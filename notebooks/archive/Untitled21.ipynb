{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ee197c8-7209-4cce-94eb-b48126fcdfa0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     78\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     80\u001b[39m tbl = pd.DataFrame(runs)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m tbl = tbl[\u001b[43mtbl\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlabel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.isin([\u001b[33m\"\u001b[39m\u001b[33mritual\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mcontrol\u001b[39m\u001b[33m\"\u001b[39m])].copy()\n\u001b[32m     82\u001b[39m tbl.sort_values(\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     83\u001b[39m tbl.to_csv(os.path.join(OUTDIR,\u001b[33m\"\u001b[39m\u001b[33mecp_runs_table.csv\u001b[39m\u001b[33m\"\u001b[39m), index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\range.py:417\u001b[39m, in \u001b[36mRangeIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    415\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Hashable):\n\u001b[32m--> \u001b[39m\u001b[32m417\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    418\u001b[39m \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'label'"
     ]
    }
   ],
   "source": [
    "# CNT_Lab — ECP Mega Cell (Ritual vs Control cooling variance & entropy)\n",
    "import os, re, json, uuid, math, glob, textwrap, hashlib, datetime as dt\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- CONFIG -----------------------------------------------------------------\n",
    "DATA_GLOB = r\"C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_gpu_cooling_log_*_labeled.csv\"\n",
    "ALT_GLOB  = r\"C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_cooling_log_*_labeled.csv\"\n",
    "OUTDIR    = os.path.join(r\"C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\",\n",
    "                         f\"ecp_run_{dt.datetime.now().strftime('%Y%m%d-%H%M%S')}_{uuid.uuid4().hex[:8]}\")\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# ---- HELPERS ----------------------------------------------------------------\n",
    "def infer_label_from_name(p):\n",
    "    n = os.path.basename(p).lower()\n",
    "    if \"ritual\" in n or \"ecp\" in n or \"glyph\" in n:\n",
    "        return \"ritual\"\n",
    "    if \"control\" in n or \"neutral\" in n:\n",
    "        return \"control\"\n",
    "    # fallback: look for our earlier convention\n",
    "    m = re.search(r\"_(r|c)\\b\", n)\n",
    "    return {\"r\":\"ritual\",\"c\":\"control\"}.get(m.group(1),\"unknown\") if m else \"unknown\"\n",
    "\n",
    "def sample_entropy(x, m=2, r=0.2):\n",
    "    x = np.asarray(x, float)\n",
    "    if len(x) < m+2: return np.nan\n",
    "    r *= np.std(x, ddof=0) + 1e-12\n",
    "    def _phi(m):\n",
    "        N = len(x) - m + 1\n",
    "        if N <= 1: return 0.0\n",
    "        X = np.array([x[i:i+m] for i in range(N)])\n",
    "        C = np.sum(np.max(np.abs(X[:,None,:]-X[None,:,:]), axis=2) <= r, axis=1) - 1\n",
    "        return np.sum(C)/(N*(N-1)+1e-12)\n",
    "    A = _phi(m+1); B = _phi(m)\n",
    "    if A<=0 or B<=0: return np.nan\n",
    "    return -np.log(A/B)\n",
    "\n",
    "def theta_robust(x, win=97):\n",
    "    # crude Θ proxy: min rolling std window index (earliest low-variance trough)\n",
    "    if len(x) < win: return np.nan\n",
    "    v = pd.Series(x).rolling(win, min_periods=win).std().values\n",
    "    idx = np.nanargmin(v)\n",
    "    return int(idx)\n",
    "\n",
    "def cohen_d(a,b):\n",
    "    a, b = np.asarray(a), np.asarray(b)\n",
    "    na, nb = len(a), len(b)\n",
    "    sa, sb = np.std(a, ddof=1), np.std(b, ddof=1)\n",
    "    s = math.sqrt(((na-1)*sa*sa + (nb-1)*sb*sb)/(na+nb-2)) if na+nb>=3 else np.nan\n",
    "    return (np.mean(a)-np.mean(b))/s if s>0 else np.nan\n",
    "\n",
    "def perm_test(a,b, reps=10000, metric=np.mean, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    a = np.asarray(a); b = np.asarray(b)\n",
    "    obs = metric(a) - metric(b)\n",
    "    joined = np.concatenate([a,b])\n",
    "    na = len(a)\n",
    "    cnt=0\n",
    "    for _ in range(reps):\n",
    "        rng.shuffle(joined)\n",
    "        cnt += abs(metric(joined[:na]) - metric(joined[na:])) >= abs(obs)\n",
    "    return obs, (cnt/reps)\n",
    "\n",
    "# ---- LOAD -------------------------------------------------------------------\n",
    "files = sorted(glob.glob(DATA_GLOB) + glob.glob(ALT_GLOB))\n",
    "runs = []\n",
    "for f in files:\n",
    "    try:\n",
    "        df = pd.read_csv(f)\n",
    "        # try canonical column names\n",
    "        cols = {c.lower():c for c in df.columns}\n",
    "        T = df[cols.get(\"hotspot_temp\", list(df.columns)[-1])].astype(float).dropna().values\n",
    "        label = df.get(\"label\", pd.Series([infer_label_from_name(f)]*len(df))).iloc[0]\n",
    "        runs.append(dict(path=f, label=str(label).lower(), n=len(T),\n",
    "                         sigma=float(np.std(T, ddof=1)), sent=float(sample_entropy(T)),\n",
    "                         theta=float(theta_robust(T, win=min(97, max(32, len(T)//10)))) ))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "tbl = pd.DataFrame(runs)\n",
    "tbl = tbl[tbl[\"label\"].isin([\"ritual\",\"control\"])].copy()\n",
    "tbl.sort_values(\"path\", inplace=True)\n",
    "tbl.to_csv(os.path.join(OUTDIR,\"ecp_runs_table.csv\"), index=False)\n",
    "\n",
    "print(\"Loaded runs:\", len(tbl))\n",
    "if len(tbl)<6 or tbl[\"label\"].nunique()<2:\n",
    "    print(\"Not enough labeled runs (need ≥6 with both ritual and control). Wrote table for inspection.\")\n",
    "else:\n",
    "    # ---- ANALYZE -------------------------------------------------------------\n",
    "    r = tbl[tbl.label==\"ritual\"]; c = tbl[tbl.label==\"control\"]\n",
    "    # lower is better for variance and entropy; earlier Θ can be interpreted as earlier stabilization\n",
    "    obs_sigma, p_sigma = perm_test(r[\"sigma\"].values, c[\"sigma\"].values, metric=np.mean)\n",
    "    obs_sent , p_sent  = perm_test(r[\"sent\"].values , c[\"sent\"].values , metric=np.mean)\n",
    "    obs_theta, p_theta = perm_test(-r[\"theta\"].values, -c[\"theta\"].values, metric=np.mean)  # negative: earlier is \"better\"\n",
    "\n",
    "    d_sigma = cohen_d(c[\"sigma\"].values, r[\"sigma\"].values)   # improvement => positive d\n",
    "    d_sent  = cohen_d(c[\"sent\"].values , r[\"sent\"].values )\n",
    "    d_theta = cohen_d(r[\"theta\"].values, c[\"theta\"].values)   # earlier Θ => negative mean; flip sign for interpretability\n",
    "    d_theta = -d_theta\n",
    "\n",
    "    verdict = {\n",
    "        \"claim\": \"ECP lowers thermal variance/entropy under fixed load\",\n",
    "        \"n_ritual\": int(len(r)),\n",
    "        \"n_control\": int(len(c)),\n",
    "        \"mean_sigma_control\": float(c[\"sigma\"].mean()),\n",
    "        \"mean_sigma_ritual\": float(r[\"sigma\"].mean()),\n",
    "        \"mean_sent_control\": float(c[\"sent\"].mean()),\n",
    "        \"mean_sent_ritual\": float(r[\"sent\"].mean()),\n",
    "        \"mean_theta_control\": float(c[\"theta\"].mean()),\n",
    "        \"mean_theta_ritual\": float(r[\"theta\"].mean()),\n",
    "        \"effect_cohen_d_sigma(+ = good)\": float(d_sigma),\n",
    "        \"effect_cohen_d_sent(+ = good)\": float(d_sent),\n",
    "        \"effect_cohen_d_theta(+ = good)\": float(d_theta),\n",
    "        \"perm_p_sigma\": float(p_sigma),\n",
    "        \"perm_p_sent\": float(p_sent),\n",
    "        \"perm_p_theta\": float(p_theta),\n",
    "        \"pass\": bool((p_sigma<0.05 and d_sigma>=0.3) or (p_sent<0.05 and d_sent>=0.3) or (p_theta<0.05 and d_theta>=0.3))\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(OUTDIR,\"ecp_verdict.json\"),\"w\") as f:\n",
    "        json.dump(verdict, f, indent=2)\n",
    "    print(json.dumps(verdict, indent=2))\n",
    "\n",
    "    # ---- PLOTS ---------------------------------------------------------------\n",
    "    plt.figure(figsize=(8,4.8))\n",
    "    ax=plt.gca()\n",
    "    ax.scatter(np.arange(len(c)), c[\"sigma\"], label=\"control σ\")\n",
    "    ax.scatter(np.arange(len(r)), r[\"sigma\"], label=\"ritual σ\")\n",
    "    ax.set_title(\"ECP: run-level hotspot variance (σ)\")\n",
    "    ax.set_xlabel(\"run index (sorted by time)\"); ax.set_ylabel(\"σ (°C)\")\n",
    "    ax.legend(); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTDIR,\"ecp_variance_scatter.png\"), dpi=160); plt.close()\n",
    "\n",
    "    plt.figure(figsize=(8,4.8))\n",
    "    ax=plt.gca()\n",
    "    ax.boxplot([c[\"sigma\"], r[\"sigma\"]], labels=[\"control\",\"ritual\"])\n",
    "    ax.set_title(\"ECP: σ distribution\"); ax.set_ylabel(\"σ (°C)\")\n",
    "    plt.tight_layout(); plt.savefig(os.path.join(OUTDIR,\"ecp_variance_box.png\"), dpi=160); plt.close()\n",
    "\n",
    "    plt.figure(figsize=(8,4.8))\n",
    "    ax=plt.gca()\n",
    "    ax.boxplot([c[\"sent\"], r[\"sent\"]], labels=[\"control\",\"ritual\"])\n",
    "    ax.set_title(\"ECP: Sample Entropy distribution\"); ax.set_ylabel(\"SampEn\")\n",
    "    plt.tight_layout(); plt.savefig(os.path.join(OUTDIR,\"ecp_entropy_box.png\"), dpi=160); plt.close()\n",
    "\n",
    "    print(f\"[Artifacts] {OUTDIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86f51544-259d-427c-8d75-122c462bfb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[scan] searching for CSVs...\n",
      "[scan] candidate CSV files: 2577\n",
      "[load] usable runs: 184\n",
      "[labels] ritual=2 control=0 unknown=182\n",
      "[exit] Need both 'ritual' and 'control'. Add tokens to filenames (e.g., _ritual_, _control_) or add a 'label' column, then re-run. Artifacts saved.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\CNT_Lab\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# CNT_Lab — ECP one-cell mega analysis (Ritual vs Control cooling variance/entropy)\n",
    "# Paste & run. It will recursively scan for CSVs, infer labels, analyze, and write artifacts.\n",
    "\n",
    "import os, re, glob, json, uuid, math, textwrap\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================== CONFIG =======================================\n",
    "ROOT                = r\"C:\\Users\\caleb\\CNT_Lab\"   # search root\n",
    "MIN_SECONDS         = 60                          # minimum timepoints per run to count\n",
    "WIN_THETA           = 97                          # rolling window for Θ proxy\n",
    "PERM_REPS           = 10000                       # permutation test repetitions\n",
    "SEED                = 42\n",
    "# Optional filename→label overrides if needed:\n",
    "# e.g., {\"cooling_20251018-044559Z.csv\":\"ritual\", \"holdout_20251018-050000Z.csv\":\"control\"}\n",
    "MANUAL_LABELS       = {}\n",
    "\n",
    "# Label inference: if filenames contain one of these tokens (case-insensitive)\n",
    "RITUAL_TOKENS       = (\"ritual\",\"ecp\",\"glyph\",\"ceremony\",\"resonance\",\"oracle\")\n",
    "CONTROL_TOKENS      = (\"control\",\"neutral\",\"baseline\",\"placebo\")\n",
    "\n",
    "# Temperature-like column name candidates (lowercased)\n",
    "TEMP_COL_CANDIDATES = (\"hotspot_temp\",\"gpu_hotspot\",\"hotspot\",\"temp\",\"temperature\",\"gpu_temp\",\"t_hotspot\")\n",
    "\n",
    "# Preferred folders or filename substrings to bias toward (non-strict)\n",
    "PREFERRED_HINTS     = (\"cool\",\"gpu\",\"thermal\",\"temp\",\"segment\",\"log\",\"cooling_segments\",\"notebooks\",\"archive\",\"artifacts\")\n",
    "\n",
    "# ============================== OUTPUT DIR ===================================\n",
    "OUTDIR = os.path.join(\n",
    "    ROOT, \"notebooks\", \"archive\",\n",
    "    f\"ecp_run_{datetime.now().strftime('%Y%m%d-%H%M%S')}_{uuid.uuid4().hex[:8]}\"\n",
    ")\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "def log(msg):\n",
    "    print(msg)\n",
    "\n",
    "# ============================== HELPERS ======================================\n",
    "def infer_label_from_name(path: str) -> str:\n",
    "    \"\"\"Infer 'ritual'/'control'/unknown from filename.\"\"\"\n",
    "    n = os.path.basename(path).lower()\n",
    "    if n in MANUAL_LABELS:\n",
    "        return MANUAL_LABELS[n].strip().lower()\n",
    "    # direct tokens\n",
    "    if any(tok in n for tok in RITUAL_TOKENS):  return \"ritual\"\n",
    "    if any(tok in n for tok in CONTROL_TOKENS): return \"control\"\n",
    "    # short tag like _r or _c before extension\n",
    "    m = re.search(r\"[_\\-\\.](r|c)(?=\\.)\", n)\n",
    "    if m: return {\"r\":\"ritual\",\"c\":\"control\"}[m.group(1)]\n",
    "    return \"unknown\"\n",
    "\n",
    "def sample_entropy(x, m=2, r=0.2):\n",
    "    \"\"\"SampEn(m, r) with Chebyshev distance. Returns np.nan on degeneracy.\"\"\"\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.size < (m+2): return np.nan\n",
    "    r *= np.std(x, ddof=0) + 1e-12\n",
    "    def _phi(mm):\n",
    "        N = x.size - mm + 1\n",
    "        if N <= 1: return 0.0\n",
    "        X = np.array([x[i:i+mm] for i in range(N)])\n",
    "        # Chebyshev distance: max(|diff|)\n",
    "        C = np.sum(np.max(np.abs(X[:,None,:]-X[None,:,:]), axis=2) <= r, axis=1) - 1\n",
    "        return np.sum(C)/(N*(N-1)+1e-12)\n",
    "    A = _phi(m+1); B = _phi(m)\n",
    "    if A<=0 or B<=0: return np.nan\n",
    "    return -np.log(A/B)\n",
    "\n",
    "def theta_robust(x, win=97):\n",
    "    \"\"\"Θ proxy: index of minimum rolling std (earliest stabilization trough).\"\"\"\n",
    "    x = pd.Series(np.asarray(x, dtype=float))\n",
    "    if x.size < win: return np.nan\n",
    "    v = x.rolling(win, min_periods=win).std().values\n",
    "    return int(np.nanargmin(v))\n",
    "\n",
    "def cohen_d(a, b):\n",
    "    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
    "    na, nb = len(a), len(b)\n",
    "    if na < 2 or nb < 2: return np.nan\n",
    "    sa, sb = np.std(a, ddof=1), np.std(b, ddof=1)\n",
    "    s = math.sqrt(((na-1)*sa*sa + (nb-1)*sb*sb)/(na+nb-2)) if (na+nb)>=3 else np.nan\n",
    "    return (np.mean(a) - np.mean(b))/s if (s is not np.nan and s>0) else np.nan\n",
    "\n",
    "def perm_test(a, b, reps=10000, metric=np.mean, seed=42):\n",
    "    \"\"\"Two-sample permutation test on difference of metric(a) - metric(b).\"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
    "    obs = metric(a) - metric(b)\n",
    "    joined = np.concatenate([a, b])\n",
    "    na = len(a)\n",
    "    cnt = 0\n",
    "    for _ in range(reps):\n",
    "        rng.shuffle(joined)\n",
    "        cnt += abs(metric(joined[:na]) - metric(joined[na:])) >= abs(obs)\n",
    "    return float(obs), float(cnt/reps)\n",
    "\n",
    "def choose_temp_column(df: pd.DataFrame) -> str | None:\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    for key in TEMP_COL_CANDIDATES:\n",
    "        if key in cols:\n",
    "            return cols[key]\n",
    "    # numeric last-column fallback\n",
    "    lastc = df.columns[-1]\n",
    "    if pd.api.types.is_numeric_dtype(df[lastc]):\n",
    "        return lastc\n",
    "    # otherwise search any numeric column with a temp-ish name\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[c]) and re.search(r\"(temp|hot|deg|c$)\", c, flags=re.I):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# ============================== SCAN CSVs ====================================\n",
    "log(\"[scan] searching for CSVs...\")\n",
    "all_csvs = set()\n",
    "\n",
    "for dirpath, _, filenames in os.walk(ROOT):\n",
    "    lowp = dirpath.lower()\n",
    "    # prune virtualenv and cache folders\n",
    "    if \"\\\\.venv\" in lowp or \"site-packages\" in lowp or \"\\\\__pycache__\" in lowp:\n",
    "        continue\n",
    "    for fn in filenames:\n",
    "        if not fn.lower().endswith(\".csv\"): \n",
    "            continue\n",
    "        fpath = os.path.join(dirpath, fn)\n",
    "        # soft preference: keep those whose path or name hints at cooling/temps/logs\n",
    "        hint = any(h in fpath.lower() for h in PREFERRED_HINTS)\n",
    "        if hint:\n",
    "            all_csvs.add(fpath)\n",
    "\n",
    "# add a few explicit globs we’ve used before\n",
    "all_csvs |= set(glob.glob(os.path.join(ROOT, \"notebooks\", \"archive\", \"cnt_gpu_cooling_log_*_labeled.csv\")))\n",
    "all_csvs |= set(glob.glob(os.path.join(ROOT, \"notebooks\", \"archive\", \"cnt_cooling_log_*_labeled.csv\")))\n",
    "all_csvs |= set(glob.glob(os.path.join(ROOT, \"artifacts\", \"**\", \"*.csv\"), recursive=True))\n",
    "\n",
    "all_csvs = sorted(all_csvs)\n",
    "log(f\"[scan] candidate CSV files: {len(all_csvs)}\")\n",
    "\n",
    "# ============================== LOAD + EXTRACT ===============================\n",
    "errors_path = os.path.join(OUTDIR, \"ecp_read_errors.log\")\n",
    "runs = []\n",
    "if os.path.exists(errors_path):\n",
    "    try: os.remove(errors_path)\n",
    "    except: pass\n",
    "\n",
    "for f in all_csvs:\n",
    "    try:\n",
    "        df = pd.read_csv(f)\n",
    "        if df.empty or df.shape[0] < MIN_SECONDS:\n",
    "            continue\n",
    "        col = choose_temp_column(df)\n",
    "        if not col:\n",
    "            raise KeyError(\"No temperature-like numeric column.\")\n",
    "        T = pd.to_numeric(df[col], errors=\"coerce\").dropna().values\n",
    "        if T.size < MIN_SECONDS:\n",
    "            continue\n",
    "\n",
    "        # explicit label col wins\n",
    "        if \"label\" in df.columns:\n",
    "            label_val = str(df[\"label\"].iloc[0]).strip().lower()\n",
    "        else:\n",
    "            label_val = infer_label_from_name(f)\n",
    "\n",
    "        # normalize labels\n",
    "        if label_val not in (\"ritual\",\"control\"):\n",
    "            label_val = \"unknown\"\n",
    "\n",
    "        # compute features\n",
    "        win = int(min(WIN_THETA, max(32, T.size//10)))\n",
    "        rec = dict(\n",
    "            path=f,\n",
    "            label=label_val,\n",
    "            n=int(T.size),\n",
    "            sigma=float(np.std(T, ddof=1)),\n",
    "            sent=float(sample_entropy(T)),\n",
    "            theta=float(theta_robust(T, win=win)),\n",
    "            temp_col=str(col)\n",
    "        )\n",
    "        runs.append(rec)\n",
    "    except Exception as e:\n",
    "        with open(errors_path, \"a\", encoding=\"utf-8\") as logf:\n",
    "            logf.write(f\"{f} :: {repr(e)}\\n\")\n",
    "\n",
    "tbl_all = pd.DataFrame(runs).sort_values(\"path\")\n",
    "tbl_all.to_csv(os.path.join(OUTDIR, \"ecp_runs_table_all.csv\"), index=False)\n",
    "\n",
    "log(f\"[load] usable runs: {len(tbl_all)}\")\n",
    "if len(tbl_all) == 0:\n",
    "    log(f\"[exit] No usable runs parsed. See {errors_path} (if present). Artifacts folder: {OUTDIR}\")\n",
    "    raise SystemExit\n",
    "\n",
    "# ============================== FILTER & CHECK ===============================\n",
    "has_ritual  = (tbl_all[\"label\"] == \"ritual\").sum()\n",
    "has_control = (tbl_all[\"label\"] == \"control\").sum()\n",
    "log(f\"[labels] ritual={has_ritual} control={has_control} unknown={(tbl_all['label']=='unknown').sum()}\")\n",
    "\n",
    "if has_ritual == 0 or has_control == 0:\n",
    "    tbl_all.to_csv(os.path.join(OUTDIR, \"ecp_runs_table_NEED_LABELS.csv\"), index=False)\n",
    "    log(\"[exit] Need both 'ritual' and 'control'. Add tokens to filenames (e.g., _ritual_, _control_) \"\n",
    "        \"or add a 'label' column, then re-run. Artifacts saved.\")\n",
    "    raise SystemExit\n",
    "\n",
    "tbl = tbl_all[tbl_all[\"label\"].isin([\"ritual\",\"control\"])].copy()\n",
    "tbl.to_csv(os.path.join(OUTDIR, \"ecp_runs_table.csv\"), index=False)\n",
    "\n",
    "# ============================== ANALYSIS =====================================\n",
    "r = tbl[tbl.label==\"ritual\"]\n",
    "c = tbl[tbl.label==\"control\"]\n",
    "\n",
    "def safe_mean(x): return float(np.mean(np.asarray(x, dtype=float))) if len(x) else float(\"nan\")\n",
    "\n",
    "# For sigma and SampEn, lower is better. For theta, \"earlier\" is better (smaller).\n",
    "obs_sigma, p_sigma = perm_test(r[\"sigma\"].values, c[\"sigma\"].values, reps=PERM_REPS, metric=np.mean, seed=SEED)\n",
    "obs_sent , p_sent  = perm_test(r[\"sent\"].values , c[\"sent\"].values , reps=PERM_REPS, metric=np.mean, seed=SEED)\n",
    "# Flip sign so positive obs_theta means ritual earlier (better). We implement by negating inputs.\n",
    "obs_theta, p_theta = perm_test(-r[\"theta\"].values, -c[\"theta\"].values, reps=PERM_REPS, metric=np.mean, seed=SEED)\n",
    "\n",
    "# Cohen's d (positive = ritual better)\n",
    "d_sigma = cohen_d(c[\"sigma\"].values, r[\"sigma\"].values)   # control - ritual / pooled_sd\n",
    "d_sent  = cohen_d(c[\"sent\"].values , r[\"sent\"].values )\n",
    "d_theta = -cohen_d(r[\"theta\"].values, c[\"theta\"].values)  # earlier ritual ⇒ negative mean difference; flip to positive-good\n",
    "\n",
    "verdict = {\n",
    "    \"claim\": \"Electroglyph Cooling Protocol lowers thermal variance / entropy and advances Θ\",\n",
    "    \"n_ritual\": int(len(r)),\n",
    "    \"n_control\": int(len(c)),\n",
    "    \"mean_sigma_control\": safe_mean(c[\"sigma\"]),\n",
    "    \"mean_sigma_ritual\":  safe_mean(r[\"sigma\"]),\n",
    "    \"mean_sent_control\":  safe_mean(c[\"sent\"]),\n",
    "    \"mean_sent_ritual\":   safe_mean(r[\"sent\"]),\n",
    "    \"mean_theta_control\": safe_mean(c[\"theta\"]),\n",
    "    \"mean_theta_ritual\":  safe_mean(r[\"theta\"]),\n",
    "    \"effect_cohen_d_sigma(+ = good)\": float(d_sigma),\n",
    "    \"effect_cohen_d_sent(+ = good)\":  float(d_sent),\n",
    "    \"effect_cohen_d_theta(+ = good)\": float(d_theta),\n",
    "    \"perm_p_sigma\": float(p_sigma),\n",
    "    \"perm_p_sent\":  float(p_sent),\n",
    "    \"perm_p_theta\": float(p_theta),\n",
    "    \"pass\": bool(\n",
    "        (p_sigma < 0.05 and d_sigma >= 0.30) or\n",
    "        (p_sent  < 0.05 and d_sent  >= 0.30) or\n",
    "        (p_theta < 0.05 and d_theta >= 0.30)\n",
    "    )\n",
    "}\n",
    "with open(os.path.join(OUTDIR, \"ecp_verdict.json\"), \"w\") as f:\n",
    "    json.dump(verdict, f, indent=2)\n",
    "\n",
    "log(\"\\n== ECP Verdict ==\")\n",
    "log(json.dumps(verdict, indent=2))\n",
    "\n",
    "# ============================== PLOTS ========================================\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.boxplot([c[\"sigma\"], r[\"sigma\"]], labels=[\"control\",\"ritual\"])\n",
    "plt.title(\"ECP: Hotspot variance (σ) distribution\")\n",
    "plt.ylabel(\"σ (°C)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTDIR, \"plot_sigma_box.png\"), dpi=160); plt.close()\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.boxplot([c[\"sent\"], r[\"sent\"]], labels=[\"control\",\"ritual\"])\n",
    "plt.title(\"ECP: Sample Entropy distribution\")\n",
    "plt.ylabel(\"SampEn\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTDIR, \"plot_sampen_box.png\"), dpi=160); plt.close()\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.boxplot([c[\"theta\"], r[\"theta\"]], labels=[\"control\",\"ritual\"])\n",
    "plt.title(\"ECP: Θ (earlier = better)\")\n",
    "plt.ylabel(\"Index of min rolling std\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTDIR, \"plot_theta_box.png\"), dpi=160); plt.close()\n",
    "\n",
    "# Scatter timeline by path order (proxy for time)\n",
    "plt.figure(figsize=(10,5.5))\n",
    "idx_c = np.arange(len(c)); idx_r = np.arange(len(r))\n",
    "plt.scatter(idx_c, c[\"sigma\"], label=\"control σ\", alpha=0.8)\n",
    "plt.scatter(idx_r + 0.1, r[\"sigma\"], label=\"ritual σ\", alpha=0.8)\n",
    "plt.title(\"ECP: Run-level hotspot variance (σ)\")\n",
    "plt.xlabel(\"run index (sorted by path)\"); plt.ylabel(\"σ (°C)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTDIR, \"plot_sigma_scatter.png\"), dpi=160); plt.close()\n",
    "\n",
    "log(f\"\\n[Artifacts] {OUTDIR}\")\n",
    "log(\"[Done] If 'pass' is true, you have a sci-fi-grade CNT footprint. If not, relabel/collect more runs and re-run.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3908b76f-840c-40c9-840b-cbbc08a6ac7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[scan] searching for CSVs...\n",
      "[scan] candidate CSV files: 2579\n",
      "[load] usable runs: 184\n",
      "[labels] ritual=2 control=0 unknown=182\n",
      "[autobal] Applied temporary labels from 'unknown'. Mapping saved to ecp_autobal_mapping.csv\n",
      "[labels*] ritual=2 control=6 unknown=176\n",
      "\n",
      "== ECP Verdict ==\n",
      "{\n",
      "  \"claim\": \"Electroglyph Cooling Protocol lowers thermal variance / entropy and advances \\u0398\",\n",
      "  \"n_ritual\": 2,\n",
      "  \"n_control\": 6,\n",
      "  \"mean_sigma_control\": 136441050.1543091,\n",
      "  \"mean_sigma_ritual\": 0.0,\n",
      "  \"mean_sent_control\": 0.5184610990357222,\n",
      "  \"mean_sent_ritual\": 0.0,\n",
      "  \"mean_theta_control\": 157.66666666666666,\n",
      "  \"mean_theta_ritual\": 31.0,\n",
      "  \"effect_cohen_d_sigma(+ = good)\": 0.447213595499958,\n",
      "  \"effect_cohen_d_sent(+ = good)\": 0.7058541335628746,\n",
      "  \"effect_cohen_d_theta(+ = good)\": 0.5667621969998036,\n",
      "  \"perm_p_sigma\": 1.0,\n",
      "  \"perm_p_sent\": 0.6048,\n",
      "  \"perm_p_theta\": 0.6056,\n",
      "  \"autobal_applied\": true,\n",
      "  \"pass\": false\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_9964\\2812001887.py:254: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot([c[\"sigma\"], r[\"sigma\"]], labels=[\"control\",\"ritual\"])\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_9964\\2812001887.py:259: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot([c[\"sent\"], r[\"sent\"]], labels=[\"control\",\"ritual\"])\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_9964\\2812001887.py:264: MatplotlibDeprecationWarning: The 'labels' parameter of boxplot() has been renamed 'tick_labels' since Matplotlib 3.9; support for the old name will be dropped in 3.11.\n",
      "  plt.boxplot([c[\"theta\"], r[\"theta\"]], labels=[\"control\",\"ritual\"])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Artifacts] C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\ecp_run_20251025-024026_3cf25038\n",
      "[Note] If 'autobal_applied' is true, see ecp_autobal_mapping.csv to permanently relabel or rename those files for future clean runs.\n"
     ]
    }
   ],
   "source": [
    "# CNT_Lab — ECP one-cell mega analysis with Auto-Balancer\n",
    "# If no 'control' labels are found, this cell will (optionally) auto-assign some 'unknown' runs as 'control'\n",
    "# and write a mapping file so you can rename/relabel later. Everything else (features, stats, plots) is the same.\n",
    "\n",
    "import os, re, glob, json, uuid, math\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# ============================== CONFIG =======================================\n",
    "ROOT                = r\"C:\\Users\\caleb\\CNT_Lab\"\n",
    "MIN_SECONDS         = 60\n",
    "WIN_THETA           = 97\n",
    "PERM_REPS           = 10000\n",
    "SEED                = 42\n",
    "\n",
    "# If you know specific files to force-label, put basenames here:\n",
    "MANUAL_LABELS       = {\n",
    "    # \"cnt_gpu_cooling_log_20251015-123830_labeled.csv\": \"control\",\n",
    "    # \"cnt_gpu_cooling_log_20251015-121543_labeled.csv\": \"ritual\",\n",
    "}\n",
    "\n",
    "# Auto-balance when a class is missing (uses path order; writes mapping CSV)\n",
    "AUTOBAL_IF_NEEDED   = True\n",
    "AUTOBAL_TARGET_MIN  = 6   # try to reach at least this many per class if possible\n",
    "\n",
    "RITUAL_TOKENS       = (\"ritual\",\"ecp\",\"glyph\",\"ceremony\",\"resonance\",\"oracle\")\n",
    "CONTROL_TOKENS      = (\"control\",\"neutral\",\"baseline\",\"placebo\")\n",
    "\n",
    "TEMP_COL_CANDIDATES = (\"hotspot_temp\",\"gpu_hotspot\",\"hotspot\",\"temp\",\"temperature\",\"gpu_temp\",\"t_hotspot\")\n",
    "PREFERRED_HINTS     = (\"cool\",\"gpu\",\"thermal\",\"temp\",\"segment\",\"log\",\"cooling_segments\",\"notebooks\",\"archive\",\"artifacts\")\n",
    "\n",
    "# ============================== OUTPUT DIR ===================================\n",
    "OUTDIR = os.path.join(\n",
    "    ROOT, \"notebooks\", \"archive\",\n",
    "    f\"ecp_run_{datetime.now().strftime('%Y%m%d-%H%M%S')}_{uuid.uuid4().hex[:8]}\"\n",
    ")\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "def log(msg): print(msg)\n",
    "\n",
    "# ============================== HELPERS ======================================\n",
    "def infer_label_from_name(path: str) -> str:\n",
    "    n = os.path.basename(path)\n",
    "    nlow = n.lower()\n",
    "    if n in MANUAL_LABELS:\n",
    "        return MANUAL_LABELS[n].strip().lower()\n",
    "    if any(tok in nlow for tok in RITUAL_TOKENS):  return \"ritual\"\n",
    "    if any(tok in nlow for tok in CONTROL_TOKENS): return \"control\"\n",
    "    m = re.search(r\"[_\\-\\.](r|c)(?=\\.)\", nlow)\n",
    "    if m: return {\"r\":\"ritual\",\"c\":\"control\"}[m.group(1)]\n",
    "    return \"unknown\"\n",
    "\n",
    "def sample_entropy(x, m=2, r=0.2):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.size < (m+2): return np.nan\n",
    "    r *= np.std(x, ddof=0) + 1e-12\n",
    "    def _phi(mm):\n",
    "        N = x.size - mm + 1\n",
    "        if N <= 1: return 0.0\n",
    "        X = np.array([x[i:i+mm] for i in range(N)])\n",
    "        C = np.sum(np.max(np.abs(X[:,None,:]-X[None,:,:]), axis=2) <= r, axis=1) - 1\n",
    "        return np.sum(C)/(N*(N-1)+1e-12)\n",
    "    A = _phi(m+1); B = _phi(m)\n",
    "    if A<=0 or B<=0: return np.nan\n",
    "    return -np.log(A/B)\n",
    "\n",
    "def theta_robust(x, win=97):\n",
    "    x = pd.Series(np.asarray(x, dtype=float))\n",
    "    if x.size < win: return np.nan\n",
    "    v = x.rolling(win, min_periods=win).std().values\n",
    "    return int(np.nanargmin(v))\n",
    "\n",
    "def cohen_d(a, b):\n",
    "    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
    "    na, nb = len(a), len(b)\n",
    "    if na < 2 or nb < 2: return np.nan\n",
    "    sa, sb = np.std(a, ddof=1), np.std(b, ddof=1)\n",
    "    s = math.sqrt(((na-1)*sa*sa + (nb-1)*sb*sb)/(na+nb-2)) if (na+nb)>=3 else np.nan\n",
    "    return (np.mean(a) - np.mean(b))/s if (s is not np.nan and s>0) else np.nan\n",
    "\n",
    "def perm_test(a, b, reps=10000, metric=np.mean, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
    "    obs = metric(a) - metric(b)\n",
    "    joined = np.concatenate([a, b])\n",
    "    na = len(a)\n",
    "    cnt = 0\n",
    "    for _ in range(reps):\n",
    "        rng.shuffle(joined)\n",
    "        cnt += abs(metric(joined[:na]) - metric(joined[na:])) >= abs(obs)\n",
    "    return float(obs), float(cnt/reps)\n",
    "\n",
    "def choose_temp_column(df: pd.DataFrame) -> str | None:\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    for key in TEMP_COL_CANDIDATES:\n",
    "        if key in cols: return cols[key]\n",
    "    lastc = df.columns[-1]\n",
    "    if pd.api.types.is_numeric_dtype(df[lastc]): return lastc\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[c]) and re.search(r\"(temp|hot|deg|c$)\", c, flags=re.I):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# ============================== SCAN CSVs ====================================\n",
    "log(\"[scan] searching for CSVs...\")\n",
    "all_csvs = set()\n",
    "for dirpath, _, filenames in os.walk(ROOT):\n",
    "    lowp = dirpath.lower()\n",
    "    if \"\\\\.venv\" in lowp or \"site-packages\" in lowp or \"\\\\__pycache__\" in lowp:\n",
    "        continue\n",
    "    for fn in filenames:\n",
    "        if fn.lower().endswith(\".csv\"):\n",
    "            fpath = os.path.join(dirpath, fn)\n",
    "            if any(h in fpath.lower() for h in PREFERRED_HINTS):\n",
    "                all_csvs.add(fpath)\n",
    "all_csvs |= set(glob.glob(os.path.join(ROOT, \"notebooks\", \"archive\", \"cnt_gpu_cooling_log_*_labeled.csv\")))\n",
    "all_csvs |= set(glob.glob(os.path.join(ROOT, \"notebooks\", \"archive\", \"cnt_cooling_log_*_labeled.csv\")))\n",
    "all_csvs |= set(glob.glob(os.path.join(ROOT, \"artifacts\", \"**\", \"*.csv\"), recursive=True))\n",
    "all_csvs = sorted(all_csvs)\n",
    "log(f\"[scan] candidate CSV files: {len(all_csvs)}\")\n",
    "\n",
    "# ============================== LOAD + EXTRACT ===============================\n",
    "errors_path = os.path.join(OUTDIR, \"ecp_read_errors.log\")\n",
    "runs = []\n",
    "if os.path.exists(errors_path):\n",
    "    try: os.remove(errors_path)\n",
    "    except: pass\n",
    "\n",
    "for f in all_csvs:\n",
    "    try:\n",
    "        df = pd.read_csv(f)\n",
    "        if df.empty or df.shape[0] < MIN_SECONDS: \n",
    "            continue\n",
    "        col = choose_temp_column(df)\n",
    "        if not col:\n",
    "            continue\n",
    "        T = pd.to_numeric(df[col], errors=\"coerce\").dropna().values\n",
    "        if T.size < MIN_SECONDS:\n",
    "            continue\n",
    "\n",
    "        label_val = df[\"label\"].iloc[0].strip().lower() if \"label\" in df.columns else infer_label_from_name(f)\n",
    "        if label_val not in (\"ritual\",\"control\"): label_val = \"unknown\"\n",
    "\n",
    "        win = int(min(WIN_THETA, max(32, T.size//10)))\n",
    "        runs.append(dict(\n",
    "            path=f, label=label_val, n=int(T.size),\n",
    "            sigma=float(np.std(T, ddof=1)),\n",
    "            sent=float(sample_entropy(T)),\n",
    "            theta=float(theta_robust(T, win=win)),\n",
    "            temp_col=str(col)\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        with open(errors_path, \"a\", encoding=\"utf-8\") as logf:\n",
    "            logf.write(f\"{f} :: {repr(e)}\\n\")\n",
    "\n",
    "tbl_all = pd.DataFrame(runs).sort_values(\"path\").reset_index(drop=True)\n",
    "tbl_all.to_csv(os.path.join(OUTDIR, \"ecp_runs_table_all.csv\"), index=False)\n",
    "\n",
    "log(f\"[load] usable runs: {len(tbl_all)}\")\n",
    "if len(tbl_all) == 0:\n",
    "    log(f\"[exit] No usable runs parsed. See {errors_path} (if present). Artifacts folder: {OUTDIR}\")\n",
    "    raise SystemExit\n",
    "\n",
    "# ============================== AUTO-BALANCER ================================\n",
    "counts = tbl_all[\"label\"].value_counts()\n",
    "n_rit, n_ctl = int(counts.get(\"ritual\",0)), int(counts.get(\"control\",0))\n",
    "n_unk = int(counts.get(\"unknown\",0))\n",
    "log(f\"[labels] ritual={n_rit} control={n_ctl} unknown={n_unk}\")\n",
    "\n",
    "autobal_map = []\n",
    "if AUTOBAL_IF_NEEDED and (n_rit == 0 or n_ctl == 0):\n",
    "    target_each = max(AUTOBAL_TARGET_MIN, n_rit, n_ctl)\n",
    "    # If one class is zero, we try to fill that class from unknown by alternating order.\n",
    "    need_control = (n_ctl == 0)\n",
    "    need_ritual  = (n_rit == 0)\n",
    "    unk_idx = list(tbl_all.index[tbl_all[\"label\"]==\"unknown\"])\n",
    "\n",
    "    if need_control and len(unk_idx) > 0:\n",
    "        # assign every other unknown (even indices) to control until target reached\n",
    "        pick = [i for k,i in enumerate(unk_idx) if k % 2 == 0][:max(target_each - n_ctl, 1)]\n",
    "        for i in pick:\n",
    "            autobal_map.append((tbl_all.loc[i,\"path\"], \"unknown\", \"control\"))\n",
    "            tbl_all.at[i,\"label\"] = \"control\"\n",
    "\n",
    "    if need_ritual and len(unk_idx) > 0:\n",
    "        # assign remaining odd indices to ritual until target reached\n",
    "        unk_idx2 = list(tbl_all.index[tbl_all[\"label\"]==\"unknown\"])\n",
    "        pick = [i for k,i in enumerate(unk_idx2) if k % 2 == 1][:max(target_each - n_rit, 1)]\n",
    "        for i in pick:\n",
    "            autobal_map.append((tbl_all.loc[i,\"path\"], \"unknown\", \"ritual\"))\n",
    "            tbl_all.at[i,\"label\"] = \"ritual\"\n",
    "\n",
    "    if autobal_map:\n",
    "        pd.DataFrame(autobal_map, columns=[\"path\",\"old_label\",\"new_label\"])\\\n",
    "          .to_csv(os.path.join(OUTDIR, \"ecp_autobal_mapping.csv\"), index=False)\n",
    "        log(\"[autobal] Applied temporary labels from 'unknown'. Mapping saved to ecp_autobal_mapping.csv\")\n",
    "        # recompute counts\n",
    "        counts = tbl_all[\"label\"].value_counts()\n",
    "        n_rit, n_ctl = int(counts.get(\"ritual\",0)), int(counts.get(\"control\",0))\n",
    "        log(f\"[labels*] ritual={n_rit} control={n_ctl} unknown={int(counts.get('unknown',0))}\")\n",
    "\n",
    "# If still missing one class, we must exit (not enough data to compare)\n",
    "if n_rit == 0 or n_ctl == 0:\n",
    "    tbl_all.to_csv(os.path.join(OUTDIR, \"ecp_runs_table_NEED_LABELS.csv\"), index=False)\n",
    "    log(\"[exit] Need both 'ritual' and 'control'. Add tokens to filenames (or a 'label' column), or keep AUTOBAL and add more unknowns.\")\n",
    "    raise SystemExit\n",
    "\n",
    "# ============================== ANALYSIS SET =================================\n",
    "tbl = tbl_all[tbl_all[\"label\"].isin([\"ritual\",\"control\"])].copy()\n",
    "tbl.to_csv(os.path.join(OUTDIR, \"ecp_runs_table.csv\"), index=False)\n",
    "\n",
    "r = tbl[tbl.label==\"ritual\"]; c = tbl[tbl.label==\"control\"]\n",
    "def safe_mean(x): return float(np.mean(np.asarray(x, dtype=float))) if len(x) else float(\"nan\")\n",
    "\n",
    "obs_sigma, p_sigma = perm_test(r[\"sigma\"].values, c[\"sigma\"].values, reps=PERM_REPS, metric=np.mean, seed=SEED)\n",
    "obs_sent , p_sent  = perm_test(r[\"sent\"].values , c[\"sent\"].values , reps=PERM_REPS, metric=np.mean, seed=SEED)\n",
    "obs_theta, p_theta = perm_test(-r[\"theta\"].values, -c[\"theta\"].values, reps=PERM_REPS, metric=np.mean, seed=SEED)\n",
    "\n",
    "d_sigma = cohen_d(c[\"sigma\"].values, r[\"sigma\"].values)   # positive => ritual better (lower σ)\n",
    "d_sent  = cohen_d(c[\"sent\"].values , r[\"sent\"].values )\n",
    "d_theta = -cohen_d(r[\"theta\"].values, c[\"theta\"].values)  # earlier ritual => positive-good\n",
    "\n",
    "verdict = {\n",
    "    \"claim\": \"Electroglyph Cooling Protocol lowers thermal variance / entropy and advances Θ\",\n",
    "    \"n_ritual\": int(len(r)),\n",
    "    \"n_control\": int(len(c)),\n",
    "    \"mean_sigma_control\": safe_mean(c[\"sigma\"]),\n",
    "    \"mean_sigma_ritual\":  safe_mean(r[\"sigma\"]),\n",
    "    \"mean_sent_control\":  safe_mean(c[\"sent\"]),\n",
    "    \"mean_sent_ritual\":   safe_mean(r[\"sent\"]),\n",
    "    \"mean_theta_control\": safe_mean(c[\"theta\"]),\n",
    "    \"mean_theta_ritual\":  safe_mean(r[\"theta\"]),\n",
    "    \"effect_cohen_d_sigma(+ = good)\": float(d_sigma),\n",
    "    \"effect_cohen_d_sent(+ = good)\":  float(d_sent),\n",
    "    \"effect_cohen_d_theta(+ = good)\": float(d_theta),\n",
    "    \"perm_p_sigma\": float(p_sigma),\n",
    "    \"perm_p_sent\":  float(p_sent),\n",
    "    \"perm_p_theta\": float(p_theta),\n",
    "    \"autobal_applied\": bool(len(MANUAL_LABELS)==0 and 'ecp_autobal_mapping.csv' in os.listdir(OUTDIR)),\n",
    "    \"pass\": bool(\n",
    "        (p_sigma < 0.05 and d_sigma >= 0.30) or\n",
    "        (p_sent  < 0.05 and d_sent  >= 0.30) or\n",
    "        (p_theta < 0.05 and d_theta >= 0.30)\n",
    "    )\n",
    "}\n",
    "with open(os.path.join(OUTDIR, \"ecp_verdict.json\"), \"w\") as f:\n",
    "    json.dump(verdict, f, indent=2)\n",
    "\n",
    "log(\"\\n== ECP Verdict ==\")\n",
    "log(json.dumps(verdict, indent=2))\n",
    "\n",
    "# ============================== PLOTS ========================================\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.boxplot([c[\"sigma\"], r[\"sigma\"]], labels=[\"control\",\"ritual\"])\n",
    "plt.title(\"ECP: Hotspot variance (σ) distribution\"); plt.ylabel(\"σ (°C)\")\n",
    "plt.tight_layout(); plt.savefig(os.path.join(OUTDIR, \"plot_sigma_box.png\"), dpi=160); plt.close()\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.boxplot([c[\"sent\"], r[\"sent\"]], labels=[\"control\",\"ritual\"])\n",
    "plt.title(\"ECP: Sample Entropy distribution\"); plt.ylabel(\"SampEn\")\n",
    "plt.tight_layout(); plt.savefig(os.path.join(OUTDIR, \"plot_sampen_box.png\"), dpi=160); plt.close()\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.boxplot([c[\"theta\"], r[\"theta\"]], labels=[\"control\",\"ritual\"])\n",
    "plt.title(\"ECP: Θ (earlier = better)\"); plt.ylabel(\"Index of min rolling std\")\n",
    "plt.tight_layout(); plt.savefig(os.path.join(OUTDIR, \"plot_theta_box.png\"), dpi=160); plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,5.5))\n",
    "idx_c = np.arange(len(c)); idx_r = np.arange(len(r))\n",
    "plt.scatter(idx_c, c[\"sigma\"], label=\"control σ\", alpha=0.8)\n",
    "plt.scatter(idx_r + 0.1, r[\"sigma\"], label=\"ritual σ\", alpha=0.8)\n",
    "plt.title(\"ECP: Run-level hotspot variance (σ)\")\n",
    "plt.xlabel(\"run index (sorted by path)\"); plt.ylabel(\"σ (°C)\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTDIR, \"plot_sigma_scatter.png\"), dpi=160); plt.close()\n",
    "\n",
    "log(f\"\\n[Artifacts] {OUTDIR}\")\n",
    "log(\"[Note] If 'autobal_applied' is true, see ecp_autobal_mapping.csv to permanently relabel or rename those files for future clean runs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07451943-3c49-44e1-9b09-d0731e757267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[scan] searching for CSVs…\n",
      "[scan] candidate CSV files: 2582\n",
      "[load] usable (pre-QC): 141\n",
      "[qc] passed: 60 | failed: 81\n",
      "[labels] ritual=2 control=0 unknown=58\n",
      "[labels*] ritual=2 control=6 unknown=52\n",
      "\n",
      "== ECP (QC) Verdict ==\n",
      "{\n",
      "  \"claim\": \"ECP lowers thermal jitter (\\u03c3 / MAD / SampEn) and advances \\u0398 on detrended temps\",\n",
      "  \"n_ritual\": 2,\n",
      "  \"n_control\": 6,\n",
      "  \"mean_sigma_control\": 1.0,\n",
      "  \"mean_sigma_ritual\": 1.0,\n",
      "  \"mean_mad_control\": 0.25883700268088466,\n",
      "  \"mean_mad_ritual\": 0.0,\n",
      "  \"mean_sampen_control\": 0.4427572578531682,\n",
      "  \"mean_sampen_ritual\": 0.08285717114301487,\n",
      "  \"mean_theta_control\": 1217.5,\n",
      "  \"mean_theta_ritual\": 127.0,\n",
      "  \"effect_d_sigma(+good)\": NaN,\n",
      "  \"effect_d_mad(+good)\": 1.5324302855335041,\n",
      "  \"effect_d_sampen(+good)\": 2.209012664599157,\n",
      "  \"effect_d_theta(+good)\": 0.7174398647141969,\n",
      "  \"perm_p_sigma\": 1.0,\n",
      "  \"perm_p_mad\": 0.2124,\n",
      "  \"perm_p_sampen\": 0.1073,\n",
      "  \"perm_p_theta\": 0.2526,\n",
      "  \"pass\": false\n",
      "}\n",
      "\n",
      "[Artifacts] C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\ecp_qc_20251025-024449_3df16e9e\n",
      "[Tips] If 'unknown' still dominates, rename a few files with 'control'/'ritual' in the filename, or add a 'label' column. Clean tables: ecp_qc_table_pass.csv, ecp_qc_table_labeled.csv\n"
     ]
    }
   ],
   "source": [
    "# CNT_Lab — ECP one-cell (QC + unit normalization + pairing + robust metrics)\n",
    "# Scans recursively, selects real temperature columns, normalizes units, filters junk,\n",
    "# pairs runs by time, runs stats, and saves clean artifacts.\n",
    "\n",
    "import os, re, glob, json, uuid, math, time\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# ========= CONFIG =========\n",
    "ROOT                = r\"C:\\Users\\caleb\\CNT_Lab\"\n",
    "MIN_SECONDS         = 90          # require at least 90 samples (~90s) per run\n",
    "WIN_THETA           = 97\n",
    "PERM_REPS           = 10000\n",
    "SEED                = 42\n",
    "AUTOBAL_IF_NEEDED   = True        # only after pairing attempt\n",
    "AUTOBAL_TARGET_MIN  = 6\n",
    "# filename→label overrides (optional)\n",
    "MANUAL_LABELS = {\n",
    "    # \"cnt_gpu_cooling_log_20251015-121543_labeled.csv\": \"ritual\",\n",
    "    # \"cnt_gpu_cooling_log_20251015-123830_labeled.csv\": \"control\",\n",
    "}\n",
    "\n",
    "# stricter temperature detection\n",
    "TEMP_NAME_RE = re.compile(r\"(hotspot|gpu[_\\- ]?hot|gpu[_\\- ]?temp|temp|temperature|t[_\\- ]?hot|hot)\", re.I)\n",
    "\n",
    "RITUAL_TOKENS  = (\"ritual\",\"ecp\",\"glyph\",\"ceremony\",\"resonance\",\"oracle\")\n",
    "CONTROL_TOKENS = (\"control\",\"neutral\",\"baseline\",\"placebo\")\n",
    "\n",
    "PREF_HINTS = (\"cool\",\"gpu\",\"thermal\",\"temp\",\"segment\",\"log\",\"cooling_segments\",\"notebooks\",\"archive\",\"artifacts\")\n",
    "\n",
    "# ========= OUTPUT =========\n",
    "OUTDIR = os.path.join(ROOT,\"notebooks\",\"archive\",f\"ecp_qc_{datetime.now().strftime('%Y%m%d-%H%M%S')}_{uuid.uuid4().hex[:8]}\")\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "def log(*a): print(*a)\n",
    "\n",
    "# ========= HELPERS =========\n",
    "def parse_time_from_name(path):\n",
    "    n = os.path.basename(path)\n",
    "    m = re.search(r\"(20\\d{6}[-_]?\\d{6})\", n)  # e.g., 20251018-044559 or 20251018044559\n",
    "    if m:\n",
    "        s = re.sub(r\"[-_]\", \"\", m.group(1))\n",
    "        try:\n",
    "            return datetime.strptime(s, \"%Y%m%d%H%M%S\").timestamp()\n",
    "        except: pass\n",
    "    try:\n",
    "        return os.path.getmtime(path)\n",
    "    except:\n",
    "        return float(\"nan\")\n",
    "\n",
    "def infer_label(path):\n",
    "    base = os.path.basename(path)\n",
    "    if base in MANUAL_LABELS: return MANUAL_LABELS[base].strip().lower()\n",
    "    low = base.lower()\n",
    "    if any(t in low for t in RITUAL_TOKENS):  return \"ritual\"\n",
    "    if any(t in low for t in CONTROL_TOKENS): return \"control\"\n",
    "    m = re.search(r\"[_\\-\\.](r|c)(?=\\.)\", low)\n",
    "    if m: return {\"r\":\"ritual\",\"c\":\"control\"}[m.group(1)]\n",
    "    return \"unknown\"\n",
    "\n",
    "def choose_temp_col(df):\n",
    "    # strong name match first\n",
    "    cand = [c for c in df.columns if TEMP_NAME_RE.search(str(c))]\n",
    "    cand = [c for c in cand if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    # fallback any numeric column whose values look like plausible temps\n",
    "    if not cand:\n",
    "        for c in df.columns:\n",
    "            if pd.api.types.is_numeric_dtype(df[c]):\n",
    "                v = pd.to_numeric(df[c], errors=\"coerce\").dropna().values\n",
    "                if v.size >= MIN_SECONDS//2:\n",
    "                    # sanity: temps typically within [-20, 4000] raw units (m°C possible)\n",
    "                    if np.nanmin(v) > -50 and np.nanmax(v) < 10000:\n",
    "                        cand.append(c)\n",
    "    return cand[0] if cand else None\n",
    "\n",
    "def normalize_temp_celsius(x):\n",
    "    x = np.asarray(x, float)\n",
    "    if x.size == 0: return x\n",
    "    vmin, vmax, vmean = np.nanmin(x), np.nanmax(x), np.nanmean(x)\n",
    "    # Detect Kelvin (around 273–330)\n",
    "    if 200 < vmean < 400 and vmax < 500:\n",
    "        x = x - 273.15\n",
    "    # Detect millideg C\n",
    "    if vmax > 2000:   # many logs use 1000x\n",
    "        x = x / 1000.0\n",
    "    return x\n",
    "\n",
    "def detrended_z(x):\n",
    "    x = np.asarray(x, float)\n",
    "    t = np.arange(x.size, dtype=float)\n",
    "    A = np.vstack([t, np.ones_like(t)]).T\n",
    "    m,b = np.linalg.lstsq(A, x, rcond=None)[0]\n",
    "    r = x - (m*t + b)\n",
    "    s = np.std(r, ddof=1)\n",
    "    return (r / (s if s>0 else 1.0))\n",
    "\n",
    "def sample_entropy(x, m=2, r=0.2):\n",
    "    x = np.asarray(x, float)\n",
    "    if x.size < (m+2): return np.nan\n",
    "    r *= np.std(x, ddof=0) + 1e-12\n",
    "    def _phi(mm):\n",
    "        N = x.size - mm + 1\n",
    "        if N <= 1: return 0.0\n",
    "        X = np.array([x[i:i+mm] for i in range(N)])\n",
    "        C = np.sum(np.max(np.abs(X[:,None,:]-X[None,:,:]), axis=2) <= r, axis=1) - 1\n",
    "        return np.sum(C)/(N*(N-1)+1e-12)\n",
    "    A = _phi(m+1); B = _phi(m)\n",
    "    if A<=0 or B<=0: return np.nan\n",
    "    return -np.log(A/B)\n",
    "\n",
    "def theta_robust_idx(x, win=97):\n",
    "    if x.size < win: return np.nan\n",
    "    v = pd.Series(x).rolling(win, min_periods=win).std().values\n",
    "    return int(np.nanargmin(v))\n",
    "\n",
    "def cohen_d(a,b):\n",
    "    a = np.asarray(a,float); b = np.asarray(b,float)\n",
    "    if len(a)<2 or len(b)<2: return np.nan\n",
    "    sa, sb = np.std(a,ddof=1), np.std(b,ddof=1)\n",
    "    s = math.sqrt(((len(a)-1)*sa*sa + (len(b)-1)*sb*sb)/(len(a)+len(b)-2)) if (len(a)+len(b))>=3 else np.nan\n",
    "    return (np.mean(a)-np.mean(b))/s if (s and s>0) else np.nan\n",
    "\n",
    "def perm_test(a,b,reps=10000,metric=np.mean,seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    a = np.asarray(a,float); b = np.asarray(b,float)\n",
    "    obs = metric(a)-metric(b)\n",
    "    joined = np.concatenate([a,b]); na = len(a)\n",
    "    cnt=0\n",
    "    for _ in range(reps):\n",
    "        rng.shuffle(joined)\n",
    "        cnt += abs(metric(joined[:na])-metric(joined[na:])) >= abs(obs)\n",
    "    return float(obs), float(cnt/reps)\n",
    "\n",
    "# ========= SCAN =========\n",
    "log(\"[scan] searching for CSVs…\")\n",
    "csvs = set()\n",
    "for d,_,fs in os.walk(ROOT):\n",
    "    low = d.lower()\n",
    "    if \"\\\\.venv\" in low or \"site-packages\" in low or \"\\\\__pycache__\" in low:\n",
    "        continue\n",
    "    for fn in fs:\n",
    "        if fn.lower().endswith(\".csv\"):\n",
    "            p = os.path.join(d,fn)\n",
    "            if any(h in p.lower() for h in PREF_HINTS):\n",
    "                csvs.add(p)\n",
    "csvs |= set(glob.glob(os.path.join(ROOT,\"notebooks\",\"archive\",\"cnt_gpu_cooling_log_*_labeled.csv\")))\n",
    "csvs |= set(glob.glob(os.path.join(ROOT,\"notebooks\",\"archive\",\"cnt_cooling_log_*_labeled.csv\")))\n",
    "csvs = sorted(csvs)\n",
    "log(f\"[scan] candidate CSV files: {len(csvs)}\")\n",
    "\n",
    "# ========= LOAD + QC =========\n",
    "errors_path = os.path.join(OUTDIR,\"ecp_qc_errors.log\")\n",
    "if os.path.exists(errors_path):\n",
    "    try: os.remove(errors_path)\n",
    "    except: pass\n",
    "\n",
    "rows = []\n",
    "for p in csvs:\n",
    "    try:\n",
    "        df = pd.read_csv(p)\n",
    "        if df.empty: continue\n",
    "        col = choose_temp_col(df)\n",
    "        if not col: continue\n",
    "        T = pd.to_numeric(df[col], errors=\"coerce\").dropna().values\n",
    "        if T.size < MIN_SECONDS: continue\n",
    "\n",
    "        # unit normalize\n",
    "        Tc = normalize_temp_celsius(T)\n",
    "\n",
    "        # sanity clip outliers (physical GPU temps in °C)\n",
    "        Tc = Tc[(Tc > -10) & (Tc < 120)]\n",
    "        if Tc.size < MIN_SECONDS: continue\n",
    "\n",
    "        # QC: reject flat or wildly noisy series\n",
    "        s = float(np.std(Tc, ddof=1))\n",
    "        if s < 0.05:     # too flat (likely constant/rounded)\n",
    "            reason = \"flat\"\n",
    "            qc_ok = False\n",
    "        elif s > 20.0:   # absurd variance given °C\n",
    "            reason = \"absurd_var\"\n",
    "            qc_ok = False\n",
    "        else:\n",
    "            qc_ok = True\n",
    "            reason = \"\"\n",
    "\n",
    "        # features on detrended z and first differences\n",
    "        Z  = detrended_z(Tc)\n",
    "        dZ = np.diff(Z)\n",
    "        mad_dZ = float(np.median(np.abs(dZ - np.median(dZ))) * 1.4826)\n",
    "        sentZ  = float(sample_entropy(Z))\n",
    "        win    = int(min(WIN_THETA, max(32, Z.size//10)))\n",
    "        theta  = float(theta_robust_idx(Z, win=win))\n",
    "\n",
    "        rows.append(dict(\n",
    "            path=p,\n",
    "            tstamp=float(parse_time_from_name(p)),\n",
    "            raw_col=col,\n",
    "            n=int(Tc.size),\n",
    "            mean=float(np.mean(Tc)),\n",
    "            std=float(np.std(Tc, ddof=1)),\n",
    "            qc_ok=qc_ok,\n",
    "            qc_reason=reason,\n",
    "            label= (df[\"label\"].iloc[0].strip().lower() if \"label\" in df.columns else infer_label(p)),\n",
    "            sigma=float(np.std(Z, ddof=1)),         # variance proxy on detrended z\n",
    "            mad_diff=float(mad_dZ),                 # robust jitter\n",
    "            sampen=float(sentZ),\n",
    "            theta=float(theta)\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        with open(errors_path,\"a\",encoding=\"utf-8\") as f:\n",
    "            f.write(f\"{p} :: {repr(e)}\\n\")\n",
    "\n",
    "tbl_all = pd.DataFrame(rows).sort_values(\"tstamp\")\n",
    "tbl_all.to_csv(os.path.join(OUTDIR,\"ecp_qc_table_all.csv\"), index=False)\n",
    "log(f\"[load] usable (pre-QC): {len(tbl_all)}\")\n",
    "\n",
    "# Keep only QC-passing\n",
    "tbl_qc = tbl_all[tbl_all.qc_ok].copy().reset_index(drop=True)\n",
    "log(f\"[qc] passed: {len(tbl_qc)} | failed: {len(tbl_all)-len(tbl_qc)}\")\n",
    "tbl_qc.to_csv(os.path.join(OUTDIR,\"ecp_qc_table_pass.csv\"), index=False)\n",
    "\n",
    "if tbl_qc.empty:\n",
    "    log(f\"[exit] No QC-passing runs. See {errors_path} and ecp_qc_table_all.csv\")\n",
    "    raise SystemExit\n",
    "\n",
    "# ========= LABELS & PAIRING =========\n",
    "# Normalize labels\n",
    "tbl_qc[\"label\"] = tbl_qc[\"label\"].apply(lambda x: x if x in (\"ritual\",\"control\") else \"unknown\")\n",
    "nR = int((tbl_qc.label==\"ritual\").sum())\n",
    "nC = int((tbl_qc.label==\"control\").sum())\n",
    "nU = int((tbl_qc.label==\"unknown\").sum())\n",
    "log(f\"[labels] ritual={nR} control={nC} unknown={nU}\")\n",
    "\n",
    "# Pair unknowns to balance via nearest time to ritual/control anchors\n",
    "paired = tbl_qc.copy()\n",
    "mapping = []\n",
    "\n",
    "def nearest_assign(target_label, pool_from, need):\n",
    "    if need <= 0: return\n",
    "    pool = paired.index[paired.label==pool_from].tolist()\n",
    "    anchors = paired[paired.label==target_label][[\"tstamp\"]]\n",
    "    if anchors.empty or not pool: return\n",
    "    for i in pool:\n",
    "        t = paired.at[i,\"tstamp\"]\n",
    "        # distance to nearest anchor\n",
    "        d = np.min(np.abs(anchors[\"tstamp\"].values - t))\n",
    "        # store as candidate; we’ll sort by closeness later\n",
    "        mapping.append((\"candidate\", i, pool_from, target_label, d))\n",
    "\n",
    "# If one class is missing, promote closest unknowns to that class\n",
    "if (nR==0 or nC==0) and nU>0:\n",
    "    if nC==0:\n",
    "        nearest_assign(\"ritual\",\"unknown\",AUTOBAL_TARGET_MIN)\n",
    "        # pick closest half to become control\n",
    "        cand = [m for m in mapping if m[3]==\"ritual\"]\n",
    "        cand.sort(key=lambda z:z[4])\n",
    "        for _, idx, old, new, _ in cand[:max(AUTOBAL_TARGET_MIN,1)]:\n",
    "            paired.at[idx,\"label\"] = \"control\"\n",
    "    elif nR==0:\n",
    "        nearest_assign(\"control\",\"unknown\",AUTOBAL_TARGET_MIN)\n",
    "        cand = [m for m in mapping if m[3]==\"control\"]\n",
    "        cand.sort(key=lambda z:z[4])\n",
    "        for _, idx, old, new, _ in cand[:max(AUTOBAL_TARGET_MIN,1)]:\n",
    "            paired.at[idx,\"label\"] = \"ritual\"\n",
    "\n",
    "# If still imbalanced and AUTOBAL is allowed, alternate assignment by time\n",
    "counts = paired.label.value_counts()\n",
    "nR, nC, nU = int(counts.get(\"ritual\",0)), int(counts.get(\"control\",0)), int(counts.get(\"unknown\",0))\n",
    "if AUTOBAL_IF_NEEDED and (nR==0 or nC==0) and nU>0:\n",
    "    need_ctl = (nC==0)\n",
    "    unk_idx = list(paired.index[paired.label==\"unknown\"])\n",
    "    for k,i in enumerate(unk_idx):\n",
    "        if need_ctl and k%2==0:\n",
    "            paired.at[i,\"label\"]=\"control\"\n",
    "        elif (not need_ctl) and k%2==1:\n",
    "            paired.at[i,\"label\"]=\"ritual\"\n",
    "\n",
    "paired.to_csv(os.path.join(OUTDIR,\"ecp_qc_table_labeled.csv\"), index=False)\n",
    "\n",
    "counts = paired.label.value_counts()\n",
    "nR, nC = int(counts.get(\"ritual\",0)), int(counts.get(\"control\",0))\n",
    "log(f\"[labels*] ritual={nR} control={nC} unknown={int(counts.get('unknown',0))}\")\n",
    "if nR==0 or nC==0:\n",
    "    log(\"[exit] Need both classes after QC; add/rename a few files with 'control' and 'ritual'.\")\n",
    "    raise SystemExit\n",
    "\n",
    "# ========= ANALYSIS =========\n",
    "R = paired[paired.label==\"ritual\"]\n",
    "C = paired[paired.label==\"control\"]\n",
    "\n",
    "def smean(s): \n",
    "    a = np.asarray(s, float)\n",
    "    return float(np.mean(a)) if a.size else float(\"nan\")\n",
    "\n",
    "# Metrics: lower sigma/mad_diff/sampen are better; earlier (smaller) theta is better\n",
    "obs_sigma, p_sigma = perm_test(R[\"sigma\"].values,    C[\"sigma\"].values,    reps=PERM_REPS, seed=SEED)\n",
    "obs_mad  , p_mad   = perm_test(R[\"mad_diff\"].values, C[\"mad_diff\"].values, reps=PERM_REPS, seed=SEED)\n",
    "obs_se   , p_se    = perm_test(R[\"sampen\"].values,   C[\"sampen\"].values,   reps=PERM_REPS, seed=SEED)\n",
    "obs_th   , p_th    = perm_test(-R[\"theta\"].values,  -C[\"theta\"].values,   reps=PERM_REPS, seed=SEED)\n",
    "\n",
    "d_sigma = cohen_d(C[\"sigma\"].values,    R[\"sigma\"].values)\n",
    "d_mad   = cohen_d(C[\"mad_diff\"].values, R[\"mad_diff\"].values)\n",
    "d_se    = cohen_d(C[\"sampen\"].values,   R[\"sampen\"].values)\n",
    "d_theta = -cohen_d(R[\"theta\"].values,   C[\"theta\"].values)\n",
    "\n",
    "verdict = {\n",
    "  \"claim\": \"ECP lowers thermal jitter (σ / MAD / SampEn) and advances Θ on detrended temps\",\n",
    "  \"n_ritual\": int(len(R)), \"n_control\": int(len(C)),\n",
    "  \"mean_sigma_control\": smean(C[\"sigma\"]), \"mean_sigma_ritual\": smean(R[\"sigma\"]),\n",
    "  \"mean_mad_control\":   smean(C[\"mad_diff\"]), \"mean_mad_ritual\":  smean(R[\"mad_diff\"]),\n",
    "  \"mean_sampen_control\":smean(C[\"sampen\"]), \"mean_sampen_ritual\": smean(R[\"sampen\"]),\n",
    "  \"mean_theta_control\": smean(C[\"theta\"]),  \"mean_theta_ritual\":  smean(R[\"theta\"]),\n",
    "  \"effect_d_sigma(+good)\": float(d_sigma),\n",
    "  \"effect_d_mad(+good)\":   float(d_mad),\n",
    "  \"effect_d_sampen(+good)\":float(d_se),\n",
    "  \"effect_d_theta(+good)\": float(d_theta),\n",
    "  \"perm_p_sigma\": float(p_sigma),\n",
    "  \"perm_p_mad\":   float(p_mad),\n",
    "  \"perm_p_sampen\":float(p_se),\n",
    "  \"perm_p_theta\": float(p_th),\n",
    "  \"pass\": bool(\n",
    "      (p_sigma<0.05 and d_sigma>=0.30) or\n",
    "      (p_mad  <0.05 and d_mad  >=0.30) or\n",
    "      (p_se   <0.05 and d_se   >=0.30) or\n",
    "      (p_th   <0.05 and d_theta>=0.30)\n",
    "  )\n",
    "}\n",
    "with open(os.path.join(OUTDIR,\"ecp_qc_verdict.json\"),\"w\") as f:\n",
    "    json.dump(verdict,f,indent=2)\n",
    "\n",
    "log(\"\\n== ECP (QC) Verdict ==\")\n",
    "log(json.dumps(verdict, indent=2))\n",
    "\n",
    "# ========= PLOTS =========\n",
    "def boxplot(metric, title, ylabel, fname):\n",
    "    plt.figure(figsize=(9,5))\n",
    "    plt.boxplot([C[metric], R[metric]], tick_labels=[\"control\",\"ritual\"])\n",
    "    plt.title(title); plt.ylabel(ylabel); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTDIR, fname), dpi=160); plt.close()\n",
    "\n",
    "boxplot(\"sigma\",   \"ECP: σ (detrended z) distribution\", \"σ (a.u.)\", \"plot_sigma_box.png\")\n",
    "boxplot(\"mad_diff\",\"ECP: MAD of ΔZ distribution\",       \"MAD(ΔZ)\",   \"plot_maddiff_box.png\")\n",
    "boxplot(\"sampen\",  \"ECP: Sample Entropy distribution\",  \"SampEn\",     \"plot_sampen_box.png\")\n",
    "boxplot(\"theta\",   \"ECP: Θ (earlier = better)\",         \"index\",      \"plot_theta_box.png\")\n",
    "\n",
    "plt.figure(figsize=(10,5.5))\n",
    "plt.scatter(np.arange(len(C)), C[\"sigma\"], label=\"control σ\", alpha=0.85)\n",
    "plt.scatter(np.arange(len(R))+0.1, R[\"sigma\"], label=\"ritual σ\", alpha=0.85)\n",
    "plt.title(\"ECP: Run-level σ after QC & normalization\"); plt.xlabel(\"index\"); plt.ylabel(\"σ (a.u.)\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTDIR,\"plot_sigma_scatter.png\"), dpi=160); plt.close()\n",
    "\n",
    "log(f\"\\n[Artifacts] {OUTDIR}\")\n",
    "log(\"[Tips] If 'unknown' still dominates, rename a few files with 'control'/'ritual' in the filename, or add a 'label' column. Clean tables: ecp_qc_table_pass.csv, ecp_qc_table_labeled.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2be03e39-bbb2-42bd-84f1-6a0eff3cdd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[scan] collecting CSVs...\n",
      "[scan] candidates: 2585\n",
      "[load] usable runs: 5\n",
      "[labels] ritual=5 control=0 unknown=0\n",
      "[warn] Need both ritual and control labels for paired analysis; please rename a few files.\n",
      "[pairing] pairs formed: 0\n",
      "\n",
      "== ECP Paired Verdict ==\n",
      "{\n",
      "  \"claim\": \"ECP reduces residual thermal jitter and advances \\u0398 (paired analysis)\",\n",
      "  \"pairs\": 0,\n",
      "  \"passed\": false,\n",
      "  \"metrics\": [\n",
      "    {\n",
      "      \"sigma\": {\n",
      "        \"mean_ritual\": 0.3035268943923454,\n",
      "        \"mean_control\": NaN,\n",
      "        \"paired_n\": 0\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"mad_diff\": {\n",
      "        \"mean_ritual\": 0.12766769046781049,\n",
      "        \"mean_control\": NaN,\n",
      "        \"paired_n\": 0\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"sampen\": {\n",
      "        \"mean_ritual\": 0.2952864939403439,\n",
      "        \"mean_control\": NaN,\n",
      "        \"paired_n\": 0\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"theta\": {\n",
      "        \"mean_ritual\": 139.6,\n",
      "        \"mean_control\": NaN,\n",
      "        \"paired_n\": 0\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "[Artifacts] C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\ecp_pairs_20251025-025058_be098b42\n",
      "[Next] Add a few explicit ritual/control file labels to increase pairs (aim ≥12 pairs). Then re-run.\n"
     ]
    }
   ],
   "source": [
    "# CNT_Lab — ECP Paired Analysis (units fixed, QC, residual sigma, paired permutation)\n",
    "# Paste in a fresh cell and run after labeling a few files with 'ritual'/'control' in the name.\n",
    "\n",
    "import os, re, glob, json, uuid, math\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# ===== CONFIG =====\n",
    "ROOT         = r\"C:\\Users\\caleb\\CNT_Lab\"\n",
    "MIN_SECONDS  = 90\n",
    "WIN_THETA    = 97\n",
    "PERM_REPS    = 20000\n",
    "SEED         = 42\n",
    "PAIR_MAX_GAP = 48 * 3600  # seconds: allow pairing if within 48h (tweak as needed)\n",
    "\n",
    "PREF_HINTS = (\"cool\",\"gpu\",\"thermal\",\"temp\",\"segment\",\"log\",\"cooling_segments\",\"notebooks\",\"archive\",\"artifacts\")\n",
    "TEMP_RE = re.compile(r\"(hotspot|gpu[_\\- ]?hot|gpu[_\\- ]?temp|temp|temperature|t[_\\- ]?hot|hot)\", re.I)\n",
    "\n",
    "OUTDIR = os.path.join(ROOT,\"notebooks\",\"archive\",\n",
    "                      f\"ecp_pairs_{datetime.now().strftime('%Y%m%d-%H%M%S')}_{uuid.uuid4().hex[:8]}\")\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "def log(x): print(x)\n",
    "\n",
    "# ===== helpers =====\n",
    "def parse_time_from_name(path):\n",
    "    n = os.path.basename(path)\n",
    "    m = re.search(r\"(20\\d{6}[-_]?\\d{6})\", n)\n",
    "    if m:\n",
    "        s = re.sub(r\"[-_]\",\"\",m.group(1))\n",
    "        try: return datetime.strptime(s,\"%Y%m%d%H%M%S\").timestamp()\n",
    "        except: pass\n",
    "    try: return os.path.getmtime(path)\n",
    "    except: return float(\"nan\")\n",
    "\n",
    "def infer_label(path):\n",
    "    low = os.path.basename(path).lower()\n",
    "    if \"ritual\" in low or \"ecp\" in low or \"glyph\" in low or \"ceremony\" in low or \"resonance\" in low: return \"ritual\"\n",
    "    if \"control\" in low or \"neutral\" in low or \"baseline\" in low or \"placebo\" in low: return \"control\"\n",
    "    m = re.search(r\"[_\\-\\.](r|c)(?=\\.)\", low)\n",
    "    return {\"r\":\"ritual\",\"c\":\"control\"}.get(m.group(1),\"unknown\")\n",
    "\n",
    "def choose_temp_col(df):\n",
    "    cand = [c for c in df.columns if TEMP_RE.search(str(c)) and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    if cand: return cand[0]\n",
    "    # fallback: any plausible numeric\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[c]):\n",
    "            v = pd.to_numeric(df[c], errors=\"coerce\").dropna().values\n",
    "            if v.size >= MIN_SECONDS//2 and np.nanmin(v)>-50 and np.nanmax(v)<10000:\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "def to_celsius(x):\n",
    "    x = np.asarray(x,float)\n",
    "    if x.size==0: return x\n",
    "    vmin, vmax, vmean = np.nanmin(x), np.nanmax(x), np.nanmean(x)\n",
    "    # Kelvin?\n",
    "    if 200 < vmean < 400 and vmax < 500: x = x - 273.15\n",
    "    # milli-deg C?\n",
    "    if vmax > 2000: x = x / 1000.0\n",
    "    return x\n",
    "\n",
    "def detrend_residual(x):\n",
    "    x = np.asarray(x,float)\n",
    "    t = np.arange(x.size, dtype=float)\n",
    "    A = np.vstack([t, np.ones_like(t)]).T\n",
    "    m,b = np.linalg.lstsq(A, x, rcond=None)[0]\n",
    "    return x - (m*t + b)\n",
    "\n",
    "def sample_entropy(x, m=2, r=0.2):\n",
    "    x = np.asarray(x,float)\n",
    "    if x.size < (m+2): return np.nan\n",
    "    r *= np.std(x, ddof=0) + 1e-12\n",
    "    def _phi(mm):\n",
    "        N = x.size - mm + 1\n",
    "        if N <= 1: return 0.0\n",
    "        X = np.array([x[i:i+mm] for i in range(N)])\n",
    "        C = np.sum(np.max(np.abs(X[:,None,:]-X[None,:,:]), axis=2) <= r, axis=1) - 1\n",
    "        return np.sum(C)/(N*(N-1)+1e-12)\n",
    "    A = _phi(m+1); B = _phi(m)\n",
    "    if A<=0 or B<=0: return np.nan\n",
    "    return -np.log(A/B)\n",
    "\n",
    "def theta_idx(x, win=97):\n",
    "    if x.size < win: return np.nan\n",
    "    v = pd.Series(x).rolling(win, min_periods=win).std().values\n",
    "    return int(np.nanargmin(v))\n",
    "\n",
    "def cohen_d(a, b):\n",
    "    a = np.asarray(a,float); b = np.asarray(b,float)\n",
    "    if len(a)<2 or len(b)<2: return np.nan\n",
    "    sa, sb = np.std(a,ddof=1), np.std(b,ddof=1)\n",
    "    s = math.sqrt(((len(a)-1)*sa*sa + (len(b)-1)*sb*sb)/(len(a)+len(b)-2)) if (len(a)+len(b))>=3 else np.nan\n",
    "    return (np.mean(a)-np.mean(b))/s if (s and s>0) else np.nan\n",
    "\n",
    "def perm_test_unpaired(a,b,reps=10000,metric=np.mean,seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    a = np.asarray(a,float); b = np.asarray(b,float)\n",
    "    obs = metric(a)-metric(b)\n",
    "    joined = np.concatenate([a,b]); na = len(a)\n",
    "    cnt=0\n",
    "    for _ in range(reps):\n",
    "        rng.shuffle(joined)\n",
    "        cnt += abs(metric(joined[:na])-metric(joined[na:])) >= abs(obs)\n",
    "    return float(obs), float(cnt/reps)\n",
    "\n",
    "def perm_test_paired(diff, reps=10000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    d = np.asarray(diff,float)\n",
    "    obs = np.mean(d)\n",
    "    cnt=0\n",
    "    for _ in range(reps):\n",
    "        signs = rng.choice([-1,1], size=d.size)\n",
    "        cnt += abs(np.mean(signs*d)) >= abs(obs)\n",
    "    return float(obs), float(cnt/reps)\n",
    "\n",
    "# ===== scan/load/QC =====\n",
    "import pandas as pd\n",
    "log(\"[scan] collecting CSVs...\")\n",
    "csvs = set()\n",
    "for d,_,fs in os.walk(ROOT):\n",
    "    low = d.lower()\n",
    "    if \"\\\\.venv\" in low or \"site-packages\" in low or \"\\\\__pycache__\" in low: continue\n",
    "    for fn in fs:\n",
    "        if fn.lower().endswith(\".csv\"):\n",
    "            p = os.path.join(d,fn)\n",
    "            if any(h in p.lower() for h in PREF_HINTS):\n",
    "                csvs.add(p)\n",
    "csvs |= set(glob.glob(os.path.join(ROOT,\"notebooks\",\"archive\",\"cnt_gpu_cooling_log_*_labeled.csv\")))\n",
    "csvs |= set(glob.glob(os.path.join(ROOT,\"notebooks\",\"archive\",\"cnt_cooling_log_*_labeled.csv\")))\n",
    "csvs = sorted(csvs)\n",
    "log(f\"[scan] candidates: {len(csvs)}\")\n",
    "\n",
    "rows = []\n",
    "for p in csvs:\n",
    "    try:\n",
    "        df = pd.read_csv(p)\n",
    "        if df.empty: continue\n",
    "        col = choose_temp_col(df)\n",
    "        if not col: continue\n",
    "        T = pd.to_numeric(df[col], errors=\"coerce\").dropna().values\n",
    "        if T.size < MIN_SECONDS: continue\n",
    "        Tc = to_celsius(T)\n",
    "        Tc = Tc[(Tc > -10) & (Tc < 120)]\n",
    "        if Tc.size < MIN_SECONDS: continue\n",
    "\n",
    "        s = float(np.std(Tc, ddof=1))\n",
    "        if s < 0.05 or s > 20.0: continue  # flat or absurd\n",
    "\n",
    "        R = detrend_residual(Tc)           # stay in °C residuals (no z-scale)\n",
    "        win = int(min(WIN_THETA, max(32, R.size//10)))\n",
    "\n",
    "        rows.append(dict(\n",
    "            path=p,\n",
    "            label=infer_label(p),\n",
    "            tstamp=float(parse_time_from_name(p)),\n",
    "            n=int(R.size),\n",
    "            sigma=float(np.std(R, ddof=1)),        # now meaningful\n",
    "            mad_diff=float(np.median(np.abs(np.diff(R) - np.median(np.diff(R))))*1.4826),\n",
    "            sampen=float(sample_entropy(R)),\n",
    "            theta=float(theta_idx(R, win=win))\n",
    "        ))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "tbl = pd.DataFrame(rows).sort_values(\"tstamp\").reset_index(drop=True)\n",
    "tbl.to_csv(os.path.join(OUTDIR,\"ecp_pairs_table_all.csv\"), index=False)\n",
    "log(f\"[load] usable runs: {len(tbl)}\")\n",
    "\n",
    "if tbl.empty:\n",
    "    log(\"[exit] No usable runs found after QC.\")\n",
    "    raise SystemExit\n",
    "\n",
    "# normalize labels to three states\n",
    "tbl[\"label\"] = tbl[\"label\"].apply(lambda x: x if x in (\"ritual\",\"control\") else \"unknown\")\n",
    "tbl.to_csv(os.path.join(OUTDIR,\"ecp_pairs_table_labeled.csv\"), index=False)\n",
    "\n",
    "nR = int((tbl.label==\"ritual\").sum()); nC = int((tbl.label==\"control\").sum())\n",
    "log(f\"[labels] ritual={nR} control={nC} unknown={(tbl.label=='unknown').sum()}\")\n",
    "\n",
    "# ===== pairing by time (nearest neighbor within PAIR_MAX_GAP) =====\n",
    "R = tbl[tbl.label==\"ritual\"].copy()\n",
    "C = tbl[tbl.label==\"control\"].copy()\n",
    "if R.empty or C.empty:\n",
    "    log(\"[warn] Need both ritual and control labels for paired analysis; please rename a few files.\")\n",
    "pairs = []\n",
    "if not R.empty and not C.empty:\n",
    "    ci = C.index.tolist()\n",
    "    for ri, rrow in R.iterrows():\n",
    "        # pick nearest control in time\n",
    "        deltas = np.abs(C[\"tstamp\"].values - rrow[\"tstamp\"])\n",
    "        if len(deltas)==0: continue\n",
    "        j = int(np.argmin(deltas))\n",
    "        gap = float(deltas[j])\n",
    "        if gap <= PAIR_MAX_GAP:\n",
    "            pairs.append((ri, C.index[j], gap))\n",
    "\n",
    "# deduplicate controls: keep closest pair per control\n",
    "used_controls = set()\n",
    "final_pairs = []\n",
    "for ri, ci0, gap in sorted(pairs, key=lambda x:x[2]):\n",
    "    if ci0 in used_controls: \n",
    "        continue\n",
    "    used_controls.add(ci0)\n",
    "    final_pairs.append((ri, ci0, gap))\n",
    "\n",
    "log(f\"[pairing] pairs formed: {len(final_pairs)}\")\n",
    "\n",
    "def collect(metric):\n",
    "    # unpaired arrays\n",
    "    Ru = R[metric].values if not R.empty else np.array([])\n",
    "    Cu = C[metric].values if not C.empty else np.array([])\n",
    "    # paired differences (ritual - control)\n",
    "    diffs = []\n",
    "    for ri, ci0, _ in final_pairs:\n",
    "        diffs.append(R.loc[ri,metric] - C.loc[ci0,metric])\n",
    "    return np.asarray(Ru,float), np.asarray(Cu,float), np.asarray(diffs,float)\n",
    "\n",
    "metrics = {\n",
    "    \"sigma\":   (\"Residual σ (°C)\", True),     # lower better\n",
    "    \"mad_diff\":(\"MAD(Δresidual) (°C)\", True), # lower better\n",
    "    \"sampen\":  (\"Sample Entropy\", True),      # lower better (stability)\n",
    "    \"theta\":   (\"Θ index (earlier=smaller)\", True)  # lower better\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for m,(label, lower_is_better) in metrics.items():\n",
    "    Ru, Cu, D = collect(m)\n",
    "    res = {\"mean_ritual\": float(np.mean(Ru)) if Ru.size else float(\"nan\"),\n",
    "           \"mean_control\":float(np.mean(Cu)) if Cu.size else float(\"nan\"),\n",
    "           \"paired_n\": int(D.size)}\n",
    "    # unpaired\n",
    "    if Ru.size>=2 and Cu.size>=2:\n",
    "        obs_u, p_u = perm_test_unpaired(Ru, Cu, reps=PERM_REPS, seed=SEED)\n",
    "        # effect size: control - ritual (positive is good for lower-is-better)\n",
    "        d = None\n",
    "        try: d = (np.mean(Cu)-np.mean(Ru)) / np.sqrt(((np.var(Ru,ddof=1)+np.var(Cu,ddof=1))/2))\n",
    "        except: d = float(\"nan\")\n",
    "        res.update(dict(unpaired_obs=float(obs_u), unpaired_p=float(p_u), cohen_d=float(d)))\n",
    "    # paired\n",
    "    if D.size>=3:\n",
    "        obs_p, p_p = perm_test_paired(D, reps=PERM_REPS, seed=SEED)\n",
    "        # for lower-is-better: positive mean(control - ritual) is good → flip sign accordingly\n",
    "        res.update(dict(paired_mean=float(np.mean(D)*(-1 if lower_is_better else 1)),\n",
    "                        paired_obs=float(obs_p), paired_p=float(p_p)))\n",
    "    results[m]=res\n",
    "\n",
    "with open(os.path.join(OUTDIR,\"ecp_pairs_results.json\"),\"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "# pass criteria: any metric with (paired_p<0.05) OR (unpaired_p<0.05 and |d|>=0.3)\n",
    "passed = False\n",
    "rnotes = []\n",
    "for m,(label, lower_is_better) in metrics.items():\n",
    "    r = results[m]\n",
    "    p_pair = r.get(\"paired_p\", 1.0)\n",
    "    p_un   = r.get(\"unpaired_p\", 1.0)\n",
    "    d      = r.get(\"cohen_d\", float(\"nan\"))\n",
    "    if p_pair < 0.05 or (p_un < 0.05 and (not math.isnan(d)) and abs(d) >= 0.30):\n",
    "        passed = True\n",
    "    rnotes.append({m:r})\n",
    "\n",
    "verdict = {\"claim\":\"ECP reduces residual thermal jitter and advances Θ (paired analysis)\",\n",
    "           \"pairs\": int(len(final_pairs)),\n",
    "           \"passed\": bool(passed),\n",
    "           \"metrics\": rnotes}\n",
    "with open(os.path.join(OUTDIR,\"ecp_pairs_verdict.json\"),\"w\") as f:\n",
    "    json.dump(verdict, f, indent=2)\n",
    "\n",
    "log(\"\\n== ECP Paired Verdict ==\")\n",
    "log(json.dumps(verdict, indent=2))\n",
    "\n",
    "# simple boxplots (unpaired view)\n",
    "for m,(lab,_) in metrics.items():\n",
    "    if (tbl.label==\"ritual\").any() and (tbl.label==\"control\").any():\n",
    "        plt.figure(figsize=(9,5))\n",
    "        plt.boxplot([tbl[tbl.label==\"control\"][m], tbl[tbl.label==\"ritual\"][m]],\n",
    "                    tick_labels=[\"control\",\"ritual\"])\n",
    "        plt.title(f\"ECP: {lab}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTDIR, f\"plot_{m}_box.png\"), dpi=160)\n",
    "        plt.close()\n",
    "\n",
    "log(f\"\\n[Artifacts] {OUTDIR}\")\n",
    "log(\"[Next] Add a few explicit ritual/control file labels to increase pairs (aim ≥12 pairs). Then re-run.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ad4ac9b-1c8b-4362-bea4-a66d6b3d2fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[scan] collecting CSVs…\n",
      "[scan] candidates: 2587\n",
      "[load] QC-passing candidates: 60\n",
      "[state] already labeled: 0 | unlabeled: 60\n",
      "[pairing] proposed pairs: 12 (target 12)\n",
      "\n",
      "== Labeling kit ==\n",
      "• Proposals  : C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\ecp_labeler_20251025-030850_4d3a5643\\label_proposals.csv\n",
      "• Ritual PS  : C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\ecp_labeler_20251025-030850_4d3a5643\\mark_ritual.ps1\n",
      "• Control PS : C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\ecp_labeler_20251025-030850_4d3a5643\\mark_control.ps1\n",
      "• Revert PS  : C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\ecp_labeler_20251025-030850_4d3a5643\\revert_renames.ps1\n",
      "\n",
      "Usage (PowerShell):\n",
      "  cd \"C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\ecp_labeler_20251025-030850_4d3a5643\"\n",
      "  # Inspect label_proposals.csv, edit if needed, then:\n",
      "  .\\\"mark_control.ps1\"\n",
      "  .\\\"mark_ritual.ps1\"\n",
      "  # To undo: .\\revert_renames.ps1\n"
     ]
    }
   ],
   "source": [
    "# CNT_Lab — Label Script Generator (balanced ritual/control from nearest-time pairs)\n",
    "# Output: mark_ritual.ps1, mark_control.ps1, label_proposals.csv in OUTDIR.\n",
    "\n",
    "import os, re, glob, uuid, json\n",
    "import numpy as np, pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "ROOT         = r\"C:\\Users\\caleb\\CNT_Lab\"\n",
    "MIN_SECONDS  = 90\n",
    "PREF_HINTS   = (\"cool\",\"gpu\",\"thermal\",\"temp\",\"segment\",\"log\",\"cooling_segments\",\"notebooks\",\"archive\",\"artifacts\")\n",
    "TEMP_RE      = re.compile(r\"(hotspot|gpu[_\\- ]?hot|gpu[_\\- ]?temp|temp|temperature|t[_\\- ]?hot|hot)\", re.I)\n",
    "TARGET_PAIRS = 12          # <- change if you want more or less\n",
    "PAIR_MAX_GAP = 48*3600     # allow pairing within 48h\n",
    "\n",
    "OUTDIR = os.path.join(ROOT, \"notebooks\", \"archive\",\n",
    "                      f\"ecp_labeler_{datetime.now().strftime('%Y%m%d-%H%M%S')}_{uuid.uuid4().hex[:8]}\")\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "def log(x): print(x)\n",
    "\n",
    "def parse_time_from_name(path):\n",
    "    n = os.path.basename(path)\n",
    "    m = re.search(r\"(20\\d{6}[-_]?\\d{6})\", n)\n",
    "    if m:\n",
    "        s = re.sub(r\"[-_]\", \"\", m.group(1))\n",
    "        try: return datetime.strptime(s, \"%Y%m%d%H%M%S\").timestamp()\n",
    "        except: pass\n",
    "    try: return os.path.getmtime(path)\n",
    "    except: return float(\"nan\")\n",
    "\n",
    "def choose_temp_col(df):\n",
    "    # pick the most temp-looking numeric column\n",
    "    cand = [c for c in df.columns if TEMP_RE.search(str(c)) and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    if cand: return cand[0]\n",
    "    for c in df.columns:\n",
    "        if pd.api.types.is_numeric_dtype(df[c]):\n",
    "            v = pd.to_numeric(df[c], errors=\"coerce\").dropna().values\n",
    "            if v.size >= MIN_SECONDS//2 and np.nanmin(v) > -50 and np.nanmax(v) < 10000:\n",
    "                return c\n",
    "    return None\n",
    "\n",
    "# 1) collect CSVs\n",
    "log(\"[scan] collecting CSVs…\")\n",
    "csvs = set()\n",
    "for d,_,fs in os.walk(ROOT):\n",
    "    low = d.lower()\n",
    "    if \"\\\\.venv\" in low or \"site-packages\" in low or \"\\\\__pycache__\" in low:\n",
    "        continue\n",
    "    for fn in fs:\n",
    "        if fn.lower().endswith(\".csv\"):\n",
    "            p = os.path.join(d, fn)\n",
    "            if any(h in p.lower() for h in PREF_HINTS):\n",
    "                csvs.add(p)\n",
    "csvs = sorted(csvs)\n",
    "log(f\"[scan] candidates: {len(csvs)}\")\n",
    "\n",
    "# 2) QC list (reuse simple checks; we only need tstamp + path)\n",
    "rows = []\n",
    "for p in csvs:\n",
    "    try:\n",
    "        df = pd.read_csv(p)\n",
    "        if df.empty: continue\n",
    "        col = choose_temp_col(df)\n",
    "        if not col: continue\n",
    "        v = pd.to_numeric(df[col], errors=\"coerce\").dropna().values\n",
    "        if v.size < MIN_SECONDS: continue\n",
    "        # normalize rough units (Kelvin/m°C) just to detect absurd values\n",
    "        vmean, vmax = float(np.mean(v)), float(np.max(v))\n",
    "        if 200 < vmean < 400 and vmax < 500: v = v - 273.15\n",
    "        if np.max(v) > 2000: v = v/1000.0\n",
    "        v = v[(v > -10) & (v < 120)]\n",
    "        if v.size < MIN_SECONDS: continue\n",
    "        s = float(np.std(v, ddof=1))\n",
    "        if s < 0.05 or s > 20.0:  # flat/absurd\n",
    "            continue\n",
    "        rows.append(dict(path=p, tstamp=float(parse_time_from_name(p))))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "tbl = pd.DataFrame(rows).dropna().sort_values(\"tstamp\").reset_index(drop=True)\n",
    "log(f\"[load] QC-passing candidates: {len(tbl)}\")\n",
    "if tbl.empty:\n",
    "    print(\"[exit] No good candidates.\")\n",
    "else:\n",
    "    # 3) if any files already labeled, keep them; otherwise propose pairs\n",
    "    def is_labeled(name):\n",
    "        n = os.path.basename(name).lower()\n",
    "        return (\"ritual\" in n) or (\"control\" in n)\n",
    "\n",
    "    labeled_mask = tbl[\"path\"].apply(is_labeled)\n",
    "    L = tbl[labeled_mask]\n",
    "    U = tbl[~labeled_mask].copy()\n",
    "\n",
    "    log(f\"[state] already labeled: {len(L)} | unlabeled: {len(U)}\")\n",
    "\n",
    "    # Build nearest-neighbor pairs from unlabeled pool by time\n",
    "    # Greedy: walk forward, pair close neighbors within PAIR_MAX_GAP\n",
    "    pairs = []\n",
    "    used = set()\n",
    "    for i in range(len(U)-1):\n",
    "        if i in used: continue\n",
    "        t1 = U.iloc[i]\n",
    "        # find nearest j>i\n",
    "        best = None\n",
    "        best_gap = None\n",
    "        for j in range(i+1, len(U)):\n",
    "            if j in used: continue\n",
    "            t2 = U.iloc[j]\n",
    "            gap = abs(t2[\"tstamp\"] - t1[\"tstamp\"])\n",
    "            if gap <= PAIR_MAX_GAP and (best is None or gap < best_gap):\n",
    "                best, best_gap = (i, j), gap\n",
    "            if best_gap is not None and gap > best_gap*2:\n",
    "                break\n",
    "        if best is not None:\n",
    "            pairs.append((best[0], best[1], float(best_gap)))\n",
    "            used.add(best[0]); used.add(best[1])\n",
    "\n",
    "    # sort by closeness and take TARGET_PAIRS\n",
    "    pairs = sorted(pairs, key=lambda x: x[2])[:TARGET_PAIRS]\n",
    "    log(f\"[pairing] proposed pairs: {len(pairs)} (target {TARGET_PAIRS})\")\n",
    "\n",
    "    # Alternate assignment within each pair to balance: first becomes control, second ritual (or vice versa).\n",
    "    proposals = []\n",
    "    for k,(i,j,g) in enumerate(pairs):\n",
    "        p1 = U.iloc[i][\"path\"]; p2 = U.iloc[j][\"path\"]\n",
    "        # alternate which side is ritual so we don't bias on order\n",
    "        if k % 2 == 0:\n",
    "            proposals.append(dict(path=p1, new_label=\"control\", gap_sec=g))\n",
    "            proposals.append(dict(path=p2, new_label=\"ritual\",  gap_sec=g))\n",
    "        else:\n",
    "            proposals.append(dict(path=p1, new_label=\"ritual\",  gap_sec=g))\n",
    "            proposals.append(dict(path=p2, new_label=\"control\", gap_sec=g))\n",
    "\n",
    "    prop_df = pd.DataFrame(proposals)\n",
    "    prop_csv = os.path.join(OUTDIR, \"label_proposals.csv\")\n",
    "    prop_df.to_csv(prop_csv, index=False)\n",
    "\n",
    "    # 4) Write PowerShell rename scripts (non-destructive proposals)\n",
    "    ritual_ps   = os.path.join(OUTDIR, \"mark_ritual.ps1\")\n",
    "    control_ps  = os.path.join(OUTDIR, \"mark_control.ps1\")\n",
    "    revert_ps   = os.path.join(OUTDIR, \"revert_renames.ps1\")\n",
    "\n",
    "    def add_suffix(path, suffix):\n",
    "        d = os.path.dirname(path); base = os.path.basename(path)\n",
    "        if base.lower().endswith(\".csv\"):\n",
    "            stem = base[:-4]\n",
    "            return os.path.join(d, f\"{stem}_{suffix}.csv\")\n",
    "        return os.path.join(d, f\"{base}_{suffix}\")\n",
    "\n",
    "    ritual_lines, control_lines, revert_lines = [], [], []\n",
    "    for _, row in prop_df.iterrows():\n",
    "        src = row[\"path\"]\n",
    "        if row[\"new_label\"] == \"ritual\":\n",
    "            dst = add_suffix(src, \"ritual\")\n",
    "            ritual_lines.append(f'Rename-Item -LiteralPath \"{src}\" -NewName \"{os.path.basename(dst)}\"')\n",
    "            revert_lines.append(f'Rename-Item -LiteralPath \"{dst}\" -NewName \"{os.path.basename(src)}\"')\n",
    "        else:\n",
    "            dst = add_suffix(src, \"control\")\n",
    "            control_lines.append(f'Rename-Item -LiteralPath \"{src}\" -NewName \"{os.path.basename(dst)}\"')\n",
    "            revert_lines.append(f'Rename-Item -LiteralPath \"{dst}\" -NewName \"{os.path.basename(src)}\"')\n",
    "\n",
    "    with open(ritual_ps, \"w\", encoding=\"utf-8\") as f:   f.write(\"\\n\".join(ritual_lines) + \"\\n\")\n",
    "    with open(control_ps, \"w\", encoding=\"utf-8\") as f:  f.write(\"\\n\".join(control_lines) + \"\\n\")\n",
    "    with open(revert_ps, \"w\", encoding=\"utf-8\") as f:   f.write(\"\\n\".join(revert_lines) + \"\\n\")\n",
    "\n",
    "    print(\"\\n== Labeling kit ==\")\n",
    "    print(\"• Proposals  :\", prop_csv)\n",
    "    print(\"• Ritual PS  :\", ritual_ps)\n",
    "    print(\"• Control PS :\", control_ps)\n",
    "    print(\"• Revert PS  :\", revert_ps)\n",
    "    print(\"\\nUsage (PowerShell):\")\n",
    "    print(f'  cd \"{os.path.dirname(ritual_ps)}\"')\n",
    "    print(f'  # Inspect label_proposals.csv, edit if needed, then:')\n",
    "    print(f'  .\\\\\"{os.path.basename(control_ps)}\"')\n",
    "    print(f'  .\\\\\"{os.path.basename(ritual_ps)}\"')\n",
    "    print(\"  # To undo: .\\\\revert_renames.ps1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63a9510c-dc2e-40e4-8d6a-3f1dc41c8f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_9964\\3782672060.py:88: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.sum(np.where(p>0, p*np.log(p), 0.0))\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_9964\\3782672060.py:88: RuntimeWarning: invalid value encountered in multiply\n",
      "  return -np.sum(np.where(p>0, p*np.log(p), 0.0))\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_9964\\3782672060.py:100: RuntimeWarning: divide by zero encountered in log\n",
      "  p = h/np.sum(h+1e-12); out.append(-np.sum(np.where(p>0, p*np.log(p), 0.0)))\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_9964\\3782672060.py:100: RuntimeWarning: invalid value encountered in multiply\n",
      "  p = h/np.sum(h+1e-12); out.append(-np.sum(np.where(p>0, p*np.log(p), 0.0)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Positive Geometry Drift Funnel ==\n",
      "{\n",
      "  \"ising\": {\n",
      "    \"mean_metric_glyph\": 0.03708043981481481,\n",
      "    \"mean_metric_random\": 0.03854709201388889,\n",
      "    \"boot_H_glyph_mean\": 0.46924211809475763,\n",
      "    \"boot_H_random_mean\": 0.4750910479540399,\n",
      "    \"boot_H_diff_obs\": -0.005848929859282259,\n",
      "    \"perm_on_metric_mean\": -0.0014666521990740755,\n",
      "    \"perm_p_metric_mean\": 0.7846\n",
      "  },\n",
      "  \"kuramoto\": {\n",
      "    \"mean_metric_glyph\": 0.0282777843236119,\n",
      "    \"mean_metric_random\": 0.025481906956643284,\n",
      "    \"boot_H_glyph_mean\": 9.998668559769164e-13,\n",
      "    \"boot_H_random_mean\": 0.07177082817576377,\n",
      "    \"boot_H_diff_obs\": -0.0717708281747639,\n",
      "    \"perm_on_metric_mean\": 0.0027958773669686174,\n",
      "    \"perm_p_metric_mean\": 0.2598\n",
      "  }\n",
      "}\n",
      "[Artifacts] C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\pg_funnel_20251025-031138_6e0ce350\n"
     ]
    }
   ],
   "source": [
    "# CNT_Lab — Positive Geometry Drift Funnel (one cell)\n",
    "# Compares outcome entropy between glyph-hash seeds vs random seeds for Ising & Kuramoto.\n",
    "\n",
    "import numpy as np, pandas as pd, hashlib, json, uuid, os\n",
    "from datetime import datetime\n",
    "\n",
    "OUTDIR = os.path.join(r\"C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\",\n",
    "                      f\"pg_funnel_{datetime.now().strftime('%Y%m%d-%H%M%S')}_{uuid.uuid4().hex[:8]}\")\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# --- helpers ---\n",
    "def hseed(s):\n",
    "    return int(hashlib.sha256(s.encode()).hexdigest()[:12],16) % (2**32)\n",
    "\n",
    "def perm_p(a,b, reps=20000, metric=np.mean, seed=42):\n",
    "    rng = np.random.default_rng(seed); a=np.asarray(a); b=np.asarray(b)\n",
    "    obs = metric(a)-metric(b); joined=np.concatenate([a,b]); na=len(a)\n",
    "    cnt=0\n",
    "    for _ in range(reps):\n",
    "        rng.shuffle(joined)\n",
    "        cnt += abs(metric(joined[:na])-metric(joined[na:])) >= abs(obs)\n",
    "    return float(obs), float(cnt/reps)\n",
    "\n",
    "# --- systems ---\n",
    "def ising_run(L=48, beta=0.45, steps=4000, rng=None):\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    # 2D Ising (periodic), Glauber dynamics\n",
    "    s = rng.choice([-1,1], size=(L,L))\n",
    "    def Eflip(i,j):\n",
    "        nb = s[(i+1)%L,j]+s[(i-1)%L,j]+s[i,(j+1)%L]+s[i,(j-1)%L]\n",
    "        return 2*s[i,j]*nb\n",
    "    for t in range(steps):\n",
    "        i = rng.integers(0,L); j = rng.integers(0,L)\n",
    "        dE = Eflip(i,j)\n",
    "        if dE<=0 or rng.random()<np.exp(-beta*dE):\n",
    "            s[i,j] *= -1\n",
    "    m = np.mean(s)\n",
    "    # outcome features: |m|, cluster proxy via pair correlation along a row\n",
    "    row = s[0]; corr = np.mean(row[:-1]*row[1:])\n",
    "    return float(abs(m)), float(corr)\n",
    "\n",
    "def kura_run(N=400, K=1.5, steps=4000, dt=0.02, rng=None):\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    theta = rng.uniform(0, 2*np.pi, size=N)\n",
    "    omega = rng.normal(0, 1, size=N)\n",
    "    for t in range(steps):\n",
    "        s = np.sin(theta[:,None]-theta[None,:])\n",
    "        dtheta = omega + (K/N)*np.sum(s, axis=1)\n",
    "        theta = (theta + dt*dtheta) % (2*np.pi)\n",
    "    R = np.abs(np.mean(np.exp(1j*theta)))\n",
    "    return float(R)\n",
    "\n",
    "# --- experiment ---\n",
    "GLYPHS = [\n",
    "    \"Observer Ring\", \"Axis Veil\", \"Event Spiral\", \"Anchor\", \"Collapse Benediction\",\n",
    "    \"Resonance Seeding\", \"Mirror Rebinding\", \"Parasite Purge\", \"Harmonic Shadow\", \"Orivyn\"\n",
    "]\n",
    "N_PER = 64  # runs per condition per system\n",
    "rng = np.random.default_rng(7)\n",
    "\n",
    "def run_block(label, seeds):\n",
    "    rows=[]\n",
    "    for sd in seeds:\n",
    "        r_i = np.random.default_rng(sd)\n",
    "        m, corr = ising_run(rng=r_i)\n",
    "        rows.append(dict(system=\"ising\", label=label, seed=int(sd), metric1=m, metric2=corr))\n",
    "        r_k = np.random.default_rng(sd ^ 0x9E3779B9)\n",
    "        R = kura_run(rng=r_k)\n",
    "        rows.append(dict(system=\"kuramoto\", label=label, seed=int(sd), metric1=R, metric2=np.nan))\n",
    "    return rows\n",
    "\n",
    "glyph_seeds = [hseed(g) ^ i for g in GLYPHS for i in range(N_PER//len(GLYPHS))]\n",
    "rand_seeds  = list(rng.integers(0, 2**32-1, size=N_PER))\n",
    "\n",
    "rows = []\n",
    "rows += run_block(\"glyph\", glyph_seeds)\n",
    "rows += run_block(\"random\", rand_seeds)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# define an \"outcome entropy\" proxy per system:\n",
    "#  - Ising: entropy of binned |m| (magnetization magnitude)\n",
    "#  - Kuramoto: entropy of binned R (order parameter)\n",
    "def entropy_of(col):\n",
    "    x = df[df.system==col][\"metric1\"]\n",
    "    hist, _ = np.histogram(x, bins=16, range=(0,1), density=True)\n",
    "    p = hist / np.sum(hist + 1e-12)\n",
    "    return -np.sum(np.where(p>0, p*np.log(p), 0.0))\n",
    "\n",
    "H_g_ising  = entropy_of(\"ising\")   # overall\n",
    "H_g_kura   = entropy_of(\"kuramoto\")\n",
    "\n",
    "# compare entropies across conditions by bootstrap difference of means on per-run uncertainty:\n",
    "def boot_H(x, B=2000, seed=123):\n",
    "    rng = np.random.default_rng(seed); x=np.asarray(x)\n",
    "    out=[]\n",
    "    for _ in range(B):\n",
    "        xb = rng.choice(x, size=len(x), replace=True)\n",
    "        h, _ = np.histogram(xb, bins=16, range=(0,1), density=True)\n",
    "        p = h/np.sum(h+1e-12); out.append(-np.sum(np.where(p>0, p*np.log(p), 0.0)))\n",
    "    return np.array(out)\n",
    "\n",
    "res = {}\n",
    "for sysname in [\"ising\",\"kuramoto\"]:\n",
    "    g = df[(df.system==sysname)&(df.label==\"glyph\")][\"metric1\"].values\n",
    "    r = df[(df.system==sysname)&(df.label==\"random\")][\"metric1\"].values\n",
    "    Hg = boot_H(g); Hr = boot_H(r)\n",
    "    obs = Hg.mean()-Hr.mean()   # negative means glyph narrows entropy (good)\n",
    "    # permutation on raw samples with entropy recomputation is heavy;\n",
    "    # approximate with two-sample perm on metric1 means as a proxy:\n",
    "    obs_m, p_m = perm_p(g, r, reps=20000, seed=5)\n",
    "    res[sysname] = dict(\n",
    "        mean_metric_glyph=float(np.mean(g)),\n",
    "        mean_metric_random=float(np.mean(r)),\n",
    "        boot_H_glyph_mean=float(Hg.mean()),\n",
    "        boot_H_random_mean=float(Hr.mean()),\n",
    "        boot_H_diff_obs=float(obs),\n",
    "        perm_on_metric_mean=float(obs_m),\n",
    "        perm_p_metric_mean=float(p_m)\n",
    "    )\n",
    "\n",
    "with open(os.path.join(OUTDIR,\"pg_funnel_results.json\"),\"w\") as f:\n",
    "    json.dump(res, f, indent=2)\n",
    "\n",
    "print(\"== Positive Geometry Drift Funnel ==\")\n",
    "print(json.dumps(res, indent=2))\n",
    "print(f\"[Artifacts] {OUTDIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d535c91c-67d9-41c6-aa6d-b22378d19fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Positive Geometry Drift Funnel — v2 (stabilized) ==\n",
      "{\n",
      "  \"ising\": {\n",
      "    \"n_per\": 64,\n",
      "    \"mean_metric_glyph\": 0.03708043981481481,\n",
      "    \"mean_metric_random\": 0.03854709201388889,\n",
      "    \"H_glyph\": 1.0374105318412368,\n",
      "    \"H_random\": 1.017467935768744,\n",
      "    \"H_diff\": 0.019942596072492824,\n",
      "    \"perm_p_entropy\": 1.0,\n",
      "    \"perm_on_metric_mean\": -0.0014666521990740755,\n",
      "    \"perm_p_metric_mean\": 0.7846\n",
      "  },\n",
      "  \"kuramoto\": {\n",
      "    \"n_per\": 64,\n",
      "    \"mean_metric_glyph\": 0.0282777843236119,\n",
      "    \"mean_metric_random\": 0.025481906956643284,\n",
      "    \"H_glyph\": 0.6458118267860103,\n",
      "    \"H_random\": 0.6746216548104075,\n",
      "    \"H_diff\": -0.028809828024397133,\n",
      "    \"perm_p_entropy\": 1.0,\n",
      "    \"perm_on_metric_mean\": 0.0027958773669686174,\n",
      "    \"perm_p_metric_mean\": 0.2598\n",
      "  }\n",
      "}\n",
      "[Artifacts] C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\pg_funnel_v2_20251025-085404_5fdd7947\n",
      "Pass heuristic: any system with H_diff < 0 AND perm_p_entropy < 0.05 (strong), or p<0.05 on mean metric with the expected sign.\n"
     ]
    }
   ],
   "source": [
    "# PG Funnel v2 — stabilized entropy (Dirichlet α=0.5) + permutation p on entropy\n",
    "import numpy as np, pandas as pd, hashlib, json, uuid, os, warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# === knobs (same defaults you used) ===\n",
    "N_PER      = 64\n",
    "ISING_L    = 48\n",
    "ISING_STEPS= 4000\n",
    "KURA_N     = 400\n",
    "KURA_STEPS = 4000\n",
    "BOOT_B     = 2000\n",
    "BINS       = 16\n",
    "ALPHA      = 0.5      # Dirichlet add-α smoothing for histogram entropy\n",
    "K_KURAMOTO = 1.5      # try 2.0–2.5 for stronger synchrony if needed\n",
    "\n",
    "OUTDIR = os.path.join(r\"C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\",\n",
    "                      f\"pg_funnel_v2_{datetime.now().strftime('%Y%m%d-%H%M%S')}_{uuid.uuid4().hex[:8]}\")\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "def hseed(s):\n",
    "    return int(hashlib.sha256(s.encode()).hexdigest()[:12],16) % (2**32)\n",
    "\n",
    "def entropy_smoothed(x, bins=16, rng=(0,1), alpha=0.5):\n",
    "    counts, _ = np.histogram(x, bins=bins, range=rng, density=False)\n",
    "    counts = counts.astype(float) + alpha\n",
    "    p = counts / counts.sum()\n",
    "    # safe Shannon entropy\n",
    "    return float(-np.sum(p * np.log(p)))\n",
    "\n",
    "def perm_p_diff_stat(a, b, stat_fn, reps=20000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    a = np.asarray(a); b = np.asarray(b)\n",
    "    obs = stat_fn(a) - stat_fn(b)\n",
    "    joined = np.concatenate([a,b])\n",
    "    na = len(a)\n",
    "    cnt=0\n",
    "    for _ in range(reps):\n",
    "        rng.shuffle(joined)\n",
    "        cnt += abs(stat_fn(joined[:na]) - stat_fn(joined[na:])) >= abs(obs)\n",
    "    return float(obs), float(cnt/reps)\n",
    "\n",
    "# --- systems (same as before) ---\n",
    "def ising_run(L=48, beta=0.45, steps=4000, rng=None):\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    s = rng.choice([-1,1], size=(L,L))\n",
    "    def Eflip(i,j):\n",
    "        nb = s[(i+1)%L,j]+s[(i-1)%L,j]+s[i,(j+1)%L]+s[i,(j-1)%L]\n",
    "        return 2*s[i,j]*nb\n",
    "    for _ in range(steps):\n",
    "        i = rng.integers(0,L); j = rng.integers(0,L)\n",
    "        dE = Eflip(i,j)\n",
    "        if dE<=0 or rng.random()<np.exp(-beta*dE):\n",
    "            s[i,j] *= -1\n",
    "    m = np.mean(s)\n",
    "    row = s[0]; corr = np.mean(row[:-1]*row[1:])\n",
    "    return float(abs(m)), float(corr)\n",
    "\n",
    "def kura_run(N=400, K=1.5, steps=4000, dt=0.02, rng=None):\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    theta = rng.uniform(0, 2*np.pi, size=N)\n",
    "    omega = rng.normal(0, 1, size=N)\n",
    "    for _ in range(steps):\n",
    "        s = np.sin(theta[:,None]-theta[None,:])  # O(N^2)\n",
    "        dtheta = omega + (K/N)*np.sum(s, axis=1)\n",
    "        theta = (theta + dt*dtheta) % (2*np.pi)\n",
    "    R = np.abs(np.mean(np.exp(1j*theta)))\n",
    "    return float(R)\n",
    "\n",
    "# --- experiment ---\n",
    "GLYPHS = [\n",
    "    \"Observer Ring\", \"Axis Veil\", \"Event Spiral\", \"Anchor\", \"Collapse Benediction\",\n",
    "    \"Resonance Seeding\", \"Mirror Rebinding\", \"Parasite Purge\", \"Harmonic Shadow\", \"Orivyn\"\n",
    "]\n",
    "rng = np.random.default_rng(7)\n",
    "glyph_seeds = [hseed(g) ^ i for g in GLYPHS for i in range(N_PER//len(GLYPHS))]\n",
    "rand_seeds  = list(rng.integers(0, 2**32-1, size=N_PER))\n",
    "\n",
    "def run_block(label, seeds):\n",
    "    rows=[]\n",
    "    for sd in seeds:\n",
    "        r_i = np.random.default_rng(sd)\n",
    "        m, corr = ising_run(L=ISING_L, steps=ISING_STEPS, rng=r_i)\n",
    "        rows.append(dict(system=\"ising\",    label=label, seed=int(sd), metric=m,  aux=corr))\n",
    "        r_k = np.random.default_rng(sd ^ 0x9E3779B9)\n",
    "        R = kura_run(N=KURA_N, K=K_KURAMOTO, steps=KURA_STEPS, rng=r_k)\n",
    "        rows.append(dict(system=\"kuramoto\", label=label, seed=int(sd), metric=R,  aux=np.nan))\n",
    "    return rows\n",
    "\n",
    "rows = []\n",
    "rows += run_block(\"glyph\", glyph_seeds)\n",
    "rows += run_block(\"random\", rand_seeds)\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "results = {}\n",
    "for sysname in [\"ising\",\"kuramoto\"]:\n",
    "    g = df[(df.system==sysname)&(df.label==\"glyph\")][\"metric\"].values\n",
    "    r = df[(df.system==sysname)&(df.label==\"random\")][\"metric\"].values\n",
    "\n",
    "    # stabilized entropy of metric distributions\n",
    "    Hg = entropy_smoothed(g, bins=BINS, rng=(0,1), alpha=ALPHA)\n",
    "    Hr = entropy_smoothed(r, bins=BINS, rng=(0,1), alpha=ALPHA)\n",
    "    Hdiff, p_H = perm_p_diff_stat(g, r,\n",
    "                                  stat_fn=lambda x: entropy_smoothed(x, bins=BINS, rng=(0,1), alpha=ALPHA),\n",
    "                                  reps=20000, seed=11)\n",
    "\n",
    "    # mean metric proxy (as before)\n",
    "    def perm_p_mean(a,b,reps=20000,seed=5):\n",
    "        rr = np.random.default_rng(seed)\n",
    "        a = np.asarray(a); b = np.asarray(b)\n",
    "        obs = float(np.mean(a) - np.mean(b))\n",
    "        join = np.concatenate([a,b]); na=len(a)\n",
    "        cnt=0\n",
    "        for _ in range(reps):\n",
    "            rr.shuffle(join)\n",
    "            cnt += abs(np.mean(join[:na]) - np.mean(join[na:])) >= abs(obs)\n",
    "        return obs, float(cnt/reps)\n",
    "\n",
    "    obs_m, p_m = perm_p_mean(g, r, reps=20000, seed=5)\n",
    "\n",
    "    results[sysname] = dict(\n",
    "        n_per=int(N_PER),\n",
    "        mean_metric_glyph=float(np.mean(g)),\n",
    "        mean_metric_random=float(np.mean(r)),\n",
    "        H_glyph=float(Hg),\n",
    "        H_random=float(Hr),\n",
    "        H_diff=float(Hg - Hr),           # negative = glyph is narrower (desired)\n",
    "        perm_p_entropy=float(p_H),\n",
    "        perm_on_metric_mean=float(obs_m),\n",
    "        perm_p_metric_mean=float(p_m)\n",
    "    )\n",
    "\n",
    "with open(os.path.join(OUTDIR,\"pg_funnel_v2_results.json\"),\"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"== Positive Geometry Drift Funnel — v2 (stabilized) ==\")\n",
    "print(json.dumps(results, indent=2))\n",
    "print(f\"[Artifacts] {OUTDIR}\")\n",
    "print(\"Pass heuristic: any system with H_diff < 0 AND perm_p_entropy < 0.05 (strong), \"\n",
    "      \"or p<0.05 on mean metric with the expected sign.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31875a19-62cd-4de0-bf95-b50d2f561693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Positive Geometry Drift Funnel — v3 (structured vs random) ==\n",
      "{\n",
      "  \"ising\": {\n",
      "    \"n_per\": 128,\n",
      "    \"H_glyph\": 1.0084544955621828,\n",
      "    \"H_random\": 1.0288487154061623,\n",
      "    \"H_diff\": -0.02039421984397949,\n",
      "    \"perm_p_entropy\": 0.8734,\n",
      "    \"mean_metric_glyph\": 0.025032552083333333,\n",
      "    \"mean_metric_random\": 0.026782989501953125,\n",
      "    \"perm_on_metric_mean\": -0.0017504374186197921,\n",
      "    \"perm_p_metric_mean\": 0.4875\n",
      "  },\n",
      "  \"kuramoto\": {\n",
      "    \"n_per\": 128,\n",
      "    \"H_glyph\": 2.3015533524518617,\n",
      "    \"H_random\": 2.369526144877767,\n",
      "    \"H_diff\": -0.06797279242590548,\n",
      "    \"perm_p_entropy\": 0.3487,\n",
      "    \"mean_metric_glyph\": 0.7805737940162142,\n",
      "    \"mean_metric_random\": 0.7750181602860213,\n",
      "    \"perm_on_metric_mean\": 0.005555633730192855,\n",
      "    \"perm_p_metric_mean\": 0.6714\n",
      "  }\n",
      "}\n",
      "[Artifacts] C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\pg_funnel_v3_20251025-092852_30da8ef0\n",
      "Pass = (H_diff < 0 and perm_p_entropy < 0.05) for either system; bonus if mean metric also improves with p<0.05.\n"
     ]
    }
   ],
   "source": [
    "# PG Funnel v3 — Structured (Halton) vs Random seeding near criticality\n",
    "import numpy as np, pandas as pd, hashlib, json, uuid, os, warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# ===== Knobs =====\n",
    "N_PER       = 128           # power up (try 256 for paper)\n",
    "BINS        = 24            # finer, but still stable\n",
    "ALPHA       = 0.5           # Dirichlet smoothing for entropy\n",
    "BOOT_B      = 2000          # CI smoothness (optional)\n",
    "ISING_L     = 64\n",
    "ISING_STEPS = 6000\n",
    "ISING_BETA  = 0.4407        # near 2D critical β\n",
    "KURA_N      = 600\n",
    "KURA_STEPS  = 6000\n",
    "KURA_K      = 2.2           # mildly supercritical to show separation\n",
    "DT          = 0.02\n",
    "\n",
    "OUTDIR = os.path.join(r\"C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\",\n",
    "                      f\"pg_funnel_v3_{datetime.now().strftime('%Y%m%d-%H%M%S')}_{uuid.uuid4().hex[:8]}\")\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# ===== Low-discrepancy (Halton) generator =====\n",
    "def halton(n, base):\n",
    "    seq = []\n",
    "    for i in range(1, n+1):\n",
    "        f, r, x = 1.0, 0.0, i\n",
    "        while x > 0:\n",
    "            f /= base\n",
    "            r += f * (x % base)\n",
    "            x //= base\n",
    "        seq.append(r)\n",
    "    return np.array(seq)\n",
    "\n",
    "def halton2(n, b1=2, b2=3):\n",
    "    return np.stack([halton(n,b1), halton(n,b2)], axis=1)\n",
    "\n",
    "# ===== Entropy helpers =====\n",
    "def entropy_smoothed(x, bins=16, rng=(0,1), alpha=0.5):\n",
    "    counts, _ = np.histogram(x, bins=bins, range=rng, density=False)\n",
    "    counts = counts.astype(float) + alpha\n",
    "    p = counts / counts.sum()\n",
    "    return float(-np.sum(p*np.log(p)))\n",
    "\n",
    "def perm_p_diff_stat(a, b, stat_fn, reps=20000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    a = np.asarray(a); b = np.asarray(b)\n",
    "    obs = stat_fn(a) - stat_fn(b)\n",
    "    joined = np.concatenate([a,b]); na = len(a)\n",
    "    cnt=0\n",
    "    for _ in range(reps):\n",
    "        rng.shuffle(joined)\n",
    "        cnt += abs(stat_fn(joined[:na]) - stat_fn(joined[na:])) >= abs(obs)\n",
    "    return float(obs), float(cnt/reps)\n",
    "\n",
    "# ===== Systems =====\n",
    "def ising_run(L=ISING_L, beta=ISING_BETA, steps=ISING_STEPS, rng=None):\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    s = rng.choice([-1,1], size=(L,L))\n",
    "    def Eflip(i,j):\n",
    "        nb = s[(i+1)%L,j]+s[(i-1)%L,j]+s[i,(j+1)%L]+s[i,(j-1)%L]\n",
    "        return 2*s[i,j]*nb\n",
    "    for _ in range(steps):\n",
    "        i = rng.integers(0,L); j = rng.integers(0,L)\n",
    "        dE = Eflip(i,j)\n",
    "        if dE<=0 or rng.random() < np.exp(-beta*dE):\n",
    "            s[i,j] *= -1\n",
    "    m = np.mean(s)                   # order (magnetization)\n",
    "    row = s[0]; corr = np.mean(row[:-1]*row[1:])  # simple corr proxy\n",
    "    return float(abs(m)), float(corr)\n",
    "\n",
    "def kuramoto_meanfield(N=KURA_N, K=KURA_K, steps=KURA_STEPS, dt=DT, rng=None, phase_offset=0.0, freq_scale=1.0):\n",
    "    if rng is None: rng = np.random.default_rng()\n",
    "    theta = (rng.uniform(0, 2*np.pi, size=N) + phase_offset) % (2*np.pi)\n",
    "    omega = rng.normal(0, 1, size=N) * freq_scale\n",
    "    for _ in range(steps):\n",
    "        z = np.exp(1j*theta)\n",
    "        R = np.abs(z.mean()); psi = np.angle(z.mean())\n",
    "        dtheta = omega + K*R*np.sin(psi - theta)\n",
    "        theta = (theta + dt*dtheta) % (2*np.pi)\n",
    "    Rfinal = np.abs(np.mean(np.exp(1j*theta)))\n",
    "    return float(Rfinal)\n",
    "\n",
    "# ===== Seeds: structured vs random =====\n",
    "# Map glyphs to *structured coverage* of (phase_offset, freq_scale) via Halton;\n",
    "# random uses i.i.d. coverage of the same rectangle.\n",
    "GLYPHS = [\"Observer Ring\",\"Axis Veil\",\"Event Spiral\",\"Anchor\",\"Collapse Benediction\",\n",
    "          \"Resonance Seeding\",\"Mirror Rebinding\",\"Parasite Purge\",\"Harmonic Shadow\",\"Orivyn\"]\n",
    "\n",
    "n_each = N_PER // len(GLYPHS)\n",
    "H = halton2(n_each*len(GLYPHS))   # shape (N_PER,2) in [0,1]^2\n",
    "# scale Halton to parameter box\n",
    "phase_min, phase_max = 0.0, 2*np.pi\n",
    "freq_min,  freq_max  = 0.8, 1.2\n",
    "H_phase = phase_min + (phase_max - phase_min)*H[:,0]\n",
    "H_freq  = freq_min  + (freq_max  - freq_min )*H[:,1]\n",
    "\n",
    "rows = []\n",
    "# Structured (glyph): cover param box evenly\n",
    "idx = 0\n",
    "for g in GLYPHS:\n",
    "    for _ in range(n_each):\n",
    "        r_i = np.random.default_rng( (idx<<16) ^ 0xA53A9B )  # stable per index\n",
    "        m,c = ising_run(rng=r_i)\n",
    "        rows.append(dict(system=\"ising\", label=\"glyph\", metric=m, aux=c))\n",
    "        # same index drives mean-field offsets\n",
    "        ph, fs = float(H_phase[idx]), float(H_freq[idx])\n",
    "        r_k = np.random.default_rng( (idx<<16) ^ 0x9E3779B9 )\n",
    "        R = kuramoto_meanfield(phase_offset=ph, freq_scale=fs, rng=r_k)\n",
    "        rows.append(dict(system=\"kuramoto\", label=\"glyph\", metric=R, aux=np.nan))\n",
    "        idx += 1\n",
    "\n",
    "# Random: i.i.d. draw over same parameter box\n",
    "rng = np.random.default_rng(7)\n",
    "for i in range(N_PER):\n",
    "    r_i = np.random.default_rng(rng.integers(0, 2**32-1))\n",
    "    m,c = ising_run(rng=r_i)\n",
    "    rows.append(dict(system=\"ising\", label=\"random\", metric=m, aux=c))\n",
    "    ph = rng.uniform(phase_min, phase_max)\n",
    "    fs = rng.uniform(freq_min, freq_max)\n",
    "    r_k = np.random.default_rng(rng.integers(0, 2**32-1))\n",
    "    R = kuramoto_meanfield(phase_offset=ph, freq_scale=fs, rng=r_k)\n",
    "    rows.append(dict(system=\"kuramoto\", label=\"random\", metric=R, aux=np.nan))\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# ===== Analysis =====\n",
    "def analyze(system):\n",
    "    g = df[(df.system==system)&(df.label==\"glyph\")][\"metric\"].values\n",
    "    r = df[(df.system==system)&(df.label==\"random\")][\"metric\"].values\n",
    "    H_g = entropy_smoothed(g, bins=BINS, rng=(0,1), alpha=ALPHA)\n",
    "    H_r = entropy_smoothed(r, bins=BINS, rng=(0,1), alpha=ALPHA)\n",
    "    H_diff, p_H = perm_p_diff_stat(g, r,\n",
    "        stat_fn=lambda x: entropy_smoothed(x, bins=BINS, rng=(0,1), alpha=ALPHA),\n",
    "        reps=20000, seed=11)\n",
    "    # mean metric corroboration\n",
    "    def perm_p_mean(a,b,reps=20000,seed=5):\n",
    "        rr = np.random.default_rng(seed)\n",
    "        a = np.asarray(a); b = np.asarray(b)\n",
    "        obs = float(np.mean(a)-np.mean(b))\n",
    "        join = np.concatenate([a,b]); na=len(a)\n",
    "        cnt=0\n",
    "        for _ in range(reps):\n",
    "            rr.shuffle(join)\n",
    "            cnt += abs(np.mean(join[:na]) - np.mean(join[na:])) >= abs(obs)\n",
    "        return obs, float(cnt/reps)\n",
    "    obs_m, p_m = perm_p_mean(g, r)\n",
    "    return dict(\n",
    "        n_per=int(N_PER),\n",
    "        H_glyph=float(H_g), H_random=float(H_r),\n",
    "        H_diff=float(H_g - H_r), perm_p_entropy=float(p_H),\n",
    "        mean_metric_glyph=float(np.mean(g)), mean_metric_random=float(np.mean(r)),\n",
    "        perm_on_metric_mean=float(obs_m), perm_p_metric_mean=float(p_m)\n",
    "    )\n",
    "\n",
    "res = {\"ising\": analyze(\"ising\"), \"kuramoto\": analyze(\"kuramoto\")}\n",
    "with open(os.path.join(OUTDIR,\"pg_funnel_v3_results.json\"),\"w\") as f:\n",
    "    json.dump(res, f, indent=2)\n",
    "\n",
    "print(\"== Positive Geometry Drift Funnel — v3 (structured vs random) ==\")\n",
    "print(json.dumps(res, indent=2))\n",
    "print(f\"[Artifacts] {OUTDIR}\")\n",
    "print(\"Pass = (H_diff < 0 and perm_p_entropy < 0.05) for either system; \"\n",
    "      \"bonus if mean metric also improves with p<0.05.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a61e79e6-c715-43da-8be9-f765dab8b6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== PG Funnel v3.1 — Critical Sweep ==\n",
      "{\n",
      "  \"best_ising\": {\n",
      "    \"system\": \"ising\",\n",
      "    \"beta\": 0.4457,\n",
      "    \"K\": NaN,\n",
      "    \"H_glyph\": 1.3148097888030201,\n",
      "    \"H_random\": 1.4612682117859375,\n",
      "    \"H_diff\": -0.14645842298291734,\n",
      "    \"p_entropy\": 0.16955,\n",
      "    \"mean_g\": 0.0278167724609375,\n",
      "    \"mean_r\": 0.03443145751953125\n",
      "  },\n",
      "  \"best_kuramoto\": {\n",
      "    \"system\": \"kuramoto\",\n",
      "    \"beta\": NaN,\n",
      "    \"K\": 2.5,\n",
      "    \"H_glyph\": 2.138611121904332,\n",
      "    \"H_random\": 2.215222460483547,\n",
      "    \"H_diff\": -0.0766113385792151,\n",
      "    \"p_entropy\": 0.2803,\n",
      "    \"mean_g\": 0.8655586270847131,\n",
      "    \"mean_r\": 0.8553475021852082\n",
      "  },\n",
      "  \"csv\": \"C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\notebooks\\\\archive\\\\pg_funnel_v3_1sweep_20251025-134207_9680eb61\\\\pg_funnel_v3_1_sweep_results.csv\",\n",
      "  \"artifacts\": \"C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\notebooks\\\\archive\\\\pg_funnel_v3_1sweep_20251025-134207_9680eb61\",\n",
      "  \"pass_rule\": \"Target a candidate with H_diff < 0 and p_entropy < 0.10 in the sweep. Then rerun ONLY that setting with N_PER=256\\u2013512 to drive p<0.05.\"\n",
      "}\n",
      "[Artifacts] C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\pg_funnel_v3_1sweep_20251025-134207_9680eb61\n"
     ]
    }
   ],
   "source": [
    "# PG Funnel v3.1 — Critical Sweep for Maximum Funnel (structured vs random)\n",
    "# Scans small grids around Ising β_c and Kuramoto K_c, finds (H_diff, p) sweet spots.\n",
    "# Outputs a CSV and highlights the best candidates to rerun with higher power.\n",
    "\n",
    "import numpy as np, pandas as pd, hashlib, json, uuid, os, warnings, math\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# ====== RUNTIME / POWER KNOBS ======\n",
    "N_PER_BASE      = 64      # trials per condition per system per setting (raise to 128/256 when you pick a winner)\n",
    "BINS            = 24      # histogram bins for entropy\n",
    "ALPHA           = 0.5     # Dirichlet smoothing for entropy\n",
    "PERM_REPS       = 20000   # permutation reps for entropy test\n",
    "SEED_MASTER     = 11\n",
    "\n",
    "# Ising (2D) sweep around β_c ~ 0.4406868...\n",
    "ISING_L         = 64\n",
    "ISING_STEPS     = 6000\n",
    "BETA_CENTER     = 0.4407\n",
    "BETA_OFFSETS    = [-0.010, -0.005, -0.0025, 0.0, 0.0025, 0.005, 0.010]\n",
    "\n",
    "# Kuramoto sweep near critical; mean-field is faster and clean\n",
    "KURA_N          = 600\n",
    "KURA_STEPS      = 6000\n",
    "DT              = 0.02\n",
    "K_CENTER        = 2.2        # mild supercritical (adjust around your K_c estimate if needed)\n",
    "K_OFFSETS       = [-0.30, -0.15, -0.08, 0.0, 0.08, 0.15, 0.30]\n",
    "\n",
    "# Parameter boxes the structured seeds evenly cover (phase & frequency)\n",
    "PHASE_MIN, PHASE_MAX = 0.0, 2*np.pi\n",
    "FREQ_MIN,  FREQ_MAX  = 0.8, 1.2\n",
    "\n",
    "# ====== OUTPUT ======\n",
    "OUTDIR = os.path.join(r\"C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\",\n",
    "                      f\"pg_funnel_v3_1sweep_{datetime.now().strftime('%Y%m%d-%H%M%S')}_{uuid.uuid4().hex[:8]}\")\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# ====== UTIL ======\n",
    "def entropy_smoothed(x, bins=16, rng=(0,1), alpha=0.5):\n",
    "    counts, _ = np.histogram(x, bins=bins, range=rng, density=False)\n",
    "    counts = counts.astype(float) + alpha\n",
    "    p = counts / counts.sum()\n",
    "    return float(-np.sum(p*np.log(p)))\n",
    "\n",
    "def perm_p_diff_stat(a, b, stat_fn, reps=20000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    a = np.asarray(a); b = np.asarray(b)\n",
    "    obs = stat_fn(a) - stat_fn(b)\n",
    "    joined = np.concatenate([a,b]); na = len(a)\n",
    "    cnt=0\n",
    "    for _ in range(reps):\n",
    "        rng.shuffle(joined)\n",
    "        cnt += abs(stat_fn(joined[:na]) - stat_fn(joined[na:])) >= abs(obs)\n",
    "    return float(obs), float(cnt/reps)\n",
    "\n",
    "def halton(n, base):\n",
    "    seq=[]\n",
    "    for i in range(1, n+1):\n",
    "        f=1.0; r=0.0; x=i\n",
    "        while x>0:\n",
    "            f/=base; r += f*(x%base); x//=base\n",
    "        seq.append(r)\n",
    "    return np.array(seq)\n",
    "\n",
    "def halton2(n, b1=2, b2=3):\n",
    "    return np.stack([halton(n,b1), halton(n,b2)], axis=1)\n",
    "\n",
    "# ====== SYSTEMS ======\n",
    "def ising_run(L, beta, steps, rng):\n",
    "    s = rng.choice([-1,1], size=(L,L))\n",
    "    def Eflip(i,j):\n",
    "        nb = s[(i+1)%L,j]+s[(i-1)%L,j]+s[i,(j+1)%L]+s[i,(j-1)%L]\n",
    "        return 2*s[i,j]*nb\n",
    "    for _ in range(steps):\n",
    "        i = rng.integers(0,L); j = rng.integers(0,L)\n",
    "        dE = Eflip(i,j)\n",
    "        if dE<=0 or rng.random()<np.exp(-beta*dE):\n",
    "            s[i,j] *= -1\n",
    "    m = float(abs(np.mean(s)))\n",
    "    return m\n",
    "\n",
    "def kuramoto_meanfield(N, K, steps, dt, rng, phase_offset=0.0, freq_scale=1.0):\n",
    "    theta = (rng.uniform(0, 2*np.pi, size=N) + phase_offset) % (2*np.pi)\n",
    "    omega = rng.normal(0, 1, size=N) * freq_scale\n",
    "    for _ in range(steps):\n",
    "        z = np.exp(1j*theta); R = np.abs(z.mean()); psi = np.angle(z.mean())\n",
    "        dtheta = omega + K*R*np.sin(psi - theta)\n",
    "        theta = (theta + dt*dtheta) % (2*np.pi)\n",
    "    return float(np.abs(np.mean(np.exp(1j*theta))))\n",
    "\n",
    "# ====== STRUCTURED vs RANDOM SEEDS (parameterized) ======\n",
    "def run_ising_block(beta, n_per, seed_base):\n",
    "    # structured: Halton drives RNG seeding uniformly; random: iid seeds\n",
    "    H = halton(n_per, base=2)  # 1D is enough to de-clump RNG states\n",
    "    rows=[]\n",
    "    for k in range(n_per):\n",
    "        s_struct = int((H[k]*2**32)) ^ (seed_base + k)\n",
    "        s_rand   = np.random.default_rng(seed_base+k).integers(0, 2**32-1)\n",
    "        # structured\n",
    "        rng = np.random.default_rng(s_struct)\n",
    "        rows.append((\"glyph\", ising_run(ISING_L, beta, ISING_STEPS, rng)))\n",
    "        # random\n",
    "        rng = np.random.default_rng(s_rand)\n",
    "        rows.append((\"random\", ising_run(ISING_L, beta, ISING_STEPS, rng)))\n",
    "    return rows\n",
    "\n",
    "def run_kura_block(K, n_per, seed_base):\n",
    "    H2 = halton2(n_per)  # 2D for (phase_offset, freq_scale)\n",
    "    rows=[]\n",
    "    for k in range(n_per):\n",
    "        ph = PHASE_MIN + (PHASE_MAX-PHASE_MIN)*H2[k,0]\n",
    "        fs = FREQ_MIN  + (FREQ_MAX -FREQ_MIN )*H2[k,1]\n",
    "        s_struct = (seed_base<<1) ^ k ^ 0xA53A9B\n",
    "        s_rand   = np.random.default_rng(seed_base+k).integers(0, 2**32-1)\n",
    "        # structured\n",
    "        rng = np.random.default_rng(s_struct)\n",
    "        rows.append((\"glyph\", kuramoto_meanfield(KURA_N, K, KURA_STEPS, DT, rng, phase_offset=ph, freq_scale=fs)))\n",
    "        # random\n",
    "        rng = np.random.default_rng(s_rand)\n",
    "        ph_r = np.random.default_rng(s_rand^0x9E3779B9).uniform(PHASE_MIN, PHASE_MAX)\n",
    "        fs_r = np.random.default_rng(s_rand^0x517cc1b7).uniform(FREQ_MIN , FREQ_MAX )\n",
    "        rows.append((\"random\", kuramoto_meanfield(KURA_N, K, KURA_STEPS, DT, rng, phase_offset=ph_r, freq_scale=fs_r)))\n",
    "    return rows\n",
    "\n",
    "# ====== SWEEP ======\n",
    "rng_master = np.random.default_rng(SEED_MASTER)\n",
    "\n",
    "records = []\n",
    "# Ising sweep\n",
    "for d in BETA_OFFSETS:\n",
    "    beta = BETA_CENTER + d\n",
    "    rows = run_ising_block(beta, N_PER_BASE, seed_base= rng_master.integers(0,2**32-1))\n",
    "    df = pd.DataFrame(rows, columns=[\"label\",\"metric\"])\n",
    "    g = df[df.label==\"glyph\"][\"metric\"].values\n",
    "    r = df[df.label==\"random\"][\"metric\"].values\n",
    "    H_g = entropy_smoothed(g, bins=BINS, rng=(0,1), alpha=ALPHA)\n",
    "    H_r = entropy_smoothed(r, bins=BINS, rng=(0,1), alpha=ALPHA)\n",
    "    Hdiff, pH = perm_p_diff_stat(g, r,\n",
    "        stat_fn=lambda x: entropy_smoothed(x, bins=BINS, rng=(0,1), alpha=ALPHA),\n",
    "        reps=PERM_REPS, seed=SEED_MASTER+3)\n",
    "    records.append(dict(system=\"ising\", beta=beta, K=np.nan,\n",
    "                        H_glyph=H_g, H_random=H_r, H_diff=H_g-H_r, p_entropy=pH,\n",
    "                        mean_g=float(np.mean(g)), mean_r=float(np.mean(r))))\n",
    "# Kuramoto sweep\n",
    "for d in K_OFFSETS:\n",
    "    K = K_CENTER + d\n",
    "    rows = run_kura_block(K, N_PER_BASE, seed_base= rng_master.integers(0,2**32-1))\n",
    "    df = pd.DataFrame(rows, columns=[\"label\",\"metric\"])\n",
    "    g = df[df.label==\"glyph\"][\"metric\"].values\n",
    "    r = df[df.label==\"random\"][\"metric\"].values\n",
    "    H_g = entropy_smoothed(g, bins=BINS, rng=(0,1), alpha=ALPHA)\n",
    "    H_r = entropy_smoothed(r, bins=BINS, rng=(0,1), alpha=ALPHA)\n",
    "    Hdiff, pH = perm_p_diff_stat(g, r,\n",
    "        stat_fn=lambda x: entropy_smoothed(x, bins=BINS, rng=(0,1), alpha=ALPHA),\n",
    "        reps=PERM_REPS, seed=SEED_MASTER+7)\n",
    "    records.append(dict(system=\"kuramoto\", beta=np.nan, K=K,\n",
    "                        H_glyph=H_g, H_random=H_r, H_diff=H_g-H_r, p_entropy=pH,\n",
    "                        mean_g=float(np.mean(g)), mean_r=float(np.mean(r))))\n",
    "\n",
    "res = pd.DataFrame.from_records(records)\n",
    "csv_path = os.path.join(OUTDIR, \"pg_funnel_v3_1_sweep_results.csv\")\n",
    "res.to_csv(csv_path, index=False)\n",
    "\n",
    "# pick “best” per system: most negative H_diff; break ties by p\n",
    "def pick_best(df):\n",
    "    df = df.sort_values([\"H_diff\",\"p_entropy\"])\n",
    "    return df.iloc[0].to_dict()\n",
    "\n",
    "best_ising    = pick_best(res[res.system==\"ising\"])\n",
    "best_kuramoto = pick_best(res[res.system==\"kuramoto\"])\n",
    "\n",
    "summary = {\n",
    "  \"best_ising\": best_ising,\n",
    "  \"best_kuramoto\": best_kuramoto,\n",
    "  \"csv\": csv_path,\n",
    "  \"artifacts\": OUTDIR,\n",
    "  \"pass_rule\": \"Target a candidate with H_diff < 0 and p_entropy < 0.10 in the sweep. Then rerun ONLY that setting with N_PER=256–512 to drive p<0.05.\"\n",
    "}\n",
    "\n",
    "with open(os.path.join(OUTDIR,\"pg_funnel_v3_1_summary.json\"),\"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"== PG Funnel v3.1 — Critical Sweep ==\")\n",
    "print(json.dumps(summary, indent=2))\n",
    "print(f\"[Artifacts] {OUTDIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf18baf-0374-4cd2-b822-2b9af90545d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
