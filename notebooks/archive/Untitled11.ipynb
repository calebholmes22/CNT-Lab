{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e0ab938-9be2-4bd0-9db1-62578903208a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 cooling logs\n",
      "Found 1 cosmology triage tables\n",
      "Found 2 EEG laterality tables\n",
      "Found 0 Kuramoto/Ising summaries\n",
      "Found 1 Gray-Scott edge images\n",
      "Scanned 50 text/log files\n",
      "Found 0 glyph label tables\n",
      "\n",
      "Saved:\n",
      " - ./cnt_correlates_report_20251015-163558.csv\n",
      " - ./cnt_correlates_report_20251015-163558.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_17252\\1189349130.py:319: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  rho, p = spearmanr(arr[:,0], arr[:,1], nan_policy=\"omit\")\n"
     ]
    }
   ],
   "source": [
    "# === CNT Correlates Audit — One-Cell Runner (paste me into Jupyter) ===\n",
    "# Scans your CNT_Lab for logs (cooling, EEG laterality, Kuramoto/Ising, cosmology triage),\n",
    "# tests 8 candidate correlates, and saves a compact report (CSV + TXT).\n",
    "#\n",
    "# Outputs:\n",
    "#   - ./cnt_correlates_report_YYYYMMDD-HHMMSS.csv\n",
    "#   - ./cnt_correlates_report_YYYYMMDD-HHMMSS.txt\n",
    "#\n",
    "# Tips:\n",
    "#   - Adjust ROOT_DIRS below if your folders differ.\n",
    "#   - The cell is robust to missing data; it marks unavailable checks as \"grey\".\n",
    "#   - No internet needed. Uses numpy/pandas/(scipy if present).\n",
    "import os, re, glob, json, math, datetime as dt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "TS = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Try SciPy; if absent, we fall back to a rank-corr helper.\n",
    "try:\n",
    "    from scipy.stats import spearmanr, linregress\n",
    "    HAVE_SCIPY = True\n",
    "except Exception:\n",
    "    HAVE_SCIPY = False\n",
    "    def _rankdata(a):\n",
    "        # simple average-rank\n",
    "        a = np.asarray(a, float)\n",
    "        n = a.size\n",
    "        order = a.argsort()\n",
    "        ranks = np.empty(n, float)\n",
    "        ranks[order] = np.arange(1, n+1)\n",
    "        # tie handling (average)\n",
    "        _, inv, counts = np.unique(a, return_inverse=True, return_counts=True)\n",
    "        sums = np.bincount(inv, ranks)\n",
    "        avg = sums[counts.cumsum()-counts]/counts\n",
    "        return avg[inv]\n",
    "    def spearmanr(x, y, nan_policy=\"omit\"):\n",
    "        x = np.asarray(x, float); y = np.asarray(y, float)\n",
    "        mask = np.isfinite(x) & np.isfinite(y)\n",
    "        if mask.sum() < 3: return np.nan, np.nan\n",
    "        xr = _rankdata(x[mask]); yr = _rankdata(y[mask])\n",
    "        r = np.corrcoef(xr, yr)[0,1]\n",
    "        return float(r), np.nan\n",
    "    def linregress(x, y):\n",
    "        x = np.asarray(x, float); y = np.asarray(y, float)\n",
    "        mask = np.isfinite(x) & np.isfinite(y)\n",
    "        if mask.sum() < 3:\n",
    "            return type(\"LR\", (), dict(slope=np.nan, intercept=np.nan, rvalue=np.nan, pvalue=np.nan))()\n",
    "        A = np.vstack([x[mask], np.ones(mask.sum())]).T\n",
    "        m, b = np.linalg.lstsq(A, y[mask], rcond=None)[0]\n",
    "        r = np.corrcoef(x[mask], y[mask])[0,1]\n",
    "        return type(\"LR\", (), dict(slope=float(m), intercept=float(b), rvalue=float(r), pvalue=np.nan))()\n",
    "\n",
    "# --------- Configure your roots here ----------\n",
    "ROOT_DIRS = [\n",
    "    r\"C:\\Users\\caleb\\CNT_Lab\",\n",
    "    r\"C:\\Users\\caleb\\cnt_genome\",\n",
    "    r\"C:\\Users\\caleb\\Desktop\\CNT_Lab\",\n",
    "]\n",
    "\n",
    "PATTERNS = {\n",
    "    \"cooling_logs\": [\n",
    "        \"**/notebooks/archive/*cooling*.csv\",\n",
    "        \"**/archive/*cooling*.csv\",\n",
    "        \"**/*unified_cooling*.csv\",\n",
    "        \"**/cooling/*.csv\",\n",
    "    ],\n",
    "    \"cosmo_triage\": [\n",
    "        \"**/cnt_ch_slope_triage/**/ch_triage_timeseries.csv\",\n",
    "        \"**/*triage*timeseries*.csv\",\n",
    "    ],\n",
    "    \"eeg_lap_erd\": [\n",
    "        \"**/pli_humans_100plus/**/tables/*lap_erd*.csv\",\n",
    "        \"**/eeg*/**/lap_*erd*.csv\",\n",
    "    ],\n",
    "    \"kuramoto_ising\": [\n",
    "        \"**/cnt_mega_out/**/results*.json\",\n",
    "        \"**/cnt_mega_out/**/summary*.json\",\n",
    "        \"**/cnt_mega_out/**/kuramoto*.csv\",\n",
    "        \"**/cnt_mega_out/**/ising*.csv\",\n",
    "        \"**/artifacts/**/metrics/*ising*.csv\",\n",
    "        \"**/artifacts/**/metrics/*kuramoto*.csv\",\n",
    "    ],\n",
    "    \"grayscott_images\": [\n",
    "        \"**/cnt_mega_out/**/topo_edge_grayscott.png\",\n",
    "        \"**/artifacts/**/figures/**/grayscott*.png\",\n",
    "        \"**/artifacts/**/figures/**/gray-scott*.png\",\n",
    "    ],\n",
    "    \"logs_text\": [\n",
    "        \"**/notebooks/**/*.txt\",\n",
    "        \"**/logs/**/*.log\",\n",
    "        \"**/*.log\",\n",
    "        \"**/*.txt\",\n",
    "    ],\n",
    "    \"glyph_labels\": [\n",
    "        \"**/artifacts/**/glyph_labels*.csv\",\n",
    "        \"**/glyphs/**/labels*.csv\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "def find_files(root_dirs, patterns, limit=None):\n",
    "    out = []\n",
    "    for root in root_dirs:\n",
    "        if not os.path.exists(root): continue\n",
    "        for pat in patterns:\n",
    "            out.extend(glob.glob(os.path.join(root, pat), recursive=True))\n",
    "    out = sorted(set(out))\n",
    "    return out[:limit] if limit else out\n",
    "\n",
    "def safe_read_csv(p):\n",
    "    try:\n",
    "        return pd.read_csv(p)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return pd.read_csv(p, sep=\"\\t\")\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def slope(x, y):\n",
    "    x = np.asarray(x); y = np.asarray(y)\n",
    "    m = np.isfinite(x) & np.isfinite(y)\n",
    "    if m.sum() < 3: return np.nan, np.nan, np.nan, np.nan\n",
    "    res = linregress(x[m], y[m])\n",
    "    return res.slope, res.intercept, res.rvalue, res.pvalue\n",
    "\n",
    "def verdict(stat, good=\"high\", thr_green=0.5, thr_yellow=0.25):\n",
    "    if not np.isfinite(stat): return \"grey\"\n",
    "    s = float(stat)\n",
    "    if good == \"low\": s = -s\n",
    "    if s >= thr_green: return \"green\"\n",
    "    if s >= thr_yellow: return \"yellow\"\n",
    "    return \"red\"\n",
    "\n",
    "def row(claim, status, evidence, notes):\n",
    "    return {\"claim\": claim, \"status\": status, \"evidence\": evidence, \"notes\": notes}\n",
    "\n",
    "RUN_LOG = []\n",
    "def log(msg): RUN_LOG.append(msg)\n",
    "\n",
    "# 1) Cooling: oscillation amplitude vs clock variance\n",
    "def analyze_cooling(files):\n",
    "    rows, used = [], []\n",
    "    for f in files:\n",
    "        df = safe_read_csv(f)\n",
    "        if df is None or df.empty: continue\n",
    "        cols = {c.lower(): c for c in df.columns}\n",
    "        temp_col  = next((cols[c] for c in cols if re.search(r\"\\btemp|gpu[_ ]?temp\", c)), None)\n",
    "        clock_col = next((cols[c] for c in cols if re.search(r\"\\bclock|gpu[_ ]?clock\", c)), None)\n",
    "        if temp_col is None or clock_col is None: continue\n",
    "        t = pd.to_numeric(df[temp_col], errors=\"coerce\").values\n",
    "        clk = pd.to_numeric(df[clock_col], errors=\"coerce\").values\n",
    "        if np.isfinite(t).sum()<30 or np.isfinite(clk).sum()<30: continue\n",
    "        osc = float(np.nanpercentile(t,95) - np.nanpercentile(t,5))\n",
    "        vclk = float(np.nanvar(clk / (np.nanmedian(clk)+1e-9)))\n",
    "        rows.append((osc, vclk, f)); used.append(f)\n",
    "    if not rows:\n",
    "        return row(\"Cooling: Temp oscillation amplitude predicts clock stability\",\n",
    "                   \"grey\",\"Need logs with GPU temp + GPU clock columns.\",\"Add both fields to your CSVs.\"), used\n",
    "    data = pd.DataFrame(rows, columns=[\"osc_amp\",\"clock_var\",\"file\"])\n",
    "    rho, p = spearmanr(data[\"osc_amp\"], data[\"clock_var\"], nan_policy=\"omit\")\n",
    "    effect = -rho if np.isfinite(rho) else np.nan\n",
    "    return row(\"Cooling: Temp oscillation amplitude predicts clock stability\",\n",
    "               verdict(effect,\"high\",0.5,0.2),\n",
    "               f\"Spearman ρ={rho:.3f} (p={p:.2g}); N={len(data)}.\",\n",
    "               \"Negative ρ supports the claim; sign inverted for scoring.\"), used\n",
    "\n",
    "# 2) EEG laterality significance\n",
    "def analyze_eeg_laterality(files):\n",
    "    dfs, used = [], []\n",
    "    for f in files:\n",
    "        df = safe_read_csv(f)\n",
    "        if df is None or df.empty: continue\n",
    "        if not any(\"laterality\" in c.lower() for c in df.columns): continue\n",
    "        dfs.append(df); used.append(f)\n",
    "    if not dfs:\n",
    "        return row(\"EEG: Laterality (|μ|/|β|) robustly separates motor vs rest\",\n",
    "                   \"grey\",\"No lap_erd laterality tables found.\",\"Export lap_erd_subject*.csv.\"), used\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    p_cols = [c for c in df.columns if re.fullmatch(r\"p|pval|p_value|p-value\", c, flags=re.I)]\n",
    "    strong_rate = np.nan\n",
    "    if p_cols:\n",
    "        pvals = pd.to_numeric(pd.concat([df[c] for c in p_cols], axis=0), errors=\"coerce\").dropna().values\n",
    "        if pvals.size: strong_rate = float((pvals < 1e-4).mean())\n",
    "    return row(\"EEG: Laterality (|μ|/|β|) robustly separates motor vs rest\",\n",
    "               verdict(strong_rate,\"high\",0.6,0.3),\n",
    "               f\"Strong p<1e-4 rate = {strong_rate:.2f}\" if np.isfinite(strong_rate) else \"No p-value columns found.\",\n",
    "               \"High strong-hit rate indicates robust laterality.\"), used\n",
    "\n",
    "# 3) Cosmology triage: T-stat vs expansion proxies\n",
    "def analyze_cosmo_triage(files):\n",
    "    used = []\n",
    "    for f in files:\n",
    "        df = safe_read_csv(f)\n",
    "        if df is None or df.empty: continue\n",
    "        used.append(f)\n",
    "        cols = {c.lower(): c for c in df.columns}\n",
    "        Tg = cols.get(\"t_grad\") or next((cols[c] for c in cols if \"t_grad\" in c), None)\n",
    "        Ts = cols.get(\"t_spec\") or next((cols[c] for c in cols if \"t_spec\" in c), None)\n",
    "        ac = cols.get(\"a_corr\") or next((cols[c] for c in cols if \"a_corr\" in c), None)\n",
    "        ap = cols.get(\"a_peak\") or next((cols[c] for c in cols if \"a_peak\" in c), None)\n",
    "        al = cols.get(\"a_len\")  or next((cols[c] for c in cols if \"a_len\"  in c), None)\n",
    "        if not ((Tg or Ts) and (ac or ap or al)): continue\n",
    "        pairs = []\n",
    "        if Tg and ac: s,_,_,_ = slope(df[Tg], df[ac]); pairs.append((\"T_grad vs a_corr\", s))\n",
    "        if Ts and ac: s,_,_,_ = slope(df[Ts], df[ac]); pairs.append((\"T_spec vs a_corr\", s))\n",
    "        if Tg and ap: s,_,_,_ = slope(df[Tg], df[ap]); pairs.append((\"T_grad vs a_peak\", s))\n",
    "        if Ts and ap: s,_,_,_ = slope(df[Ts], df[ap]); pairs.append((\"T_spec vs a_peak\", s))\n",
    "        if Tg and al: s,_,_,_ = slope(df[Tg], df[al]); pairs.append((\"T_grad vs a_len\",  s))\n",
    "        if Ts and al: s,_,_,_ = slope(df[Ts], df[al]); pairs.append((\"T_spec vs a_len\",  s))\n",
    "        if not pairs: continue\n",
    "        names, slopes_ = zip(*pairs)\n",
    "        abs_slopes = np.abs(np.array(slopes_))\n",
    "        try:\n",
    "            rank_ap = min([i for i,n in enumerate(names) if \"a_peak\" in n], default=np.nan)\n",
    "            rank_al = min([i for i,n in enumerate(names) if \"a_len\"  in n], default=np.nan)\n",
    "            rank_ac = min([i for i,n in enumerate(names) if \"a_corr\" in n], default=np.nan)\n",
    "            ranks = [r for r in [rank_ac, rank_al] if np.isfinite(r)]\n",
    "            score = float(np.mean([r - rank_ap for r in ranks])) if np.isfinite(rank_ap) and ranks else np.nan\n",
    "        except Exception:\n",
    "            score = np.nan\n",
    "        return row(\"Cosmo triage: T-statistics favor a_peak over a_corr/a_len\",\n",
    "                   verdict(score,\"high\",0.7,0.3),\n",
    "                   f\"'a_peak' rank-advantage score = {score:.2f} from {len(pairs)} regressions.\",\n",
    "                   \"Higher means T-statistics better explain a_peak.\"), used\n",
    "    return row(\"Cosmo triage: T-statistics favor a_peak over a_corr/a_len\",\n",
    "               \"grey\",\"No triage timeseries found.\",\"Save ch_triage_timeseries.csv.\"), used\n",
    "\n",
    "# 4) Cross-model echo: Kuramoto ↔ Ising\n",
    "def analyze_kuramoto_ising(files):\n",
    "    used, kur_rows, isi_rows = [], [], []\n",
    "    for f in files:\n",
    "        if f.lower().endswith(\".json\"):\n",
    "            try:\n",
    "                js = json.loads(Path(f).read_text())\n",
    "            except Exception:\n",
    "                continue\n",
    "            used.append(f)\n",
    "            Kc = js.get(\"Kuramoto\", {}).get(\"Kc_est\", np.nan)\n",
    "            bf = js.get(\"Kuramoto\", {}).get(\"beta_fit\", np.nan)\n",
    "            if np.isfinite(Kc) or np.isfinite(bf): kur_rows.append((Kc, bf, f))\n",
    "            if \"Ising_FSS\" in js:\n",
    "                xw = js[\"Ising_FSS\"].get(\"crossing_spread\", np.nan)\n",
    "                bnu = js[\"Ising_FSS\"].get(\"beta_over_nu\", np.nan)\n",
    "                isi_rows.append((xw, bnu, f))\n",
    "        elif f.lower().endswith(\".csv\"):\n",
    "            df = safe_read_csv(f)\n",
    "            if df is None or df.empty: continue\n",
    "            used.append(f)\n",
    "            cols = {c.lower(): c for c in df.columns}\n",
    "            if \"beta_fit\" in cols or \"kc_est\" in cols:\n",
    "                Kc = pd.to_numeric(df.get(cols.get(\"kc_est\",\"kc_est\"), pd.Series([np.nan])), errors=\"coerce\").median()\n",
    "                bf = pd.to_numeric(df.get(cols.get(\"beta_fit\",\"beta_fit\"), pd.Series([np.nan])), errors=\"coerce\").median()\n",
    "                kur_rows.append((Kc, bf, f))\n",
    "            if \"crossing_spread\" in cols or \"beta_over_nu\" in cols:\n",
    "                xw = pd.to_numeric(df.get(cols.get(\"crossing_spread\",\"crossing_spread\"), pd.Series([np.nan])), errors=\"coerce\").median()\n",
    "                bnu = pd.to_numeric(df.get(cols.get(\"beta_over_nu\",\"beta_over_nu\"), pd.Series([np.nan])), errors=\"coerce\").median()\n",
    "                isi_rows.append((xw, bnu, f))\n",
    "    if not kur_rows or not isi_rows:\n",
    "        return row(\"Cross-model echo: Kuramoto onset sharpness predicts Ising FSS crispness\",\n",
    "                   \"grey\",\"Need Kuramoto (Kc_est/beta_fit) and Ising (crossing_spread/beta_over_nu) summaries.\",\n",
    "                   \"Save json/csv summaries for both.\"), used\n",
    "    kur = pd.DataFrame(kur_rows, columns=[\"Kc_est\",\"beta_fit\",\"file\"])\n",
    "    isi = pd.DataFrame(isi_rows, columns=[\"crossing_spread\",\"beta_over_nu\",\"file\"])\n",
    "    n = min(len(kur), len(isi))\n",
    "    df = pd.concat([kur.head(n).reset_index(drop=True),\n",
    "                    isi.head(n).reset_index(drop=True)], axis=1)\n",
    "    rho, p = spearmanr(df[\"beta_fit\"], df[\"crossing_spread\"], nan_policy=\"omit\")\n",
    "    effect = -rho if np.isfinite(rho) else np.nan\n",
    "    return row(\"Cross-model echo: Kuramoto onset sharpness predicts Ising FSS crispness\",\n",
    "               verdict(effect,\"high\",0.5,0.2),\n",
    "               f\"Spearman β_fit vs crossing_spread: ρ={rho:.3f} (p={p:.2g}); N={n}. Lower β_fit ↔ crisper crossings.\",\n",
    "               \"Negative ρ supports portability of 'crispness' across models.\"), used\n",
    "\n",
    "# 5) Gray-Scott morphology ↔ Kuramoto dispersion (placeholder if inputs exist)\n",
    "def analyze_grayscott_kuramoto(gs_images, kfiles):\n",
    "    used = []\n",
    "    if not gs_images:\n",
    "        return row(\"Morphology→Dynamics: Gray-Scott edge density predicts Kuramoto dispersion near Kc\",\n",
    "                   \"grey\",\"No Gray-Scott edge images found.\",\"Export topo_edge_grayscott.png.\"), used\n",
    "    disp_csvs = [f for f in kfiles if f.lower().endswith(\".csv\") and \"kuramoto\" in os.path.basename(f).lower()]\n",
    "    if not disp_csvs:\n",
    "        return row(\"Morphology→Dynamics: Gray-Scott edge density predicts Kuramoto dispersion near Kc\",\n",
    "                   \"grey\",\"No Kuramoto dispersion CSV found.\",\"Export seed,K,R near K≈Kc.\"), used\n",
    "    used.extend(gs_images[:1] + disp_csvs[:1])\n",
    "    return row(\"Morphology→Dynamics: Gray-Scott edge density predicts Kuramoto dispersion near Kc\",\n",
    "               \"yellow\",\"Inputs detected; run detailed edge-density/homology in extended notebook.\",\n",
    "               \"Compute Canny/homology edge density and regress vs σ(R) near K≈Kc.\"), used\n",
    "\n",
    "# 6) Control economics: step size ↔ ΔT per energy\n",
    "def analyze_cooling_econ(files):\n",
    "    used, rows = [], []\n",
    "    for f in files:\n",
    "        df = safe_read_csv(f)\n",
    "        if df is None or df.empty: continue\n",
    "        cols = {c.lower(): c for c in df.columns}\n",
    "        temp_col  = next((cols[c] for c in cols if re.search(r\"\\btemp|gpu[_ ]?temp\", c)), None)\n",
    "        power_col = next((cols[c] for c in cols if re.search(r\"\\bpower|watts|pwr\", c)), None)\n",
    "        step_col  = next((cols[c] for c in cols if re.search(r\"\\bstep|schedule|mode\", c)), None)\n",
    "        if temp_col is None or power_col is None: continue\n",
    "        used.append(f)\n",
    "        t   = pd.to_numeric(df[temp_col], errors=\"coerce\")\n",
    "        pwr = pd.to_numeric(df[power_col], errors=\"coerce\")\n",
    "        if t.notna().sum()<10 or pwr.notna().sum()<10: continue\n",
    "        dT = float(np.nanpercentile(t,50) - np.nanpercentile(t,95))  # negative if cooling\n",
    "        E  = float(np.nansum(pwr))  # watt-samples (assumes constant dt)\n",
    "        eff = -dT/(E+1e-9)          # higher = better cooling per energy\n",
    "        stepiness = np.nan\n",
    "        if step_col is not None:\n",
    "            try:\n",
    "                stepiness = float(df.shape[0] / (df[step_col].nunique() + 1e-9))\n",
    "            except Exception:\n",
    "                stepiness = np.nan\n",
    "        rows.append((eff, stepiness))\n",
    "    if not rows:\n",
    "        return row(\"Cooling economics: smaller, frequent steps yield better ΔT per energy\",\n",
    "                   \"grey\",\"Need cooling logs with power + step/schedule labels.\",\"Log W and step labels.\"), used\n",
    "    arr = np.array(rows, float)\n",
    "    rho, p = spearmanr(arr[:,0], arr[:,1], nan_policy=\"omit\")\n",
    "    return row(\"Cooling economics: smaller, frequent steps yield better ΔT per energy\",\n",
    "               verdict(rho,\"high\",0.4,0.2),\n",
    "               f\"Spearman ρ={rho:.3f} (p={p:.2g}); N={len(arr)}.\",\n",
    "               \"Positive ρ supports the claim.\"), used\n",
    "\n",
    "# 7) Brittleness sentinel: ConstantInputWarning ↔ high-|ρ| windows\n",
    "def analyze_brittleness(log_files, metric_csvs):\n",
    "    used, warnings = [], []\n",
    "    for f in log_files:\n",
    "        try:\n",
    "            txt = Path(f).read_text(errors=\"ignore\")\n",
    "            if \"ConstantInputWarning\" in txt:\n",
    "                warnings.append(f); used.append(f)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not warnings:\n",
    "        return row(\"Brittleness sentinel: constant-input warnings forecast spurious correlations\",\n",
    "                   \"grey\",\"No ConstantInputWarning logs found.\",\"If they appear, we’ll cross-check.\"), used\n",
    "    high, total = 0, 0\n",
    "    for f in metric_csvs[:10]:\n",
    "        df = safe_read_csv(f)\n",
    "        if df is None or df.empty: continue\n",
    "        corr_cols = [c for c in df.columns if \"corr\" in c.lower()]\n",
    "        for c in corr_cols:\n",
    "            s = pd.to_numeric(df[c], errors=\"coerce\").dropna()\n",
    "            total += len(s)\n",
    "            high  += int((np.abs(s) > 0.95).sum())\n",
    "    rate = (high/total) if total else np.nan\n",
    "    return row(\"Brittleness sentinel: constant-input warnings forecast spurious correlations\",\n",
    "               verdict(rate,\"low\",0.05,0.15),\n",
    "               f\"High-ρ window rate near warnings ≈ {rate:.2f}.\",\n",
    "               \"Lower is safer; treat nearby 'discoveries' with caution.\"), used\n",
    "\n",
    "# 8) EEG laterality magnitude ↔ glyph label stability (placeholder)\n",
    "def analyze_laterality_vs_glyph(eeg_files, glyph_label_files):\n",
    "    used = []\n",
    "    if not eeg_files or not glyph_label_files:\n",
    "        return row(\"EEG laterality magnitude predicts glyph label stability\",\n",
    "                   \"grey\",\"Need laterality tables + glyph label time-series.\",\n",
    "                   \"Export per-session glyph labels (time windows).\"), used\n",
    "    used.extend(eeg_files[:1] + glyph_label_files[:1])\n",
    "    return row(\"EEG laterality magnitude predicts glyph label stability\",\n",
    "               \"yellow\",\"Inputs detected; join on session_id to compute flip-rate vs laterality.\",\n",
    "               \"Regress flip-rate on |μ|/|β| laterality with SNR covariates.\"), used\n",
    "\n",
    "# --------- Run all ----------\n",
    "def run_all():\n",
    "    results = []\n",
    "    used = set()\n",
    "\n",
    "    cooling = find_files(ROOT_DIRS, PATTERNS[\"cooling_logs\"])\n",
    "    cosmo   = find_files(ROOT_DIRS, PATTERNS[\"cosmo_triage\"])\n",
    "    eeg     = find_files(ROOT_DIRS, PATTERNS[\"eeg_lap_erd\"])\n",
    "    ki      = find_files(ROOT_DIRS, PATTERNS[\"kuramoto_ising\"])\n",
    "    gs      = find_files(ROOT_DIRS, PATTERNS[\"grayscott_images\"])\n",
    "    logs    = find_files(ROOT_DIRS, PATTERNS[\"logs_text\"], limit=50)\n",
    "    glyphs  = find_files(ROOT_DIRS, PATTERNS[\"glyph_labels\"])\n",
    "\n",
    "    log(f\"Found {len(cooling)} cooling logs\")\n",
    "    log(f\"Found {len(cosmo)} cosmology triage tables\")\n",
    "    log(f\"Found {len(eeg)} EEG laterality tables\")\n",
    "    log(f\"Found {len(ki)} Kuramoto/Ising summaries\")\n",
    "    log(f\"Found {len(gs)} Gray-Scott edge images\")\n",
    "    log(f\"Scanned {len(logs)} text/log files\")\n",
    "    log(f\"Found {len(glyphs)} glyph label tables\")\n",
    "\n",
    "    for r,u in [\n",
    "        analyze_cooling(cooling),\n",
    "        analyze_eeg_laterality(eeg),\n",
    "        analyze_cosmo_triage(cosmo),\n",
    "        analyze_kuramoto_ising(ki),\n",
    "        analyze_grayscott_kuramoto(gs, ki),\n",
    "        analyze_cooling_econ(cooling),\n",
    "        analyze_brittleness(logs, ki + eeg + cosmo + cooling),\n",
    "        analyze_laterality_vs_glyph(eeg, glyphs),\n",
    "    ]:\n",
    "        results.append(r); used.update(u)\n",
    "\n",
    "    df = pd.DataFrame(results, columns=[\"claim\",\"status\",\"evidence\",\"notes\"])\n",
    "    csv_path = f\"./cnt_correlates_report_{TS}.csv\"\n",
    "    txt_path = f\"./cnt_correlates_report_{TS}.txt\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"== CNT Correlates Audit ==\\n\")\n",
    "        f.write(f\"Timestamp: {TS}\\n\\n\")\n",
    "        for k,v in [\n",
    "            (\"Cooling logs\", len(cooling)), (\"Cosmo triage tables\", len(cosmo)),\n",
    "            (\"EEG laterality tables\", len(eeg)), (\"Kuramoto/Ising summaries\", len(ki)),\n",
    "            (\"Gray-Scott edge images\", len(gs)), (\"Scanned logs/text\", len(logs)),\n",
    "            (\"Glyph label tables\", len(glyphs)),\n",
    "        ]:\n",
    "            f.write(f\"- {k}: {v}\\n\")\n",
    "        f.write(\"\\n== Results ==\\n\")\n",
    "        for _,rowd in df.iterrows():\n",
    "            f.write(f\"[{rowd['status'].upper()}] {rowd['claim']}\\n  {rowd['evidence']}\\n  {rowd['notes']}\\n\\n\")\n",
    "        if used:\n",
    "            f.write(\"== Used Files (subset) ==\\n\")\n",
    "            for p in list(sorted(used))[:50]:\n",
    "                f.write(f\"- {p}\\n\")\n",
    "\n",
    "    print(\"\\n\".join(RUN_LOG))\n",
    "    print(\"\\nSaved:\")\n",
    "    print(\" -\", csv_path)\n",
    "    print(\" -\", txt_path)\n",
    "\n",
    "run_all()\n",
    "# === end cell ===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c96f66d2-2a01-4f1d-be54-6d9cd980ee1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 cooling logs\n",
      "Found 1 cosmology triage tables\n",
      "Found 2 EEG laterality tables\n",
      "Found 0 Kuramoto/Ising summaries\n",
      "Found 1 Gray-Scott edge images\n",
      "Scanned 50 text/log files\n",
      "Found 0 glyph label tables\n",
      "Found 0 Kuramoto dispersion CSVs\n",
      "\n",
      "Saved:\n",
      " - ./cnt_correlates_report_20251015-164130.csv\n",
      " - ./cnt_correlates_report_20251015-164130.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_17252\\462416049.py:198: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  jumps = np.abs(np.diff(v.fillna(method=\"ffill\").fillna(method=\"bfill\").values))\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_17252\\462416049.py:198: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  jumps = np.abs(np.diff(v.fillna(method=\"ffill\").fillna(method=\"bfill\").values))\n"
     ]
    }
   ],
   "source": [
    "# === CNT Correlates Audit — Fused Update (paste into Jupyter and run) ===\n",
    "# What this cell does:\n",
    "#  • Scans your CNT_Lab for logs (cooling, EEG, cosmology triage, Kuramoto/Ising, Gray–Scott images).\n",
    "#  • Computes 8 candidate correlates with safe guards and better features.\n",
    "#  • Adds: safe Spearman; robust \"stepiness\"; Gray–Scott edge density; Kuramoto dispersion support.\n",
    "#  • Saves a verdict table (CSV + TXT) next to your notebook.\n",
    "\n",
    "import os, re, glob, json, math, warnings, datetime as dt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "TS = dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Try SciPy; fall back to in-cell implementations if absent\n",
    "try:\n",
    "    from scipy.stats import spearmanr, linregress\n",
    "    from scipy.stats import ConstantInputWarning\n",
    "    warnings.filterwarnings(\"ignore\", category=ConstantInputWarning)\n",
    "    HAVE_SCIPY = True\n",
    "except Exception:\n",
    "    HAVE_SCIPY = False\n",
    "    def _rankdata(a):\n",
    "        a = np.asarray(a, float)\n",
    "        order = np.argsort(a)\n",
    "        ranks = np.empty_like(a, float)\n",
    "        ranks[order] = np.arange(1, a.size+1)\n",
    "        # average ties\n",
    "        vals, inv, cnt = np.unique(a, return_inverse=True, return_counts=True)\n",
    "        sums = np.bincount(inv, ranks)\n",
    "        avg = sums[cnt.cumsum()-cnt]/cnt\n",
    "        return avg[inv]\n",
    "    def spearmanr(x, y, nan_policy=\"omit\"):\n",
    "        x = np.asarray(x, float); y = np.asarray(y, float)\n",
    "        m = np.isfinite(x) & np.isfinite(y)\n",
    "        if m.sum() < 3: return np.nan, np.nan\n",
    "        xr = _rankdata(x[m]); yr = _rankdata(y[m])\n",
    "        r = float(np.corrcoef(xr, yr)[0,1])\n",
    "        return r, np.nan\n",
    "    def linregress(x, y):\n",
    "        x = np.asarray(x, float); y = np.asarray(y, float)\n",
    "        m = np.isfinite(x) & np.isfinite(y)\n",
    "        if m.sum() < 3:\n",
    "            return type(\"LR\", (), dict(slope=np.nan, intercept=np.nan, rvalue=np.nan, pvalue=np.nan))()\n",
    "        A = np.vstack([x[m], np.ones(m.sum())]).T\n",
    "        m_, b_ = np.linalg.lstsq(A, y[m], rcond=None)[0]\n",
    "        r = float(np.corrcoef(x[m], y[m])[0,1])\n",
    "        return type(\"LR\", (), dict(slope=float(m_), intercept=float(b_), rvalue=r, pvalue=np.nan))()\n",
    "\n",
    "# ---------- Config ----------\n",
    "ROOT_DIRS = [\n",
    "    r\"C:\\Users\\caleb\\CNT_Lab\",\n",
    "    r\"C:\\Users\\caleb\\cnt_genome\",\n",
    "    r\"C:\\Users\\caleb\\Desktop\\CNT_Lab\",\n",
    "]\n",
    "\n",
    "PATTERNS = {\n",
    "    \"cooling_logs\": [\n",
    "        \"**/notebooks/archive/*cooling*.csv\",\n",
    "        \"**/archive/*cooling*.csv\",\n",
    "        \"**/*unified_cooling*.csv\",\n",
    "        \"**/cooling/*.csv\",\n",
    "    ],\n",
    "    \"cosmo_triage\": [\n",
    "        \"**/cnt_ch_slope_triage/**/ch_triage_timeseries.csv\",\n",
    "        \"**/*triage*timeseries*.csv\",\n",
    "    ],\n",
    "    \"eeg_lap_erd\": [\n",
    "        \"**/pli_humans_100plus/**/tables/*lap_erd*.csv\",\n",
    "        \"**/eeg*/**/lap_*erd*.csv\",\n",
    "    ],\n",
    "    \"kuramoto_ising\": [\n",
    "        \"**/cnt_mega_out/**/results*.json\",\n",
    "        \"**/cnt_mega_out/**/summary*.json\",\n",
    "        \"**/cnt_mega_out/**/kuramoto*.csv\",\n",
    "        \"**/cnt_mega_out/**/ising*.csv\",\n",
    "        \"**/artifacts/**/metrics/*ising*.csv\",\n",
    "        \"**/artifacts/**/metrics/*kuramoto*.csv\",\n",
    "    ],\n",
    "    \"grayscott_images\": [\n",
    "        \"**/cnt_mega_out/**/topo_edge_grayscott.png\",\n",
    "        \"**/artifacts/**/figures/**/grayscott*.png\",\n",
    "        \"**/artifacts/**/figures/**/gray-scott*.png\",\n",
    "    ],\n",
    "    \"logs_text\": [\n",
    "        \"**/notebooks/**/*.txt\",\n",
    "        \"**/logs/**/*.log\",\n",
    "        \"**/*.log\",\n",
    "        \"**/*.txt\",\n",
    "    ],\n",
    "    \"glyph_labels\": [\n",
    "        \"**/artifacts/**/glyph_labels*.csv\",\n",
    "        \"**/glyphs/**/labels*.csv\",\n",
    "    ],\n",
    "    # NEW: optional Kuramoto dispersion CSV (seed,K,R) near K≈Kc\n",
    "    \"kuramoto_disp\": [\n",
    "        \"**/cnt_mega_out/**/kuramoto_dispersion*.csv\",\n",
    "        \"**/artifacts/**/metrics/**/kuramoto_dispersion*.csv\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "RUN_LOG = []\n",
    "def log(m): \n",
    "    RUN_LOG.append(m)\n",
    "\n",
    "def find_files(root_dirs, patterns, limit=None):\n",
    "    out = []\n",
    "    for root in root_dirs:\n",
    "        if not os.path.exists(root): continue\n",
    "        for pat in patterns:\n",
    "            out.extend(glob.glob(os.path.join(root, pat), recursive=True))\n",
    "    out = sorted(set(out))\n",
    "    return out[:limit] if limit else out\n",
    "\n",
    "def safe_read_csv(p):\n",
    "    for sep in [\",\", \"\\t\", \"|\", \";\"]:\n",
    "        try:\n",
    "            df = pd.read_csv(p, sep=sep)\n",
    "            if df.shape[1] >= 2: return df\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def slope(x, y):\n",
    "    x = np.asarray(x); y = np.asarray(y)\n",
    "    m = np.isfinite(x) & np.isfinite(y)\n",
    "    if m.sum() < 3: return np.nan, np.nan, np.nan, np.nan\n",
    "    lr = linregress(x[m], y[m])\n",
    "    return lr.slope, lr.intercept, lr.rvalue, lr.pvalue\n",
    "\n",
    "def safe_spearman(x, y):\n",
    "    x = np.asarray(x, float); y = np.asarray(y, float)\n",
    "    m = np.isfinite(x) & np.isfinite(y)\n",
    "    if m.sum() < 3: return np.nan, np.nan\n",
    "    if np.nanstd(x[m]) == 0 or np.nanstd(y[m]) == 0:\n",
    "        return np.nan, np.nan\n",
    "    return spearmanr(x[m], y[m], nan_policy=\"omit\")\n",
    "\n",
    "def verdict(stat, good=\"high\", thr_green=0.5, thr_yellow=0.25):\n",
    "    if not np.isfinite(stat): return \"grey\"\n",
    "    s = float(stat)\n",
    "    if good == \"low\": s = -s\n",
    "    if s >= thr_green: return \"green\"\n",
    "    if s >= thr_yellow: return \"yellow\"\n",
    "    return \"red\"\n",
    "\n",
    "def row(claim, status, evidence, notes):\n",
    "    return {\"claim\": claim, \"status\": status, \"evidence\": evidence, \"notes\": notes}\n",
    "\n",
    "# --- Image edge density (no extra deps): simple Sobel + percentile threshold ---\n",
    "def image_edge_density(path, thresh_pct=90):\n",
    "    try:\n",
    "        import matplotlib.image as mpimg\n",
    "        img = mpimg.imread(path)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "    arr = np.asarray(img)\n",
    "    if arr.ndim == 3:\n",
    "        arr = 0.299*arr[...,0] + 0.587*arr[...,1] + 0.114*arr[...,2]\n",
    "    arr = arr.astype(float)\n",
    "    # Sobel kernels\n",
    "    Kx = np.array([[1,0,-1],[2,0,-2],[1,0,-1]], float)\n",
    "    Ky = Kx.T\n",
    "    # pad\n",
    "    pad = np.pad(arr, 1, mode=\"reflect\")\n",
    "    Gx = (Kx[0,0]*pad[:-2,:-2] + Kx[0,1]*pad[:-2,1:-1] + Kx[0,2]*pad[:-2,2:] +\n",
    "          Kx[1,0]*pad[1:-1,:-2] + Kx[1,1]*pad[1:-1,1:-1] + Kx[1,2]*pad[1:-1,2:] +\n",
    "          Kx[2,0]*pad[2:,  :-2] + Kx[2,1]*pad[2:,  1:-1] + Kx[2,2]*pad[2:,  2:])\n",
    "    Gy = (Ky[0,0]*pad[:-2,:-2] + Ky[0,1]*pad[:-2,1:-1] + Ky[0,2]*pad[:-2,2:] +\n",
    "          Ky[1,0]*pad[1:-1,:-2] + Ky[1,1]*pad[1:-1,1:-1] + Ky[1,2]*pad[1:-1,2:] +\n",
    "          Ky[2,0]*pad[2:,  :-2] + Ky[2,1]*pad[2:,  1:-1] + Ky[2,2]*pad[2:,  2:])\n",
    "    mag = np.hypot(Gx, Gy)\n",
    "    thr = np.nanpercentile(mag, thresh_pct)\n",
    "    edges = (mag >= thr)\n",
    "    return float(np.mean(edges))  # fraction of \"edge\" pixels\n",
    "\n",
    "# --- Robust \"stepiness\" detector: uses explicit step/mode if present, else change-points in power/fan ---\n",
    "def compute_stepiness(df, step_cols=(\"step\",\"schedule\",\"mode\"), change_cols=(\"power\",\"watts\",\"pwr\",\"fan\",\"target\")):\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    # 1) explicit step column\n",
    "    for k in step_cols:\n",
    "        if any(k in c for c in cols):\n",
    "            c = [cols[c] for c in cols if k in c][0]\n",
    "            vals = pd.Series(df[c]).astype(str)\n",
    "            uniq = vals.nunique()\n",
    "            if uniq > 1:\n",
    "                return float(df.shape[0] / uniq)\n",
    "    # 2) change points on power/fan-like columns\n",
    "    for k in change_cols:\n",
    "        hits = [cols[c] for c in cols if k in c]\n",
    "        for c in hits:\n",
    "            v = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "            if v.notna().sum() < 5: continue\n",
    "            # count big jumps (>= 5% of range) as \"steps\"\n",
    "            rng = np.nanpercentile(v, 95) - np.nanpercentile(v, 5)\n",
    "            if rng <= 0: continue\n",
    "            jumps = np.abs(np.diff(v.fillna(method=\"ffill\").fillna(method=\"bfill\").values))\n",
    "            steps = int((jumps >= 0.05 * rng).sum())\n",
    "            if steps > 0:\n",
    "                return float(df.shape[0] / max(1, steps))\n",
    "    return np.nan\n",
    "\n",
    "# ---------- 1) Cooling: oscillation amplitude vs clock variance ----------\n",
    "def analyze_cooling(cooling_files):\n",
    "    rows, used = [], []\n",
    "    for f in cooling_files:\n",
    "        df = safe_read_csv(f)\n",
    "        if df is None or df.empty: continue\n",
    "        cols = {c.lower(): c for c in df.columns}\n",
    "        temp_col  = next((cols[c] for c in cols if re.search(r\"\\btemp|gpu[_ ]?temp\", c)), None)\n",
    "        clock_col = next((cols[c] for c in cols if re.search(r\"\\bclock|gpu[_ ]?clock\", c)), None)\n",
    "        if temp_col is None or clock_col is None: continue\n",
    "        t = pd.to_numeric(df[temp_col], errors=\"coerce\").values\n",
    "        clk = pd.to_numeric(df[clock_col], errors=\"coerce\").values\n",
    "        if np.isfinite(t).sum()<30 or np.isfinite(clk).sum()<30: continue\n",
    "        osc = float(np.nanpercentile(t,95) - np.nanpercentile(t,5))\n",
    "        vclk = float(np.nanvar(clk / (np.nanmedian(clk)+1e-9)))\n",
    "        rows.append((osc, vclk, f)); used.append(f)\n",
    "    if not rows:\n",
    "        return row(\"Cooling: Temp oscillation amplitude predicts clock stability\",\n",
    "                   \"grey\",\"Need logs with GPU temp + GPU clock columns.\",\"Add both fields to your CSVs.\"), used\n",
    "    data = pd.DataFrame(rows, columns=[\"osc_amp\",\"clock_var\",\"file\"])\n",
    "    rho, p = safe_spearman(data[\"osc_amp\"], data[\"clock_var\"])\n",
    "    effect = -rho if np.isfinite(rho) else np.nan\n",
    "    return row(\"Cooling: Temp oscillation amplitude predicts clock stability\",\n",
    "               verdict(effect,\"high\",0.5,0.2),\n",
    "               f\"Spearman ρ={rho:.3f} (p={p:.2g}); N={len(data)}.\",\n",
    "               \"Negative ρ supports the claim; sign inverted for scoring.\"), used\n",
    "\n",
    "# ---------- 2) EEG laterality significance ----------\n",
    "def analyze_eeg_laterality(eeg_files):\n",
    "    dfs, used = [], []\n",
    "    for f in eeg_files:\n",
    "        df = safe_read_csv(f)\n",
    "        if df is None or df.empty: continue\n",
    "        if not any(\"laterality\" in c.lower() for c in df.columns): continue\n",
    "        dfs.append(df); used.append(f)\n",
    "    if not dfs:\n",
    "        return row(\"EEG: Laterality (|μ|/|β|) robustly separates motor vs rest\",\n",
    "                   \"grey\",\"No lap_erd laterality tables found.\",\"Export lap_erd_subject*.csv.\"), used\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    p_cols = [c for c in df.columns if re.fullmatch(r\"p|pval|p_value|p-value\", c, flags=re.I)]\n",
    "    strong_rate = np.nan\n",
    "    if p_cols:\n",
    "        pvals = pd.to_numeric(pd.concat([df[c] for c in p_cols], axis=0), errors=\"coerce\").dropna().values\n",
    "        if pvals.size: strong_rate = float((pvals < 1e-4).mean())\n",
    "    return row(\"EEG: Laterality (|μ|/|β|) robustly separates motor vs rest\",\n",
    "               verdict(strong_rate,\"high\",0.6,0.3),\n",
    "               f\"Strong p<1e-4 rate = {strong_rate:.2f}\" if np.isfinite(strong_rate) else \"No p-value columns found.\",\n",
    "               \"High strong-hit rate indicates robust laterality.\"), used\n",
    "\n",
    "# ---------- 3) Cosmology triage ----------\n",
    "def analyze_cosmo_triage(cosmo_files):\n",
    "    used = []\n",
    "    for f in cosmo_files:\n",
    "        df = safe_read_csv(f)\n",
    "        if df is None or df.empty: continue\n",
    "        used.append(f)\n",
    "        cols = {c.lower(): c for c in df.columns}\n",
    "        Tg = cols.get(\"t_grad\") or next((cols[c] for c in cols if \"t_grad\" in c), None)\n",
    "        Ts = cols.get(\"t_spec\") or next((cols[c] for c in cols if \"t_spec\" in c), None)\n",
    "        ac = cols.get(\"a_corr\") or next((cols[c] for c in cols if \"a_corr\" in c), None)\n",
    "        ap = cols.get(\"a_peak\") or next((cols[c] for c in cols if \"a_peak\" in c), None)\n",
    "        al = cols.get(\"a_len\")  or next((cols[c] for c in cols if \"a_len\"  in c), None)\n",
    "        if not ((Tg or Ts) and (ac or ap or al)): continue\n",
    "        pairs = []\n",
    "        if Tg and ac: s,_,_,_ = slope(df[Tg], df[ac]); pairs.append((\"T_grad vs a_corr\", s))\n",
    "        if Ts and ac: s,_,_,_ = slope(df[Ts], df[ac]); pairs.append((\"T_spec vs a_corr\", s))\n",
    "        if Tg and ap: s,_,_,_ = slope(df[Tg], df[ap]); pairs.append((\"T_grad vs a_peak\", s))\n",
    "        if Ts and ap: s,_,_,_ = slope(df[Ts], df[ap]); pairs.append((\"T_spec vs a_peak\", s))\n",
    "        if Tg and al: s,_,_,_ = slope(df[Tg], df[al]); pairs.append((\"T_grad vs a_len\",  s))\n",
    "        if Ts and al: s,_,_,_ = slope(df[Ts], df[al]); pairs.append((\"T_spec vs a_len\",  s))\n",
    "        if not pairs: continue\n",
    "        names, slopes_ = zip(*pairs)\n",
    "        abs_slopes = np.abs(np.array(slopes_))\n",
    "        # rank advantage: a_peak should rank above a_corr/a_len\n",
    "        def first_rank(substr):\n",
    "            idxs = [i for i,n in enumerate(names) if substr in n]\n",
    "            return min(idxs) if idxs else np.nan\n",
    "        rank_ap = first_rank(\"a_peak\"); rank_al = first_rank(\"a_len\"); rank_ac = first_rank(\"a_corr\")\n",
    "        others = [r for r in [rank_ac, rank_al] if np.isfinite(r)]\n",
    "        score = float(np.mean([r - rank_ap for r in others])) if np.isfinite(rank_ap) and others else np.nan\n",
    "        return row(\"Cosmo triage: T-statistics favor a_peak over a_corr/a_len\",\n",
    "                   verdict(score,\"high\",0.7,0.3),\n",
    "                   f\"'a_peak' rank-advantage score = {score:.2f} from {len(pairs)} regressions.\",\n",
    "                   \"Higher means T-statistics better explain a_peak.\"), used\n",
    "    return row(\"Cosmo triage: T-statistics favor a_peak over a_corr/a_len\",\n",
    "               \"grey\",\"No triage timeseries found.\",\"Save ch_triage_timeseries.csv.\"), used\n",
    "\n",
    "# ---------- 4) Cross-model echo: Kuramoto ↔ Ising ----------\n",
    "def analyze_kuramoto_ising(ki_files):\n",
    "    used, kur_rows, isi_rows = [], [], []\n",
    "    for f in ki_files:\n",
    "        if f.lower().endswith(\".json\"):\n",
    "            try:\n",
    "                js = json.loads(Path(f).read_text())\n",
    "            except Exception:\n",
    "                continue\n",
    "            used.append(f)\n",
    "            Kc = js.get(\"Kuramoto\", {}).get(\"Kc_est\", np.nan)\n",
    "            bf = js.get(\"Kuramoto\", {}).get(\"beta_fit\", np.nan)\n",
    "            if np.isfinite(Kc) or np.isfinite(bf): kur_rows.append((Kc, bf, f))\n",
    "            if \"Ising_FSS\" in js:\n",
    "                xw = js[\"Ising_FSS\"].get(\"crossing_spread\", np.nan)\n",
    "                bnu = js[\"Ising_FSS\"].get(\"beta_over_nu\", np.nan)\n",
    "                isi_rows.append((xw, bnu, f))\n",
    "        elif f.lower().endswith(\".csv\"):\n",
    "            df = safe_read_csv(f)\n",
    "            if df is None or df.empty: continue\n",
    "            used.append(f)\n",
    "            cols = {c.lower(): c for c in df.columns}\n",
    "            if \"beta_fit\" in cols or \"kc_est\" in cols:\n",
    "                Kc = pd.to_numeric(df.get(cols.get(\"kc_est\",\"kc_est\"), pd.Series([np.nan])), errors=\"coerce\").median()\n",
    "                bf = pd.to_numeric(df.get(cols.get(\"beta_fit\",\"beta_fit\"), pd.Series([np.nan])), errors=\"coerce\").median()\n",
    "                kur_rows.append((Kc, bf, f))\n",
    "            if \"crossing_spread\" in cols or \"beta_over_nu\" in cols:\n",
    "                xw = pd.to_numeric(df.get(cols.get(\"crossing_spread\",\"crossing_spread\"), pd.Series([np.nan])), errors=\"coerce\").median()\n",
    "                bnu = pd.to_numeric(df.get(cols.get(\"beta_over_nu\",\"beta_over_nu\"), pd.Series([np.nan])), errors=\"coerce\").median()\n",
    "                isi_rows.append((xw, bnu, f))\n",
    "    if not kur_rows or not isi_rows:\n",
    "        return row(\"Cross-model echo: Kuramoto onset sharpness predicts Ising FSS crispness\",\n",
    "                   \"grey\",\"Need Kuramoto (Kc_est/beta_fit) and Ising (crossing_spread/beta_over_nu) summaries.\",\n",
    "                   \"Save json/csv summaries for both.\"), used\n",
    "    kur = pd.DataFrame(kur_rows, columns=[\"Kc_est\",\"beta_fit\",\"file\"])\n",
    "    isi = pd.DataFrame(isi_rows, columns=[\"crossing_spread\",\"beta_over_nu\",\"file\"])\n",
    "    n = min(len(kur), len(isi))\n",
    "    df = pd.concat([kur.head(n).reset_index(drop=True),\n",
    "                    isi.head(n).reset_index(drop=True)], axis=1)\n",
    "    rho, p = safe_spearman(df[\"beta_fit\"], df[\"crossing_spread\"])\n",
    "    effect = -rho if np.isfinite(rho) else np.nan\n",
    "    return row(\"Cross-model echo: Kuramoto onset sharpness predicts Ising FSS crispness\",\n",
    "               verdict(effect,\"high\",0.5,0.2),\n",
    "               f\"Spearman β_fit vs crossing_spread: ρ={rho:.3f} (p={p:.2g}); N={n}. Lower β_fit ↔ crisper crossings.\",\n",
    "               \"Negative ρ supports portability of 'crispness' across models.\"), used\n",
    "\n",
    "# ---------- 5) Morphology→Dynamics: Gray–Scott edge density ↔ Kuramoto dispersion ----------\n",
    "def analyze_grayscott_kuramoto(gs_images, disp_files):\n",
    "    used = []\n",
    "    if not gs_images:\n",
    "        return row(\"Morphology→Dynamics: Gray-Scott edge density predicts Kuramoto dispersion near Kc\",\n",
    "                   \"grey\",\"No Gray-Scott edge images found.\",\"Export topo_edge_grayscott.png.\"), used\n",
    "    # Take first image (or aggregate later if you have many)\n",
    "    ed = image_edge_density(gs_images[0])\n",
    "    used.append(gs_images[0])\n",
    "\n",
    "    if not disp_files:\n",
    "        return row(\"Morphology→Dynamics: Gray-Scott edge density predicts Kuramoto dispersion near Kc\",\n",
    "                   \"yellow\", f\"Edge density ~ {ed:.3f}; no Kuramoto dispersion CSV found.\",\n",
    "                   \"Export seed,K,R near K≈Kc as 'kuramoto_dispersion.csv'.\"), used\n",
    "\n",
    "    # Compute dispersion metric: std of R in a narrow band around median K\n",
    "    disp = []\n",
    "    for f in disp_files:\n",
    "        df = safe_read_csv(f)\n",
    "        if df is None or df.empty: continue\n",
    "        cols = {c.lower(): c for c in df.columns}\n",
    "        if not all(k in cols for k in [\"seed\",\"k\",\"r\"]): \n",
    "            # try lowercase fallback names\n",
    "            need = [\"seed\",\"k\",\"r\"]; got = list(cols.keys())\n",
    "            continue\n",
    "        K = pd.to_numeric(df[cols[\"k\"]], errors=\"coerce\")\n",
    "        R = pd.to_numeric(df[cols[\"r\"]], errors=\"coerce\")\n",
    "        if K.notna().sum()<10: continue\n",
    "        K0 = np.nanmedian(K)\n",
    "        win = np.abs(K - K0) <= (0.02 * (np.nanpercentile(K,95)-np.nanpercentile(K,5)) + 1e-9)\n",
    "        sigma_R = float(np.nanstd(R[win]))\n",
    "        disp.append(sigma_R)\n",
    "        used.append(f)\n",
    "\n",
    "    if not disp:\n",
    "        return row(\"Morphology→Dynamics: Gray-Scott edge density predicts Kuramoto dispersion near Kc\",\n",
    "                   \"yellow\", f\"Edge density ~ {ed:.3f}; dispersion metric not computed (K/R missing or too sparse).\",\n",
    "                   \"Ensure columns seed,K,R and sufficient rows near K≈median(K).\"), used\n",
    "\n",
    "    # If you eventually have multiple images+disp files, correlate; with one pair, report metric only.\n",
    "    if len(disp) == 1:\n",
    "        return row(\"Morphology→Dynamics: Gray-Scott edge density predicts Kuramoto dispersion near Kc\",\n",
    "                   \"yellow\", f\"Edge density ~ {ed:.3f}; σ(R|K≈Kc) ~ {disp[0]:.4f}.\",\n",
    "                   \"Add more runs to correlate edge density vs σ(R).\"), used\n",
    "\n",
    "    # Multi-run: correlate edge density (per image) with dispersion across runs (here we only used first image, but structure supports list)\n",
    "    # Placeholder association if multiple pairs are available\n",
    "    return row(\"Morphology→Dynamics: Gray-Scott edge density predicts Kuramoto dispersion near Kc\",\n",
    "               \"yellow\", \"Multiple runs detected; extend mapping to pair each image with its dispersion.\", \n",
    "               \"Name images/CSVs with matching tags to correlate per-run.\"), used\n",
    "\n",
    "# ---------- 6) Control economics: step size ↔ ΔT per energy ----------\n",
    "def analyze_cooling_econ(cooling_files):\n",
    "    used, rows = [], []\n",
    "    for f in cooling_files:\n",
    "        df = safe_read_csv(f)\n",
    "        if df is None or df.empty: continue\n",
    "        cols = {c.lower(): c for c in df.columns}\n",
    "        temp_col  = next((cols[c] for c in cols if re.search(r\"\\btemp|gpu[_ ]?temp\", c)), None)\n",
    "        power_col = next((cols[c] for c in cols if re.search(r\"\\bpower|watts|pwr\", c)), None)\n",
    "        if temp_col is None or power_col is None: continue\n",
    "        used.append(f)\n",
    "        t   = pd.to_numeric(df[temp_col], errors=\"coerce\")\n",
    "        pwr = pd.to_numeric(df[power_col], errors=\"coerce\")\n",
    "        if t.notna().sum()<10 or pwr.notna().sum()<10: continue\n",
    "        # ΔT per \"joule\" proxy (assuming constant dt sampling)\n",
    "        dT = float(np.nanpercentile(t,50) - np.nanpercentile(t,95))  # negative if cooling\n",
    "        E  = float(np.nansum(pwr))\n",
    "        eff = -dT/(E+1e-9)          # higher = better cooling per energy\n",
    "        stepiness = compute_stepiness(df)\n",
    "        rows.append((eff, stepiness))\n",
    "    if not rows:\n",
    "        return row(\"Cooling economics: smaller, frequent steps yield better ΔT per energy\",\n",
    "                   \"grey\",\"Need cooling logs with power + step/schedule/targets.\",\"Log W and step/target labels.\"), used\n",
    "    arr = np.array(rows, float)\n",
    "    rho, p = safe_spearman(arr[:,0], arr[:,1])\n",
    "    return row(\"Cooling economics: smaller, frequent steps yield better ΔT per energy\",\n",
    "               verdict(rho,\"high\",0.4,0.2),\n",
    "               f\"Spearman ρ={rho:.3f} (p={p:.2g}); N={len(arr)}. (auto stepiness detector)\",\n",
    "               \"Positive ρ supports the claim; guards avoid constant-input errors.\"), used\n",
    "\n",
    "# ---------- 7) Brittleness sentinel ----------\n",
    "def analyze_brittleness(log_files, metric_csvs):\n",
    "    used, warnings_hit = [], []\n",
    "    for f in log_files:\n",
    "        try:\n",
    "            txt = Path(f).read_text(errors=\"ignore\")\n",
    "        except Exception:\n",
    "            continue\n",
    "        if \"ConstantInputWarning\" in txt or \"ConstantInput\" in txt:\n",
    "            warnings_hit.append(f); used.append(f)\n",
    "    if not warnings_hit:\n",
    "        return row(\"Brittleness sentinel: constant-input warnings forecast spurious correlations\",\n",
    "                   \"grey\",\"No ConstantInputWarning logs found.\",\"If they appear, we’ll cross-check.\"), used\n",
    "    high, total = 0, 0\n",
    "    for f in metric_csvs[:10]:\n",
    "        df = safe_read_csv(f)\n",
    "        if df is None or df.empty: continue\n",
    "        corr_cols = [c for c in df.columns if \"corr\" in c.lower()]\n",
    "        for c in corr_cols:\n",
    "            s = pd.to_numeric(df[c], errors=\"coerce\").dropna()\n",
    "            total += len(s)\n",
    "            high  += int((np.abs(s) > 0.95).sum())\n",
    "    rate = (high/total) if total else np.nan\n",
    "    return row(\"Brittleness sentinel: constant-input warnings forecast spurious correlations\",\n",
    "               verdict(rate,\"low\",0.05,0.15),\n",
    "               f\"High-ρ window rate near warnings ≈ {rate:.2f}.\",\n",
    "               \"Lower is safer; treat nearby 'discoveries' with caution.\"), used\n",
    "\n",
    "# ---------- 8) Laterality ↔ glyph stability (placeholder until glyph labels exist) ----------\n",
    "def analyze_laterality_vs_glyph(eeg_files, glyph_label_files):\n",
    "    used = []\n",
    "    if not eeg_files or not glyph_label_files:\n",
    "        return row(\"EEG laterality magnitude predicts glyph label stability\",\n",
    "                   \"grey\",\"Need laterality tables + glyph label time-series.\",\n",
    "                   \"Export per-session glyph labels (time windows) with session_id.\"), used\n",
    "    used.extend(eeg_files[:1] + glyph_label_files[:1])\n",
    "    return row(\"EEG laterality magnitude predicts glyph label stability\",\n",
    "               \"yellow\",\"Inputs detected; join on session_id to compute flip-rate vs laterality.\",\n",
    "               \"Regress flip-rate on |μ|/|β| laterality with SNR covariates.\"), used\n",
    "\n",
    "# ---------- Runner ----------\n",
    "def run_all():\n",
    "    results = []\n",
    "    used = set()\n",
    "\n",
    "    cooling = find_files(ROOT_DIRS, PATTERNS[\"cooling_logs\"])\n",
    "    cosmo   = find_files(ROOT_DIRS, PATTERNS[\"cosmo_triage\"])\n",
    "    eeg     = find_files(ROOT_DIRS, PATTERNS[\"eeg_lap_erd\"])\n",
    "    ki      = find_files(ROOT_DIRS, PATTERNS[\"kuramoto_ising\"])\n",
    "    gs      = find_files(ROOT_DIRS, PATTERNS[\"grayscott_images\"])\n",
    "    logs    = find_files(ROOT_DIRS, PATTERNS[\"logs_text\"], limit=50)\n",
    "    glyphs  = find_files(ROOT_DIRS, PATTERNS[\"glyph_labels\"])\n",
    "    kdisp   = find_files(ROOT_DIRS, PATTERNS[\"kuramoto_disp\"])\n",
    "\n",
    "    log(f\"Found {len(cooling)} cooling logs\")\n",
    "    log(f\"Found {len(cosmo)} cosmology triage tables\")\n",
    "    log(f\"Found {len(eeg)} EEG laterality tables\")\n",
    "    log(f\"Found {len(ki)} Kuramoto/Ising summaries\")\n",
    "    log(f\"Found {len(gs)} Gray-Scott edge images\")\n",
    "    log(f\"Scanned {len(logs)} text/log files\")\n",
    "    log(f\"Found {len(glyphs)} glyph label tables\")\n",
    "    log(f\"Found {len(kdisp)} Kuramoto dispersion CSVs\")\n",
    "\n",
    "    for r,u in [\n",
    "        analyze_cooling(cooling),\n",
    "        analyze_eeg_laterality(eeg),\n",
    "        analyze_cosmo_triage(cosmo),\n",
    "        analyze_kuramoto_ising(ki),\n",
    "        analyze_grayscott_kuramoto(gs, kdisp),\n",
    "        analyze_cooling_econ(cooling),\n",
    "        analyze_brittleness(logs, ki + eeg + cosmo + cooling),\n",
    "        analyze_laterality_vs_glyph(eeg, glyphs),\n",
    "    ]:\n",
    "        results.append(r); used.update(u)\n",
    "\n",
    "    df = pd.DataFrame(results, columns=[\"claim\",\"status\",\"evidence\",\"notes\"])\n",
    "    csv_path = f\"./cnt_correlates_report_{TS}.csv\"\n",
    "    txt_path = f\"./cnt_correlates_report_{TS}.txt\"\n",
    "    df.to_csv(csv_path, index=False)\n",
    "\n",
    "    with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"== CNT Correlates Audit (Fused) ==\\n\")\n",
    "        f.write(f\"Timestamp: {TS}\\n\\n\")\n",
    "        for k,v in [\n",
    "            (\"Cooling logs\", len(cooling)), (\"Cosmo triage tables\", len(cosmo)),\n",
    "            (\"EEG laterality tables\", len(eeg)), (\"Kuramoto/Ising summaries\", len(ki)),\n",
    "            (\"Kuramoto dispersion CSVs\", len(kdisp)), (\"Gray-Scott edge images\", len(gs)),\n",
    "            (\"Scanned logs/text\", len(logs)), (\"Glyph label tables\", len(glyphs)),\n",
    "        ]:\n",
    "            f.write(f\"- {k}: {v}\\n\")\n",
    "        f.write(\"\\n== Results ==\\n\")\n",
    "        for _,rowd in df.iterrows():\n",
    "            f.write(f\"[{rowd['status'].upper()}] {rowd['claim']}\\n  {rowd['evidence']}\\n  {rowd['notes']}\\n\\n\")\n",
    "        if used:\n",
    "            f.write(\"== Used Files (subset) ==\\n\")\n",
    "            for p in list(sorted(used))[:50]:\n",
    "                f.write(f\"- {p}\\n\")\n",
    "\n",
    "    print(\"\\n\".join(RUN_LOG))\n",
    "    print(\"\\nSaved:\")\n",
    "    print(\" -\", csv_path)\n",
    "    print(\" -\", txt_path)\n",
    "\n",
    "run_all()\n",
    "# === end cell ===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f010d72a-3e2b-4632-b105-14977dd91ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
