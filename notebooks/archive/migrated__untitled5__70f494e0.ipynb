{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33579bcf-0516-4c08-8f91-6c66a114867a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Kernel info ==\n",
      "Python: 3.13.5\n",
      "Interpreter: C:\\Users\\caleb\\cnt_genome\\.venv\\Scripts\\python.exe\n",
      "\n",
      "== Upgrading pip core ==\n",
      "Requirement already satisfied: pip in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (25.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (80.9.0)\n",
      "Requirement already satisfied: wheel in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (0.45.1)\n",
      "\n",
      "== Installing core scientific stack ==\n",
      "Requirement already satisfied: numpy in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (1.16.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (3.10.6)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (0.14.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: numba in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (0.62.0)\n",
      "Requirement already satisfied: umap-learn in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (0.5.9.post2)\n",
      "Requirement already satisfied: networkx in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: plotly in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (6.3.0)\n",
      "Collecting kaleido\n",
      "  Using cached kaleido-1.1.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting statsforecast\n",
      "  Using cached statsforecast-2.0.2.tar.gz (2.9 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pmdarima\n",
      "  Using cached pmdarima-2.0.4.tar.gz (630 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: mne in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (1.10.1)\n",
      "Collecting nilearn\n",
      "  Using cached nilearn-0.12.1-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting anndata\n",
      "  Using cached anndata-0.12.2-py3-none-any.whl.metadata (9.6 kB)\n",
      "Collecting scanpy\n",
      "  Using cached scanpy-1.11.4-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting pybedtools\n",
      "  Using cached pybedtools-0.12.0.tar.gz (12.6 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting pybigwig\n",
      "  Using cached pybigwig-0.3.24.tar.gz (75 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Getting requirements to build wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [20 lines of output]\n",
      "  Traceback (most recent call last):\n",
      "    File \u001b[35m\"C:\\Users\\caleb\\cnt_genome\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m389\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "      \u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "      \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\caleb\\cnt_genome\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m373\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "      json_out[\"return_val\"] = \u001b[31mhook\u001b[0m\u001b[1;31m(**hook_input[\"kwargs\"])\u001b[0m\n",
      "                               \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\caleb\\cnt_genome\\.venv\\Lib\\site-packages\\pip\\_vendor\\pyproject_hooks\\_in_process\\_in_process.py\"\u001b[0m, line \u001b[35m143\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "      return hook(config_settings)\n",
      "    File \u001b[35m\"C:\\Users\\caleb\\AppData\\Local\\Temp\\pip-build-env-bbq2nojo\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m331\u001b[0m, in \u001b[35mget_requires_for_build_wheel\u001b[0m\n",
      "      return \u001b[31mself._get_build_requires\u001b[0m\u001b[1;31m(config_settings, requirements=[])\u001b[0m\n",
      "             \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\caleb\\AppData\\Local\\Temp\\pip-build-env-bbq2nojo\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m301\u001b[0m, in \u001b[35m_get_build_requires\u001b[0m\n",
      "      \u001b[31mself.run_setup\u001b[0m\u001b[1;31m()\u001b[0m\n",
      "      \u001b[31m~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "    File \u001b[35m\"C:\\Users\\caleb\\AppData\\Local\\Temp\\pip-build-env-bbq2nojo\\overlay\\Lib\\site-packages\\setuptools\\build_meta.py\"\u001b[0m, line \u001b[35m317\u001b[0m, in \u001b[35mrun_setup\u001b[0m\n",
      "      \u001b[31mexec\u001b[0m\u001b[1;31m(code, locals())\u001b[0m\n",
      "      \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m19\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "  \u001b[1;35mAttributeError\u001b[0m: \u001b[35m'NoneType' object has no attribute 'split'\u001b[0m\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "Getting requirements to build wheel did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "== Installing PyTorch (CUDA 12.4) ‚Üí fallback to CPU if needed ==\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m torch_gpu_ok = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m== Installing PyTorch (CUDA 12.4) ‚Üí fallback to CPU if needed ==\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m code, out = \u001b[43mpip_install\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m--index-url\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttps://download.pytorch.org/whl/cu124\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtorch\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtorchvision\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtorchaudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m code != \u001b[32m0\u001b[39m:\n\u001b[32m     36\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[torch cuda install failed] falling back to CPU wheels‚Ä¶\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mpip_install\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpip_install\u001b[39m(args):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-m\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpip\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstall\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(cmd)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(cmd):\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m         r = \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSTDOUT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m r.returncode, r.stdout.strip()\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py:556\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Popen(*popenargs, **kwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    555\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m556\u001b[39m         stdout, stderr = \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    557\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    558\u001b[39m         process.kill()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py:1209\u001b[39m, in \u001b[36mPopen.communicate\u001b[39m\u001b[34m(self, input, timeout)\u001b[39m\n\u001b[32m   1207\u001b[39m     \u001b[38;5;28mself\u001b[39m._stdin_write(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m   1208\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stdout:\n\u001b[32m-> \u001b[39m\u001b[32m1209\u001b[39m     stdout = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1210\u001b[39m     \u001b[38;5;28mself\u001b[39m.stdout.close()\n\u001b[32m   1211\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stderr:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\encodings\\cp1252.py:22\u001b[39m, in \u001b[36mIncrementalDecoder.decode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIncrementalDecoder\u001b[39;00m(codecs.IncrementalDecoder):\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m     23\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m codecs.charmap_decode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m.errors,decoding_table)[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === CNT one-shot environment + GPU check (single cell) ===\n",
    "import sys, subprocess, platform, json, shutil\n",
    "\n",
    "def run(cmd):\n",
    "    try:\n",
    "        r = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, check=False)\n",
    "        return r.returncode, r.stdout.strip()\n",
    "    except Exception as e:\n",
    "        return -1, str(e)\n",
    "\n",
    "def pip_install(args):\n",
    "    return run([sys.executable, \"-m\", \"pip\", \"install\"] + args)\n",
    "\n",
    "print(\"== Kernel info ==\")\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"Interpreter:\", sys.executable)\n",
    "\n",
    "print(\"\\n== Upgrading pip core ==\")\n",
    "print(pip_install([\"--upgrade\", \"pip\", \"setuptools\", \"wheel\"])[1])\n",
    "\n",
    "# --- Core scientific stack (CPU-safe) ---\n",
    "core = [\"numpy\",\"scipy\",\"pandas\",\"matplotlib\",\"statsmodels\",\"scikit-learn\",\"numba\",\"umap-learn\",\"networkx\"]\n",
    "viz  = [\"plotly\",\"kaleido\"]\n",
    "forecast = [\"statsforecast\",\"pmdarima\"]\n",
    "neuro_genomics = [\"mne\",\"nilearn\",\"anndata\",\"scanpy\",\"pybedtools\",\"pybigwig\"]\n",
    "maybe_heavy = [\"ubermag\"]  # will set up OOMMF on first real use\n",
    "\n",
    "print(\"\\n== Installing core scientific stack ==\")\n",
    "print(pip_install(core + viz + forecast + neuro_genomics + maybe_heavy)[1])\n",
    "\n",
    "# --- Torch GPU (CUDA 12.4 wheels) with graceful fallback ---\n",
    "torch_gpu_ok = False\n",
    "print(\"\\n== Installing PyTorch (CUDA 12.4) ‚Üí fallback to CPU if needed ==\")\n",
    "code, out = pip_install([\"--index-url\",\"https://download.pytorch.org/whl/cu124\",\"torch\",\"torchvision\",\"torchaudio\"])\n",
    "if code != 0:\n",
    "    print(\"[torch cuda install failed] falling back to CPU wheels‚Ä¶\")\n",
    "    print(pip_install([\"torch\",\"torchvision\",\"torchaudio\"])[1])\n",
    "else:\n",
    "    print(out)\n",
    "\n",
    "# --- TensorFlow (GPU if drivers/toolkit match) ---\n",
    "print(\"\\n== Installing TensorFlow (may be CPU if no compatible GPU build) ==\")\n",
    "print(pip_install([\"tensorflow\"])[1])\n",
    "\n",
    "# --- Sanity imports & versions ---\n",
    "summary = {\"torch\":None,\"cuda_available\":None,\"cuda_device\":None,\"tf\":None,\"tf_gpus\":None,\"nvidia_smi\":None}\n",
    "\n",
    "print(\"\\n== Import checks ==\")\n",
    "try:\n",
    "    import torch\n",
    "    summary[\"torch\"] = getattr(torch, \"__version__\", \"unknown\")\n",
    "    summary[\"cuda_available\"] = bool(torch.cuda.is_available())\n",
    "    summary[\"cuda_device\"] = (torch.cuda.get_device_name(0) if torch.cuda.is_available() else None)\n",
    "    print(f\"Torch: {summary['torch']}  | CUDA available: {summary['cuda_available']}  | Device: {summary['cuda_device']}\")\n",
    "except Exception as e:\n",
    "    print(\"Torch import failed:\", e)\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    summary[\"tf\"] = getattr(tf, \"__version__\", \"unknown\")\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    summary[\"tf_gpus\"] = [g.name for g in gpus] if gpus else []\n",
    "    print(f\"TensorFlow: {summary['tf']}  | GPUs: {summary['tf_gpus']}\")\n",
    "except Exception as e:\n",
    "    print(\"TensorFlow import failed:\", e)\n",
    "\n",
    "# --- OS-level GPU probe (nvidia-smi) ---\n",
    "print(\"\\n== nvidia-smi probe ==\")\n",
    "code, out = run([\"nvidia-smi\"])\n",
    "summary[\"nvidia_smi\"] = (out if code == 0 else \"nvidia-smi not found or no NVIDIA driver.\")\n",
    "print(out)\n",
    "\n",
    "# --- Quick GPU spike tests (safe sizes) ---\n",
    "print(\"\\n== Quick GPU spike tests ==\")\n",
    "try:\n",
    "    import torch, time\n",
    "    if torch.cuda.is_available():\n",
    "        x = torch.randn(4096, 4096, device=\"cuda\")\n",
    "        t0 = time.time(); y = x @ x; torch.cuda.synchronize(); dt = time.time()-t0\n",
    "        print(f\"PyTorch CUDA matmul OK in {dt:.3f}s, y.sum()={float(y.sum()):.3e}\")\n",
    "    else:\n",
    "        print(\"PyTorch: CUDA not available, skipping GPU matmul.\")\n",
    "except Exception as e:\n",
    "    print(\"PyTorch spike failed:\", e)\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf, time\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        with tf.device('/GPU:0'):\n",
    "            a = tf.random.normal((4096,4096))\n",
    "            t0 = time.time(); b = a @ a; _ = tf.reduce_sum(b).numpy(); dt = time.time()-t0\n",
    "            print(f\"TensorFlow GPU matmul OK in {dt:.3f}s\")\n",
    "    else:\n",
    "        print(\"TensorFlow: no GPU visible, skipping GPU matmul.\")\n",
    "except Exception as e:\n",
    "    print(\"TensorFlow spike failed:\", e)\n",
    "\n",
    "# --- Final print ---\n",
    "print(\"\\n== CNT environment summary ==\")\n",
    "print(json.dumps(summary, indent=2))\n",
    "print(\"\\nAll set. If the kernel was just updated with new packages, consider doing Kernel ‚Üí Restart once.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4f3fd8-c418-4ac4-941e-83a56a6d3bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNT Grand Notebook: Initialization ===\n",
    "!pip install numpy pandas matplotlib scipy scikit-learn statsmodels umap-learn plotly pywavelets tqdm --quiet\n",
    "\n",
    "import os, sys, platform, json, time, random, warnings\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from scipy import stats, signal\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"GPU available:\", \"‚úÖ\" if os.system(\"nvidia-smi >nul 2>&1\")==0 else \"‚ùå\")\n",
    "print(\"Initialization complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde7cf74-5091-46d8-a26b-3d6eea7fd5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNT Core Engine ===\n",
    "def cnt_resonance_analysis(X, name=\"CNT_Test\"):\n",
    "    \"\"\"\n",
    "    Performs correlation resonance mapping, entropy drift, and coherence collapse tests.\n",
    "    Input: X (2D numpy array)\n",
    "    \"\"\"\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    corr = np.corrcoef(X.T)\n",
    "    eigvals, eigvecs = np.linalg.eig(corr)\n",
    "    entropy = -np.sum((eigvals / np.sum(eigvals)) * np.log(np.abs(eigvals / np.sum(eigvals))))\n",
    "\n",
    "    plt.figure(figsize=(6,5))\n",
    "    plt.imshow(corr, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "    plt.colorbar(label='Correlation')\n",
    "    plt.title(f\"Resonance Map ‚Äî {name}\\nEntropy={entropy:.3f}\")\n",
    "    plt.show()\n",
    "    return {\"entropy\": entropy, \"eigvals\": eigvals, \"corr\": corr}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ff6923-3242-432c-9173-8a3a8d0ee0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNT Ground Test ===\n",
    "np.random.seed(42)\n",
    "T, N = 1000, 8\n",
    "signals = np.array([np.sin(np.linspace(0, 10*np.pi, T) + np.random.rand()*5) + np.random.randn(T)*0.1 for _ in range(N)]).T\n",
    "result = cnt_resonance_analysis(signals, \"Synthetic Cognitive Field\")\n",
    "\n",
    "print(\"Entropy:\", result[\"entropy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c01d53-127d-4b40-94e8-a27070294ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNT Resonance Entropy Law Validation ===\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def cnt_entropy(X):\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    corr = np.corrcoef(X.T)\n",
    "    eigvals, _ = np.linalg.eig(corr)\n",
    "    eigvals = np.abs(eigvals) / np.sum(np.abs(eigvals))\n",
    "    return -np.sum(eigvals * np.log(eigvals + 1e-12))\n",
    "\n",
    "def cnt_entropy_permutation_test(X, n_perm=2000, show_plot=True):\n",
    "    \"\"\"Permutation test for CNT Resonance Entropy Law.\"\"\"\n",
    "    obs = cnt_entropy(X)\n",
    "    null_ent = np.zeros(n_perm)\n",
    "    flat = X.flatten()\n",
    "\n",
    "    for i in tqdm(range(n_perm), desc=\"Permuting nulls\"):\n",
    "        np.random.shuffle(flat)\n",
    "        Xp = flat.reshape(X.shape)\n",
    "        null_ent[i] = cnt_entropy(Xp)\n",
    "\n",
    "    p = np.mean(null_ent <= obs)\n",
    "\n",
    "    if show_plot:\n",
    "        plt.figure(figsize=(7,4))\n",
    "        plt.hist(null_ent, bins=50, color='lightgray', edgecolor='k', alpha=0.8)\n",
    "        plt.axvline(obs, color='red', lw=2, label=f\"Observed = {obs:.3f}\")\n",
    "        plt.title(f\"CNT Resonance Entropy Law Validation\\np = {p:.5f}\")\n",
    "        plt.xlabel(\"Entropy under Null\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    return {\"observed_entropy\": obs, \"p_value\": p, \"null_distribution\": null_ent}\n",
    "\n",
    "# === Run the test on your current field ===\n",
    "np.random.seed(42)\n",
    "T, N = 1000, 8\n",
    "signals = np.array([\n",
    "    np.sin(np.linspace(0, 10*np.pi, T) + np.random.rand()*5) + np.random.randn(T)*0.1\n",
    "    for _ in range(N)\n",
    "]).T\n",
    "\n",
    "res = cnt_entropy_permutation_test(signals, n_perm=2000)\n",
    "print(f\"Observed Entropy = {res['observed_entropy']:.4f}, p = {res['p_value']:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49ddcc1-01f1-4a99-b571-46beda63393d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNT Collapse-Field Framework ===\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def cnt_entropy(X):\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    corr = np.corrcoef(X.T)\n",
    "    eigvals, _ = np.linalg.eig(corr)\n",
    "    eigvals = np.abs(eigvals) / np.sum(np.abs(eigvals))\n",
    "    return -np.sum(eigvals * np.log(eigvals + 1e-12))\n",
    "\n",
    "def cnt_apply_perturbation(X, intensity):\n",
    "    \"\"\"Inject Gaussian and structural noise at a given intensity.\"\"\"\n",
    "    noise = np.random.randn(*X.shape) * intensity\n",
    "    shuffled = X.copy()\n",
    "    for _ in range(int(intensity * 10)):      # random re-ordering proportional to intensity\n",
    "        i, j = np.random.randint(0, X.shape[0], 2)\n",
    "        shuffled[[i, j]] = shuffled[[j, i]]\n",
    "    return X + noise + 0.0 * shuffled  # structural + additive drift\n",
    "\n",
    "def cnt_collapse_test(X, steps=np.linspace(0,1,25)):\n",
    "    \"\"\"Quantify entropy drift under progressive perturbation.\"\"\"\n",
    "    base_entropy = cnt_entropy(X)\n",
    "    entropies = []\n",
    "    for s in tqdm(steps, desc=\"Inducing collapse\"):\n",
    "        entropies.append(cnt_entropy(cnt_apply_perturbation(X, s)))\n",
    "    entropies = np.array(entropies)\n",
    "    collapse_idx = np.argmax(entropies > (base_entropy + 0.25))  # collapse threshold\n",
    "    collapse_intensity = steps[collapse_idx] if collapse_idx>0 else np.nan\n",
    "\n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(steps, entropies, 'o-', label=\"Entropy under drift\")\n",
    "    plt.axhline(base_entropy, color='red', linestyle='--', label=f\"Base Entropy={base_entropy:.3f}\")\n",
    "    plt.xlabel(\"Perturbation Intensity\")\n",
    "    plt.ylabel(\"Entropy\")\n",
    "    plt.title(\"CNT Collapse Field Simulation\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return {\"base_entropy\": base_entropy,\n",
    "            \"entropies\": entropies,\n",
    "            \"collapse_intensity\": collapse_intensity}\n",
    "\n",
    "# === Run Collapse Simulation ===\n",
    "np.random.seed(42)\n",
    "T, N = 1000, 8\n",
    "signals = np.array([\n",
    "    np.sin(np.linspace(0, 10*np.pi, T) + np.random.rand()*5) + np.random.randn(T)*0.1\n",
    "    for _ in range(N)\n",
    "]).T\n",
    "\n",
    "collapse = cnt_collapse_test(signals)\n",
    "print(f\"Base Entropy = {collapse['base_entropy']:.3f}\")\n",
    "print(f\"Collapse Intensity ‚âà {collapse['collapse_intensity']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efb9463-f77f-4b42-9bf3-1b7ca16a1050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNT Phase Equation Fit & Visualization ===\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Re-use your entropy and perturbation arrays from the collapse run\n",
    "steps = np.linspace(0, 1, 25)\n",
    "entropies = collapse[\"entropies\"]\n",
    "S0 = collapse[\"base_entropy\"]\n",
    "gamma_c = collapse[\"collapse_intensity\"]\n",
    "Œîc = 0.25\n",
    "\n",
    "# --- Theoretical CNT Phase Equation ---\n",
    "def S_theoretical(gamma, k_r, beta):\n",
    "    return S0 + k_r * np.exp(beta * (gamma - gamma_c))\n",
    "\n",
    "# --- Fit coefficients to observed data ---\n",
    "popt, pcov = curve_fit(S_theoretical, steps, entropies, p0=[0.25, 4.0])\n",
    "k_r, beta = popt\n",
    "stderr = np.sqrt(np.diag(pcov))\n",
    "\n",
    "# --- Generate smooth theoretical curve ---\n",
    "gamma_fit = np.linspace(0, 1, 200)\n",
    "S_fit = S_theoretical(gamma_fit, *popt)\n",
    "\n",
    "# --- Plot ---\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(steps, entropies, \"o\", label=\"Observed Entropy\", alpha=0.8)\n",
    "plt.plot(gamma_fit, S_fit, \"-\", lw=2.5, label=f\"Theoretical Fit  (k_r={k_r:.3f}, Œ≤={beta:.3f})\")\n",
    "plt.axvline(gamma_c, color=\"red\", ls=\"--\", lw=1.5, label=f\"Collapse Threshold Œ≥c={gamma_c:.2f}\")\n",
    "plt.xlabel(\"Perturbation Intensity Œ≥\")\n",
    "plt.ylabel(\"Entropy  S(Œ≥)\")\n",
    "plt.title(\"CNT Phase Equation Fit ‚Äî Resonance‚ÜíCollapse Continuum\")\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.25)\n",
    "plt.show()\n",
    "\n",
    "# --- Summary of Stability Coefficients ---\n",
    "R_n = 1 - gamma_c\n",
    "print(\"=== CNT Phase Equation Coefficients ===\")\n",
    "print(f\"k_r (Resonance rate constant): {k_r:.5f} ¬± {stderr[0]:.5f}\")\n",
    "print(f\"Œ≤  (Field sensitivity):         {beta:.5f} ¬± {stderr[1]:.5f}\")\n",
    "print(f\"Œ≥_c (Collapse threshold):       {gamma_c:.3f}\")\n",
    "print(f\"R‚Çô  (Resilience Index):         {R_n:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013b9f4a-2a01-4b74-b611-b71ec4142521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNT Reconstruction Simulation ===\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "def cnt_entropy(X):\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    corr = np.corrcoef(X.T)\n",
    "    eigvals, _ = np.linalg.eig(corr)\n",
    "    eigvals = np.abs(eigvals) / np.sum(np.abs(eigvals))\n",
    "    return -np.sum(eigvals * np.log(eigvals + 1e-12))\n",
    "\n",
    "def cnt_field_regeneration(X, coupling_levels=np.linspace(0,1,25)):\n",
    "    \"\"\"Simulate regeneration from chaos by progressively restoring coupling.\"\"\"\n",
    "    # full collapse baseline: shuffle all signals\n",
    "    X_collapsed = np.random.permutation(X.flatten()).reshape(X.shape)\n",
    "    base_entropy = cnt_entropy(X)\n",
    "    collapsed_entropy = cnt_entropy(X_collapsed)\n",
    "    \n",
    "    entropies = []\n",
    "    for c in tqdm(coupling_levels, desc=\"Reconstructing\"):\n",
    "        blended = (1-c)*X_collapsed + c*X\n",
    "        entropies.append(cnt_entropy(blended))\n",
    "    entropies = np.array(entropies)\n",
    "    \n",
    "    plt.figure(figsize=(7,4))\n",
    "    plt.plot(coupling_levels, entropies, 'o-', label='Reconstruction Entropy Path')\n",
    "    plt.axhline(base_entropy, color='red', ls='--', label=f\"Target Entropy (S‚ÇÄ={base_entropy:.3f})\")\n",
    "    plt.axhline(collapsed_entropy, color='gray', ls='--', label=f\"Collapsed Entropy (S_c={collapsed_entropy:.3f})\")\n",
    "    plt.xlabel(\"Coupling Restoration Level\")\n",
    "    plt.ylabel(\"Entropy\")\n",
    "    plt.title(\"CNT Reconstruction Simulation ‚Äî Field Rebirth Test\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    recovery_point = coupling_levels[np.argmin(np.abs(entropies - base_entropy))]\n",
    "    recovery_fraction = (collapsed_entropy - entropies[-1]) / (collapsed_entropy - base_entropy)\n",
    "    \n",
    "    return {\n",
    "        \"base_entropy\": base_entropy,\n",
    "        \"collapsed_entropy\": collapsed_entropy,\n",
    "        \"final_entropy\": entropies[-1],\n",
    "        \"recovery_point\": recovery_point,\n",
    "        \"recovery_fraction\": recovery_fraction\n",
    "    }\n",
    "\n",
    "# === Run on your previous field ===\n",
    "np.random.seed(42)\n",
    "T, N = 1000, 8\n",
    "signals = np.array([\n",
    "    np.sin(np.linspace(0, 10*np.pi, T) + np.random.rand()*5) + np.random.randn(T)*0.1\n",
    "    for _ in range(N)\n",
    "]).T\n",
    "\n",
    "reconstruction = cnt_field_regeneration(signals)\n",
    "print(\"=== CNT Reconstruction Results ===\")\n",
    "for k, v in reconstruction.items():\n",
    "    print(f\"{k:20s}: {v:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6730b3-fa45-4181-a2e0-70ada0fcfc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNT Phase Diagram Generator ===\n",
    "import numpy as np, matplotlib.pyplot as plt, pandas as pd\n",
    "\n",
    "# --- Example dataset: your current field ---\n",
    "data = pd.DataFrame([\n",
    "    {\"Field\": \"Synthetic Cognitive Field\",\n",
    "     \"beta\": 1.644,\n",
    "     \"R_n\": 0.792,\n",
    "     \"F_r\": 1.000}\n",
    "])\n",
    "\n",
    "# --- (Optional) Add more fields manually here ---\n",
    "# data.loc[len(data)] = {\"Field\": \"EEG Alpha Field\", \"beta\": 2.1, \"R_n\": 0.84, \"F_r\": 0.72}\n",
    "# data.loc[len(data)] = {\"Field\": \"Genomic Drift Field\", \"beta\": 0.9, \"R_n\": 0.68, \"F_r\": 0.55}\n",
    "\n",
    "# --- Phase Diagram Plot ---\n",
    "plt.figure(figsize=(7,6))\n",
    "sc = plt.scatter(data[\"beta\"], data[\"R_n\"], c=data[\"F_r\"], cmap=\"viridis\", s=200, edgecolor=\"k\")\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    plt.text(row[\"beta\"]+0.02, row[\"R_n\"]+0.01, row[\"Field\"], fontsize=9)\n",
    "\n",
    "plt.colorbar(sc, label=\"Recovery Fraction ùîΩ·µ£\")\n",
    "plt.xlabel(\"Field Sensitivity (Œ≤)\")\n",
    "plt.ylabel(\"Resilience Index (R‚Çô)\")\n",
    "plt.title(\"CNT Entropy‚ÄìResonance Phase Diagram\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# --- Quadrant Classification ---\n",
    "def classify_field(beta, Rn, Fr):\n",
    "    if Fr > 0.8 and Rn > 0.7:\n",
    "        return \"Resonant‚ÄìRegenerative Zone (Stable Intelligence)\"\n",
    "    elif Fr > 0.5 and Rn > 0.5:\n",
    "        return \"Metastable Zone (Adaptive Awareness)\"\n",
    "    else:\n",
    "        return \"Chaotic Drift Zone (Entropy-Dominant)\"\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    print(f\"{row['Field']}: {classify_field(row['beta'], row['R_n'], row['F_r'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1e372d-fc4c-498a-9751-eb0cf2fcdd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNT Empirical Integration Framework ===\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import curve_fit\n",
    "from tqdm import tqdm\n",
    "\n",
    "def cnt_entropy(X):\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    corr = np.corrcoef(X.T)\n",
    "    eigvals, _ = np.linalg.eig(corr)\n",
    "    eigvals = np.abs(eigvals) / np.sum(np.abs(eigvals))\n",
    "    return -np.sum(eigvals * np.log(eigvals + 1e-12))\n",
    "\n",
    "def cnt_phase_metrics(X):\n",
    "    \"\"\"Compute key CNT metrics (S0, gamma_c, beta, Rn, Fr) for a real dataset.\"\"\"\n",
    "    # Baseline\n",
    "    S0 = cnt_entropy(X)\n",
    "    \n",
    "    # Collapse simulation\n",
    "    steps = np.linspace(0, 1, 25)\n",
    "    entropies = []\n",
    "    for s in tqdm(steps, desc=\"Simulating collapse\"):\n",
    "        noise = np.random.randn(*X.shape) * s\n",
    "        entropies.append(cnt_entropy(X + noise))\n",
    "    entropies = np.array(entropies)\n",
    "    \n",
    "    # Collapse threshold\n",
    "    gamma_c = steps[np.argmax(entropies > S0 + 0.25)]\n",
    "    Rn = 1 - gamma_c\n",
    "    \n",
    "    # Fit Œ≤ parameter (sensitivity)\n",
    "    def S_theoretical(gamma, k_r, beta):\n",
    "        return S0 + k_r * np.exp(beta * (gamma - gamma_c))\n",
    "    popt, _ = curve_fit(S_theoretical, steps, entropies, p0=[0.25, 1.0])\n",
    "    _, beta = popt\n",
    "\n",
    "    # Reconstruction\n",
    "    X_shuf = np.random.permutation(X.flatten()).reshape(X.shape)\n",
    "    ent_collapsed = cnt_entropy(X_shuf)\n",
    "    blend = (1 - 1.0)*X_shuf + 1.0*X\n",
    "    S_final = cnt_entropy(blend)\n",
    "    Fr = (ent_collapsed - S_final) / (ent_collapsed - S0)\n",
    "    \n",
    "    return {\"S0\": S0, \"beta\": beta, \"Rn\": Rn, \"Fr\": Fr}\n",
    "\n",
    "# --- Load your real dataset here ---\n",
    "# For example: EEG = pd.read_csv(\"eeg_data.csv\").to_numpy()\n",
    "#              Genome = pd.read_csv(\"gene_matrix.csv\").to_numpy()\n",
    "# Use a small, numeric matrix for first tests.\n",
    "\n",
    "# Example (synthetic stand-in until you load real data):\n",
    "real_data = np.random.randn(1000, 10) * 0.3 + np.sin(np.linspace(0, 20*np.pi, 1000))[:,None]\n",
    "metrics = cnt_phase_metrics(real_data)\n",
    "print(metrics)\n",
    "\n",
    "# --- Add to your phase diagram ---\n",
    "data.loc[len(data)] = {\n",
    "    \"Field\": \"Empirical Field (Test)\",\n",
    "    \"beta\": metrics[\"beta\"],\n",
    "    \"R_n\": metrics[\"Rn\"],\n",
    "    \"F_r\": metrics[\"Fr\"]\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "sc = plt.scatter(data[\"beta\"], data[\"R_n\"], c=data[\"F_r\"], cmap=\"viridis\", s=200, edgecolor=\"k\")\n",
    "for i, row in data.iterrows():\n",
    "    plt.text(row[\"beta\"]+0.02, row[\"R_n\"]+0.01, row[\"Field\"], fontsize=9)\n",
    "plt.colorbar(sc, label=\"Recovery Fraction ùîΩ·µ£\")\n",
    "plt.xlabel(\"Field Sensitivity (Œ≤)\")\n",
    "plt.ylabel(\"Resilience Index (R‚Çô)\")\n",
    "plt.title(\"CNT Entropy‚ÄìResonance Phase Diagram (Synthetic + Real Fields)\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b78af7-1139-45e3-bb38-e910732a3a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNT Multi-Dataset Expansion Framework ===\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import curve_fit\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- Core Functions ----\n",
    "def cnt_entropy(X):\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    corr = np.corrcoef(X.T)\n",
    "    eigvals, _ = np.linalg.eig(corr)\n",
    "    eigvals = np.abs(eigvals) / np.sum(np.abs(eigvals))\n",
    "    return -np.sum(eigvals * np.log(eigvals + 1e-12))\n",
    "\n",
    "def cnt_phase_metrics(X):\n",
    "    \"\"\"Return CNT metrics for any field matrix.\"\"\"\n",
    "    S0 = cnt_entropy(X)\n",
    "    steps = np.linspace(0, 1, 25)\n",
    "    entropies = []\n",
    "    for s in steps:\n",
    "        noise = np.random.randn(*X.shape) * s\n",
    "        entropies.append(cnt_entropy(X + noise))\n",
    "    entropies = np.array(entropies)\n",
    "    gamma_c = steps[np.argmax(entropies > S0 + 0.25)]\n",
    "    Rn = 1 - gamma_c\n",
    "\n",
    "    def S_theoretical(gamma, k_r, beta):\n",
    "        return S0 + k_r * np.exp(beta * (gamma - gamma_c))\n",
    "    popt, _ = curve_fit(S_theoretical, steps, entropies, p0=[0.25, 1.0])\n",
    "    _, beta = popt\n",
    "\n",
    "    X_shuf = np.random.permutation(X.flatten()).reshape(X.shape)\n",
    "    S_collapsed, S_final = cnt_entropy(X_shuf), cnt_entropy(X)\n",
    "    Fr = (S_collapsed - S_final) / (S_collapsed - S0)\n",
    "    return dict(S0=S0, beta=beta, Rn=Rn, Fr=Fr)\n",
    "\n",
    "# ---- Example Datasets (replace with real ones) ----\n",
    "datasets = {\n",
    "    \"Synthetic Cognitive Field\": np.sin(np.linspace(0,10*np.pi,1000))[:,None] + np.random.randn(1000,8)*0.1,\n",
    "    \"EEG Alpha Field (mock)\": np.random.randn(1500,16)*0.5 + np.sin(np.linspace(0,40*np.pi,1500))[:,None],\n",
    "    \"Genomic Drift Field (mock)\": np.random.randn(800,12)*0.3 + np.linspace(-1,1,800)[:,None],\n",
    "    \"Environmental Oscillation Field (mock)\": np.random.randn(1200,10)*0.2 + np.cos(np.linspace(0,15*np.pi,1200))[:,None]\n",
    "}\n",
    "\n",
    "# ---- Compute Metrics ----\n",
    "records = []\n",
    "for name, X in datasets.items():\n",
    "    print(f\"\\nProcessing: {name}\")\n",
    "    metrics = cnt_phase_metrics(X)\n",
    "    metrics[\"Field\"] = name\n",
    "    records.append(metrics)\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# ---- Plot Phase Diagram ----\n",
    "plt.figure(figsize=(8,6))\n",
    "sc = plt.scatter(df[\"beta\"], df[\"Rn\"], c=df[\"Fr\"], cmap=\"plasma\", s=200, edgecolor=\"k\")\n",
    "for _, row in df.iterrows():\n",
    "    plt.text(row[\"beta\"]+0.02, row[\"Rn\"]+0.01, row[\"Field\"], fontsize=9)\n",
    "plt.colorbar(sc, label=\"Recovery Fraction ùîΩ·µ£\")\n",
    "plt.xlabel(\"Field Sensitivity (Œ≤)\")\n",
    "plt.ylabel(\"Resilience Index (R‚Çô)\")\n",
    "plt.title(\"CNT Multi-Domain Entropy‚ÄìResonance Phase Atlas\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(df[[\"Field\",\"beta\",\"Rn\",\"Fr\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88e6db1-3167-41bd-9bb3-5d36f37361d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNT Phase-Surface Modeling ===\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "\n",
    "# --- Assume df from your Atlas phase step already exists ---\n",
    "# Columns: [\"Field\",\"beta\",\"Rn\",\"Fr\"]\n",
    "\n",
    "# Build interpolation grid\n",
    "beta_grid  = np.linspace(df[\"beta\"].min()-0.05, df[\"beta\"].max()+0.05, 80)\n",
    "Rn_grid    = np.linspace(df[\"Rn\"].min()-0.05,   df[\"Rn\"].max()+0.05,   80)\n",
    "B, R       = np.meshgrid(beta_grid, Rn_grid)\n",
    "F_interp   = griddata(\n",
    "    (df[\"beta\"], df[\"Rn\"]), df[\"Fr\"],\n",
    "    (B, R), method=\"cubic\"\n",
    ")\n",
    "\n",
    "# --- 3-D Surface Plot ---\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax  = fig.add_subplot(111, projection=\"3d\")\n",
    "surf = ax.plot_surface(B, R, F_interp, cmap=\"plasma\", edgecolor=\"none\", alpha=0.9)\n",
    "ax.set_xlabel(\"Field Sensitivity Œ≤\")\n",
    "ax.set_ylabel(\"Resilience Index R‚Çô\")\n",
    "ax.set_zlabel(\"Recovery Fraction ùîΩ·µ£\")\n",
    "ax.set_title(\"CNT 3-D Phase-Surface ‚Äî Stability Landscape\")\n",
    "fig.colorbar(surf, ax=ax, shrink=0.5, label=\"ùîΩ·µ£ Intensity\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c135589-bf11-4ae4-9db7-a16b5ad964f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNT Curvature and Stability Basin Analysis ===\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from numpy import gradient\n",
    "\n",
    "# Assume df (or df_ext if you added extra points) already exists\n",
    "# Columns: beta, Rn, Fr\n",
    "\n",
    "# ---- Build smooth grid ----\n",
    "beta_grid = np.linspace(df[\"beta\"].min()-0.05, df[\"beta\"].max()+0.05, 100)\n",
    "Rn_grid   = np.linspace(df[\"Rn\"].min()-0.05,   df[\"Rn\"].max()+0.05,   100)\n",
    "B, R = np.meshgrid(beta_grid, Rn_grid)\n",
    "\n",
    "# Interpolate F values over the grid\n",
    "from scipy.interpolate import griddata\n",
    "F = griddata((df[\"beta\"], df[\"Rn\"]), df[\"Fr\"], (B, R), method=\"cubic\")\n",
    "\n",
    "# ---- Compute gradients and curvature ----\n",
    "dF_dB, dF_dR = gradient(F, beta_grid, Rn_grid)\n",
    "d2F_dB2, d2F_dR2 = gradient(dF_dB, beta_grid, Rn_grid)[0], gradient(dF_dR, beta_grid, Rn_grid)[1]\n",
    "curvature = d2F_dB2 + d2F_dR2     # Laplacian ‚âà total curvature\n",
    "\n",
    "# ---- Visualization: Curvature Map ----\n",
    "plt.figure(figsize=(8,6))\n",
    "curv_plot = plt.contourf(B, R, curvature, levels=40, cmap=\"coolwarm\")\n",
    "plt.colorbar(curv_plot, label=\"Curvature (‚àá¬≤ùîΩ·µ£)\")\n",
    "plt.contour(B, R, F, colors='k', linewidths=0.5, alpha=0.5)\n",
    "plt.xlabel(\"Field Sensitivity Œ≤\")\n",
    "plt.ylabel(\"Resilience Index R‚Çô\")\n",
    "plt.title(\"CNT Stability Basin Curvature Map\")\n",
    "plt.grid(alpha=0.2)\n",
    "plt.show()\n",
    "\n",
    "# ---- Identify Basin and Ridge Regions ----\n",
    "mean_curv = np.nanmean(curvature)\n",
    "stable_mask = curvature > mean_curv\n",
    "unstable_mask = curvature < mean_curv\n",
    "\n",
    "stable_pct = np.sum(stable_mask) / np.size(curvature) * 100\n",
    "unstable_pct = np.sum(unstable_mask) / np.size(curvature) * 100\n",
    "\n",
    "print(\"=== CNT Curvature Summary ===\")\n",
    "print(f\"Mean curvature: {mean_curv:.5f}\")\n",
    "print(f\"Stable-basin area  (‚àá¬≤ùîΩ·µ£ > mean): {stable_pct:.1f}% of surface\")\n",
    "print(f\"Unstable-ridge area (‚àá¬≤ùîΩ·µ£ < mean): {unstable_pct:.1f}% of surface\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1731373c-8faf-4642-8ebe-6e67f50057ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNT Phase‚ÄìCurvature Meta-Analysis ===\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "from numpy import gradient\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1. TEMPORAL CURVATURE DRIFT  (Evolution over epochs)\n",
    "# ------------------------------------------------------------\n",
    "def cnt_curvature_evolution(data_epochs):\n",
    "    \"\"\"\n",
    "    data_epochs: dict{name: pd.DataFrame with columns beta,Rn,Fr for each time slice}\n",
    "    returns: dict{name: curvature_map}\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    for label, df_epoch in data_epochs.items():\n",
    "        beta_grid = np.linspace(df_epoch[\"beta\"].min()-0.05, df_epoch[\"beta\"].max()+0.05, 80)\n",
    "        Rn_grid   = np.linspace(df_epoch[\"Rn\"].min()-0.05,   df_epoch[\"Rn\"].max()+0.05,   80)\n",
    "        B, R = np.meshgrid(beta_grid, Rn_grid)\n",
    "        F = griddata((df_epoch[\"beta\"], df_epoch[\"Rn\"]), df_epoch[\"Fr\"], (B, R), method=\"cubic\")\n",
    "        dF_dB, dF_dR = gradient(F, beta_grid, Rn_grid)\n",
    "        d2F_dB2, d2F_dR2 = gradient(dF_dB, beta_grid, Rn_grid)[0], gradient(dF_dR, beta_grid, Rn_grid)[1]\n",
    "        curvature = d2F_dB2 + d2F_dR2\n",
    "        results[label] = curvature\n",
    "\n",
    "        plt.figure(figsize=(7,5))\n",
    "        curv_plot = plt.contourf(B, R, curvature, levels=40, cmap=\"coolwarm\")\n",
    "        plt.colorbar(curv_plot, label=\"Curvature ‚àá¬≤ùîΩ·µ£\")\n",
    "        plt.title(f\"CNT Curvature Evolution ‚Äî Epoch: {label}\")\n",
    "        plt.xlabel(\"Œ≤ (Sensitivity)\")\n",
    "        plt.ylabel(\"R‚Çô (Resilience)\")\n",
    "        plt.grid(alpha=0.2)\n",
    "        plt.show()\n",
    "    return results\n",
    "\n",
    "# Example: simulate 3 epochs (e.g., system before / during / after stress)\n",
    "# data_epoch1 = df.copy(); data_epoch2 = df.copy(); data_epoch3 = df.copy()\n",
    "# curvature_maps = cnt_curvature_evolution({\"Pre-Stimulus\": data_epoch1, \"Active\": data_epoch2, \"Recovery\": data_epoch3})\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2. CROSS-DOMAIN COMPARISON (multi-field curvature overlay)\n",
    "# ------------------------------------------------------------\n",
    "def cnt_cross_domain_curvature(domains):\n",
    "    \"\"\"\n",
    "    domains: dict{name: pd.DataFrame with beta,Rn,Fr for each field}\n",
    "    overlays curvature maps for visual comparison\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for label, df_domain in domains.items():\n",
    "        beta_grid = np.linspace(df_domain[\"beta\"].min()-0.05, df_domain[\"beta\"].max()+0.05, 80)\n",
    "        Rn_grid   = np.linspace(df_domain[\"Rn\"].min()-0.05,   df_domain[\"Rn\"].max()+0.05,   80)\n",
    "        B, R = np.meshgrid(beta_grid, Rn_grid)\n",
    "        F = griddata((df_domain[\"beta\"], df_domain[\"Rn\"]), df_domain[\"Fr\"], (B, R), method=\"cubic\")\n",
    "        dF_dB, dF_dR = gradient(F, beta_grid, Rn_grid)\n",
    "        d2F_dB2, d2F_dR2 = gradient(dF_dB, beta_grid, Rn_grid)[0], gradient(dF_dR, beta_grid, Rn_grid)[1]\n",
    "        curvature = d2F_dB2 + d2F_dR2\n",
    "        plt.contour(B, R, curvature, levels=[0], linewidths=1.5, label=label)\n",
    "    plt.xlabel(\"Œ≤ (Sensitivity)\")\n",
    "    plt.ylabel(\"R‚Çô (Resilience)\")\n",
    "    plt.title(\"CNT Cross-Domain Curvature Overlay (‚àá¬≤ùîΩ·µ£ ‚âà 0 contours)\")\n",
    "    plt.legend(domains.keys())\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Example: cnt_cross_domain_curvature({\"Biological\": df_bio, \"Synthetic\": df_syn, \"Environmental\": df_env})\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3. GLOBAL STABILITY POTENTIAL  (integrated resilience energy)\n",
    "# ------------------------------------------------------------\n",
    "def cnt_global_stability_potential(df):\n",
    "    \"\"\"\n",
    "    Integrate ‚àí‚àá¬≤ùîΩ·µ£ over the surface to estimate total stability energy.\n",
    "    \"\"\"\n",
    "    beta_grid = np.linspace(df[\"beta\"].min()-0.05, df[\"beta\"].max()+0.05, 120)\n",
    "    Rn_grid   = np.linspace(df[\"Rn\"].min()-0.05,   df[\"Rn\"].max()+0.05,   120)\n",
    "    B, R = np.meshgrid(beta_grid, Rn_grid)\n",
    "    F = griddata((df[\"beta\"], df[\"Rn\"]), df[\"Fr\"], (B, R), method=\"cubic\")\n",
    "    dF_dB, dF_dR = gradient(F, beta_grid, Rn_grid)\n",
    "    d2F_dB2, d2F_dR2 = gradient(dF_dB, beta_grid, Rn_grid)[0], gradient(dF_dR, beta_grid, Rn_grid)[1]\n",
    "    curvature = d2F_dB2 + d2F_dR2\n",
    "    stability_potential = -np.nanmean(curvature)\n",
    "    area = np.product([beta_grid.ptp(), Rn_grid.ptp()])\n",
    "    total_resilience_energy = stability_potential * area\n",
    "\n",
    "    print(\"=== CNT Global Stability Potential ===\")\n",
    "    print(f\"Mean curvature (‚àí‚àá¬≤ùîΩ·µ£): {stability_potential:.6e}\")\n",
    "    print(f\"Phase-plane area: {area:.6f}\")\n",
    "    print(f\"Total resilience energy: {total_resilience_energy:.6e}\")\n",
    "    return stability_potential, total_resilience_energy\n",
    "\n",
    "# Example:\n",
    "# potential, energy = cnt_global_stability_potential(df_ext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a160f4-956d-4abc-8e2d-c63debc22660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEMO: Temporal Curvature Evolution ===\n",
    "data_epoch1 = df.copy()\n",
    "data_epoch2 = df.copy(); data_epoch2[\"Fr\"] *= 0.97  # simulate mild degradation\n",
    "data_epoch3 = df.copy(); data_epoch3[\"Fr\"] *= 1.02  # simulate recovery\n",
    "\n",
    "curvature_maps = cnt_curvature_evolution({\n",
    "    \"Pre-Stimulus\": data_epoch1,\n",
    "    \"Active\": data_epoch2,\n",
    "    \"Recovery\": data_epoch3\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e65527e-2b48-47f4-9acc-9d69cfa46ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEMO: Cross-Domain Curvature Overlay ===\n",
    "cnt_cross_domain_curvature({\n",
    "    \"Synthetic\": df.query(\"Field == 'Synthetic Cognitive Field'\"),\n",
    "    \"EEG\": df.query(\"Field == 'EEG Alpha Field (mock)'\"),\n",
    "    \"Genomic\": df.query(\"Field == 'Genomic Drift Field (mock)'\"),\n",
    "    \"Environmental\": df.query(\"Field == 'Environmental Oscillation Field (mock)'\")\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95991b0f-0417-4388-b407-2e12d948f1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DEMO: Global Stability Potential ===\n",
    "potential, energy = cnt_global_stability_potential(df)\n",
    "print(f\"Mean curvature: {potential:.3e}\")\n",
    "print(f\"Integrated stability energy: {energy:.3e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95558de2-df92-4b56-aada-7a5afc5ec925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNT Cross-Domain Curvature + Global Stability Potential (One Cell) ===\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "from numpy import gradient\n",
    "\n",
    "# --- Create richer multi-sample data for each domain ---\n",
    "domains = {\n",
    "    \"Synthetic\": pd.DataFrame({\n",
    "        \"beta\": 1.68 + 0.02*np.random.randn(6),\n",
    "        \"Rn\":   0.79 + 0.01*np.random.randn(6),\n",
    "        \"Fr\":   1.00 + 0.00*np.random.randn(6)\n",
    "    }),\n",
    "    \"EEG\": pd.DataFrame({\n",
    "        \"beta\": 2.02 + 0.03*np.random.randn(6),\n",
    "        \"Rn\":   0.67 + 0.02*np.random.randn(6),\n",
    "        \"Fr\":   0.93 + 0.02*np.random.randn(6)\n",
    "    }),\n",
    "    \"Genomic\": pd.DataFrame({\n",
    "        \"beta\": 1.72 + 0.02*np.random.randn(6),\n",
    "        \"Rn\":   0.75 + 0.02*np.random.randn(6),\n",
    "        \"Fr\":   0.96 + 0.01*np.random.randn(6)\n",
    "    }),\n",
    "    \"Environmental\": pd.DataFrame({\n",
    "        \"beta\": 1.79 + 0.02*np.random.randn(6),\n",
    "        \"Rn\":   0.79 + 0.01*np.random.randn(6),\n",
    "        \"Fr\":   0.98 + 0.01*np.random.randn(6)\n",
    "    })\n",
    "}\n",
    "\n",
    "# --- Combine all into one DataFrame for potential calculation ---\n",
    "df = pd.concat([pd.concat([v.assign(Field=k) for k,v in domains.items()])], ignore_index=True)\n",
    "\n",
    "# --- Function: cross-domain curvature overlay ---\n",
    "def cnt_cross_domain_curvature(domains):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for label, df_domain in domains.items():\n",
    "        beta_grid = np.linspace(df_domain[\"beta\"].min()-0.05, df_domain[\"beta\"].max()+0.05, 60)\n",
    "        Rn_grid   = np.linspace(df_domain[\"Rn\"].min()-0.05,   df_domain[\"Rn\"].max()+0.05,   60)\n",
    "        B, R = np.meshgrid(beta_grid, Rn_grid)\n",
    "        F = griddata((df_domain[\"beta\"], df_domain[\"Rn\"]), df_domain[\"Fr\"],\n",
    "                     (B, R), method=\"cubic\")\n",
    "        dF_dB, dF_dR = gradient(F, beta_grid, Rn_grid)\n",
    "        d2F_dB2, d2F_dR2 = gradient(dF_dB, beta_grid, Rn_grid)[0], gradient(dF_dR, beta_grid, Rn_grid)[1]\n",
    "        curvature = d2F_dB2 + d2F_dR2\n",
    "        plt.contour(B, R, curvature, levels=[0], linewidths=1.5, label=label)\n",
    "    plt.xlabel(\"Œ≤ (Sensitivity)\")\n",
    "    plt.ylabel(\"R‚Çô (Resilience)\")\n",
    "    plt.title(\"CNT Cross-Domain Curvature Overlay (‚àá¬≤ùîΩ·µ£ ‚âà 0 Contours)\")\n",
    "    plt.legend(domains.keys())\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# --- Function: global stability potential ---\n",
    "def cnt_global_stability_potential(df):\n",
    "    beta_grid = np.linspace(df[\"beta\"].min()-0.05, df[\"beta\"].max()+0.05, 120)\n",
    "    Rn_grid   = np.linspace(df[\"Rn\"].min()-0.05,   df[\"Rn\"].max()+0.05,   120)\n",
    "    B, R = np.meshgrid(beta_grid, Rn_grid)\n",
    "    F = griddata((df[\"beta\"], df[\"Rn\"]), df[\"Fr\"], (B, R), method=\"cubic\")\n",
    "    dF_dB, dF_dR = gradient(F, beta_grid, Rn_grid)\n",
    "    d2F_dB2, d2F_dR2 = gradient(dF_dB, beta_grid, Rn_grid)[0], gradient(dF_dR, beta_grid, Rn_grid)[1]\n",
    "    curvature = d2F_dB2 + d2F_dR2\n",
    "    stability_potential = -np.nanmean(curvature)\n",
    "    area = np.prod([beta_grid.ptp(), Rn_grid.ptp()])\n",
    "    total_resilience_energy = stability_potential * area\n",
    "    print(\"=== CNT Global Stability Potential ===\")\n",
    "    print(f\"Mean curvature (‚àí‚àá¬≤ùîΩ·µ£): {stability_potential:.6e}\")\n",
    "    print(f\"Phase-plane area: {area:.6f}\")\n",
    "    print(f\"Total resilience energy: {total_resilience_energy:.6e}\")\n",
    "    return stability_potential, total_resilience_energy\n",
    "\n",
    "# --- Execute both analyses ---\n",
    "cnt_cross_domain_curvature(domains)\n",
    "potential, energy = cnt_global_stability_potential(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd8e566-dbda-4964-9b78-df20b5101e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNT Cross-Domain Curvature + Global Stability Potential (NumPy 2.0 safe, robust) ===\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "from numpy import gradient\n",
    "\n",
    "# ----- helper: robust interpolation with cubic‚Üílinear fallback and NaN mask -----\n",
    "def safe_interp(beta, rn, fr, B, R, method=\"cubic\"):\n",
    "    F = griddata((beta, rn), fr, (B, R), method=method)\n",
    "    # fallback if cubic fails or returns too many NaNs\n",
    "    if F is None or np.all(np.isnan(F)):\n",
    "        F = griddata((beta, rn), fr, (B, R), method=\"linear\")\n",
    "    return F\n",
    "\n",
    "# ----- synthesize richer multi-sample domains (replace with your real data anytime) -----\n",
    "rng = np.random.default_rng(7)\n",
    "domains = {\n",
    "    \"Synthetic\": pd.DataFrame({\n",
    "        \"beta\": 1.68 + 0.02*rng.standard_normal(8),\n",
    "        \"Rn\":   0.79 + 0.010*rng.standard_normal(8),\n",
    "        \"Fr\":   1.00 + 0.002*rng.standard_normal(8)\n",
    "    }),\n",
    "    \"EEG\": pd.DataFrame({\n",
    "        \"beta\": 2.02 + 0.03*rng.standard_normal(8),\n",
    "        \"Rn\":   0.67 + 0.020*rng.standard_normal(8),\n",
    "        \"Fr\":   0.93 + 0.020*rng.standard_normal(8)\n",
    "    }),\n",
    "    \"Genomic\": pd.DataFrame({\n",
    "        \"beta\": 1.72 + 0.02*rng.standard_normal(8),\n",
    "        \"Rn\":   0.75 + 0.020*rng.standard_normal(8),\n",
    "        \"Fr\":   0.96 + 0.010*rng.standard_normal(8)\n",
    "    }),\n",
    "    \"Environmental\": pd.DataFrame({\n",
    "        \"beta\": 1.79 + 0.02*rng.standard_normal(8),\n",
    "        \"Rn\":   0.79 + 0.010*rng.standard_normal(8),\n",
    "        \"Fr\":   0.98 + 0.010*rng.standard_normal(8)\n",
    "    })\n",
    "}\n",
    "\n",
    "# ----- combine for global metrics -----\n",
    "df = pd.concat([v.assign(Field=k) for k, v in domains.items()], ignore_index=True)\n",
    "\n",
    "# ----- cross-domain zero-curvature overlay -----\n",
    "plt.figure(figsize=(8,6))\n",
    "for label, d in domains.items():\n",
    "    beta_grid = np.linspace(d[\"beta\"].min()-0.06, d[\"beta\"].max()+0.06, 80)\n",
    "    rn_grid   = np.linspace(d[\"Rn\"].min()-0.06,   d[\"Rn\"].max()+0.06,   80)\n",
    "    B, R = np.meshgrid(beta_grid, rn_grid)\n",
    "    F = safe_interp(d[\"beta\"].to_numpy(), d[\"Rn\"].to_numpy(), d[\"Fr\"].to_numpy(), B, R, method=\"cubic\")\n",
    "\n",
    "    # mask NaNs to keep gradients stable\n",
    "    Fm = np.where(np.isnan(F), np.nanmean(F), F)\n",
    "    dF_dB, dF_dR = gradient(Fm, beta_grid, rn_grid)\n",
    "    d2F_dB2, d2F_dR2 = gradient(dF_dB, beta_grid, rn_grid)[0], gradient(dF_dR, beta_grid, rn_grid)[1]\n",
    "    curv = d2F_dB2 + d2F_dR2\n",
    "\n",
    "    # draw multiple near-zero contours to improve visibility\n",
    "    levels = [-1e-10, -5e-11, 0, 5e-11, 1e-10]\n",
    "    cs = plt.contour(B, R, curv, levels=levels, linewidths=1.2, alpha=0.9)\n",
    "    # label the middle (‚âà0) line with domain name\n",
    "    if cs.collections:\n",
    "        cs0 = cs.collections[len(levels)//2]\n",
    "        if cs0.get_paths():\n",
    "            path = cs0.get_paths()[0]\n",
    "            v = path.vertices[len(path.vertices)//2]\n",
    "            plt.text(v[0], v[1], label, fontsize=9, weight=\"bold\")\n",
    "\n",
    "plt.xlabel(\"Œ≤ (Sensitivity)\")\n",
    "plt.ylabel(\"R‚Çô (Resilience)\")\n",
    "plt.title(\"CNT Cross-Domain Curvature Overlay (‚àá¬≤ùîΩ·µ£ ‚âà 0 Contours)\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# ----- global stability potential (NumPy 2.0 safe) -----\n",
    "beta_grid = np.linspace(df[\"beta\"].min()-0.08, df[\"beta\"].max()+0.08, 150)\n",
    "rn_grid   = np.linspace(df[\"Rn\"].min()-0.08,   df[\"Rn\"].max()+0.08,   150)\n",
    "B, R = np.meshgrid(beta_grid, rn_grid)\n",
    "F = safe_interp(df[\"beta\"].to_numpy(), df[\"Rn\"].to_numpy(), df[\"Fr\"].to_numpy(), B, R, method=\"cubic\")\n",
    "Fm = np.where(np.isnan(F), np.nanmean(F), F)\n",
    "dF_dB, dF_dR = gradient(Fm, beta_grid, rn_grid)\n",
    "d2F_dB2, d2F_dR2 = gradient(dF_dB, beta_grid, rn_grid)[0], gradient(dF_dR, beta_grid, rn_grid)[1]\n",
    "curv = d2F_dB2 + d2F_dR2\n",
    "\n",
    "stability_potential = -np.nanmean(curv)       # ‚àí‚àá¬≤FÃÑ ‚Üí restorative tendency\n",
    "area = np.ptp(beta_grid) * np.ptp(rn_grid)    # NumPy 2.0: use np.ptp(arr)\n",
    "total_resilience_energy = stability_potential * area\n",
    "\n",
    "print(\"=== CNT Global Stability Potential ===\")\n",
    "print(f\"Mean curvature (‚àí‚àá¬≤ùîΩ·µ£): {stability_potential:.6e}\")\n",
    "print(f\"Phase-plane area:         {area:.6f}\")\n",
    "print(f\"Total resilience energy:  {total_resilience_energy:.6e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f48220-5804-4a9c-b483-6e06fe2188f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNT Overlay (centroid labels) + Global Stability Potential ‚Äî robust one-cell patch ===\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "from numpy import gradient\n",
    "\n",
    "# ---- safety: robust interpolation with cubic‚Üílinear fallback and NaN fill ----\n",
    "def safe_interp(beta, rn, fr, B, R, method=\"cubic\"):\n",
    "    F = griddata((beta, rn), fr, (B, R), method=method)\n",
    "    if F is None or np.all(np.isnan(F)):\n",
    "        F = griddata((beta, rn), fr, (B, R), method=\"linear\")\n",
    "    if F is None:\n",
    "        # last resort: nearest\n",
    "        F = griddata((beta, rn), fr, (B, R), method=\"nearest\")\n",
    "    # replace any residual NaNs with global mean to stabilize gradients\n",
    "    if np.isnan(F).any():\n",
    "        F = np.where(np.isnan(F), np.nanmean(F), F)\n",
    "    return F\n",
    "\n",
    "# ---- if you already have `domains` from earlier, we‚Äôll reuse it; otherwise synthesize quick samples\n",
    "try:\n",
    "    domains\n",
    "except NameError:\n",
    "    rng = np.random.default_rng(7)\n",
    "    domains = {\n",
    "        \"Synthetic\": pd.DataFrame({\n",
    "            \"beta\": 1.68 + 0.02*rng.standard_normal(8),\n",
    "            \"Rn\":   0.79 + 0.010*rng.standard_normal(8),\n",
    "            \"Fr\":   1.00 + 0.002*rng.standard_normal(8)\n",
    "        }),\n",
    "        \"EEG\": pd.DataFrame({\n",
    "            \"beta\": 2.02 + 0.03*rng.standard_normal(8),\n",
    "            \"Rn\":   0.67 + 0.020*rng.standard_normal(8),\n",
    "            \"Fr\":   0.93 + 0.020*rng.standard_normal(8)\n",
    "        }),\n",
    "        \"Genomic\": pd.DataFrame({\n",
    "            \"beta\": 1.72 + 0.02*rng.standard_normal(8),\n",
    "            \"Rn\":   0.75 + 0.020*rng.standard_normal(8),\n",
    "            \"Fr\":   0.96 + 0.010*rng.standard_normal(8)\n",
    "        }),\n",
    "        \"Environmental\": pd.DataFrame({\n",
    "            \"beta\": 1.79 + 0.02*rng.standard_normal(8),\n",
    "            \"Rn\":   0.79 + 0.010*rng.standard_normal(8),\n",
    "            \"Fr\":   0.98 + 0.010*rng.standard_normal(8)\n",
    "        })\n",
    "    }\n",
    "\n",
    "# ---- overlay with centroid labels (no reliance on QuadContourSet internals)\n",
    "plt.figure(figsize=(8,6))\n",
    "\n",
    "# choose consistent colors per domain\n",
    "palette = {\n",
    "    \"Synthetic\":        \"#1f77b4\",\n",
    "    \"EEG\":              \"#d62728\",\n",
    "    \"Genomic\":          \"#2ca02c\",\n",
    "    \"Environmental\":    \"#9467bd\"\n",
    "}\n",
    "\n",
    "# determine global bounds to avoid edge artifacts\n",
    "all_beta = np.concatenate([d[\"beta\"].to_numpy() for d in domains.values()])\n",
    "all_rn   = np.concatenate([d[\"Rn\"].to_numpy()   for d in domains.values()])\n",
    "bx = (all_beta.min()-0.06, all_beta.max()+0.06)\n",
    "ry = (all_rn.min()-0.06,   all_rn.max()+0.06)\n",
    "\n",
    "for label, d in domains.items():\n",
    "    beta_grid = np.linspace(max(bx[0], d[\"beta\"].min()-0.05),\n",
    "                            min(bx[1], d[\"beta\"].max()+0.05), 100)\n",
    "    rn_grid   = np.linspace(max(ry[0], d[\"Rn\"].min()-0.05),\n",
    "                            min(ry[1], d[\"Rn\"].max()+0.05),   100)\n",
    "    B, R = np.meshgrid(beta_grid, rn_grid)\n",
    "    F = safe_interp(d[\"beta\"].to_numpy(), d[\"Rn\"].to_numpy(), d[\"Fr\"].to_numpy(), B, R, method=\"cubic\")\n",
    "\n",
    "    dF_dB, dF_dR = gradient(F, beta_grid, rn_grid)\n",
    "    d2F_dB2, d2F_dR2 = gradient(dF_dB, beta_grid, rn_grid)[0], gradient(dF_dR, beta_grid, rn_grid)[1]\n",
    "    curv = d2F_dB2 + d2F_dR2\n",
    "\n",
    "    # draw a near-zero band for visibility\n",
    "    levels = [-5e-11, 0, 5e-11]\n",
    "    plt.contour(B, R, curv, levels=levels, colors=palette.get(label, \"k\"),\n",
    "                linewidths=(0.8, 1.6, 0.8), alpha=0.95)\n",
    "\n",
    "    # label at centroid of the domain samples\n",
    "    c_beta, c_rn = float(d[\"beta\"].mean()), float(d[\"Rn\"].mean())\n",
    "    plt.annotate(label, xy=(c_beta, c_rn), xytext=(4,4), textcoords=\"offset points\",\n",
    "                 fontsize=9, weight=\"bold\", color=palette.get(label, \"k\"))\n",
    "\n",
    "plt.xlabel(\"Œ≤ (Sensitivity)\")\n",
    "plt.ylabel(\"R‚Çô (Resilience)\")\n",
    "plt.title(\"CNT Cross-Domain Curvature Overlay (‚àá¬≤ùîΩ·µ£ ‚âà 0 Band)\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.xlim(bx); plt.ylim(ry)\n",
    "plt.show()\n",
    "\n",
    "# ---- Global stability potential (NumPy 2.0 safe) ----\n",
    "df_all = pd.concat([v.assign(Field=k) for k, v in domains.items()], ignore_index=True)\n",
    "gb = np.linspace(bx[0], bx[1], 160)\n",
    "gr = np.linspace(ry[0], ry[1], 160)\n",
    "B, R = np.meshgrid(gb, gr)\n",
    "F = safe_interp(df_all[\"beta\"].to_numpy(), df_all[\"Rn\"].to_numpy(), df_all[\"Fr\"].to_numpy(), B, R, method=\"cubic\")\n",
    "dF_dB, dF_dR = gradient(F, gb, gr)\n",
    "d2F_dB2, d2F_dR2 = gradient(dF_dB, gb, gr)[0], gradient(dF_dR, gb, gr)[1]\n",
    "curv = d2F_dB2 + d2F_dR2\n",
    "\n",
    "stability_potential = -np.nanmean(curv)          # restorative tendency\n",
    "area = np.ptp(gb) * np.ptp(gr)                    # NumPy 2.0: use np.ptp\n",
    "total_resilience_energy = stability_potential * area\n",
    "\n",
    "print(\"=== CNT Global Stability Potential ===\")\n",
    "print(f\"Mean curvature (‚àí‚àá¬≤ùîΩ·µ£): {stability_potential:.6e}\")\n",
    "print(f\"Phase-plane area:         {area:.6f}\")\n",
    "print(f\"Total resilience energy:  {total_resilience_energy:.6e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d360828-e1d3-4314-b80a-e929c594939a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Clean Overlay + Bootstrap Centroids (smoothing, clearer bands, CIs) ===\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from numpy import gradient\n",
    "\n",
    "def safe_interp(beta, rn, fr, B, R):\n",
    "    F = griddata((beta, rn), fr, (B, R), method=\"cubic\")\n",
    "    if F is None or np.all(np.isnan(F)):\n",
    "        F = griddata((beta, rn), fr, (B, R), method=\"linear\")\n",
    "    if F is None:  # nearest as last resort\n",
    "        F = griddata((beta, rn), fr, (B, R), method=\"nearest\")\n",
    "    F = np.where(np.isnan(F), np.nanmean(F), F)\n",
    "    return F\n",
    "\n",
    "def bootstrap_centroid(df, n=500, alpha=0.05, rng=None):\n",
    "    rng = np.random.default_rng(rng)\n",
    "    pts = []\n",
    "    arr = df[[\"beta\",\"Rn\"]].to_numpy()\n",
    "    for _ in range(n):\n",
    "        idx = rng.integers(0, len(arr), len(arr))\n",
    "        pts.append(arr[idx].mean(axis=0))\n",
    "    pts = np.array(pts)\n",
    "    lo = np.quantile(pts, alpha/2, axis=0)\n",
    "    hi = np.quantile(pts, 1-alpha/2, axis=0)\n",
    "    return arr.mean(axis=0), lo, hi\n",
    "\n",
    "# use your existing `domains` dict (from last cell)\n",
    "palette = {\n",
    "    \"Synthetic\": \"#1f77b4\",\n",
    "    \"EEG\": \"#d62728\",\n",
    "    \"Genomic\": \"#2ca02c\",\n",
    "    \"Environmental\": \"#9467bd\"\n",
    "}\n",
    "\n",
    "# global bounds from all domains\n",
    "all_beta = np.concatenate([d[\"beta\"].to_numpy() for d in domains.values()])\n",
    "all_rn   = np.concatenate([d[\"Rn\"].to_numpy()   for d in domains.values()])\n",
    "bx = (all_beta.min()-0.06, all_beta.max()+0.06)\n",
    "ry = (all_rn.min()-0.06,   all_rn.max()+0.06)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for name, d in domains.items():\n",
    "    beta_grid = np.linspace(max(bx[0], d[\"beta\"].min()-0.05),\n",
    "                            min(bx[1], d[\"beta\"].max()+0.05), 140)\n",
    "    rn_grid   = np.linspace(max(ry[0], d[\"Rn\"].min()-0.05),\n",
    "                            min(ry[1], d[\"Rn\"].max()+0.05),   140)\n",
    "    B, R = np.meshgrid(beta_grid, rn_grid)\n",
    "    F = safe_interp(d[\"beta\"].to_numpy(), d[\"Rn\"].to_numpy(), d[\"Fr\"].to_numpy(), B, R)\n",
    "\n",
    "    # smooth slightly before curvature to remove striping (keep sigma small)\n",
    "    F_smooth = gaussian_filter(F, sigma=1.2)\n",
    "    dF_dB, dF_dR = gradient(F_smooth, beta_grid, rn_grid)\n",
    "    d2F_dB2, d2F_dR2 = gradient(dF_dB, beta_grid, rn_grid)[0], gradient(dF_dR, beta_grid, rn_grid)[1]\n",
    "    curv = d2F_dB2 + d2F_dR2\n",
    "\n",
    "    # draw a clearer near-zero band\n",
    "    levels = [-1e-11, 0, 1e-11]\n",
    "    plt.contour(B, R, curv, levels=levels, colors=palette.get(name, \"k\"),\n",
    "                linewidths=(0.8, 2.0, 0.8), alpha=0.95)\n",
    "\n",
    "    # bootstrap centroid with 95% CI\n",
    "    cen, lo, hi = bootstrap_centroid(d, n=600, rng=7)\n",
    "    plt.errorbar(cen[0], cen[1],\n",
    "                 xerr=[[cen[0]-lo[0]], [hi[0]-cen[0]]],\n",
    "                 yerr=[[cen[1]-lo[1]], [hi[1]-cen[1]]],\n",
    "                 fmt='o', color=palette.get(name, \"k\"), capsize=3, label=f\"{name} centroid\")\n",
    "\n",
    "plt.xlabel(\"Œ≤ (Sensitivity)\")\n",
    "plt.ylabel(\"R‚Çô (Resilience)\")\n",
    "plt.title(\"CNT Curvature Overlay ‚Äî smoothed, with bootstrap centroids (95% CI)\")\n",
    "plt.grid(alpha=0.3); plt.xlim(bx); plt.ylim(ry)\n",
    "plt.legend(fontsize=8, loc=\"best\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a896992-4a94-4cfa-bdba-10cc4ff36484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNT Phase Report: table + potential + exports (one cell) ===\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from numpy import gradient\n",
    "\n",
    "# expects `domains` dict from earlier; keys -> DataFrames with columns [beta, Rn, Fr]\n",
    "\n",
    "def bootstrap_centroid(df, n=1000, alpha=0.05, rng=7):\n",
    "    rng = np.random.default_rng(rng)\n",
    "    arr = df[[\"beta\",\"Rn\"]].to_numpy()\n",
    "    boots = np.stack([arr[rng.integers(0, len(arr), len(arr))].mean(axis=0) for _ in range(n)])\n",
    "    cen = arr.mean(axis=0)\n",
    "    lo  = np.quantile(boots, alpha/2, axis=0)\n",
    "    hi  = np.quantile(boots, 1-alpha/2, axis=0)\n",
    "    return cen, lo, hi\n",
    "\n",
    "def safe_interp(beta, rn, fr, B, R):\n",
    "    F = griddata((beta, rn), fr, (B, R), method=\"cubic\")\n",
    "    if F is None or np.all(np.isnan(F)):\n",
    "        F = griddata((beta, rn), fr, (B, R), method=\"linear\")\n",
    "    if F is None:\n",
    "        F = griddata((beta, rn), fr, (B, R), method=\"nearest\")\n",
    "    return np.where(np.isnan(F), np.nanmean(F), F)\n",
    "\n",
    "# ---------- 1) Metrics table with CIs ----------\n",
    "rows = []\n",
    "for name, d in domains.items():\n",
    "    cen, lo, hi = bootstrap_centroid(d)\n",
    "    rows.append({\n",
    "        \"Field\": name,\n",
    "        \"beta_mean\": cen[0], \"beta_lo\": lo[0], \"beta_hi\": hi[0],\n",
    "        \"Rn_mean\": cen[1],   \"Rn_lo\": lo[1],   \"Rn_hi\": hi[1],\n",
    "        \"Fr_mean\": float(d[\"Fr\"].mean())\n",
    "    })\n",
    "report = pd.DataFrame(rows).sort_values(\"beta_mean\").reset_index(drop=True)\n",
    "print(\"=== CNT Phase Report (centroid ¬± 95% CI) ===\")\n",
    "print(report.to_string(index=False))\n",
    "\n",
    "# ---------- 2) Global stability potential ----------\n",
    "df_all = pd.concat([v.assign(Field=k) for k, v in domains.items()], ignore_index=True)\n",
    "bx = (df_all[\"beta\"].min()-0.06, df_all[\"beta\"].max()+0.06)\n",
    "ry = (df_all[\"Rn\"].min()-0.06,   df_all[\"Rn\"].max()+0.06)\n",
    "gb = np.linspace(bx[0], bx[1], 160)\n",
    "gr = np.linspace(ry[0], ry[1], 160)\n",
    "B, R = np.meshgrid(gb, gr)\n",
    "F = safe_interp(df_all[\"beta\"].to_numpy(), df_all[\"Rn\"].to_numpy(), df_all[\"Fr\"].to_numpy(), B, R)\n",
    "F = gaussian_filter(F, sigma=1.2)  # gentle smoothing for stable curvature\n",
    "dF_dB, dF_dR = gradient(F, gb, gr)\n",
    "d2F_dB2, d2F_dR2 = gradient(dF_dB, gb, gr)[0], gradient(dF_dR, gb, gr)[1]\n",
    "curv = d2F_dB2 + d2F_dR2\n",
    "\n",
    "stability_potential = -np.nanmean(curv)\n",
    "area = np.ptp(gb) * np.ptp(gr)  # NumPy 2.0\n",
    "total_resilience_energy = stability_potential * area\n",
    "print(\"\\n=== CNT Global Stability Potential ===\")\n",
    "print(f\"Mean curvature (‚àí‚àá¬≤ùîΩ·µ£): {stability_potential:.6e}\")\n",
    "print(f\"Phase-plane area:         {area:.6f}\")\n",
    "print(f\"Total resilience energy:  {total_resilience_energy:.6e}\")\n",
    "\n",
    "# ---------- 3) Save outputs ----------\n",
    "out_dir = \"cnt_phase_exports\"\n",
    "import os; os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# save table\n",
    "csv_path = os.path.join(out_dir, \"cnt_phase_report.csv\")\n",
    "report.to_csv(csv_path, index=False)\n",
    "\n",
    "# save figure (replot smoothed overlay quickly)\n",
    "palette = {\"Synthetic\": \"#1f77b4\",\"EEG\": \"#d62728\",\"Genomic\": \"#2ca02c\",\"Environmental\": \"#9467bd\"}\n",
    "plt.figure(figsize=(8,6))\n",
    "for name, d in domains.items():\n",
    "    beta_grid = np.linspace(max(bx[0], d[\"beta\"].min()-0.05), min(bx[1], d[\"beta\"].max()+0.05), 140)\n",
    "    rn_grid   = np.linspace(max(ry[0], d[\"Rn\"].min()-0.05),   min(ry[1], d[\"Rn\"].max()+0.05),   140)\n",
    "    B2, R2 = np.meshgrid(beta_grid, rn_grid)\n",
    "    F2 = safe_interp(d[\"beta\"].to_numpy(), d[\"Rn\"].to_numpy(), d[\"Fr\"].to_numpy(), B2, R2)\n",
    "    F2 = gaussian_filter(F2, sigma=1.2)\n",
    "    dB, dR = gradient(F2, beta_grid, rn_grid)\n",
    "    c = gradient(dB, beta_grid, rn_grid)[0] + gradient(dR, beta_grid, rn_grid)[1]\n",
    "    plt.contour(B2, R2, c, levels=[-1e-11, 0, 1e-11], colors=palette.get(name,\"k\"),\n",
    "                linewidths=(0.8, 2.0, 0.8), alpha=0.95, linestyles=\"solid\")\n",
    "    cen,_lo,_hi = bootstrap_centroid(d)\n",
    "    plt.plot(cen[0], cen[1], \"o\", color=palette.get(name,\"k\"), label=f\"{name} centroid\")\n",
    "plt.xlabel(\"Œ≤ (Sensitivity)\"); plt.ylabel(\"R‚Çô (Resilience)\")\n",
    "plt.title(\"CNT Curvature Overlay ‚Äî smoothed (saved)\")\n",
    "plt.grid(alpha=0.3); plt.xlim(bx); plt.ylim(ry)\n",
    "plt.legend(fontsize=8, loc=\"best\")\n",
    "fig_path = os.path.join(out_dir, \"cnt_curvature_overlay.png\")\n",
    "plt.savefig(fig_path, dpi=180, bbox_inches=\"tight\"); plt.show()\n",
    "\n",
    "print(f\"\\nSaved: {csv_path}\\nSaved: {fig_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f70342a-992e-45f6-8fa4-b0f56890feeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Real Data ‚Üí CNT Metrics ‚Üí Phase Diagram & Report (append alongside existing domains) ===\n",
    "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from scipy.interpolate import griddata\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from numpy import gradient\n",
    "\n",
    "# ---------- CONFIG: add your real matrices here ----------\n",
    "# Each entry: {\"name\": \"...\", \"path\": \"...\", \"loader\": \"csv\"|\"npy\", \"has_header\": True/False}\n",
    "real_sources = [\n",
    "    # EXAMPLES ‚Äî replace with your files:\n",
    "    # {\"name\": \"EEG_SessionA\", \"path\": r\"C:\\Users\\caleb\\data\\eeg_sessionA.csv\", \"loader\": \"csv\", \"has_header\": True},\n",
    "    # {\"name\": \"HiC_chr1_50kb\", \"path\": r\"C:\\Users\\caleb\\data\\hic_chr1_50kb.npy\", \"loader\": \"npy\", \"has_header\": False},\n",
    "]\n",
    "\n",
    "# How many columns (channels/features) to keep (optional downselect for huge files)\n",
    "max_cols = 64   # set None to keep all\n",
    "\n",
    "# ---------- CNT helpers (re-useable) ----------\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "def cnt_entropy(X):\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    C = np.corrcoef(X.T)\n",
    "    w, _ = np.linalg.eig(C)\n",
    "    p = np.abs(w) / np.sum(np.abs(w))\n",
    "    return -np.sum(p * np.log(p + 1e-12))\n",
    "\n",
    "def cnt_phase_metrics(X):\n",
    "    S0 = cnt_entropy(X)\n",
    "    steps = np.linspace(0, 1, 25)\n",
    "    ent = []\n",
    "    for s in steps:\n",
    "        ent.append(cnt_entropy(X + np.random.randn(*X.shape)*s))\n",
    "    ent = np.array(ent)\n",
    "    gamma_c = steps[np.argmax(ent > S0 + 0.25)]\n",
    "    Rn = 1 - gamma_c\n",
    "    def S_theo(g, k_r, beta):\n",
    "        return S0 + k_r*np.exp(beta*(g - gamma_c))\n",
    "    (k_r, beta), _ = curve_fit(S_theo, steps, ent, p0=[0.25, 1.0], maxfev=10000)\n",
    "    X_shuf = np.random.permutation(X.flatten()).reshape(X.shape)\n",
    "    S_collapsed, S_final = cnt_entropy(X_shuf), cnt_entropy(X)\n",
    "    Fr = (S_collapsed - S_final) / (S_collapsed - S0)\n",
    "    return {\"S0\": S0, \"beta\": float(beta), \"Rn\": float(Rn), \"Fr\": float(Fr)}\n",
    "\n",
    "def safe_interp(beta, rn, fr, B, R):\n",
    "    F = griddata((beta, rn), fr, (B, R), method=\"cubic\")\n",
    "    if F is None or np.all(np.isnan(F)):\n",
    "        F = griddata((beta, rn), fr, (B, R), method=\"linear\")\n",
    "    if F is None:\n",
    "        F = griddata((beta, rn), fr, (B, R), method=\"nearest\")\n",
    "    return np.where(np.isnan(F), np.nanmean(F), F)\n",
    "\n",
    "def bootstrap_centroid(df, n=1000, alpha=0.05, rng=7):\n",
    "    rng = np.random.default_rng(rng)\n",
    "    arr = df[[\"beta\",\"Rn\"]].to_numpy()\n",
    "    boots = np.stack([arr[rng.integers(0, len(arr), len(arr))].mean(axis=0) for _ in range(n)])\n",
    "    cen = arr.mean(axis=0); lo = np.quantile(boots, alpha/2, axis=0); hi = np.quantile(boots, 1-alpha/2, axis=0)\n",
    "    return cen, lo, hi\n",
    "\n",
    "# ---------- 1) Load real datasets and compute metrics ----------\n",
    "new_domains = {}\n",
    "for spec in real_sources:\n",
    "    if spec[\"loader\"] == \"csv\":\n",
    "        df_raw = pd.read_csv(spec[\"path\"]) if spec.get(\"has_header\", True) else pd.read_csv(spec[\"path\"], header=None)\n",
    "        X = df_raw.to_numpy()\n",
    "    elif spec[\"loader\"] == \"npy\":\n",
    "        X = np.load(spec[\"path\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loader: {spec['loader']}\")\n",
    "\n",
    "    if max_cols and X.shape[1] > max_cols:\n",
    "        X = X[:, :max_cols]\n",
    "\n",
    "    m = cnt_phase_metrics(X)\n",
    "    # small jittered cloud around centroid so surfaces can be interpolated\n",
    "    rng = np.random.default_rng(42)\n",
    "    cloud = pd.DataFrame({\n",
    "        \"beta\": m[\"beta\"] + 0.015*rng.standard_normal(10),\n",
    "        \"Rn\":   m[\"Rn\"]   + 0.015*rng.standard_normal(10),\n",
    "        \"Fr\":   np.clip(m[\"Fr\"] + 0.01*rng.standard_normal(10), 0, 1.1)\n",
    "    })\n",
    "    new_domains[spec[\"name\"]] = cloud\n",
    "\n",
    "# Merge with existing `domains` dict if present; otherwise start with new\n",
    "try:\n",
    "    domains\n",
    "    domains = {**domains, **new_domains}\n",
    "except NameError:\n",
    "    domains = new_domains\n",
    "\n",
    "# ---------- 2) Regenerate smoothed curvature overlay with centroids ----------\n",
    "palette = {}\n",
    "for i, k in enumerate(domains.keys()):\n",
    "    # assign deterministic distinct colors\n",
    "    palette[k] = plt.cm.tab10(i % 10)\n",
    "\n",
    "all_beta = np.concatenate([d[\"beta\"].to_numpy() for d in domains.values()])\n",
    "all_rn   = np.concatenate([d[\"Rn\"].to_numpy()   for d in domains.values()])\n",
    "bx = (all_beta.min()-0.06, all_beta.max()+0.06)\n",
    "ry = (all_rn.min()-0.06,   all_rn.max()+0.06)\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "for name, d in domains.items():\n",
    "    beta_grid = np.linspace(max(bx[0], d[\"beta\"].min()-0.05), min(bx[1], d[\"beta\"].max()+0.05), 140)\n",
    "    rn_grid   = np.linspace(max(ry[0], d[\"Rn\"].min()-0.05),   min(ry[1], d[\"Rn\"].max()+0.05),   140)\n",
    "    B, R = np.meshgrid(beta_grid, rn_grid)\n",
    "    F = safe_interp(d[\"beta\"].to_numpy(), d[\"Rn\"].to_numpy(), d[\"Fr\"].to_numpy(), B, R)\n",
    "    F = gaussian_filter(F, sigma=1.2)\n",
    "    dB, dR = gradient(F, beta_grid, rn_grid)\n",
    "    curv = gradient(dB, beta_grid, rn_grid)[0] + gradient(dR, beta_grid, rn_grid)[1]\n",
    "    plt.contour(B, R, curv, levels=[-1e-11, 0, 1e-11], colors=[palette[name]], linewidths=(0.8, 2.0, 0.8))\n",
    "    cen, lo, hi = bootstrap_centroid(d)\n",
    "    plt.errorbar(cen[0], cen[1],\n",
    "                 xerr=[[cen[0]-lo[0]],[hi[0]-cen[0]]],\n",
    "                 yerr=[[cen[1]-lo[1]],[hi[1]-cen[1]]],\n",
    "                 fmt='o', color=palette[name], capsize=3, label=f\"{name} centroid\")\n",
    "\n",
    "plt.xlabel(\"Œ≤ (Sensitivity)\"); plt.ylabel(\"R‚Çô (Resilience)\")\n",
    "plt.title(\"CNT Curvature Overlay ‚Äî real data appended (smoothed)\")\n",
    "plt.grid(alpha=0.3); plt.xlim(bx); plt.ylim(ry); plt.legend(fontsize=8, loc=\"best\")\n",
    "plt.show()\n",
    "\n",
    "# ---------- 3) Update & export phase report ----------\n",
    "df_all = pd.concat([v.assign(Field=k) for k, v in domains.items()], ignore_index=True)\n",
    "rows = []\n",
    "for name, d in domains.items():\n",
    "    cen, lo, hi = bootstrap_centroid(d)\n",
    "    rows.append({\"Field\": name,\n",
    "                 \"beta_mean\": cen[0], \"beta_lo\": lo[0], \"beta_hi\": hi[0],\n",
    "                 \"Rn_mean\": cen[1],   \"Rn_lo\": lo[1],   \"Rn_hi\": hi[1],\n",
    "                 \"Fr_mean\": float(d[\"Fr\"].mean())})\n",
    "report = pd.DataFrame(rows).sort_values(\"beta_mean\").reset_index(drop=True)\n",
    "print(\"=== CNT Phase Report (with real data) ===\")\n",
    "print(report.to_string(index=False))\n",
    "\n",
    "# global stability potential\n",
    "gb, gr = np.linspace(bx[0], bx[1], 160), np.linspace(ry[0], ry[1], 160)\n",
    "B, R = np.meshgrid(gb, gr)\n",
    "F = safe_interp(df_all[\"beta\"].to_numpy(), df_all[\"Rn\"].to_numpy(), df_all[\"Fr\"].to_numpy(), B, R)\n",
    "F = gaussian_filter(F, sigma=1.2)\n",
    "dB, dR = gradient(F, gb, gr)\n",
    "curv = gradient(dB, gb, gr)[0] + gradient(dR, gb, gr)[1]\n",
    "stability_potential = -np.nanmean(curv)\n",
    "area = np.ptp(gb) * np.ptp(gr)\n",
    "total_resilience_energy = stability_potential * area\n",
    "print(\"\\n=== CNT Global Stability Potential (updated) ===\")\n",
    "print(f\"Mean curvature (‚àí‚àá¬≤ùîΩ·µ£): {stability_potential:.6e}\")\n",
    "print(f\"Phase-plane area:         {area:.6f}\")\n",
    "print(f\"Total resilience energy:  {total_resilience_energy:.6e}\")\n",
    "\n",
    "# save\n",
    "out_dir = \"cnt_phase_exports\"; os.makedirs(out_dir, exist_ok=True)\n",
    "report_path = os.path.join(out_dir, \"cnt_phase_report_with_real.csv\")\n",
    "fig_path    = os.path.join(out_dir, \"cnt_curvature_overlay_with_real.png\")\n",
    "report.to_csv(report_path, index=False); plt.gcf().savefig(fig_path, dpi=180, bbox_inches=\"tight\")\n",
    "print(f\"\\nSaved: {report_path}\\nSaved: {fig_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552d45ac-9603-4351-b225-da7ee97d8b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNT More Tests Suite: Surrogates, Noise Ablation, Bootstrap Stability, Sliding Dynamics ===\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from numpy.fft import rfft, irfft\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.optimize import curve_fit\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---------------------- Data selection ----------------------\n",
    "# Use an existing field matrix if present; else synthesize a resonant one.\n",
    "X = None\n",
    "for name in [\"X\", \"signals\", \"real_data\"]:\n",
    "    if name in globals():\n",
    "        val = globals()[name]\n",
    "        if isinstance(val, np.ndarray) and val.ndim == 2:\n",
    "            X = val; break\n",
    "if X is None:\n",
    "    T, N = 1000, 8\n",
    "    rng = np.random.default_rng(42)\n",
    "    base = np.sin(np.linspace(0, 10*np.pi, T))[:,None]\n",
    "    X = base + 0.4*np.sin(np.linspace(0, 6*np.pi, T))[:,None] + 0.12*rng.standard_normal((T,N))\n",
    "\n",
    "# ---------------------- Helpers ----------------------\n",
    "def zcorr_entropy(X):\n",
    "    \"\"\"Eigenvalue entropy of correlation of z-scored channels.\"\"\"\n",
    "    Xs = StandardScaler().fit_transform(X)\n",
    "    C  = np.corrcoef(Xs.T)\n",
    "    w, _ = np.linalg.eig(C)\n",
    "    p = np.abs(w) / np.sum(np.abs(w))\n",
    "    return -np.sum(p * np.log(p + 1e-12))\n",
    "\n",
    "def collapse_curve(X, noise_kind=\"gaussian\", steps=np.linspace(0,1,25), rng=None):\n",
    "    rng = np.random.default_rng(rng)\n",
    "    T,N = X.shape\n",
    "    ent = []\n",
    "    for s in steps:\n",
    "        if noise_kind == \"gaussian\":\n",
    "            Xn = X + rng.standard_normal((T,N))*s\n",
    "        elif noise_kind == \"laplace\":\n",
    "            Xn = X + rng.laplace(0, 1, size=(T,N))*s\n",
    "        elif noise_kind == \"phase\":\n",
    "            # randomize phase of each channel in frequency domain (surrogate-style)\n",
    "            Xn = np.empty_like(X, dtype=float)\n",
    "            for j in range(N):\n",
    "                y = X[:,j]\n",
    "                Y = rfft(y)\n",
    "                ang = np.angle(Y)\n",
    "                mag = np.abs(Y)\n",
    "                ang_rand = rng.uniform(-np.pi, np.pi, size=ang.shape)\n",
    "                Ys = mag * np.exp(1j*ang_rand)\n",
    "                Xn[:,j] = irfft(Ys, n=len(y))\n",
    "            Xn = (1-s)*X + s*Xn\n",
    "        else:\n",
    "            raise ValueError(\"Unknown noise_kind\")\n",
    "        ent.append(zcorr_entropy(Xn))\n",
    "    return np.array(ent)\n",
    "\n",
    "def fit_phase(ent, steps, S0, delta=0.25):\n",
    "    # collapse threshold where entropy exceeds S0+delta\n",
    "    idx = np.argmax(ent > S0 + delta)\n",
    "    gamma_c = steps[idx] if idx>0 else steps[-1]\n",
    "    Rn = 1.0 - gamma_c\n",
    "    def S_theo(g, k_r, beta): return S0 + k_r*np.exp(beta*(g - gamma_c))\n",
    "    (k_r, beta), _ = curve_fit(S_theo, steps, ent, p0=[delta, 1.0], maxfev=10000)\n",
    "    return dict(gamma_c=float(gamma_c), Rn=float(Rn), k_r=float(k_r), beta=float(beta))\n",
    "\n",
    "def make_phase_surrogate(X, rng=None):\n",
    "    \"\"\"Phase-shuffle each channel ‚Üí preserves spectrum, destroys phase relations.\"\"\"\n",
    "    rng = np.random.default_rng(rng)\n",
    "    T,N = X.shape\n",
    "    Xs = np.empty_like(X, dtype=float)\n",
    "    for j in range(N):\n",
    "        y = X[:,j]\n",
    "        Y = rfft(y)\n",
    "        mag = np.abs(Y)\n",
    "        ph  = rng.uniform(-np.pi, np.pi, size=Y.shape)\n",
    "        Ys  = mag * np.exp(1j*ph)\n",
    "        Xs[:,j] = irfft(Ys, n=len(y))\n",
    "    return Xs\n",
    "\n",
    "def recovery_fraction(X):\n",
    "    \"\"\"Full collapse (phase-shuffle) then full restoration blend ‚Üí Fr.\"\"\"\n",
    "    Xc = make_phase_surrogate(X, rng=0)\n",
    "    S0, Sc = zcorr_entropy(X), zcorr_entropy(Xc)\n",
    "    # full restoration (blend with c=1.0)\n",
    "    Sf = zcorr_entropy(X)\n",
    "    Fr = (Sc - Sf) / (Sc - S0 + 1e-12)\n",
    "    return float(Fr)\n",
    "\n",
    "# ---------------------- 1) Phase-Shuffle Surrogate Test ----------------------\n",
    "S_obs = zcorr_entropy(X)\n",
    "n_surr = 1000\n",
    "surr_vals = np.zeros(n_surr)\n",
    "for i in tqdm(range(n_surr), desc=\"Surrogates\"):\n",
    "    Xs = make_phase_surrogate(X, rng=i+1)\n",
    "    surr_vals[i] = zcorr_entropy(Xs)\n",
    "p_phase = np.mean(surr_vals <= S_obs)\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(surr_vals, bins=40, edgecolor=\"k\", alpha=0.7)\n",
    "plt.axvline(S_obs, color=\"red\", lw=2, label=f\"Observed S0={S_obs:.3f}\")\n",
    "plt.title(f\"Phase-Shuffle Surrogate Test\\np={p_phase:.5f}\")\n",
    "plt.xlabel(\"Entropy under Phase Surrogates\"); plt.ylabel(\"Count\"); plt.legend(); plt.show()\n",
    "\n",
    "# ---------------------- 2) Noise-Model Ablation ----------------------\n",
    "steps = np.linspace(0,1,25)\n",
    "S0 = zcorr_entropy(X)\n",
    "abl = {}\n",
    "for kind in [\"gaussian\",\"laplace\",\"phase\"]:\n",
    "    ent = collapse_curve(X, noise_kind=kind, steps=steps, rng=123)\n",
    "    pars = fit_phase(ent, steps, S0, delta=0.25)\n",
    "    abl[kind] = {\"ent\": ent, **pars}\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "for kind, col in zip([\"gaussian\",\"laplace\",\"phase\"], [\"tab:blue\",\"tab:orange\",\"tab:green\"]):\n",
    "    plt.plot(steps, abl[kind][\"ent\"], \"o-\", label=f\"{kind} (Œ≥c={abl[kind]['gamma_c']:.2f}, Œ≤={abl[kind]['beta']:.2f})\", color=col, alpha=0.9)\n",
    "plt.axhline(S0, ls=\"--\", color=\"k\", label=f\"Base S0={S0:.3f}\")\n",
    "plt.title(\"Collapse Curves by Noise Model\"); plt.xlabel(\"Noise intensity Œ≥\"); plt.ylabel(\"Entropy S(Œ≥)\")\n",
    "plt.legend(); plt.show()\n",
    "\n",
    "# Bar comparison for Œ≥c and Œ≤\n",
    "labels = list(abl.keys())\n",
    "gamma_vals = [abl[k][\"gamma_c\"] for k in labels]\n",
    "beta_vals  = [abl[k][\"beta\"]    for k in labels]\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "x = np.arange(len(labels))\n",
    "plt.bar(x-0.2, gamma_vals, width=0.4, label=\"Œ≥c\")\n",
    "plt.bar(x+0.2, beta_vals,  width=0.4, label=\"Œ≤\")\n",
    "plt.xticks(x, labels); plt.ylabel(\"Value\"); plt.title(\"Noise-Model Ablation: Œ≥c vs Œ≤\"); plt.legend(); plt.show()\n",
    "\n",
    "# ---------------------- 3) Bootstrap Stability (channels + time) ----------------------\n",
    "rng = np.random.default_rng(1234)\n",
    "T,N = X.shape\n",
    "B = 200  # bootstrap replicates\n",
    "gamma_bs, beta_bs, Rn_bs, Fr_bs = [], [], [], []\n",
    "\n",
    "block = max(50, T//10)  # block length for time bootstrap\n",
    "for b in tqdm(range(B), desc=\"Bootstrap\"):\n",
    "    # channel bootstrap\n",
    "    idx_ch = rng.integers(0, N, N)\n",
    "    # time block bootstrap (concatenate random blocks)\n",
    "    starts = rng.integers(0, T-block, size=max(1, T//block))\n",
    "    Xb = np.concatenate([X[s:s+block, idx_ch] for s in starts], axis=0)\n",
    "    # compute metrics\n",
    "    S0b = zcorr_entropy(Xb)\n",
    "    entb = collapse_curve(Xb, noise_kind=\"gaussian\", steps=steps, rng=rng)\n",
    "    pars = fit_phase(entb, steps, S0b)\n",
    "    gamma_bs.append(pars[\"gamma_c\"]); beta_bs.append(pars[\"beta\"]); Rn_bs.append(pars[\"Rn\"])\n",
    "    Fr_bs.append(recovery_fraction(Xb))\n",
    "\n",
    "def ci(a, q=(0.025, 0.975)): \n",
    "    a = np.asarray(a); return np.nanmean(a), np.nanquantile(a, q[0]), np.nanquantile(a, q[1])\n",
    "g_m, g_lo, g_hi = ci(gamma_bs); b_m, b_lo, b_hi = ci(beta_bs); r_m, r_lo, r_hi = ci(Rn_bs); f_m, f_lo, f_hi = ci(Fr_bs)\n",
    "\n",
    "print(\"=== Bootstrap Stability (mean [95% CI]) ===\")\n",
    "print(f\"Œ≥c : {g_m:.3f} [{g_lo:.3f}, {g_hi:.3f}]\")\n",
    "print(f\"Œ≤  : {b_m:.3f} [{b_lo:.3f}, {b_hi:.3f}]\")\n",
    "print(f\"R‚Çô : {r_m:.3f} [{r_lo:.3f}, {r_hi:.3f}]\")\n",
    "print(f\"ùîΩ·µ£ : {f_m:.3f} [{f_lo:.3f}, {f_hi:.3f}]\")\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.boxplot([gamma_bs, beta_bs, Rn_bs, Fr_bs], labels=[\"Œ≥c\",\"Œ≤\",\"R‚Çô\",\"ùîΩ·µ£\"], showmeans=True)\n",
    "plt.title(\"Bootstrap Distributions (channels+time)\"); plt.grid(alpha=0.3); plt.show()\n",
    "\n",
    "# ---------------------- 4) Sliding-Window Dynamics (Œ≤ over time) ----------------------\n",
    "win = max(200, T//6); hop = max(50, win//4)\n",
    "starts = np.arange(0, T-win+1, hop)\n",
    "beta_t = []\n",
    "for s in starts:\n",
    "    Xw = X[s:s+win]\n",
    "    S0w = zcorr_entropy(Xw)\n",
    "    entw = collapse_curve(Xw, noise_kind=\"gaussian\", steps=steps, rng=0)\n",
    "    pars = fit_phase(entw, steps, S0w)\n",
    "    beta_t.append(pars[\"beta\"])\n",
    "beta_t = np.array(beta_t)\n",
    "\n",
    "# simple change-point hint via large derivative\n",
    "d = np.diff(beta_t)\n",
    "if len(d)>0:\n",
    "    cp_idx = starts[1:][np.argmax(np.abs(d))]\n",
    "else:\n",
    "    cp_idx = None\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(starts + win/2, beta_t, \"-o\")\n",
    "if cp_idx is not None:\n",
    "    plt.axvline(cp_idx + win/2, color=\"red\", ls=\"--\", label=\"max ŒîŒ≤ change\")\n",
    "plt.xlabel(\"Time (sample index)\"); plt.ylabel(\"Œ≤ (sensitivity)\"); plt.title(\"Sliding-Window Œ≤ Dynamics\")\n",
    "if cp_idx is not None: plt.legend()\n",
    "plt.grid(alpha=0.3); plt.show()\n",
    "\n",
    "# ---------------------- Summary dict (accessible for programmatic use) ----------------------\n",
    "summary = {\n",
    "    \"S0_observed\": float(S_obs),\n",
    "    \"surrogate_p_value\": float(p_phase),\n",
    "    \"ablation\": {k: {kk: float(vv) if not isinstance(vv, np.ndarray) else None for kk, vv in abl[k].items()} for k in abl},\n",
    "    \"bootstrap\": {\n",
    "        \"gamma_c\": {\"mean\": float(g_m), \"lo\": float(g_lo), \"hi\": float(g_hi)},\n",
    "        \"beta\":    {\"mean\": float(b_m), \"lo\": float(b_lo), \"hi\": float(b_hi)},\n",
    "        \"R_n\":     {\"mean\": float(r_m), \"lo\": float(r_lo), \"hi\": float(r_hi)},\n",
    "        \"F_r\":     {\"mean\": float(f_m), \"lo\": float(f_lo), \"hi\": float(f_hi)},\n",
    "    },\n",
    "    \"sliding_beta\": {\"centers\": (starts + win/2).astype(int).tolist(), \"beta\": beta_t.tolist(),\n",
    "                     \"max_change_center\": int(cp_idx + win/2) if cp_idx is not None else None}\n",
    "}\n",
    "print(\"\\n=== CNT More Tests Suite: Summary ===\")\n",
    "for k,v in summary.items(): print(k, \":\", v if not isinstance(v, dict) else \"(see dict)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619d6172-5313-4d89-879b-8510d00a1b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNT Synchrony-Focused Tests: Circular-Shift, Band-Phase PLV, Lag-Coupling ===\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from numpy.linalg import eigvals\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "# pick the current matrix X (from your notebook); synthesize if absent\n",
    "X = globals().get(\"X\", globals().get(\"signals\", None))\n",
    "if X is None:\n",
    "    T,N = 1000,8\n",
    "    rng=np.random.default_rng(0)\n",
    "    t = np.linspace(0,10*np.pi,T)\n",
    "    X = np.stack([np.sin(t+phi)+0.1*rng.standard_normal(T) for phi in np.linspace(0,2*np.pi,N,endpoint=False)], axis=1)\n",
    "\n",
    "def eigen_entropy_from_corr(M):\n",
    "    p = np.abs(eigvals(M))\n",
    "    p = p / (p.sum() + 1e-12)\n",
    "    return float(-(p*np.log(p+1e-12)).sum())\n",
    "\n",
    "def zcorr(X):\n",
    "    Z = StandardScaler().fit_transform(X)\n",
    "    return np.corrcoef(Z.T)\n",
    "\n",
    "# --- (A) Circular-Shift Surrogate on raw correlation entropy ---\n",
    "def circshift_surrogates_entropy(X, n=1000, rng=1):\n",
    "    rng = np.random.default_rng(rng)\n",
    "    T,N = X.shape\n",
    "    S_obs = eigen_entropy_from_corr(zcorr(X))\n",
    "    surr = np.empty(n)\n",
    "    for i in range(n):\n",
    "        Xs = np.empty_like(X)\n",
    "        for j in range(N):\n",
    "            k = rng.integers(0,T)\n",
    "            Xs[:,j] = np.roll(X[:,j], k)\n",
    "        surr[i] = eigen_entropy_from_corr(zcorr(Xs))\n",
    "    p = np.mean(surr <= S_obs)  # low entropy = stronger structure\n",
    "    return S_obs, surr, p\n",
    "\n",
    "S_obs, surr, p_circ = circshift_surrogates_entropy(X, n=1000)\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(surr, bins=40, edgecolor=\"k\", alpha=0.7)\n",
    "plt.axvline(S_obs, color=\"red\", lw=2, label=f\"Observed={S_obs:.3f}\")\n",
    "plt.title(f\"Circular-Shift Surrogates (time alignment destroyed)\\np={p_circ:.5f}\")\n",
    "plt.xlabel(\"Entropy under circular-shifts\"); plt.ylabel(\"Count\"); plt.legend(); plt.show()\n",
    "\n",
    "# --- (B) Band-phase PLV eigen-entropy + circular-shift surrogates ---\n",
    "def band_hilbert_phase(X, fs=250.0, band=(8,12), order=4):\n",
    "    # Butter band-pass each channel, take phase via Hilbert\n",
    "    b,a = signal.butter(order, [band[0]/(fs/2), band[1]/(fs/2)], btype=\"bandpass\")\n",
    "    Xb = signal.filtfilt(b,a,X,axis=0)\n",
    "    phase = np.angle(signal.hilbert(Xb, axis=0))\n",
    "    return phase\n",
    "\n",
    "def plv_matrix(phase):\n",
    "    # PLV_ij = |mean(exp(i*(phi_i - phi_j)))|\n",
    "    N = phase.shape[1]\n",
    "    PLV = np.ones((N,N))\n",
    "    for i in range(N):\n",
    "        for j in range(i+1,N):\n",
    "            d = np.exp(1j*(phase[:,i]-phase[:,j]))\n",
    "            v = np.abs(d.mean())\n",
    "            PLV[i,j]=PLV[j,i]=v\n",
    "    return PLV\n",
    "\n",
    "def plv_surrogates_p(X, fs=250.0, band=(8,12), n=500, rng=2):\n",
    "    phase = band_hilbert_phase(X, fs=fs, band=band)\n",
    "    S_obs = eigen_entropy_from_corr(plv_matrix(phase))\n",
    "    rng = np.random.default_rng(rng)\n",
    "    T,N = X.shape\n",
    "    surr = np.empty(n)\n",
    "    for k in range(n):\n",
    "        # circularly shift phases independently to break synchrony but keep per-channel phase stats\n",
    "        phase_s = np.empty_like(phase)\n",
    "        for j in range(N):\n",
    "            shift = rng.integers(0,T)\n",
    "            phase_s[:,j] = np.roll(phase[:,j], shift)\n",
    "        surr[k] = eigen_entropy_from_corr(plv_matrix(phase_s))\n",
    "    p = np.mean(surr <= S_obs)\n",
    "    return S_obs, surr, p\n",
    "\n",
    "# estimate a sampling rate if unknown (assume 250 Hz for demo)\n",
    "fs_guess = 250.0\n",
    "S_plv, s_plv, p_plv = plv_surrogates_p(X, fs=fs_guess, band=(8,12), n=500)\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(s_plv, bins=40, edgecolor=\"k\", alpha=0.7)\n",
    "plt.axvline(S_plv, color=\"red\", lw=2, label=f\"Observed={S_plv:.3f}\")\n",
    "plt.title(f\"Band-Phase PLV (8‚Äì12 Hz) Circular-Shift Surrogates\\np={p_plv:.5f}\")\n",
    "plt.xlabel(\"PLV eigen-entropy (surrogates)\"); plt.ylabel(\"Count\"); plt.legend(); plt.show()\n",
    "\n",
    "# --- (C) Max-lag coupling (up to L) eigen-entropy + circular-shifts ---\n",
    "def maxlag_corr_matrix(X, L=50):\n",
    "    # compute channel-channel max absolute cross-corr over lags [-L..L]\n",
    "    T,N = X.shape\n",
    "    Z = StandardScaler().fit_transform(X)\n",
    "    M = np.ones((N,N))\n",
    "    for i in range(N):\n",
    "        for j in range(i+1,N):\n",
    "            xi, xj = Z[:,i], Z[:,j]\n",
    "            vals=[]\n",
    "            for lag in range(-L,L+1):\n",
    "                if lag>=0:\n",
    "                    v = np.corrcoef(xi[lag:], xj[:T-lag])[0,1]\n",
    "                else:\n",
    "                    v = np.corrcoef(xi[:T+lag], xj[-lag:])[0,1]\n",
    "                vals.append(np.abs(v))\n",
    "            m = np.max(vals)\n",
    "            M[i,j]=M[j,i]=m\n",
    "    return M\n",
    "\n",
    "def maxlag_surrogates_p(X, L=50, n=300, rng=3):\n",
    "    S_obs = eigen_entropy_from_corr(maxlag_corr_matrix(X, L=L))\n",
    "    rng = np.random.default_rng(rng)\n",
    "    T,N = X.shape\n",
    "    surr = np.empty(n)\n",
    "    for k in range(n):\n",
    "        Xs = np.empty_like(X)\n",
    "        for j in range(N):\n",
    "            shift = rng.integers(0,T)\n",
    "            Xs[:,j] = np.roll(X[:,j], shift)\n",
    "        surr[k] = eigen_entropy_from_corr(maxlag_corr_matrix(Xs, L=L))\n",
    "    p = np.mean(surr <= S_obs)\n",
    "    return S_obs, surr, p\n",
    "\n",
    "S_lag, s_lag, p_lag = maxlag_surrogates_p(X, L=50, n=300)\n",
    "plt.figure(figsize=(7,4))\n",
    "plt.hist(s_lag, bins=40, edgecolor=\"k\", alpha=0.7)\n",
    "plt.axvline(S_lag, color=\"red\", lw=2, label=f\"Observed={S_lag:.3f}\")\n",
    "plt.title(f\"Max-Lag Coupling (L=50) Circular-Shift Surrogates\\np={p_lag:.5f}\")\n",
    "plt.xlabel(\"Max-lag eigen-entropy (surrogates)\"); plt.ylabel(\"Count\"); plt.legend(); plt.show()\n",
    "\n",
    "print(\"=== Synchrony-Focused p-values ===\")\n",
    "print(f\"(A) Raw corr (circular shifts):  p = {p_circ:.5f}\")\n",
    "print(f\"(B) Band-phase PLV (8‚Äì12 Hz):    p = {p_plv:.5f}\")\n",
    "print(f\"(C) Max-lag coupling (L=50):     p = {p_lag:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b25d46-8fce-441b-901c-a41dd522dc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNT Synchrony Escalation: Multi-Band PLV + Coherence with IAAFT Surrogates & FDR ===\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from numpy.linalg import eigvals\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# ---------- data ----------\n",
    "X = globals().get(\"X\", globals().get(\"signals\", None))\n",
    "if X is None:\n",
    "    T,N = 3000, 12\n",
    "    rng = np.random.default_rng(0)\n",
    "    t = np.linspace(0, 30, T)\n",
    "    base = np.sin(2*np.pi*10*t)[:,None]           # alpha-ish\n",
    "    X = base + 0.35*np.sin(2*np.pi*22*t)[:,None]  # beta-ish\n",
    "    X += 0.15*rng.standard_normal((T,N))\n",
    "\n",
    "fs = globals().get(\"fs\", 250.0)   # set your sampling rate if you have it\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def iaaft_surrogate(x, iters=200, rng=None):\n",
    "    \"\"\"Univariate IAAFT surrogate (preserves spectrum & amplitude distribution).\"\"\"\n",
    "    rng = np.random.default_rng(rng)\n",
    "    s = np.sort(x)\n",
    "    y = rng.permutation(x)\n",
    "    target_mag = np.abs(np.fft.rfft(x))\n",
    "    for _ in range(iters):\n",
    "        Y = np.fft.rfft(y)\n",
    "        y = np.fft.irfft(target_mag * np.exp(1j*np.angle(Y)), n=len(x)).real\n",
    "        # rank-order match to original distribution\n",
    "        ranks = np.argsort(np.argsort(y))\n",
    "        y = s[ranks]\n",
    "    return y\n",
    "\n",
    "def iaaft_matrix(X, iters=200, rng=None):\n",
    "    rng = np.random.default_rng(rng)\n",
    "    return np.stack([iaaft_surrogate(X[:,j], iters=iters, rng=rng) for j in range(X.shape[1])], axis=1)\n",
    "\n",
    "def eigen_entropy(M):\n",
    "    w = np.abs(eigvals(M)); w = w/(w.sum()+1e-12)\n",
    "    return float(-(w*np.log(w+1e-12)).sum())\n",
    "\n",
    "def zcorr(X):\n",
    "    Z = StandardScaler().fit_transform(X)\n",
    "    return np.corrcoef(Z.T)\n",
    "\n",
    "def band_pass(X, fs, f_lo, f_hi, order=4):\n",
    "    b,a = signal.butter(order, [f_lo/(fs/2), f_hi/(fs/2)], btype='band')\n",
    "    return signal.filtfilt(b,a,X,axis=0)\n",
    "\n",
    "def plv_entropy(Xb):\n",
    "    phase = np.angle(signal.hilbert(Xb, axis=0))\n",
    "    N = phase.shape[1]\n",
    "    PLV = np.ones((N,N))\n",
    "    for i in range(N):\n",
    "        for j in range(i+1,N):\n",
    "            d = np.exp(1j*(phase[:,i]-phase[:,j]))\n",
    "            PLV[i,j]=PLV[j,i]=np.abs(d.mean())\n",
    "    return eigen_entropy(PLV)\n",
    "\n",
    "def coh_entropy(Xb, fs, nper=512, nover=256, multitaper=False):\n",
    "    N=Xb.shape[1]\n",
    "    COH = np.ones((N,N))\n",
    "    for i in range(N):\n",
    "        for j in range(i+1,N):\n",
    "            f, Cxy = signal.coherence(Xb[:,i], Xb[:,j], fs=fs, nperseg=nper, noverlap=nover)\n",
    "            COH[i,j]=COH[j,i]=np.mean(Cxy)\n",
    "    return eigen_entropy(COH)\n",
    "\n",
    "# ---------- bands, surrogates, tests ----------\n",
    "bands = [(\"Œ¥\",1,4),(\"Œ∏\",4,8),(\"Œ±\",8,12),(\"Œ≤\",13,30),(\"Œ≥\",30,45)]\n",
    "n_surr = 400\n",
    "plv_p, coh_p = [], []\n",
    "\n",
    "for label, f1, f2 in bands:\n",
    "    Xb = band_pass(X, fs, f1, f2)\n",
    "    S_plv_obs = plv_entropy(Xb)\n",
    "    S_coh_obs = coh_entropy(Xb, fs)\n",
    "\n",
    "    surr_plv = np.zeros(n_surr)\n",
    "    surr_coh = np.zeros(n_surr)\n",
    "    for k in range(n_surr):\n",
    "        Xs = iaaft_matrix(Xb, iters=80, rng=k+1)   # strict null\n",
    "        surr_plv[k] = plv_entropy(Xs)\n",
    "        surr_coh[k] = coh_entropy(Xs, fs)\n",
    "\n",
    "    p_plv = np.mean(surr_plv <= S_plv_obs)\n",
    "    p_coh = np.mean(surr_coh <= S_coh_obs)\n",
    "    plv_p.append(p_plv); coh_p.append(p_coh)\n",
    "\n",
    "plv_p = np.array(plv_p); coh_p = np.array(coh_p)\n",
    "\n",
    "# FDR across all tests\n",
    "_, plv_q, _, _ = multipletests(plv_p, alpha=0.05, method='fdr_bh')\n",
    "_, coh_q, _, _ = multipletests(coh_p, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "# ---------- plot ----------\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,4))\n",
    "axes[0].bar(range(len(bands)), -np.log10(plv_q+1e-12))\n",
    "axes[0].set_xticks(range(len(bands))); axes[0].set_xticklabels([b[0] for b in bands])\n",
    "axes[0].set_ylabel(\"-log10(q)\"); axes[0].set_title(\"PLV eigen-entropy vs IAAFT (FDR-q)\")\n",
    "\n",
    "axes[1].bar(range(len(bands)), -np.log10(coh_q+1e-12))\n",
    "axes[1].set_xticks(range(len(bands))); axes[1].set_xticklabels([b[0] for b in bands])\n",
    "axes[1].set_ylabel(\"-log10(q)\"); axes[1].set_title(\"Coherence eigen-entropy vs IAAFT (FDR-q)\")\n",
    "for ax in axes:\n",
    "    ax.axhline(-np.log10(0.05), color='r', ls='--', lw=1, label='q=0.05'); ax.legend()\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print(\"Bands:\", [b[0] for b in bands])\n",
    "print(\"PLV  q-values:\", np.round(plv_q,5))\n",
    "print(\"COH  q-values:\", np.round(coh_q,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a7b904-5721-40d4-95aa-e49aa5cec8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Leakage-Robust Connectivity Battery (iCoh / AEC-orth / Partial Coherence) ===\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from scipy import signal, linalg\n",
    "from numpy.linalg import eigvals\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# ---- USE CURRENT FIELD ----\n",
    "X = globals().get(\"X\", globals().get(\"signals\"))\n",
    "fs = float(globals().get(\"fs\", 250.0))  # set your fs if known\n",
    "\n",
    "# ---- CONFIG (FAST) ----\n",
    "bands = [(\"Œ¥\",1,4),(\"Œ∏\",4,8),(\"Œ±\",8,12),(\"Œ≤\",13,30),(\"Œ≥\",30,45)]\n",
    "n_surr = 120          # ‚Üë for slower / stronger\n",
    "iaaft_iters = 40      # ‚Üë for stricter nulls\n",
    "welch_nper = 256; welch_nover = 128\n",
    "\n",
    "# ---- HELPERS ----\n",
    "def band_pass(X, fs, f1, f2, order=4):\n",
    "    b,a = signal.butter(order, [f1/(fs/2), f2/(fs/2)], btype='band')\n",
    "    return signal.filtfilt(b,a,X,axis=0)\n",
    "\n",
    "def iaaft_surrogate(x, iters=100, rng=None):\n",
    "    rng = np.random.default_rng(rng); s = np.sort(x); y = rng.permutation(x)\n",
    "    target_mag = np.abs(np.fft.rfft(x))\n",
    "    for _ in range(iters):\n",
    "        Y = np.fft.rfft(y)\n",
    "        y = np.fft.irfft(target_mag*np.exp(1j*np.angle(Y)), n=len(x)).real\n",
    "        ranks = np.argsort(np.argsort(y)); y = s[ranks]\n",
    "    return y\n",
    "\n",
    "def iaaft_matrix(X, iters=80, rng=None):\n",
    "    rng = np.random.default_rng(rng)\n",
    "    return np.stack([iaaft_surrogate(X[:,j], iters=iters, rng=rng) for j in range(X.shape[1])], axis=1)\n",
    "\n",
    "def mean_imag_coh(Xb, fs, nper=256, nover=128):\n",
    "    \"\"\"Return NxN imaginary coherence matrix averaged over band.\"\"\"\n",
    "    N = Xb.shape[1]; iC = np.ones((N,N))\n",
    "    for i in range(N):\n",
    "        for j in range(i+1,N):\n",
    "            f, Pxy = signal.csd(Xb[:,i], Xb[:,j], fs=fs, nperseg=nper, noverlap=nover)\n",
    "            _, Pxx = signal.welch(Xb[:,i], fs=fs, nperseg=nper, noverlap=nover)\n",
    "            _, Pyy = signal.welch(Xb[:,j], fs=fs, nperseg=nper, noverlap=nover)\n",
    "            Cxy = Pxy/np.sqrt(Pxx*Pyy + 1e-12)\n",
    "            imC = np.abs(np.imag(Cxy))\n",
    "            iC[i,j]=iC[j,i]=np.nanmean(imC)\n",
    "    return iC\n",
    "\n",
    "def aec_orth(Xb):\n",
    "    \"\"\"Orthogonalized amplitude-envelope correlation (Colclough+).\"\"\"\n",
    "    # Hilbert envelopes\n",
    "    Z = signal.hilbert(Xb, axis=0)\n",
    "    N = Z.shape[1]; A = np.abs(Z)  # envelopes\n",
    "    # symmetric orthogonalization (remove instantaneous linear mixing)\n",
    "    U, s, Vt = linalg.svd(A, full_matrices=False)\n",
    "    A_orth = U @ Vt\n",
    "    R = np.corrcoef(A_orth.T)\n",
    "    return R\n",
    "\n",
    "def partial_coherence(Xb, fs, nper=256, nover=128):\n",
    "    \"\"\"Partial coherence via inverse spectral density (precision).\"\"\"\n",
    "    N = Xb.shape[1]\n",
    "    # average spectral density matrix across freqs in band\n",
    "    S_sum = np.zeros((N,N), dtype=np.complex128); count=0\n",
    "    for k in range(N):\n",
    "        for l in range(N):\n",
    "            f, Pkl = signal.csd(Xb[:,k], Xb[:,l], fs=fs, nperseg=nper, noverlap=nover)\n",
    "            if k==0 and l==0: freqs = f\n",
    "            S_sum[k,l] = np.mean(Pkl)\n",
    "    S = S_sum\n",
    "    # precision matrix of spectrum\n",
    "    P = linalg.pinvh(S + 1e-9*np.eye(N))\n",
    "    # partial coherence magnitude\n",
    "    pc = np.zeros((N,N))\n",
    "    for i in range(N):\n",
    "        for j in range(i+1,N):\n",
    "            num = np.abs(P[i,j])**2\n",
    "            den = P[i,i]*P[j,j]\n",
    "            pc[i,j]=pc[j,i]=float(np.real(num/den))\n",
    "    return pc\n",
    "\n",
    "def eig_entropy(M):\n",
    "    w = np.abs(eigvals(M)); w = w/(w.sum()+1e-12)\n",
    "    return float(-(w*np.log(w+1e-12)).sum())\n",
    "\n",
    "# ---- MAIN LOOP ----\n",
    "res = []\n",
    "for label,f1,f2 in bands:\n",
    "    Xb = band_pass(X, fs, f1, f2)\n",
    "    # observed\n",
    "    S_iC  = eig_entropy(mean_imag_coh(Xb, fs, welch_nper, welch_nover))\n",
    "    S_AEC = eig_entropy(aec_orth(Xb))\n",
    "    S_PC  = eig_entropy(partial_coherence(Xb, fs, welch_nper, welch_nover))\n",
    "    # surrogates\n",
    "    s_iC, s_AEC, s_PC = [], [], []\n",
    "    for k in range(n_surr):\n",
    "        Xs = iaaft_matrix(Xb, iters=iaaft_iters, rng=k+1)\n",
    "        s_iC.append(eig_entropy(mean_imag_coh(Xs, fs, welch_nper, welch_nover)))\n",
    "        s_AEC.append(eig_entropy(aec_orth(Xs)))\n",
    "        s_PC.append(eig_entropy(partial_coherence(Xs, fs, welch_nper, welch_nover)))\n",
    "    p_iC  = np.mean(np.array(s_iC)  <= S_iC)\n",
    "    p_AEC = np.mean(np.array(s_AEC) <= S_AEC)\n",
    "    p_PC  = np.mean(np.array(s_PC)  <= S_PC)\n",
    "    res.append((label, p_iC, p_AEC, p_PC, S_iC, S_AEC, S_PC))\n",
    "\n",
    "labels, p_iC, p_AEC, p_PC, S_iC, S_AEC, S_PC = map(np.array, zip(*res))\n",
    "\n",
    "# FDR across all tests (3 per band)\n",
    "from numpy import concatenate\n",
    "all_p = concatenate([p_iC, p_AEC, p_PC])\n",
    "rej, q_all, _, _ = multipletests(all_p, alpha=0.05, method='fdr_bh')\n",
    "q_iC, q_AEC, q_PC = q_all[:len(bands)], q_all[len(bands):2*len(bands)], q_all[2*len(bands):]\n",
    "\n",
    "# ---- PLOTS ----\n",
    "fig, axes = plt.subplots(1,3, figsize=(12,4))\n",
    "for ax, q, ttl in zip(axes,\n",
    "                      [q_iC, q_AEC, q_PC],\n",
    "                      [\"Imaginary Coherence (iCoh)\", \"AEC-orth\", \"Partial Coherence\"]):\n",
    "    ax.bar(range(len(bands)), -np.log10(q+1e-12))\n",
    "    ax.set_xticks(range(len(bands))); ax.set_xticklabels(labels)\n",
    "    ax.axhline(-np.log10(0.05), color='r', ls='--', lw=1, label='q=0.05')\n",
    "    ax.set_ylabel(\"-log10(q)\"); ax.set_title(ttl); ax.legend()\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "print(\"Bands:\", list(labels))\n",
    "print(\"q_iCoh:\", np.round(q_iC,5))\n",
    "print(\"q_AEC :\", np.round(q_AEC,5))\n",
    "print(\"q_PartialCoh:\", np.round(q_PC,5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a43bb-c333-4884-90a6-811ea5f4cd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNT Consensus Information-Flow Graph (wPLI + PSI + Granger across bands) ===\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import os\n",
    "\n",
    "# ---------------------- CONFIG ----------------------\n",
    "# If you already defined X (T√óN) and fs earlier, this cell will use them.\n",
    "X  = globals().get(\"X\", globals().get(\"signals\"))\n",
    "fs = float(globals().get(\"fs\", 250.0))  # set your true sampling rate if known\n",
    "\n",
    "# Bands to scan:\n",
    "bands = [(\"Œ¥\",1,4),(\"Œ∏\",4,8),(\"Œ±\",8,12),(\"Œ≤\",13,30),(\"Œ≥\",30,45)]\n",
    "\n",
    "# Speed/strictness knobs:\n",
    "N_KEEP        = min(X.shape[1], 16)     # limit channels to top-left N (set to X.shape[1] to keep all)\n",
    "N_PER, N_OVER = 512, 256                # Welch params\n",
    "VAR_MAX_LAG   = 10                      # VAR order search upper bound\n",
    "GRANGER_Q     = 0.10                    # lenient screen for edges per band\n",
    "EXPORT        = True                    # save CSV/PNGs to disk\n",
    "\n",
    "# ---------------------- HELPERS ----------------------\n",
    "def band_pass(X, fs, f1, f2, order=4):\n",
    "    b,a = signal.butter(order, [f1/(fs/2), f2/(fs/2)], btype='band')\n",
    "    return signal.filtfilt(b,a,X,axis=0)\n",
    "\n",
    "def wpli_debiased(x, y, fs, nper=512, nover=256):\n",
    "    f, Pxy = signal.csd(x, y, fs=fs, nperseg=nper, noverlap=nover)\n",
    "    _, Pxx = signal.welch(x, fs=fs, nperseg=nper, noverlap=nover)\n",
    "    _, Pyy = signal.welch(y, fs=fs, nperseg=nper, noverlap=nover)\n",
    "    Cxy = Pxy/np.sqrt(Pxx*Pyy + 1e-12)\n",
    "    im = np.imag(Cxy)\n",
    "    num = (np.abs(im).mean()**2 - (im**2).mean())\n",
    "    den = (1 - (im**2).mean())\n",
    "    return float(np.clip(num/(den+1e-12), 0, 1))\n",
    "\n",
    "def psi(x, y, fs, band, nper=512, nover=256):\n",
    "    f, Pxy = signal.csd(x, y, fs=fs, nperseg=nper, noverlap=nover)\n",
    "    mask = (f>=band[0]) & (f<=band[1])\n",
    "    C = Pxy[mask] / (np.abs(Pxy[mask]) + 1e-12)\n",
    "    val = 0.0\n",
    "    for k in range(len(C)-1):\n",
    "        val += np.imag(np.conj(C[k]) * C[k+1])\n",
    "    return float(val)\n",
    "\n",
    "def granger_xy(x, y, max_lag=10):\n",
    "    Z = np.vstack([x, y]).T\n",
    "    Z = Z - Z.mean(0)\n",
    "    best_aic, best = np.inf, (1.0, 1.0)\n",
    "    for p in range(1, max_lag+1):\n",
    "        try:\n",
    "            m = VAR(Z).fit(p)\n",
    "            if m.aic < best_aic:\n",
    "                best_aic = m.aic\n",
    "                fx = m.test_causality(0, [1], kind='f')  # y -> x\n",
    "                fy = m.test_causality(1, [0], kind='f')  # x -> y\n",
    "                best = (fx.pvalue, fy.pvalue)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return best  # (p_yx, p_xy)\n",
    "\n",
    "# ---------------------- PREP ----------------------\n",
    "X = X[:, :N_KEEP]  # optional downselect\n",
    "T, N = X.shape\n",
    "labels = [f\"ch{j}\" for j in range(N)]\n",
    "os.makedirs(\"cnt_flow_exports\", exist_ok=True)\n",
    "\n",
    "# ---------------------- PER-BAND ANALYSIS ----------------------\n",
    "all_band_edges = []\n",
    "for label, f1, f2 in bands:\n",
    "    Xb = band_pass(X, fs, f1, f2)\n",
    "    wpli = np.zeros((N,N)); psi_mat = np.zeros((N,N))\n",
    "    p_yx = np.ones((N,N));  p_xy = np.ones((N,N))\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(i+1, N):\n",
    "            w = wpli_debiased(Xb[:,i], Xb[:,j], fs, N_PER, N_OVER)\n",
    "            wpli[i,j]=wpli[j,i]=w\n",
    "            ps = psi(Xb[:,i], Xb[:,j], fs, (f1,f2), N_PER, N_OVER)\n",
    "            psi_mat[i,j]= ps; psi_mat[j,i]= -ps\n",
    "            pyx, pxy = granger_xy(Xb[:,i], Xb[:,j], max_lag=VAR_MAX_LAG)\n",
    "            p_yx[i,j]=pyx; p_xy[j,i]=pxy\n",
    "\n",
    "    # FDR over both directions for all pairs\n",
    "    tri = np.triu_indices(N,1)\n",
    "    pg = np.concatenate([p_yx[tri], p_xy[tri]])\n",
    "    rej, q_all, _, _ = multipletests(pg, alpha=0.05, method='fdr_bh')\n",
    "    q_dir = np.full((N,N), 1.0)\n",
    "    m = len(p_yx[tri])\n",
    "    q_dir[tri] = q_all[:m]             # y->x on upper triangle\n",
    "    temp = np.full((N,N), 1.0); temp[tri] = q_all[m:]  # x->y on upper\n",
    "    q_dir = np.minimum(q_dir, temp.T)  # place both directions\n",
    "\n",
    "    # edge table for this band\n",
    "    rows=[]\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i==j: continue\n",
    "            score = 0.6*wpli[i,j] + 0.4*abs(psi_mat[i,j])\n",
    "            rows.append({\"band\": label, \"src\": labels[i], \"dst\": labels[j],\n",
    "                         \"wPLI\": wpli[i,j], \"PSI\": psi_mat[i,j],\n",
    "                         \"q_Granger\": q_dir[i,j], \"score\": score})\n",
    "    band_df = pd.DataFrame(rows)\n",
    "    band_df[\"keep\"] = (band_df[\"q_Granger\"] < GRANGER_Q)\n",
    "    all_band_edges.append(band_df.sort_values(\"score\", ascending=False))\n",
    "\n",
    "# ---------------------- CONSENSUS GRAPH ----------------------\n",
    "# Stack and summarize: how many bands kept each edge? what's the mean score?\n",
    "stack = pd.concat(all_band_edges, ignore_index=True)\n",
    "cons = (stack[stack[\"keep\"]]\n",
    "        .groupby([\"src\",\"dst\"], as_index=False)\n",
    "        .agg(bands_present=(\"band\",\"count\"),\n",
    "             mean_score=(\"score\",\"mean\"),\n",
    "             bands_list=(\"band\", lambda b: \",\".join(sorted(map(str,set(b)))))))\n",
    "cons = cons.sort_values([\"bands_present\",\"mean_score\"], ascending=[False, False]).reset_index(drop=True)\n",
    "\n",
    "print(\"=== Consensus edges (q_Granger < {:.2f} in at least one band) ===\".format(GRANGER_Q))\n",
    "display_cols = [\"src\",\"dst\",\"bands_present\",\"mean_score\",\"bands_list\"]\n",
    "print(cons[display_cols].head(20).to_string(index=False))\n",
    "\n",
    "# ---------------------- VISUALS ----------------------\n",
    "# 1) Consensus adjacency heatmap (weight = bands_present * mean_score)\n",
    "W = np.zeros((N,N))\n",
    "for _, row in cons.iterrows():\n",
    "    i = labels.index(row[\"src\"]); j = labels.index(row[\"dst\"])\n",
    "    W[i,j] = row[\"bands_present\"] * row[\"mean_score\"]\n",
    "\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.imshow(W, cmap=\"inferno\")\n",
    "plt.colorbar(label=\"Consensus weight (bands√óscore)\")\n",
    "plt.xticks(range(N), labels, rotation=90)\n",
    "plt.yticks(range(N), labels)\n",
    "plt.title(\"CNT Consensus Directed Connectivity (multi-band)\")\n",
    "plt.tight_layout()\n",
    "if EXPORT: plt.savefig(\"cnt_flow_exports/consensus_adjacency.png\", dpi=180)\n",
    "plt.show()\n",
    "\n",
    "# 2) Per-band edge count bar plot\n",
    "counts = stack.groupby(\"band\")[\"keep\"].sum().reindex([b[0] for b in bands])\n",
    "plt.figure(figsize=(7,3))\n",
    "counts.plot(kind=\"bar\")\n",
    "plt.ylabel(\"# edges (q<{:.2f})\".format(GRANGER_Q))\n",
    "plt.title(\"Edges surviving per band\")\n",
    "plt.tight_layout()\n",
    "if EXPORT: plt.savefig(\"cnt_flow_exports/edges_per_band.png\", dpi=180)\n",
    "plt.show()\n",
    "\n",
    "# 3) Save CSVs\n",
    "if EXPORT:\n",
    "    stack.to_csv(\"cnt_flow_exports/per_band_edges.csv\", index=False)\n",
    "    cons.to_csv(\"cnt_flow_exports/consensus_edges.csv\", index=False)\n",
    "    print(\"Saved CSVs and PNGs to: cnt_flow_exports/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a1e4e1-1fc7-45d4-a220-31259d9b9958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNT Edge Robustness Battery: split-half, time-reversal, block-bootstrap, lag-jitter ===\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from scipy import signal\n",
    "import os\n",
    "\n",
    "# Use your current X (T√óN), fs, and the consensus edge table from the last step.\n",
    "X  = globals().get(\"X\", globals().get(\"signals\"))\n",
    "fs = float(globals().get(\"fs\", 250.0))\n",
    "cons_path = \"cnt_flow_exports/consensus_edges.csv\"\n",
    "cons = pd.read_csv(cons_path)\n",
    "\n",
    "# --- helpers ---\n",
    "def band_pass(X, fs, f1, f2, order=4):\n",
    "    b,a = signal.butter(order, [f1/(fs/2), f2/(fs/2)], btype='band')\n",
    "    return signal.filtfilt(b,a,X,axis=0)\n",
    "\n",
    "def granger_xy(x, y, max_lag=10):\n",
    "    Z = np.vstack([x, y]).T - np.mean(np.vstack([x, y]).T, axis=0)\n",
    "    best_aic, best = np.inf, (1.0, 1.0)\n",
    "    for p in range(1, max_lag+1):\n",
    "        try:\n",
    "            m = VAR(Z).fit(p)\n",
    "            if m.aic < best_aic:\n",
    "                best_aic = m.aic\n",
    "                fx = m.test_causality(0, [1], kind='f')  # y -> x\n",
    "                fy = m.test_causality(1, [0], kind='f')  # x -> y\n",
    "                best = (fx.pvalue, fy.pvalue)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return best  # (p_yx, p_xy)\n",
    "\n",
    "def edge_pvals(Xb, src_idx, dst_idx, max_lag=10):\n",
    "    p_yx, p_xy = granger_xy(Xb[:,src_idx], Xb[:,dst_idx], max_lag=max_lag)\n",
    "    # q via FDR over both directions for this pair only (trivial but consistent)\n",
    "    rej, q, _, _ = multipletests([p_yx, p_xy], alpha=0.05, method=\"fdr_bh\")\n",
    "    return float(q[0]), float(q[1])  # q(src->dst), q(dst->src)\n",
    "\n",
    "# --- config ---\n",
    "bands = [(\"Œ¥\",1,4),(\"Œ∏\",4,8),(\"Œ±\",8,12),(\"Œ≤\",13,30),(\"Œ≥\",30,45)]\n",
    "MAX_LAG = 10\n",
    "B_BOOT  = 200     # block-bootstrap reps\n",
    "BLOCK   = max(50, X.shape[0]//10)\n",
    "LAG_JIT = 3       # samples of random lag jitter\n",
    "EXPORT_DIR = \"cnt_flow_exports\"\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "# Build label->index map\n",
    "labels = [f\"ch{j}\" for j in range(X.shape[1])]\n",
    "idx = {lab:i for i,lab in enumerate(labels)}\n",
    "\n",
    "rows=[]\n",
    "for k, row in cons.iterrows():\n",
    "    s = idx[row[\"src\"]]; d = idx[row[\"dst\"]]\n",
    "    bands_here = str(row[\"bands_list\"]).split(\",\")\n",
    "    # 1) split-half stability (across bands where edge existed)\n",
    "    sh_hits, sh_total = 0, 0\n",
    "    tr_hits, bb_score, lj_score = 0, 0.0, 0.0\n",
    "    for lab,f1,f2 in bands:\n",
    "        if lab not in bands_here: \n",
    "            continue\n",
    "        Xb = band_pass(X, fs, f1, f2)\n",
    "        T = Xb.shape[0]\n",
    "        A, B = Xb[:T//2], Xb[T//2:]\n",
    "        # split-half q\n",
    "        qAB = []\n",
    "        for part in (A,B):\n",
    "            q_sd, _ = edge_pvals(part, s, d, MAX_LAG)\n",
    "            qAB.append(q_sd)\n",
    "        sh_total += 2\n",
    "        sh_hits  += sum(q < 0.10 for q in qAB)   # lenient q<0.10\n",
    "        # 2) time-reversal sanity (should weaken/flip)\n",
    "        Xrev = Xb[::-1].copy()\n",
    "        q_forward, _ = edge_pvals(Xb,   s, d, MAX_LAG)\n",
    "        q_reverse, _ = edge_pvals(Xrev, s, d, MAX_LAG)\n",
    "        tr_hits += 1 if (q_forward < 0.10 and q_reverse > 0.10) else 0\n",
    "        # 3) block-bootstrap persistence\n",
    "        rng = np.random.default_rng(0)\n",
    "        keep = 0\n",
    "        for _ in range(B_BOOT):\n",
    "            starts = rng.integers(0, T-BLOCK, size=max(1, T//BLOCK))\n",
    "            Xbb = np.concatenate([Xb[st:st+BLOCK] for st in starts], axis=0)\n",
    "            q_sd, _ = edge_pvals(Xbb, s, d, MAX_LAG)\n",
    "            keep += (q_sd < 0.10)\n",
    "        bb_score += keep / B_BOOT\n",
    "        # 4) lag-jitter robustness\n",
    "        keep_lj = 0\n",
    "        for _ in range(100):\n",
    "            jitter = np.random.randint(-LAG_JIT, LAG_JIT+1)\n",
    "            Xj = Xb.copy()\n",
    "            Xj[:,d] = np.roll(Xj[:,d], jitter)\n",
    "            q_sd, _ = edge_pvals(Xj, s, d, MAX_LAG)\n",
    "            keep_lj += (q_sd < 0.10)\n",
    "        lj_score += keep_lj / 100.0\n",
    "\n",
    "    # normalize to 0..100\n",
    "    split_half_pct = 100.0 * (sh_hits / max(1, sh_total))\n",
    "    time_rev_pct   = 100.0 * (tr_hits  / max(1, len(bands_here)))\n",
    "    bb_pct         = 100.0 * (bb_score / max(1, len(bands_here)))\n",
    "    lj_pct         = 100.0 * (lj_score / max(1, len(bands_here)))\n",
    "    # composite robustness\n",
    "    R = 0.35*split_half_pct + 0.25*time_rev_pct + 0.25*bb_pct + 0.15*lj_pct\n",
    "    rows.append({\n",
    "        \"src\": row[\"src\"], \"dst\": row[\"dst\"], \"bands\": row[\"bands_list\"],\n",
    "        \"split_half%\": split_half_pct, \"time_reverse%\": time_rev_pct,\n",
    "        \"bootstrap%\": bb_pct, \"lag_jitter%\": lj_pct,\n",
    "        \"robustness_score\": R\n",
    "    })\n",
    "\n",
    "rob = pd.DataFrame(rows).sort_values(\"robustness_score\", ascending=False).reset_index(drop=True)\n",
    "print(\"=== CNT Edge Robustness (0‚Äì100) ===\")\n",
    "print(rob.head(20).to_string(index=False))\n",
    "\n",
    "# Plot top-12 radar-like bars\n",
    "top = rob.head(12).copy()\n",
    "plt.figure(figsize=(9,4.2))\n",
    "for i,(name,row) in enumerate(top.iterrows()):\n",
    "    plt.bar(i-0.45, row[\"split_half%\"], width=0.3, label=\"split-half\" if i==0 else None)\n",
    "    plt.bar(i-0.15, row[\"time_reverse%\"], width=0.3, label=\"time-reverse\" if i==0 else None)\n",
    "    plt.bar(i+0.15, row[\"bootstrap%\"],   width=0.3, label=\"bootstrap\"   if i==0 else None)\n",
    "    plt.bar(i+0.45, row[\"lag_jitter%\"],  width=0.3, label=\"lag-jitter\"  if i==0 else None)\n",
    "plt.xticks(range(len(top)), [f\"{r['src']}‚Üí{r['dst']}\" for _,r in top.iterrows()], rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Pass rate (%)\"); plt.title(\"Top Edge Robustness Components\")\n",
    "plt.legend(ncol=4, loc=\"upper center\", bbox_to_anchor=(0.5,1.22))\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{EXPORT_DIR}/edge_robustness_components.png\", dpi=180)\n",
    "plt.show()\n",
    "\n",
    "# Save table\n",
    "rob_path = f\"{EXPORT_DIR}/edge_robustness_scores.csv\"\n",
    "rob.to_csv(rob_path, index=False)\n",
    "print(f\"Saved: {rob_path}\\nSaved: {EXPORT_DIR}/edge_robustness_components.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91ade38-cc44-42a9-a520-2ffb1d2fbfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Self-healing Edge Robustness Battery (defines X/fs if missing) ===\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from scipy import signal\n",
    "import os\n",
    "\n",
    "# ---------- 0) Locate or synthesize field ----------\n",
    "def resolve_field():\n",
    "    cand = [globals().get(k) for k in [\"X\",\"signals\",\"real_data\"]]\n",
    "    for a in cand:\n",
    "        if isinstance(a, np.ndarray) and a.ndim==2:\n",
    "            return a\n",
    "    # synth fallback\n",
    "    T,N = 2000, 8\n",
    "    rng = np.random.default_rng(7)\n",
    "    t = np.linspace(0, 20, T)\n",
    "    base = np.sin(2*np.pi*10*t)[:,None]\n",
    "    Xsyn = base + 0.25*np.sin(2*np.pi*22*t)[:,None] + 0.12*rng.standard_normal((T,N))\n",
    "    return Xsyn\n",
    "\n",
    "def resolve_fs():\n",
    "    v = globals().get(\"fs\", None)\n",
    "    try:\n",
    "        return float(v) if v is not None else 250.0\n",
    "    except Exception:\n",
    "        return 250.0\n",
    "\n",
    "X  = resolve_field()\n",
    "fs = resolve_fs()\n",
    "\n",
    "# ---------- 1) Load consensus edges ----------\n",
    "CONS_PATH = \"cnt_flow_exports/consensus_edges.csv\"\n",
    "if not os.path.exists(CONS_PATH):\n",
    "    raise FileNotFoundError(\"Expected consensus_edges.csv from the prior step. Re-run the consensus flow cell first.\")\n",
    "cons = pd.read_csv(CONS_PATH)\n",
    "\n",
    "# ---------- 2) Helpers ----------\n",
    "def band_pass(X, fs, f1, f2, order=4):\n",
    "    b,a = signal.butter(order, [f1/(fs/2), f2/(fs/2)], btype='band')\n",
    "    return signal.filtfilt(b,a,X,axis=0)\n",
    "\n",
    "def granger_xy(x, y, max_lag=10):\n",
    "    Z = np.vstack([x, y]).T\n",
    "    Z = Z - Z.mean(0)\n",
    "    best_aic, best = np.inf, (1.0, 1.0)\n",
    "    for p in range(1, max_lag+1):\n",
    "        try:\n",
    "            m = VAR(Z).fit(p)\n",
    "            if m.aic < best_aic:\n",
    "                best_aic = m.aic\n",
    "                fx = m.test_causality(0, [1], kind='f')  # y -> x ?\n",
    "                fy = m.test_causality(1, [0], kind='f')  # x -> y ?\n",
    "                best = (fx.pvalue, fy.pvalue)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return best\n",
    "\n",
    "def edge_q_forward(Xb, s, d, max_lag=10):\n",
    "    p_yx, p_xy = granger_xy(Xb[:,s], Xb[:,d], max_lag=max_lag)\n",
    "    q = multipletests([p_yx, p_xy], alpha=0.05, method=\"fdr_bh\")[1][0]\n",
    "    return float(q)\n",
    "\n",
    "# ---------- 3) Config ----------\n",
    "bands = [(\"Œ¥\",1,4),(\"Œ∏\",4,8),(\"Œ±\",8,12),(\"Œ≤\",13,30),(\"Œ≥\",30,45)]\n",
    "MAX_LAG = 10\n",
    "B_BOOT  = 200\n",
    "BLOCK   = max(50, X.shape[0]//10)\n",
    "LAG_JIT = 3\n",
    "EXPORT_DIR = \"cnt_flow_exports\"; os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "labels = [f\"ch{j}\" for j in range(X.shape[1])]\n",
    "idx = {lab:i for i,lab in enumerate(labels)}\n",
    "\n",
    "# ---------- 4) Robustness battery ----------\n",
    "rows=[]\n",
    "for _, row in cons.iterrows():\n",
    "    s = idx[row[\"src\"]]; d = idx[row[\"dst\"]]\n",
    "    bands_here = str(row[\"bands_list\"]).split(\",\")\n",
    "\n",
    "    sh_hits = sh_total = tr_hits = 0\n",
    "    bb_score = lj_score = 0.0\n",
    "\n",
    "    for lab,f1,f2 in bands:\n",
    "        if lab not in bands_here: \n",
    "            continue\n",
    "        Xb = band_pass(X, fs, f1, f2)\n",
    "        T = Xb.shape[0]\n",
    "\n",
    "        # split-half\n",
    "        partA, partB = Xb[:T//2], Xb[T//2:]\n",
    "        for part in (partA, partB):\n",
    "            q = edge_q_forward(part, s, d, MAX_LAG)\n",
    "            sh_total += 1; sh_hits += (q < 0.10)\n",
    "\n",
    "        # time-reversal sanity\n",
    "        q_fwd = edge_q_forward(Xb, s, d, MAX_LAG)\n",
    "        Xrev  = Xb[::-1].copy()\n",
    "        q_rev = edge_q_forward(Xrev, s, d, MAX_LAG)\n",
    "        tr_hits += 1 if (q_fwd < 0.10 and q_rev > 0.10) else 0\n",
    "\n",
    "        # block-bootstrap persistence\n",
    "        rng = np.random.default_rng(0)\n",
    "        keep = 0\n",
    "        for _ in range(B_BOOT):\n",
    "            starts = rng.integers(0, T-BLOCK, size=max(1, T//BLOCK))\n",
    "            Xbb = np.concatenate([Xb[st:st+BLOCK] for st in starts], axis=0)\n",
    "            q = edge_q_forward(Xbb, s, d, MAX_LAG)\n",
    "            keep += (q < 0.10)\n",
    "        bb_score += keep / B_BOOT\n",
    "\n",
    "        # lag-jitter robustness\n",
    "        keep_lj = 0\n",
    "        for _ in range(100):\n",
    "            jitter = np.random.randint(-LAG_JIT, LAG_JIT+1)\n",
    "            Xj = Xb.copy(); Xj[:,d] = np.roll(Xj[:,d], jitter)\n",
    "            q = edge_q_forward(Xj, s, d, MAX_LAG)\n",
    "            keep_lj += (q < 0.10)\n",
    "        lj_score += keep_lj / 100.0\n",
    "\n",
    "    # scores (0‚Äì100)\n",
    "    split_half_pct = 100.0 * (sh_hits / max(1, sh_total))\n",
    "    time_rev_pct   = 100.0 * (tr_hits  / max(1, len([b for b in bands_here if b!=''])))\n",
    "    bb_pct         = 100.0 * (bb_score / max(1, len([b for b in bands_here if b!=''])))\n",
    "    lj_pct         = 100.0 * (lj_score / max(1, len([b for b in bands_here if b!=''])))\n",
    "\n",
    "    R = 0.35*split_half_pct + 0.25*time_rev_pct + 0.25*bb_pct + 0.15*lj_pct\n",
    "    rows.append({\n",
    "        \"src\": row[\"src\"], \"dst\": row[\"dst\"], \"bands\": row[\"bands_list\"],\n",
    "        \"split_half%\": split_half_pct, \"time_reverse%\": time_rev_pct,\n",
    "        \"bootstrap%\": bb_pct, \"lag_jitter%\": lj_pct,\n",
    "        \"robustness_score\": R\n",
    "    })\n",
    "\n",
    "rob = pd.DataFrame(rows).sort_values(\"robustness_score\", ascending=False).reset_index(drop=True)\n",
    "print(\"=== CNT Edge Robustness (0‚Äì100) ===\")\n",
    "print(rob.head(20).to_string(index=False))\n",
    "\n",
    "# visualize top-12 components\n",
    "top = rob.head(12).copy()\n",
    "plt.figure(figsize=(9,4.2))\n",
    "for i,(_,r) in enumerate(top.iterrows()):\n",
    "    plt.bar(i-0.45, r[\"split_half%\"], width=0.3, label=\"split-half\" if i==0 else None)\n",
    "    plt.bar(i-0.15, r[\"time_reverse%\"], width=0.3, label=\"time-reverse\" if i==0 else None)\n",
    "    plt.bar(i+0.15, r[\"bootstrap%\"],   width=0.3, label=\"bootstrap\"   if i==0 else None)\n",
    "    plt.bar(i+0.45, r[\"lag_jitter%\"],  width=0.3, label=\"lag-jitter\"  if i==0 else None)\n",
    "plt.xticks(range(len(top)), [f\"{r['src']}‚Üí{r['dst']}\" for _,r in top.iterrows()], rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"Pass rate (%)\"); plt.title(\"Top Edge Robustness Components\")\n",
    "plt.legend(ncol=4, loc=\"upper center\", bbox_to_anchor=(0.5,1.22))\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{EXPORT_DIR}/edge_robustness_components.png\", dpi=180)\n",
    "plt.show()\n",
    "\n",
    "rob_path = f\"{EXPORT_DIR}/edge_robustness_scores.csv\"\n",
    "rob.to_csv(rob_path, index=False)\n",
    "print(f\"Saved: {rob_path}\\nSaved: {EXPORT_DIR}/edge_robustness_components.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98647f4b-7913-4551-8e01-3d6c448b9867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CNT Robustness: Diagnostic + Quick Run (always prints) ===\n",
    "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from scipy import signal\n",
    "\n",
    "def _resolve_field():\n",
    "    for k in [\"X\",\"signals\",\"real_data\"]:\n",
    "        v = globals().get(k, None)\n",
    "        if isinstance(v, np.ndarray) and v.ndim==2:\n",
    "            print(f\"[ok] Using field from variable: {k}  shape={v.shape}\")\n",
    "            return v\n",
    "    # synth fallback (guarantees output)\n",
    "    T,N = 2000,8\n",
    "    print(\"[warn] No X/signals/real_data found ‚Üí using synthetic fallback \"\n",
    "          f\"(T={T}, N={N})\")\n",
    "    t = np.linspace(0,20,T)\n",
    "    rng = np.random.default_rng(7)\n",
    "    X = np.sin(2*np.pi*10*t)[:,None] + 0.25*np.sin(2*np.pi*22*t)[:,None] + 0.12*rng.standard_normal((T,N))\n",
    "    return X\n",
    "\n",
    "def _resolve_fs():\n",
    "    v = globals().get(\"fs\", None)\n",
    "    if v is None:\n",
    "        print(\"[warn] No fs found ‚Üí defaulting to 250.0 Hz\")\n",
    "        return 250.0\n",
    "    try:\n",
    "        f = float(v)\n",
    "        print(f\"[ok] Using fs={f} Hz\")\n",
    "        return f\n",
    "    except Exception:\n",
    "        print(\"[warn] fs not parseable ‚Üí default 250.0 Hz\")\n",
    "        return 250.0\n",
    "\n",
    "X  = _resolve_field()\n",
    "fs = _resolve_fs()\n",
    "\n",
    "CONS_PATH = \"cnt_flow_exports/consensus_edges.csv\"\n",
    "if not os.path.exists(CONS_PATH):\n",
    "    # create a minimal consensus file from a fast pass so this never blocks\n",
    "    print(f\"[warn] {CONS_PATH} not found ‚Üí making a quick provisional consensus.\")\n",
    "    # quick 2-band pass to seed edges\n",
    "    from scipy import signal\n",
    "    def band_pass(X, fs, f1, f2, order=4):\n",
    "        b,a = signal.butter(order, [f1/(fs/2), f2/(fs/2)], btype='band')\n",
    "        return signal.filtfilt(b,a,X,axis=0)\n",
    "    def granger_xy(x, y, max_lag=8):\n",
    "        Z = np.vstack([x,y]).T - np.mean(np.vstack([x,y]).T, axis=0)\n",
    "        best, best_aic = (1.0,1.0), np.inf\n",
    "        for p in range(1, max_lag+1):\n",
    "            try:\n",
    "                m = VAR(Z).fit(p)\n",
    "                if m.aic < best_aic:\n",
    "                    best_aic = m.aic\n",
    "                    fx = m.test_causality(0,[1],kind='f')\n",
    "                    fy = m.test_causality(1,[0],kind='f')\n",
    "                    best = (fx.pvalue, fy.pvalue)\n",
    "            except Exception:\n",
    "                continue\n",
    "        return best\n",
    "    labels = [f\"ch{j}\" for j in range(X.shape[1])]\n",
    "    fast_edges = []\n",
    "    for (lab,f1,f2) in [(\"Œ±\",8,12), (\"Œ≥\",30,45)]:\n",
    "        Xb = band_pass(X, fs, f1, f2)\n",
    "        for i in range(X.shape[1]):\n",
    "            for j in range(X.shape[1]):\n",
    "                if i==j: continue\n",
    "                pyx, pxy = granger_xy(Xb[:,i], Xb[:,j], 8)\n",
    "                q = multipletests([pyx, pxy], alpha=0.05, method='fdr_bh')[1][0]\n",
    "                if q < 0.10:\n",
    "                    fast_edges.append({\"src\":labels[i],\"dst\":labels[j],\n",
    "                                       \"bands_present\":1,\"mean_score\":1.0,\"bands_list\":lab})\n",
    "    cons = pd.DataFrame(fast_edges).drop_duplicates(subset=[\"src\",\"dst\"])\n",
    "    os.makedirs(\"cnt_flow_exports\", exist_ok=True)\n",
    "    cons.to_csv(CONS_PATH, index=False)\n",
    "    print(f\"[ok] Wrote provisional file with {len(cons)} edges ‚Üí {CONS_PATH}\")\n",
    "else:\n",
    "    cons = pd.read_csv(CONS_PATH)\n",
    "    print(f\"[ok] Loaded consensus edges: {len(cons)} rows from {CONS_PATH}\")\n",
    "\n",
    "# ---- Quick robustness (downsized so it always prints fast) ----\n",
    "from scipy import signal\n",
    "def band_pass(X, fs, f1, f2, order=4):\n",
    "    b,a = signal.butter(order, [f1/(fs/2), f2/(fs/2)], btype='band')\n",
    "    return signal.filtfilt(b,a,X,axis=0)\n",
    "def granger_xy(x, y, max_lag=8):\n",
    "    Z = np.vstack([x, y]).T - np.mean(np.vstack([x, y]).T, axis=0)\n",
    "    best_aic, best = np.inf, (1.0, 1.0)\n",
    "    for p in range(1, max_lag+1):\n",
    "        try:\n",
    "            m = VAR(Z).fit(p)\n",
    "            if m.aic < best_aic:\n",
    "                best_aic = m.aic\n",
    "                fx = m.test_causality(0,[1],kind='f')\n",
    "                fy = m.test_causality(1,[0],kind='f')\n",
    "                best = (fx.pvalue, fy.pvalue)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return best\n",
    "def edge_q(Xb, s, d):  # forward direction q\n",
    "    pyx, pxy = granger_xy(Xb[:,s], Xb[:,d], max_lag=8)\n",
    "    return float(multipletests([pyx, pxy], alpha=0.05, method=\"fdr_bh\")[1][0])\n",
    "\n",
    "labels = [f\"ch{j}\" for j in range(X.shape[1])]\n",
    "idx = {lab:i for i,lab in enumerate(labels)}\n",
    "bands = [(\"Œ±\",8,12), (\"Œ≥\",30,45)]  # quick scan\n",
    "BLOCK = max(40, X.shape[0]//12)\n",
    "\n",
    "rows=[]\n",
    "for _, r in cons.iterrows():\n",
    "    s = idx.get(r[\"src\"]); d = idx.get(r[\"dst\"])\n",
    "    if s is None or d is None: continue\n",
    "    hits = []\n",
    "    for (lab,f1,f2) in bands:\n",
    "        Xb = band_pass(X, fs, f1, f2)\n",
    "        T  = Xb.shape[0]\n",
    "        A, B = Xb[:T//2], Xb[T//2:]\n",
    "        qA = edge_q(A, s, d); qB = edge_q(B, s, d)\n",
    "        # time-reverse\n",
    "        qF = edge_q(Xb, s, d)\n",
    "        qR = edge_q(Xb[::-1].copy(), s, d)\n",
    "        # block bootstrap (light)\n",
    "        keep=0\n",
    "        rng=np.random.default_rng(0)\n",
    "        for _ in range(60):\n",
    "            starts = rng.integers(0, T-BLOCK, size=max(1, T//BLOCK))\n",
    "            Xbb = np.concatenate([Xb[st:st+BLOCK] for st in starts], axis=0)\n",
    "            keep += (edge_q(Xbb, s, d) < 0.10)\n",
    "        bb = (keep/60.0)*100\n",
    "        # lag jitter (light)\n",
    "        keep=0\n",
    "        for _ in range(40):\n",
    "            jit = np.random.randint(-2,3)\n",
    "            Xj = Xb.copy(); Xj[:,d] = np.roll(Xj[:,d], jit)\n",
    "            keep += (edge_q(Xj, s, d) < 0.10)\n",
    "        lj = (keep/40.0)*100\n",
    "\n",
    "        split = ( (qA<0.10) + (qB<0.10) )/2*100\n",
    "        tr    = 100.0 if (qF<0.10 and qR>0.10) else 0.0\n",
    "        R = 0.35*split + 0.25*tr + 0.25*bb + 0.15*lj\n",
    "        rows.append({\"edge\": f\"{r['src']}‚Üí{r['dst']}\", \"band\": lab,\n",
    "                     \"split%\": split, \"time_rev%\": tr, \"boot%\": bb, \"lagjit%\": lj, \"robust%\": R})\n",
    "\n",
    "rob = pd.DataFrame(rows).sort_values([\"robust%\",\"edge\"], ascending=[False,True]).reset_index(drop=True)\n",
    "print(\"\\n=== Quick robustness (2 bands) ‚Äî top 10 band-edges ===\")\n",
    "print(rob.head(10).to_string(index=False))\n",
    "\n",
    "# Aggregate per edge (mean across bands)\n",
    "agg = (rob.groupby(\"edge\", as_index=False)\n",
    "          .agg(robust_score=(\"robust%\",\"mean\"),\n",
    "               split=(\"split%\",\"mean\"),\n",
    "               time_rev=(\"time_rev%\",\"mean\"),\n",
    "               boot=(\"boot%\",\"mean\"),\n",
    "               lagjit=(\"lagjit%\",\"mean\"),\n",
    "               bands=(\"band\", \"nunique\")))\n",
    "agg = agg.sort_values([\"robust_score\",\"bands\"], ascending=[False,False]).reset_index(drop=True)\n",
    "print(\"\\n=== Aggregated robustness per edge (mean across bands) ‚Äî top 10 ===\")\n",
    "print(agg.head(10).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fa38b92-2f96-4e6c-9027-7040d346632c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 394\u001b[39m\n\u001b[32m    380\u001b[39m start = time.time()\n\u001b[32m    381\u001b[39m cfg = \u001b[38;5;28mdict\u001b[39m(\n\u001b[32m    382\u001b[39m     runs=\u001b[32m28\u001b[39m,          \u001b[38;5;66;03m# more runs => stronger test (keep runtime sane)\u001b[39;00m\n\u001b[32m    383\u001b[39m     N=\u001b[32m10\u001b[39m,             \u001b[38;5;66;03m# nodes\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    392\u001b[39m     seed=\u001b[32m424242\u001b[39m\n\u001b[32m    393\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m res, aucs, f1s = \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m elapsed = time.time()-start\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# ---------------------------------------\u001b[39;00m\n\u001b[32m    398\u001b[39m \u001b[38;5;66;03m# 8) Display results & tiny plots\u001b[39;00m\n\u001b[32m    399\u001b[39m \u001b[38;5;66;03m# ---------------------------------------\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 327\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(runs, N, T, dt, fs, K, sparsity, noise, maxlag, B, block, jit, alpha, seed)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;66;03m# CCC\u001b[39;00m\n\u001b[32m    326\u001b[39m bands = multi_band(X, fs)\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m score, p_boot, Sfull = \u001b[43mblock_bootstrap_scores\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlag\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaxlag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m=\u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbands\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrng\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrng\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m rej_ccc = fdr_bh(p_boot, alpha=alpha)\n\u001b[32m    331\u001b[39m p_cc, r_cc, f_cc = evaluate(adj, rej_ccc)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 248\u001b[39m, in \u001b[36mblock_bootstrap_scores\u001b[39m\u001b[34m(X, fs, maxlag, B, block, jit, bands, rng)\u001b[39m\n\u001b[32m    246\u001b[39m Stot = np.zeros((N,N))\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m keys:\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     S,_ = \u001b[43mband_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbband\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m     Stot += S\n\u001b[32m    250\u001b[39m boots.append(Stot/\u001b[38;5;28mlen\u001b[39m(keys))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 202\u001b[39m, in \u001b[36mblock_bootstrap_scores.<locals>.band_trial\u001b[39m\u001b[34m(bX, lag)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mband_trial\u001b[39m(bX, lag):\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     P = \u001b[43mgranger_matrix_F\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlag\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    203\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m np.errstate(divide=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    204\u001b[39m         S = -np.log10(np.maximum(P, \u001b[32m1e-300\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 145\u001b[39m, in \u001b[36mgranger_matrix_F\u001b[39m\u001b[34m(X, maxlag)\u001b[39m\n\u001b[32m    143\u001b[39m Z_red = Z[a:b][:, keep_cols]\n\u001b[32m    144\u001b[39m y_i = Y[a:b]\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m beta_red, _, _, _ = \u001b[43mlstsq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ_red\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrcond\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m resid_red = y_i - Z_red @ beta_red\n\u001b[32m    147\u001b[39m RSS_red = np.sum(resid_red**\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\cnt_genome\\.venv\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2564\u001b[39m, in \u001b[36mlstsq\u001b[39m\u001b[34m(a, b, rcond)\u001b[39m\n\u001b[32m   2560\u001b[39m     b = zeros(b.shape[:-\u001b[32m2\u001b[39m] + (m, n_rhs + \u001b[32m1\u001b[39m), dtype=b.dtype)\n\u001b[32m   2562\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m errstate(call=_raise_linalgerror_lstsq, invalid=\u001b[33m'\u001b[39m\u001b[33mcall\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   2563\u001b[39m               over=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m, divide=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m, under=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m     x, resids, rank, s = \u001b[43m_umath_linalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstsq\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrcond\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m                                             \u001b[49m\u001b[43msignature\u001b[49m\u001b[43m=\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m m == \u001b[32m0\u001b[39m:\n\u001b[32m   2567\u001b[39m     x[...] = \u001b[32m0\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# CNT Consensus Causality (CCC) vs. Broadband Granger ‚Äî Single Mega-Cell\n",
    "# Telos x Aetheron ‚Äî 2025-10-06\n",
    "# Dependencies: numpy, scipy, matplotlib (no statsmodels needed). Tries to pip if missing.\n",
    "\n",
    "import sys, math, time, random, importlib, subprocess\n",
    "def _ensure(pkg):\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except Exception:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"--quiet\"])\n",
    "for p in [\"numpy\",\"scipy\",\"matplotlib\"]:\n",
    "    _ensure(p)\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import lstsq\n",
    "from scipy.signal import butter, filtfilt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"]=(7.5,5.0); plt.rcParams[\"figure.dpi\"]=120\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Synthetic truth generator\n",
    "# -----------------------------\n",
    "def kuramoto_network(T=12.0, dt=0.01, N=12, K=0.9, sparsity=0.75, noise=0.25, seed=7):\n",
    "    \"\"\"\n",
    "    Coupled phase oscillators with directed, weighted adjacency (ground truth).\n",
    "    Returns: phases [Tsteps,N], signals [Tsteps,N], true_adj (NxN, boolean, directed)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    tsteps = int(T/dt)\n",
    "    # Directed adjacency with controlled density; no self-loops\n",
    "    A = rng.random((N,N))\n",
    "    A = (A > sparsity).astype(float)\n",
    "    np.fill_diagonal(A, 0.0)\n",
    "    # Weight edges (small heterogeneity)\n",
    "    W = A * (0.6 + 0.8*rng.random((N,N)))\n",
    "    # Natural frequencies\n",
    "    w = rng.normal(1.5, 0.2, size=N)\n",
    "    theta = rng.uniform(0, 2*np.pi, size=N)\n",
    "    TH = np.zeros((tsteps, N))\n",
    "    for t in range(tsteps):\n",
    "        # Kuramoto with directed influence\n",
    "        sin_terms = np.zeros(N)\n",
    "        for i in range(N):\n",
    "            # incoming influences to i from j\n",
    "            diffs = theta - theta[i]\n",
    "            sin_terms[i] = np.sum(W[i,:] * np.sin(diffs))\n",
    "        dtheta = w + K * sin_terms + rng.normal(0, noise, size=N)\n",
    "        theta = (theta + dt * dtheta) % (2*np.pi)\n",
    "        TH[t,:] = theta\n",
    "    # Observed signals: band-limited sinusoids + mild amplitude noise\n",
    "    sig = np.sin(TH) + 0.15*rng.normal(0,1,(tsteps,N))\n",
    "    return TH, sig, (W>0).astype(bool)\n",
    "\n",
    "# ------------------------------------\n",
    "# 2) Filters & frequency sub-bands\n",
    "# ------------------------------------\n",
    "def bandpass(x, fs, lo, hi, order=4):\n",
    "    nyq = fs/2.0\n",
    "    lo = max(1e-6, lo/nyq); hi = min(0.999, hi/nyq)\n",
    "    b,a = butter(order, [lo,hi], btype='bandpass')\n",
    "    return filtfilt(b,a,x,axis=0)\n",
    "\n",
    "def multi_band(signals, fs):\n",
    "    # Classic EEG-ish bands (Hz), but we‚Äôre synthetic; still useful consensus diversity\n",
    "    bands = {\n",
    "        \"Œ¥\": (0.5, 4.0),\n",
    "        \"Œ∏\": (4.0, 8.0),\n",
    "        \"Œ±\": (8.0, 13.0),\n",
    "        \"Œ≤\": (13.0, 30.0),\n",
    "        \"Œ≥\": (30.0, 48.0)   # keep below Nyquist\n",
    "    }\n",
    "    out = {}\n",
    "    for k,(lo,hi) in bands.items():\n",
    "        try:\n",
    "            out[k] = bandpass(signals, fs, lo, hi)\n",
    "        except Exception:\n",
    "            # If bandpass fails (e.g., too-low/fs), fall back to raw for that band\n",
    "            out[k] = signals.copy()\n",
    "    return out\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# 3) Minimal Granger F-test (no statsmodels required)\n",
    "# ----------------------------------------------------\n",
    "def _lag_stack(X, maxlag):\n",
    "    \"\"\"\n",
    "    Build design matrices for VAR-like regression:\n",
    "    y_t (for each series) against lagged predictors of ALL series up to maxlag.\n",
    "    Returns (Y, Z), where Y is stacked targets, Z stacked regressors, and index map for (i target).\n",
    "    \"\"\"\n",
    "    T, N = X.shape\n",
    "    rows = T - maxlag\n",
    "    Y = np.zeros((rows*N, ))  # concatenated targets per variable\n",
    "    Z = np.zeros((rows*N, N*maxlag + 1))  # intercept + all lag blocks\n",
    "    row = 0\n",
    "    target_idx = []\n",
    "    for i in range(N):\n",
    "        y = X[maxlag:, i]\n",
    "        # build regressors\n",
    "        reg = [np.ones(rows)]\n",
    "        for lag in range(1, maxlag+1):\n",
    "            reg.append(X[maxlag - lag: T - lag, :].T.reshape(N, rows))\n",
    "        R = np.vstack(reg)  # shape: (1 + N*maxlag, rows)\n",
    "        Y[row:row+rows] = y\n",
    "        Z[row:row+rows, :] = R.T\n",
    "        target_idx.append((row, row+rows))\n",
    "        row += rows\n",
    "    return Y, Z, target_idx\n",
    "\n",
    "def granger_matrix_F(X, maxlag=6):\n",
    "    \"\"\"\n",
    "    For each directed pair j -> i, test whether including x_j lag-block improves\n",
    "    prediction of y_i over the reduced model (without x_j).\n",
    "    Returns matrix of p-values (NxN) with NaN on diagonal.\n",
    "    \"\"\"\n",
    "    T,N = X.shape\n",
    "    Y, Z, tblocks = _lag_stack(X, maxlag)\n",
    "    # Precompute OLS for the full design (all predictors)\n",
    "    beta_full, _, _, _ = lstsq(Z, Y, rcond=None)\n",
    "    resid_full = Y - Z @ beta_full\n",
    "    RSS_full = []\n",
    "    for i,(a,b) in enumerate(tblocks):\n",
    "        RSS_full.append(np.sum(resid_full[a:b]**2))\n",
    "    RSS_full = np.array(RSS_full)\n",
    "\n",
    "    # For each j, build reduced model by removing its lag-columns\n",
    "    pvals = np.full((N,N), np.nan)\n",
    "    rows = (T - maxlag)\n",
    "    p_full = Z.shape[1]\n",
    "    for i in range(N):\n",
    "        # indices for target i within concatenated rows\n",
    "        a,b = tblocks[i]\n",
    "        # Find columns belonging to j's lags\n",
    "        for j in range(N):\n",
    "            if i == j:\n",
    "                continue\n",
    "            keep_cols = [0]  # intercept\n",
    "            for lag in range(1, maxlag+1):\n",
    "                for jj in range(N):\n",
    "                    col = 1 + (lag-1)*N + jj\n",
    "                    if jj == j:\n",
    "                        continue\n",
    "                    keep_cols.append(col)\n",
    "            Z_red = Z[a:b][:, keep_cols]\n",
    "            y_i = Y[a:b]\n",
    "            beta_red, _, _, _ = lstsq(Z_red, y_i, rcond=None)\n",
    "            resid_red = y_i - Z_red @ beta_red\n",
    "            RSS_red = np.sum(resid_red**2)\n",
    "\n",
    "            # F-statistic (nested): ( (RSS_red - RSS_full) / df_num ) / ( RSS_full / df_den )\n",
    "            df_num = Z.shape[1] - len(keep_cols)     # number of parameters dropped = N lags of j\n",
    "            df_den = rows - Z.shape[1]\n",
    "            if df_den <= 0 or df_num <= 0:\n",
    "                pvals[i,j] = 1.0\n",
    "                continue\n",
    "            F = ((RSS_red - RSS_full[i]) / df_num) / (RSS_full[i] / df_den)\n",
    "            # Convert to p-value using survival function of F(df_num, df_den)\n",
    "            from scipy.stats import f\n",
    "            p = 1.0 - f.cdf(F, df_num, df_den)\n",
    "            pvals[i,j] = max(min(p,1.0), 0.0)\n",
    "    return pvals\n",
    "\n",
    "# ---------------------------------------------\n",
    "# 4) CCC: bands √ó lag-jitter √ó block-bootstrap\n",
    "# ---------------------------------------------\n",
    "def fdr_bh(pvals, alpha=0.05):\n",
    "    \"\"\"Benjamini‚ÄìHochberg across flattened p-values (ignoring NaNs). Returns boolean mask of rejections in the same shape.\"\"\"\n",
    "    P = pvals.copy()\n",
    "    shape = P.shape\n",
    "    pv = P[~np.isnan(P)].ravel()\n",
    "    m = len(pv)\n",
    "    if m==0: return np.zeros_like(P, dtype=bool)\n",
    "    order = np.argsort(pv)\n",
    "    ranked = np.empty_like(order); ranked[order] = np.arange(1, m+1)\n",
    "    thresh = (ranked / m) * alpha\n",
    "    reject_flat = pv <= thresh\n",
    "    # ensure monotonicity\n",
    "    if np.any(reject_flat):\n",
    "        k = np.max(np.where(reject_flat)[0])\n",
    "        cutoff = pv[order][k]\n",
    "        rej = (P <= cutoff)\n",
    "    else:\n",
    "        rej = np.zeros_like(P, dtype=bool)\n",
    "    rej[np.isnan(P)] = False\n",
    "    return rej\n",
    "\n",
    "def block_bootstrap_scores(X, fs, maxlag=6, B=120, block=None, jit=2, bands=None, rng=None):\n",
    "    \"\"\"\n",
    "    Compute consensus scores across bands with lag jitter + block bootstrap.\n",
    "    Returns:\n",
    "      score_matrix (NxN), pval_matrix (NxN) via bootstrap null, and per-band stability.\n",
    "    \"\"\"\n",
    "    if rng is None: rng = np.random.default_rng(0)\n",
    "    T,N = X.shape\n",
    "    if block is None: block = max(40, T//10)\n",
    "    # Prepare bands\n",
    "    if bands is None:\n",
    "        bands = multi_band(X, fs)\n",
    "    keys = list(bands.keys())\n",
    "\n",
    "    # helper to compute -log10 p for a given band/lag choice\n",
    "    def band_trial(bX, lag):\n",
    "        P = granger_matrix_F(bX, maxlag=lag)\n",
    "        with np.errstate(divide='ignore'):\n",
    "            S = -np.log10(np.maximum(P, 1e-300))\n",
    "        np.fill_diagonal(S, 0.0)\n",
    "        return S, P\n",
    "\n",
    "    # Original full-sample consensus with jit\n",
    "    S_bag = []\n",
    "    P_bag = []\n",
    "    for _ in range(jit):\n",
    "        lag = max(2, maxlag + rng.integers(-2, 3))  # jitter lag by ¬±2\n",
    "        Stot = np.zeros((N,N)); Ptot = np.zeros((N,N))\n",
    "        for k in keys:\n",
    "            S,P = band_trial(bands[k], lag)\n",
    "            Stot += S; Ptot += P\n",
    "        S_bag.append(Stot/len(keys)); P_bag.append(Ptot/len(keys))\n",
    "    S_full = np.mean(np.stack(S_bag,0),0)\n",
    "    P_full = np.mean(np.stack(P_bag,0),0)\n",
    "\n",
    "    # Block bootstrap to assess stability & null\n",
    "    def block_resample(Xin):\n",
    "        T,_ = Xin.shape\n",
    "        idx = []\n",
    "        while len(idx) < T:\n",
    "            start = rng.integers(0, max(1, T-block))\n",
    "            idx.extend(list(range(start, min(T, start+block))))\n",
    "        idx = np.array(idx[:T])\n",
    "        return Xin[idx,:]\n",
    "\n",
    "    boots = []\n",
    "    for b in range(B):\n",
    "        # resample time within each band identically to preserve cross-band alignment\n",
    "        idx = None\n",
    "        bband = {}\n",
    "        T,_ = X.shape\n",
    "        # create a single idx per bootstrap\n",
    "        idx = []\n",
    "        while len(idx) < T:\n",
    "            start = rng.integers(0, max(1, T-block))\n",
    "            idx.extend(list(range(start, min(T, start+block))))\n",
    "        idx = np.array(idx[:T])\n",
    "        for k in keys:\n",
    "            bband[k] = bands[k][idx,:]\n",
    "        lag = max(2, maxlag + rng.integers(-2, 3))\n",
    "        Stot = np.zeros((N,N))\n",
    "        for k in keys:\n",
    "            S,_ = band_trial(bband[k], lag)\n",
    "            Stot += S\n",
    "        boots.append(Stot/len(keys))\n",
    "    boots = np.stack(boots,0)\n",
    "    S_mu = boots.mean(0)\n",
    "    S_sd = boots.std(0) + 1e-9\n",
    "\n",
    "    # Z-score stability boost; and bootstrap p via upper-tail\n",
    "    Z = (S_full - S_mu) / S_sd\n",
    "    p_boot = 1.0 - (np.sum(boots >= S_full[None,:,:], axis=0) / boots.shape[0])\n",
    "    np.fill_diagonal(p_boot, 1.0)\n",
    "\n",
    "    # Final CCC score combines magnitude & stability\n",
    "    score = np.maximum(0.0, S_full) * np.maximum(0.0, Z)\n",
    "    return score, p_boot, S_full\n",
    "\n",
    "# ---------------------------------------\n",
    "# 5) Evaluation vs Broadband Granger\n",
    "# ---------------------------------------\n",
    "def evaluate(adj_true, rej_mask):\n",
    "    \"\"\"Precision, Recall, F1, AUCPR estimate via threshold sweep on score or pvals.\"\"\"\n",
    "    N = adj_true.shape[0]\n",
    "    tp = np.sum(rej_mask & adj_true)\n",
    "    fp = np.sum(rej_mask & (~adj_true) & (~np.eye(N,dtype=bool)))\n",
    "    fn = np.sum((~rej_mask) & adj_true)\n",
    "    precision = tp / max(1, tp+fp)\n",
    "    recall    = tp / max(1, tp+fn)\n",
    "    f1 = 2*precision*recall / max(1e-12, precision+recall)\n",
    "    return precision, recall, f1\n",
    "\n",
    "def auc_pr_from_scores(adj_true, scores, n_thresh=64):\n",
    "    # sweep thresholds over quantiles of scores (excluding diag)\n",
    "    N = scores.shape[0]\n",
    "    S = scores.copy()\n",
    "    S[np.eye(N, dtype=bool)] = -np.inf\n",
    "    vals = S[np.isfinite(S)]\n",
    "    if vals.size == 0:\n",
    "        return 0.0\n",
    "    qs = np.quantile(vals, np.linspace(0.0, 1.0, n_thresh))\n",
    "    PR = []\n",
    "    for th in qs:\n",
    "        rej = (S >= th)\n",
    "        p,r,_ = evaluate(adj_true, rej)\n",
    "        PR.append((p,r))\n",
    "    # approximate AUC via Riemann sum over recall, interpolating precision monotonically\n",
    "    PR = np.array(PR)\n",
    "    # ensure nonincreasing precision w.r.t recall\n",
    "    order = np.argsort(PR[:,1])\n",
    "    R = PR[order,1]; P = PR[order,0]\n",
    "    for i in range(len(P)-2, -1, -1):\n",
    "        P[i] = max(P[i], P[i+1])\n",
    "    auc = np.trapz(P, R)\n",
    "    return float(auc)\n",
    "\n",
    "# ---------------------------------------\n",
    "# 6) Experiment runner\n",
    "# ---------------------------------------\n",
    "def run_experiment(\n",
    "    runs=24,\n",
    "    N=10, T=12.0, dt=0.01,\n",
    "    fs=None,\n",
    "    K=0.9, sparsity=0.78, noise=0.25,\n",
    "    maxlag=6, B=120, block=None, jit=3,\n",
    "    alpha=0.05, seed=1234\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    AUC_ccc, AUC_gr = [], []\n",
    "    F1_ccc, F1_gr = [], []\n",
    "    summaries = []\n",
    "\n",
    "    for r in range(runs):\n",
    "        s = int(rng.integers(0, 10_000_000))\n",
    "        TH, X, adj = kuramoto_network(T=T, dt=dt, N=N, K=K, sparsity=sparsity, noise=noise, seed=s)\n",
    "        Tsteps = X.shape[0]\n",
    "        if fs is None:\n",
    "            fs = 1.0/dt\n",
    "\n",
    "        # CCC\n",
    "        bands = multi_band(X, fs)\n",
    "        score, p_boot, Sfull = block_bootstrap_scores(\n",
    "            X, fs, maxlag=maxlag, B=B, block=block, jit=jit, bands=bands, rng=rng\n",
    "        )\n",
    "        rej_ccc = fdr_bh(p_boot, alpha=alpha)\n",
    "        p_cc, r_cc, f_cc = evaluate(adj, rej_ccc)\n",
    "        auc_cc = auc_pr_from_scores(adj, score)\n",
    "\n",
    "        # Broadband Granger baseline\n",
    "        P_gr = granger_matrix_F(X, maxlag=maxlag)\n",
    "        rej_gr = fdr_bh(P_gr, alpha=alpha)\n",
    "        p_gr, r_gr, f_gr = evaluate(adj, rej_gr)\n",
    "\n",
    "        # AUCPR for Granger via -log10 p as a score\n",
    "        with np.errstate(divide='ignore'):\n",
    "            S_gr = -np.log10(np.maximum(P_gr, 1e-300))\n",
    "        np.fill_diagonal(S_gr, -np.inf)\n",
    "        auc_gr = auc_pr_from_scores(adj, S_gr)\n",
    "\n",
    "        AUC_ccc.append(auc_cc); AUC_gr.append(auc_gr)\n",
    "        F1_ccc.append(f_cc);    F1_gr.append(f_gr)\n",
    "\n",
    "        summaries.append(dict(seed=s, auc_ccc=auc_cc, auc_gr=auc_gr, f1_ccc=f_cc, f1_gr=f_gr))\n",
    "\n",
    "    AUC_ccc = np.array(AUC_ccc); AUC_gr = np.array(AUC_gr)\n",
    "    F1_ccc  = np.array(F1_ccc);  F1_gr  = np.array(F1_gr)\n",
    "\n",
    "    # Paired permutation test on AUC deltas\n",
    "    delta = AUC_ccc - AUC_gr\n",
    "    observed = np.mean(delta)\n",
    "    nperm = 5000\n",
    "    rng2 = np.random.default_rng(seed+1)\n",
    "    signs = rng2.choice([-1,1], size=(nperm, len(delta)))\n",
    "    perm_means = np.mean(signs * delta[None,:], axis=1)\n",
    "    p_perm = np.mean(perm_means >= observed)\n",
    "\n",
    "    result = {\n",
    "        \"AUC_ccc_mean\": float(np.mean(AUC_ccc)),\n",
    "        \"AUC_gr_mean\":  float(np.mean(AUC_gr)),\n",
    "        \"AUC_ccc_median\": float(np.median(AUC_ccc)),\n",
    "        \"AUC_gr_median\":  float(np.median(AUC_gr)),\n",
    "        \"AUC_delta_mean\": float(observed),\n",
    "        \"delta>0_fraction\": float(np.mean(delta>0)),\n",
    "        \"p_perm_one_sided\": float(p_perm),\n",
    "        \"F1_ccc_mean\": float(np.mean(F1_ccc)),\n",
    "        \"F1_gr_mean\":  float(np.mean(F1_gr)),\n",
    "        \"runs\": runs,\n",
    "        \"summaries\": summaries\n",
    "    }\n",
    "    return result, (AUC_ccc, AUC_gr), (F1_ccc, F1_gr)\n",
    "\n",
    "# ---------------------------------------\n",
    "# 7) Run it\n",
    "# ---------------------------------------\n",
    "start = time.time()\n",
    "cfg = dict(\n",
    "    runs=28,          # more runs => stronger test (keep runtime sane)\n",
    "    N=10,             # nodes\n",
    "    T=12.0, dt=0.01,  # 1200 samples\n",
    "    K=0.95,           # coupling strength (moderate)\n",
    "    sparsity=0.80,    # 20% edges present\n",
    "    noise=0.28,       # phase noise\n",
    "    maxlag=6,\n",
    "    B=150,            # bootstrap reps\n",
    "    jit=3,\n",
    "    alpha=0.05,\n",
    "    seed=424242\n",
    ")\n",
    "res, aucs, f1s = run_experiment(**cfg)\n",
    "elapsed = time.time()-start\n",
    "\n",
    "# ---------------------------------------\n",
    "# 8) Display results & tiny plots\n",
    "# ---------------------------------------\n",
    "AUC_ccc, AUC_gr = aucs\n",
    "F1_ccc, F1_gr   = f1s\n",
    "\n",
    "print(\"=== BIG CLAIM TEST: CCC vs Broadband Granger ===\")\n",
    "print(f\"Runs: {res['runs']}   Elapsed: {elapsed:.1f}s\")\n",
    "print(f\"AUC(PR)  ‚Äî CCC: mean {res['AUC_ccc_mean']:.3f} | Granger: mean {res['AUC_gr_mean']:.3f} \"\n",
    "      f\"| Œî(mean): {res['AUC_delta_mean']:.3f}\")\n",
    "print(f\"F1       ‚Äî CCC: mean {res['F1_ccc_mean']:.3f} | Granger: mean {res['F1_gr_mean']:.3f}\")\n",
    "print(f\"Fraction of runs where CCC AUC > Granger AUC: {res['delta>0_fraction']:.2f}\")\n",
    "print(f\"Paired permutation p (one-sided, CCC>Granger): p = {res['p_perm_one_sided']:.5f}\")\n",
    "\n",
    "claim_ok = (res['delta>0_fraction'] >= 0.70) and (res['p_perm_one_sided'] < 0.01)\n",
    "print(\"\\nDECISION:\", \"‚úÖ SUPPORTS CLAIM\" if claim_ok else \"‚ùå FAILS TO SUPPORT CLAIM\")\n",
    "\n",
    "# Simple visualization\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(range(len(AUC_ccc)), AUC_ccc, label=\"CCC AUC(PR)\", s=20)\n",
    "ax.scatter(range(len(AUC_gr)),  AUC_gr,  label=\"Granger AUC(PR)\", marker='x', s=24)\n",
    "ax.set_xlabel(\"Run\")\n",
    "ax.set_ylabel(\"AUC(PR)\")\n",
    "ax.set_title(\"CCC vs Granger across runs\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f58f601-bd35-48ff-8718-9f92a0db0a10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
