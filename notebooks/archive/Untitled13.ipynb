{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54615156-84b3-4b28-80b9-05f5ad9a5463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] pytrends unavailable; skipping Google Trends.\n",
      "Fetching data...\n",
      "[WARN] wiki fail Mass_surveillance_in_the_United_States: 403 Client Error: Forbidden for url: https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/user/Mass_surveillance_in_the_United_States/daily/2012010100/2025101600\n",
      "[WARN] wiki fail First_Amendment_to_the_United_States_Constitution: 403 Client Error: Forbidden for url: https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/user/First_Amendment_to_the_United_States_Constitution/daily/2012010100/2025101600\n",
      "[WARN] wiki fail Censorship_in_the_United_States: 403 Client Error: Forbidden for url: https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/user/Censorship_in_the_United_States/daily/2012010100/2025101600\n",
      "[WARN] wiki fail Civil_liberties_in_the_United_States: 403 Client Error: Forbidden for url: https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/user/Civil_liberties_in_the_United_States/daily/2012010100/2025101600\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No data sources available.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 101\u001b[39m\n\u001b[32m     99\u001b[39m wiki   = fetch_wikiviews(CFG[\u001b[33m\"\u001b[39m\u001b[33mWIKI_PAGES\u001b[39m\u001b[33m\"\u001b[39m], since=CFG[\u001b[33m\"\u001b[39m\u001b[33mBACKTEST_START\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    100\u001b[39m frames = [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m [trends, wiki] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.empty]\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m frames: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo data sources available.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    102\u001b[39m df = pd.concat(frames, axis=\u001b[32m1\u001b[39m).sort_index().fillna(method=\u001b[33m\"\u001b[39m\u001b[33mffill\u001b[39m\u001b[33m\"\u001b[39m).fillna(method=\u001b[33m\"\u001b[39m\u001b[33mbfill\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# === FEATURES + FORECASTABILITY ==============================================\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: No data sources available."
     ]
    }
   ],
   "source": [
    "# === CNT \"Wake-Up Field\" Mega Cell ============================================\n",
    "# Requirements: numpy, pandas, matplotlib, scikit-learn, statsmodels, torch, requests\n",
    "# Optional (auto-skippable): pytrends (Google Trends), prophet (forecast), plotly (interactive)\n",
    "# -----------------------------------------------------------------------------\n",
    "import os, sys, time, json, math, datetime as dt, warnings, requests\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.tsa.statespace.structural import UnobservedComponents\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# === CONFIGURATION ===========================================================\n",
    "CFG = dict(\n",
    "    ROOT = Path.cwd(),\n",
    "    REGION = \"US\",\n",
    "    HORIZON_WEEKS = 52,\n",
    "    THRESHOLD_TAU = 0.65,\n",
    "    RUN_LENGTH_K = 6,\n",
    "    BACKTEST_START = \"2012-01-01\",\n",
    "    TOPICS = [\"government surveillance\",\"censorship\",\"civil liberties\",\"free speech\"],\n",
    "    WIKI_PAGES = [\n",
    "        \"Mass_surveillance_in_the_United_States\",\n",
    "        \"First_Amendment_to_the_United_States_Constitution\",\n",
    "        \"Censorship_in_the_United_States\",\n",
    "        \"Civil_liberties_in_the_United_States\"\n",
    "    ],\n",
    "    SAVE_DIR = \"artifacts\",\n",
    "    RNG_SEED = 1337\n",
    ")\n",
    "\n",
    "os.makedirs(CFG[\"SAVE_DIR\"], exist_ok=True)\n",
    "np.random.seed(CFG[\"RNG_SEED\"])\n",
    "\n",
    "# === UTILITY FUNCTIONS =======================================================\n",
    "def as_week_index(dts): return pd.to_datetime(dts).to_period('W-MON').to_timestamp()\n",
    "\n",
    "def spectral_entropy(x, nseg=8, eps=1e-12):\n",
    "    x = pd.Series(x).dropna().values\n",
    "    if len(x) < 16: return np.nan\n",
    "    x = (x - x.mean()) / (x.std()+1e-9)\n",
    "    seg = max(16, len(x)//nseg)\n",
    "    ps = []\n",
    "    for i in range(0, len(x)-seg+1, seg):\n",
    "        X = np.fft.rfft(x[i:i+seg]); P = (X*np.conj(X)).real; P /= P.sum()+eps; ps.append(P)\n",
    "    Pm = np.mean(ps, axis=0)\n",
    "    H = -(Pm*np.log(Pm+eps)).sum(); Hmax = math.log(len(Pm))\n",
    "    return float(1 - H/Hmax)\n",
    "\n",
    "def zscore(s): return (s - np.nanmean(s)) / (np.nanstd(s) + 1e-9)\n",
    "def logistic_scale(s):\n",
    "    q1,q2,q3 = np.nanquantile(s,[.1,.5,.9]); scale = (q3-q1)/2 or np.nanstd(s) or 1.0\n",
    "    return 1/(1+np.exp(-(s-q2)/(scale+1e-9)))\n",
    "\n",
    "# === SAFE IMPORT FOR PYTRENDS ================================================\n",
    "try:\n",
    "    from pytrends.request import TrendReq\n",
    "    _HAS_TRENDS = True\n",
    "except Exception:\n",
    "    print(\"[INFO] pytrends unavailable; skipping Google Trends.\")\n",
    "    TrendReq = None; _HAS_TRENDS = False\n",
    "\n",
    "# === FETCHERS ================================================================\n",
    "def fetch_trends(topics, geo=\"US\", since=\"2012-01-01\"):\n",
    "    if not _HAS_TRENDS: return pd.DataFrame()\n",
    "    pytrends = TrendReq(hl=\"en-US\", tz=360)\n",
    "    frames = []\n",
    "    for kw in topics:\n",
    "        try:\n",
    "            pytrends.build_payload([kw], timeframe=f\"{since} {dt.date.today():%Y-%m-%d}\", geo=geo)\n",
    "            df = pytrends.interest_over_time()\n",
    "            if not df.empty:\n",
    "                s = df[kw].rename(kw); s.index = as_week_index(s.index); frames.append(s)\n",
    "            time.sleep(1.0)\n",
    "        except Exception as e: print(f\"[WARN] pytrends failed for {kw}: {e}\")\n",
    "    return pd.concat(frames, axis=1).sort_index() if frames else pd.DataFrame()\n",
    "\n",
    "def fetch_wikiviews(pages, since=\"2012-01-01\", project=\"en.wikipedia\", agent=\"user\"):\n",
    "    def one_page(title):\n",
    "        start = pd.Timestamp(since).strftime(\"%Y%m0100\")\n",
    "        end = (pd.Timestamp(dt.date.today()) + pd.offsets.Day(0)).strftime(\"%Y%m%d00\")\n",
    "        url = (f\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/\"\n",
    "               f\"{project}/all-access/{agent}/{title}/daily/{start}/{end}\")\n",
    "        try:\n",
    "            r = requests.get(url, timeout=20); r.raise_for_status()\n",
    "            data = r.json().get(\"items\", [])\n",
    "            ts = {pd.to_datetime(i[\"timestamp\"][:8]): i[\"views\"] for i in data}\n",
    "            s = pd.Series(ts, name=title).sort_index().resample(\"W-MON\").sum()\n",
    "            return s\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] wiki fail {title}: {e}\"); return pd.Series(dtype=float)\n",
    "    cols=[one_page(p.replace(\" \",\"_\")) for p in pages]; cols=[c for c in cols if c.shape[0]]\n",
    "    return pd.concat(cols, axis=1).sort_index() if cols else pd.DataFrame()\n",
    "\n",
    "# === INGEST ==================================================================\n",
    "print(\"Fetching data...\")\n",
    "trends = fetch_trends(CFG[\"TOPICS\"], geo=CFG[\"REGION\"], since=CFG[\"BACKTEST_START\"])\n",
    "wiki   = fetch_wikiviews(CFG[\"WIKI_PAGES\"], since=CFG[\"BACKTEST_START\"])\n",
    "frames = [f for f in [trends, wiki] if not f.empty]\n",
    "if not frames: raise RuntimeError(\"No data sources available.\")\n",
    "df = pd.concat(frames, axis=1).sort_index().fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "\n",
    "# === FEATURES + FORECASTABILITY ==============================================\n",
    "feat = pd.DataFrame(index=df.index)\n",
    "for c in df.columns:\n",
    "    feat[c] = df[c]; feat[c+\"_z\"]=zscore(df[c]); feat[c+\"_vol4\"]=df[c].pct_change().rolling(4).std()\n",
    "weights = {c: spectral_entropy(df[c].values) for c in df.columns}\n",
    "w = pd.Series(weights).fillna(0.5); w=(w-w.min())/(w.max()-w.min()+1e-9); w=w.clip(0.05,1.0)\n",
    "\n",
    "# === DYNAMIC FACTOR (OAI) ====================================================\n",
    "scaler = StandardScaler(); X = scaler.fit_transform(df.values)\n",
    "W = np.diag(np.sqrt(w[df.columns].values)); Xw = X.dot(W)\n",
    "pca = PCA(n_components=1, random_state=CFG[\"RNG_SEED\"])\n",
    "oai_raw = pca.fit_transform(Xw).ravel()\n",
    "OAI = logistic_scale(zscore(oai_raw)); OAI = pd.Series(OAI, index=df.index, name=\"OAI\")\n",
    "\n",
    "# === STATE-SPACE NOWCAST =====================================================\n",
    "exo = feat.filter(regex=\"_z$|_vol4$\").fillna(0)\n",
    "model = UnobservedComponents(endog=OAI, level='local', exog=exo)\n",
    "res = model.fit(disp=False)\n",
    "h = CFG[\"HORIZON_WEEKS\"]\n",
    "lastX = exo.iloc[-1:].values; Xf = np.repeat(lastX, h, axis=0)\n",
    "fc = res.get_forecast(steps=h, exog=Xf)\n",
    "idxf = pd.date_range(OAI.index[-1]+pd.offsets.Week(1), periods=h, freq=\"W-MON\")\n",
    "OAI_fc = pd.Series(fc.predicted_mean.clip(0,1), index=idxf)\n",
    "\n",
    "# === CHANGE-POINT DETECTION (CUSUM) =========================================\n",
    "resid = OAI - res.fittedvalues.reindex_like(OAI).fillna(method=\"bfill\")\n",
    "k,hc = resid.std()*0.25, resid.std()*3.0\n",
    "pos=neg=0; alarms=[]\n",
    "for t,e in resid.items():\n",
    "    pos=max(0,pos+e-k); neg=min(0,neg+e+k)\n",
    "    if pos>hc or abs(neg)>hc: alarms.append(t); pos=neg=0\n",
    "cp_dates = alarms[-5:]\n",
    "\n",
    "# === EVENT FORECAST ==========================================================\n",
    "tau,k = CFG[\"THRESHOLD_TAU\"], CFG[\"RUN_LENGTH_K\"]\n",
    "sig = float(resid.std() or 0.05)\n",
    "n_sims = 1500\n",
    "paths = np.clip(OAI_fc.values + np.random.normal(0,sig,(n_sims,h)),0,1)\n",
    "def sustained(sim,tau,k):\n",
    "    run=0\n",
    "    for i,a in enumerate(sim>=tau):\n",
    "        run=run+1 if a else 0\n",
    "        if run>=k: return i\n",
    "    return None\n",
    "hits=[sustained(p,tau,k) for p in paths]; hit_idxs=[x for x in hits if x is not None]\n",
    "if hit_idxs:\n",
    "    dates=[idxf[i] for i in hit_idxs]\n",
    "    med=np.median(pd.to_datetime(dates))\n",
    "    pH=len(hit_idxs)/n_sims\n",
    "    d80=(np.percentile(pd.to_datetime(dates),10),np.percentile(pd.to_datetime(dates),90))\n",
    "else: med,pH,d80=None,0.0,(None,None)\n",
    "\n",
    "# === VISUALS ================================================================\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(OAI.index,OAI,label=\"OAI\")\n",
    "plt.plot(res.fittedvalues.index,res.fittedvalues.clip(0,1),\"--\",label=\"Fit\")\n",
    "plt.axhline(tau,linestyle=\":\",label=f\"τ={tau}\")\n",
    "for d in cp_dates: plt.axvline(d,linestyle=\":\",alpha=.5)\n",
    "plt.legend(); plt.title(\"Overreach Awareness Index (OAI)\")\n",
    "plt.tight_layout(); plt.savefig(\"artifacts/oai_fit.png\",dpi=160); plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(OAI_fc.index,OAI_fc,label=\"Forecast mean\")\n",
    "plt.axhline(tau,linestyle=\":\",label=f\"τ={tau}\")\n",
    "plt.legend(); plt.title(\"OAI Forecast Horizon\")\n",
    "plt.tight_layout(); plt.savefig(\"artifacts/oai_fc.png\",dpi=160); plt.close()\n",
    "\n",
    "prob_curve=[np.mean([(x is not None and x<=t) for x in hits]) for t in range(h)]\n",
    "pd.Series(prob_curve,index=idxf).plot(figsize=(10,3),ylim=(0,1),title=\"Probability of sustained crossing\"); \n",
    "plt.tight_layout(); plt.savefig(\"artifacts/oai_prob.png\",dpi=160); plt.close()\n",
    "\n",
    "# === SUMMARY ================================================================\n",
    "summary = dict(\n",
    "    generated_at = dt.datetime.utcnow().isoformat()+\"Z\",\n",
    "    horizon_weeks = h,\n",
    "    threshold_tau = tau,\n",
    "    run_length_k = k,\n",
    "    last_week = str(OAI.index[-1].date()),\n",
    "    change_points = [str(pd.Timestamp(d).date()) for d in cp_dates],\n",
    "    prob_within_horizon = round(pH,3),\n",
    "    median_event_date = (str(pd.Timestamp(med).date()) if med else None),\n",
    "    event_window_80 = tuple(str(pd.Timestamp(d).date()) if d else None for d in d80),\n",
    "    weights = {k:float(v) for k,v in w.to_dict().items()},\n",
    "    pca_var = float(pca.explained_variance_ratio_[0])\n",
    ")\n",
    "with open(CFG[\"SAVE_DIR\"]+\"/oai_summary.json\",\"w\") as f: json.dump(summary,f,indent=2)\n",
    "print(json.dumps(summary,indent=2))\n",
    "print(\"\\nFigures saved in ./artifacts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef983c9-83ae-4efc-bf6e-b1761f7edf6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] pytrends unavailable; skipping Google Trends.\n",
      "Fetching data...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid level/trend specification: 'local'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 132\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# === NOWCAST =================================================================\u001b[39;00m\n\u001b[32m    131\u001b[39m exo=feat.filter(regex=\u001b[33m\"\u001b[39m\u001b[33m_z$|_vol4$\u001b[39m\u001b[33m\"\u001b[39m).fillna(\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m model=\u001b[43mUnobservedComponents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOAI\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlocal\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m res=model.fit(disp=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    134\u001b[39m h=CFG[\u001b[33m\"\u001b[39m\u001b[33mHORIZON_WEEKS\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\structural.py:490\u001b[39m, in \u001b[36mUnobservedComponents.__init__\u001b[39m\u001b[34m(self, endog, level, trend, seasonal, freq_seasonal, cycle, autoregressive, exog, irregular, stochastic_level, stochastic_trend, stochastic_seasonal, stochastic_freq_seasonal, stochastic_cycle, damped_cycle, cycle_period_bounds, mle_regression, use_exact_diffuse, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28mself\u001b[39m.trend_specification = \u001b[33m'\u001b[39m\u001b[33mrandom trend\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInvalid level/trend specification: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m                          % spec)\n\u001b[32m    493\u001b[39m \u001b[38;5;66;03m# Check for a model that makes sense\u001b[39;00m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trend \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m level:\n",
      "\u001b[31mValueError\u001b[39m: Invalid level/trend specification: 'local'"
     ]
    }
   ],
   "source": [
    "# === CNT \"Wake-Up Field\" Mega-Cell (Resilient Edition) ========================\n",
    "# Works online or offline; includes static fallback.\n",
    "# -----------------------------------------------------------------------------\n",
    "import os, sys, time, json, math, datetime as dt, warnings, requests\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.tsa.statespace.structural import UnobservedComponents\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# === CONFIG ==================================================================\n",
    "CFG = dict(\n",
    "    ROOT = Path.cwd(),\n",
    "    REGION = \"US\",\n",
    "    HORIZON_WEEKS = 52,\n",
    "    THRESHOLD_TAU = 0.65,\n",
    "    RUN_LENGTH_K = 6,\n",
    "    BACKTEST_START = \"2012-01-01\",\n",
    "    TOPICS = [\"government surveillance\",\"censorship\",\"civil liberties\",\"free speech\"],\n",
    "    WIKI_PAGES = [\n",
    "        \"Mass_surveillance_in_the_United_States\",\n",
    "        \"First_Amendment_to_the_United_States_Constitution\",\n",
    "        \"Censorship_in_the_United_States\",\n",
    "        \"Civil_liberties_in_the_United_States\"\n",
    "    ],\n",
    "    SAVE_DIR = \"artifacts\",\n",
    "    RNG_SEED = 1337\n",
    ")\n",
    "os.makedirs(CFG[\"SAVE_DIR\"], exist_ok=True)\n",
    "np.random.seed(CFG[\"RNG_SEED\"])\n",
    "\n",
    "# === REQUESTS SESSION (fixes 403s) ===========================================\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\n",
    "    \"User-Agent\": \"CNTLab/1.0 (https://example.org; contact: researcher@fieldwalker.org)\"\n",
    "})\n",
    "requests_get = SESSION.get\n",
    "\n",
    "# === SAFE IMPORTS ============================================================\n",
    "try:\n",
    "    from pytrends.request import TrendReq\n",
    "    _HAS_TRENDS = True\n",
    "except Exception:\n",
    "    print(\"[INFO] pytrends unavailable; skipping Google Trends.\")\n",
    "    TrendReq = None; _HAS_TRENDS = False\n",
    "\n",
    "# === UTILITIES ===============================================================\n",
    "def as_week_index(dts): return pd.to_datetime(dts).to_period('W-MON').to_timestamp()\n",
    "def spectral_entropy(x, nseg=8, eps=1e-12):\n",
    "    x = pd.Series(x).dropna().values\n",
    "    if len(x)<16: return np.nan\n",
    "    x=(x-x.mean())/(x.std()+1e-9)\n",
    "    seg=max(16,len(x)//nseg); ps=[]\n",
    "    for i in range(0,len(x)-seg+1,seg):\n",
    "        X=np.fft.rfft(x[i:i+seg]); P=(X*np.conj(X)).real; P/=P.sum()+eps; ps.append(P)\n",
    "    Pm=np.mean(ps,axis=0); H=-(Pm*np.log(Pm+eps)).sum(); Hmax=math.log(len(Pm))\n",
    "    return float(1-H/Hmax)\n",
    "def zscore(s): return (s-np.nanmean(s))/(np.nanstd(s)+1e-9)\n",
    "def logistic_scale(s):\n",
    "    q1,q2,q3=np.nanquantile(s,[.1,.5,.9]); scale=(q3-q1)/2 or np.nanstd(s) or 1.0\n",
    "    return 1/(1+np.exp(-(s-q2)/(scale+1e-9)))\n",
    "\n",
    "# === DATA FETCHERS ===========================================================\n",
    "def fetch_trends(topics, geo=\"US\", since=\"2012-01-01\"):\n",
    "    if not _HAS_TRENDS: return pd.DataFrame()\n",
    "    pytrends = TrendReq(hl=\"en-US\", tz=360)\n",
    "    frames=[]\n",
    "    for kw in topics:\n",
    "        try:\n",
    "            pytrends.build_payload([kw], timeframe=f\"{since} {dt.date.today():%Y-%m-%d}\", geo=geo)\n",
    "            df=pytrends.interest_over_time()\n",
    "            if not df.empty:\n",
    "                s=df[kw].rename(kw); s.index=as_week_index(s.index); frames.append(s)\n",
    "            time.sleep(1.0)\n",
    "        except Exception as e: print(f\"[WARN] pytrends failed for {kw}: {e}\")\n",
    "    return pd.concat(frames,axis=1).sort_index() if frames else pd.DataFrame()\n",
    "\n",
    "def fetch_wikiviews(pages, since=\"2012-01-01\", project=\"en.wikipedia\", agent=\"user\"):\n",
    "    def one_page(title):\n",
    "        start=pd.Timestamp(since).strftime(\"%Y%m0100\")\n",
    "        end=(pd.Timestamp(dt.date.today())+pd.offsets.Day(0)).strftime(\"%Y%m%d00\")\n",
    "        url=(f\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/\"\n",
    "             f\"{project}/all-access/{agent}/{title}/daily/{start}/{end}\")\n",
    "        try:\n",
    "            r=requests_get(url,timeout=20); r.raise_for_status()\n",
    "            data=r.json().get(\"items\",[])\n",
    "            ts={pd.to_datetime(i[\"timestamp\"][:8]):i[\"views\"] for i in data}\n",
    "            s=pd.Series(ts,name=title).sort_index().resample(\"W-MON\").sum()\n",
    "            return s\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] wiki fail {title}: {e}\")\n",
    "            return pd.Series(dtype=float)\n",
    "    cols=[one_page(p.replace(\" \",\"_\")) for p in pages]; cols=[c for c in cols if c.shape[0]]\n",
    "    return pd.concat(cols,axis=1).sort_index() if cols else pd.DataFrame()\n",
    "\n",
    "# === INGEST ==================================================================\n",
    "print(\"Fetching data...\")\n",
    "trends = fetch_trends(CFG[\"TOPICS\"], geo=CFG[\"REGION\"], since=CFG[\"BACKTEST_START\"])\n",
    "wiki   = fetch_wikiviews(CFG[\"WIKI_PAGES\"], since=CFG[\"BACKTEST_START\"])\n",
    "frames=[f for f in [trends,wiki] if not f.empty]\n",
    "\n",
    "# --- Fallback if all remote sources fail -------------------------------------\n",
    "if not frames:\n",
    "    print(\"[FALLBACK] Using static Pew dataset from OWID.\")\n",
    "    pew = pd.read_csv(\n",
    "        \"https://raw.githubusercontent.com/owid/owid-datasets/master/datasets/Trust%20in%20government%20(Pew)/Trust%20in%20government%20(Pew).csv\"\n",
    "    )\n",
    "    pew = pew.rename(columns={\"Year\":\"date\",\"Trust in government (Pew)\":\"trust\"}).dropna()\n",
    "    pew[\"date\"] = pd.to_datetime(pew[\"date\"], format=\"%Y\")\n",
    "    pew = pew.set_index(\"date\")[\"trust\"].resample(\"W-MON\").ffill().to_frame()\n",
    "    frames = [pew]\n",
    "\n",
    "df = pd.concat(frames,axis=1).sort_index().fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "\n",
    "# === FEATURES ================================================================\n",
    "feat=pd.DataFrame(index=df.index)\n",
    "for c in df.columns:\n",
    "    feat[c]=df[c]; feat[c+\"_z\"]=zscore(df[c]); feat[c+\"_vol4\"]=df[c].pct_change().rolling(4).std()\n",
    "weights={c:spectral_entropy(df[c].values) for c in df.columns}\n",
    "w=pd.Series(weights).fillna(0.5); w=(w-w.min())/(w.max()-w.min()+1e-9); w=w.clip(0.05,1.0)\n",
    "\n",
    "# === OVERREACH AWARENESS INDEX (OAI) ========================================\n",
    "scaler=StandardScaler(); X=scaler.fit_transform(df.values)\n",
    "W=np.diag(np.sqrt(w[df.columns].values)); Xw=X.dot(W)\n",
    "pca=PCA(n_components=1, random_state=CFG[\"RNG_SEED\"])\n",
    "oai_raw=pca.fit_transform(Xw).ravel()\n",
    "OAI=pd.Series(logistic_scale(zscore(oai_raw)), index=df.index, name=\"OAI\")\n",
    "\n",
    "# === NOWCAST =================================================================\n",
    "exo=feat.filter(regex=\"_z$|_vol4$\").fillna(0)\n",
    "model=UnobservedComponents(endog=OAI, level='local', exog=exo)\n",
    "res=model.fit(disp=False)\n",
    "h=CFG[\"HORIZON_WEEKS\"]\n",
    "lastX=exo.iloc[-1:].values; Xf=np.repeat(lastX,h,axis=0)\n",
    "fc=res.get_forecast(steps=h, exog=Xf)\n",
    "idxf=pd.date_range(OAI.index[-1]+pd.offsets.Week(1), periods=h, freq=\"W-MON\")\n",
    "OAI_fc=pd.Series(fc.predicted_mean.clip(0,1), index=idxf)\n",
    "\n",
    "# === CHANGE POINTS ===========================================================\n",
    "resid=OAI - res.fittedvalues.reindex_like(OAI).fillna(method=\"bfill\")\n",
    "k_cusum,res_thr=resid.std()*0.25,resid.std()*3.0\n",
    "pos=neg=0; alarms=[]\n",
    "for t,e in resid.items():\n",
    "    pos=max(0,pos+e-k_cusum); neg=min(0,neg+e+k_cusum)\n",
    "    if pos>res_thr or abs(neg)>res_thr: alarms.append(t); pos=neg=0\n",
    "cp_dates=alarms[-5:]\n",
    "\n",
    "# === EVENT SIMULATION ========================================================\n",
    "tau,k=CFG[\"THRESHOLD_TAU\"],CFG[\"RUN_LENGTH_K\"]\n",
    "sig=float(resid.std() or 0.05); n_sims=1500\n",
    "paths=np.clip(OAI_fc.values+np.random.normal(0,sig,(n_sims,h)),0,1)\n",
    "def sustained(sim,tau,k):\n",
    "    run=0\n",
    "    for i,a in enumerate(sim>=tau):\n",
    "        run=run+1 if a else 0\n",
    "        if run>=k: return i\n",
    "    return None\n",
    "hits=[sustained(p,tau,k) for p in paths]; hit_idxs=[x for x in hits if x is not None]\n",
    "if hit_idxs:\n",
    "    dates=[idxf[i] for i in hit_idxs]\n",
    "    med=np.median(pd.to_datetime(dates)); pH=len(hit_idxs)/n_sims\n",
    "    d80=(np.percentile(pd.to_datetime(dates),10),np.percentile(pd.to_datetime(dates),90))\n",
    "else: med,pH,d80=None,0.0,(None,None)\n",
    "\n",
    "# === PLOTS ===================================================================\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(OAI.index,OAI,label=\"OAI\"); plt.plot(res.fittedvalues.index,res.fittedvalues.clip(0,1),\"--\",label=\"Fit\")\n",
    "plt.axhline(tau,linestyle=\":\",label=f\"τ={tau}\")\n",
    "for d in cp_dates: plt.axvline(d,linestyle=\":\",alpha=.5)\n",
    "plt.legend(); plt.title(\"Overreach Awareness Index (OAI)\")\n",
    "plt.tight_layout(); plt.savefig(\"artifacts/oai_fit.png\",dpi=160); plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(OAI_fc.index,OAI_fc,label=\"Forecast mean\"); plt.axhline(tau,linestyle=\":\",label=f\"τ={tau}\")\n",
    "plt.legend(); plt.title(\"OAI Forecast Horizon\")\n",
    "plt.tight_layout(); plt.savefig(\"artifacts/oai_fc.png\",dpi=160); plt.close()\n",
    "\n",
    "prob_curve=[np.mean([(x is not None and x<=t) for x in hits]) for t in range(h)]\n",
    "pd.Series(prob_curve,index=idxf).plot(figsize=(10,3),ylim=(0,1),title=\"Probability of sustained crossing\")\n",
    "plt.tight_layout(); plt.savefig(\"artifacts/oai_prob.png\",dpi=160); plt.close()\n",
    "\n",
    "# === SUMMARY OUTPUT ==========================================================\n",
    "summary=dict(\n",
    "    generated_at=dt.datetime.utcnow().isoformat()+\"Z\",\n",
    "    horizon_weeks=h,\n",
    "    threshold_tau=tau,\n",
    "    run_length_k=k,\n",
    "    last_week=str(OAI.index[-1].date()),\n",
    "    change_points=[str(pd.Timestamp(d).date()) for d in cp_dates],\n",
    "    prob_within_horizon=round(pH,3),\n",
    "    median_event_date=(str(pd.Timestamp(med).date()) if med else None),\n",
    "    event_window_80=tuple(str(pd.Timestamp(d).date()) if d else None for d in d80),\n",
    "    weights={k:float(v) for k,v in w.to_dict().items()},\n",
    "    pca_var=float(pca.explained_variance_ratio_[0])\n",
    ")\n",
    "with open(CFG[\"SAVE_DIR\"]+\"/oai_summary.json\",\"w\") as f: json.dump(summary,f,indent=2)\n",
    "print(json.dumps(summary,indent=2))\n",
    "print(\"\\nFigures saved in ./artifacts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ca5047e-5fdd-46ff-9b82-24a5a12dd6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] pytrends unavailable; skipping Google Trends.\n",
      "Fetching data...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'shape' elements cannot be negative",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 172\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# Fallback for other builds (trend=True creates a stochastic local level)\u001b[39;00m\n\u001b[32m    170\u001b[39m     model = UnobservedComponents(endog=OAI, trend=\u001b[38;5;28;01mTrue\u001b[39;00m, exog=exo)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m res = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisp\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    174\u001b[39m h = CFG[\u001b[33m\"\u001b[39m\u001b[33mHORIZON_WEEKS\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    175\u001b[39m lastX = exo.iloc[-\u001b[32m1\u001b[39m:].values\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\mlemodel.py:651\u001b[39m, in \u001b[36mMLEModel.fit\u001b[39m\u001b[34m(self, start_params, transformed, includes_fixed, cov_type, cov_kwds, method, maxiter, full_output, disp, callback, return_params, optim_score, optim_complex_step, optim_hessian, flags, low_memory, **kwargs)\u001b[39m\n\u001b[32m    531\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    532\u001b[39m \u001b[33;03mFits the model by maximum likelihood via Kalman filter.\u001b[39;00m\n\u001b[32m    533\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    648\u001b[39m \u001b[33;03mstatsmodels.tsa.statespace.structural.UnobservedComponentsResults\u001b[39;00m\n\u001b[32m    649\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    650\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m start_params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m651\u001b[39m     start_params = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstart_params\u001b[49m\n\u001b[32m    652\u001b[39m     transformed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    653\u001b[39m     includes_fixed = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\structural.py:859\u001b[39m, in \u001b[36mUnobservedComponents.start_params\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    857\u001b[39m _start_params = {}\n\u001b[32m    858\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.level:\n\u001b[32m--> \u001b[39m\u001b[32m859\u001b[39m     resid, trend1 = \u001b[43mhpfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    861\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stochastic_trend:\n\u001b[32m    862\u001b[39m         cycle2, trend2 = hpfilter(trend1)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\statsmodels\\tsa\\filters\\hp_filter.py:97\u001b[39m, in \u001b[36mhpfilter\u001b[39m\u001b[34m(x, lamb)\u001b[39m\n\u001b[32m     95\u001b[39m offsets = np.array([\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m])\n\u001b[32m     96\u001b[39m data = np.repeat([[\u001b[32m1.\u001b[39m], [-\u001b[32m2.\u001b[39m], [\u001b[32m1.\u001b[39m]], nobs, axis=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m K = \u001b[43msparse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdia_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffsets\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnobs\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnobs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m use_umfpack = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    100\u001b[39m trend = spsolve(I+lamb*K.T.dot(K), x, use_umfpack=use_umfpack)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\scipy\\sparse\\_dia.py:66\u001b[39m, in \u001b[36m_dia_base.__init__\u001b[39m\u001b[34m(self, arg1, shape, dtype, copy, maxprint)\u001b[39m\n\u001b[32m     62\u001b[39m             offsets = np.array(arg1[\u001b[32m1\u001b[39m],\n\u001b[32m     63\u001b[39m                                dtype=\u001b[38;5;28mself\u001b[39m._get_index_dtype(maxval=\u001b[38;5;28mmax\u001b[39m(shape)),\n\u001b[32m     64\u001b[39m                                copy=copy)\n\u001b[32m     65\u001b[39m             \u001b[38;5;28mself\u001b[39m.offsets = np.atleast_1d(offsets)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m             \u001b[38;5;28mself\u001b[39m._shape = \u001b[43mcheck_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# must be dense, convert to COO first, then to DIA\u001b[39;00m\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\scipy\\sparse\\_sputils.py:478\u001b[39m, in \u001b[36mcheck_shape\u001b[39m\u001b[34m(args, current_shape, allow_nd)\u001b[39m\n\u001b[32m    476\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mshape must have length in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mallow_nd\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_shape\u001b[38;5;132;01m=}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(d < \u001b[32m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m new_shape):\n\u001b[32m--> \u001b[39m\u001b[32m478\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mshape\u001b[39m\u001b[33m'\u001b[39m\u001b[33m elements cannot be negative\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    480\u001b[39m     \u001b[38;5;66;03m# Check the current size only if needed\u001b[39;00m\n\u001b[32m    481\u001b[39m     current_size = prod(current_shape)\n",
      "\u001b[31mValueError\u001b[39m: 'shape' elements cannot be negative"
     ]
    }
   ],
   "source": [
    "# === CNT \"Wake-Up Field\" Mega-Cell — Resilient + Statsmodels-Patched =========\n",
    "# Runs with or without internet. Builds OAI, nowcasts, detects regime shifts,\n",
    "# and forecasts sustained threshold-crossing dates.\n",
    "# -----------------------------------------------------------------------------\n",
    "import os, sys, time, json, math, datetime as dt, warnings, requests\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.tsa.statespace.structural import UnobservedComponents\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# === CONFIG ==================================================================\n",
    "CFG = dict(\n",
    "    ROOT = Path.cwd(),\n",
    "    REGION = \"US\",\n",
    "    HORIZON_WEEKS = 52,        # forecast horizon (weeks)\n",
    "    THRESHOLD_TAU = 0.65,      # sustained-awareness threshold in [0,1]\n",
    "    RUN_LENGTH_K = 6,          # require k consecutive weeks above τ\n",
    "    BACKTEST_START = \"2012-01-01\",\n",
    "    TOPICS = [\"government surveillance\",\"censorship\",\"civil liberties\",\"free speech\"],\n",
    "    WIKI_PAGES = [\n",
    "        \"Mass_surveillance_in_the_United_States\",\n",
    "        \"First_Amendment_to_the_United_States_Constitution\",\n",
    "        \"Censorship_in_the_United_States\",\n",
    "        \"Civil_liberties_in_the_United_States\"\n",
    "    ],\n",
    "    SAVE_DIR = \"artifacts\",\n",
    "    RNG_SEED = 1337\n",
    ")\n",
    "os.makedirs(CFG[\"SAVE_DIR\"], exist_ok=True)\n",
    "np.random.seed(CFG[\"RNG_SEED\"])\n",
    "\n",
    "# === NETWORK SESSION (fix Wikimedia 403s via User-Agent) =====================\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\n",
    "    \"User-Agent\": \"CNTLab/1.0 (https://cnt.local; contact: telos@cnt.local)\"\n",
    "})\n",
    "requests_get = SESSION.get\n",
    "\n",
    "# === OPTIONAL: Google Trends (skip gracefully if unavailable) ================\n",
    "try:\n",
    "    from pytrends.request import TrendReq\n",
    "    _HAS_TRENDS = True\n",
    "except Exception:\n",
    "    print(\"[INFO] pytrends unavailable; skipping Google Trends.\")\n",
    "    TrendReq = None\n",
    "    _HAS_TRENDS = False\n",
    "\n",
    "# === UTILS ===================================================================\n",
    "def as_week_index(dts):\n",
    "    return pd.to_datetime(dts).to_period('W-MON').to_timestamp()\n",
    "\n",
    "def spectral_entropy(x, nseg=8, eps=1e-12):\n",
    "    x = pd.Series(x).dropna().values\n",
    "    if len(x) < 16: return np.nan\n",
    "    x = (x - x.mean()) / (x.std() + 1e-9)\n",
    "    seg = max(16, len(x)//nseg)\n",
    "    ps = []\n",
    "    for i in range(0, len(x)-seg+1, seg):\n",
    "        X = np.fft.rfft(x[i:i+seg])\n",
    "        P = (X*np.conj(X)).real\n",
    "        P = P / (P.sum() + eps)\n",
    "        ps.append(P)\n",
    "    Pm = np.mean(ps, axis=0)\n",
    "    H = -(Pm * np.log(Pm + eps)).sum()\n",
    "    Hmax = math.log(len(Pm))\n",
    "    return float(1.0 - H/Hmax)\n",
    "\n",
    "def zscore(s):\n",
    "    s = pd.Series(s)\n",
    "    return (s - s.mean()) / (s.std() + 1e-9)\n",
    "\n",
    "def logistic_scale(s):\n",
    "    s = pd.Series(s)\n",
    "    q1, q2, q3 = s.quantile([0.10, 0.50, 0.90])\n",
    "    scale = (q3 - q1)/2.0 if q3 > q1 else (s.std() or 1.0)\n",
    "    return 1.0 / (1.0 + np.exp(-(s - q2) / (scale + 1e-9)))\n",
    "\n",
    "# === FETCHERS ================================================================\n",
    "def fetch_trends(topics, geo=\"US\", since=\"2012-01-01\"):\n",
    "    if not _HAS_TRENDS: return pd.DataFrame()\n",
    "    pytrends = TrendReq(hl=\"en-US\", tz=360)\n",
    "    frames = []\n",
    "    for kw in topics:\n",
    "        try:\n",
    "            pytrends.build_payload([kw], timeframe=f\"{since} {dt.date.today():%Y-%m-%d}\", geo=geo)\n",
    "            df = pytrends.interest_over_time()\n",
    "            if df.empty: \n",
    "                continue\n",
    "            s = df[kw].rename(kw)\n",
    "            s.index = as_week_index(s.index)\n",
    "            frames.append(s)\n",
    "            time.sleep(1.0)  # be polite\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] pytrends failed for {kw}: {e}\")\n",
    "    return pd.concat(frames, axis=1).sort_index() if frames else pd.DataFrame()\n",
    "\n",
    "def fetch_wikiviews(pages, since=\"2012-01-01\", project=\"en.wikipedia\", agent=\"user\"):\n",
    "    def one_page(title):\n",
    "        start = pd.Timestamp(since).strftime(\"%Y%m0100\")\n",
    "        end   = (pd.Timestamp(dt.date.today()) + pd.offsets.Day(0)).strftime(\"%Y%m%d00\")\n",
    "        url = (f\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/\"\n",
    "               f\"{project}/all-access/{agent}/{title}/daily/{start}/{end}\")\n",
    "        try:\n",
    "            r = requests_get(url, timeout=20); r.raise_for_status()\n",
    "            data = r.json().get(\"items\", [])\n",
    "            ts = {pd.to_datetime(i[\"timestamp\"][:8]): i[\"views\"] for i in data}\n",
    "            s  = pd.Series(ts, name=title).sort_index().resample(\"W-MON\").sum()\n",
    "            return s\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] wiki fail {title}: {e}\")\n",
    "            return pd.Series(dtype=float)\n",
    "    cols = []\n",
    "    for p in pages:\n",
    "        s = one_page(p.replace(\" \", \"_\"))\n",
    "        if s.shape[0]: cols.append(s)\n",
    "        time.sleep(0.3)\n",
    "    return pd.concat(cols, axis=1).sort_index() if cols else pd.DataFrame()\n",
    "\n",
    "# === INGEST (with resilient fallback) =======================================\n",
    "print(\"Fetching data...\")\n",
    "trends = fetch_trends(CFG[\"TOPICS\"], geo=CFG[\"REGION\"], since=CFG[\"BACKTEST_START\"])\n",
    "wiki   = fetch_wikiviews(CFG[\"WIKI_PAGES\"], since=CFG[\"BACKTEST_START\"])\n",
    "frames = [f for f in [trends, wiki] if not f.empty]\n",
    "\n",
    "if not frames:\n",
    "    print(\"[FALLBACK] Using static Pew dataset from OWID (trust in government).\")\n",
    "    pew = pd.read_csv(\n",
    "        \"https://raw.githubusercontent.com/owid/owid-datasets/master/datasets/Trust%20in%20government%20(Pew)/Trust%20in%20government%20(Pew).csv\"\n",
    "    )\n",
    "    pew = pew.rename(columns={\"Year\":\"date\",\"Trust in government (Pew)\":\"trust\"}).dropna(subset=[\"date\",\"trust\"])\n",
    "    pew[\"date\"] = pd.to_datetime(pew[\"date\"], format=\"%Y\")\n",
    "    pew = pew.set_index(\"date\")[\"trust\"].resample(\"W-MON\").ffill().to_frame()\n",
    "    frames = [pew]\n",
    "\n",
    "df = pd.concat(frames, axis=1).sort_index()\n",
    "df = df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "\n",
    "# === FEATURES & FORECASTABILITY =============================================\n",
    "feat = pd.DataFrame(index=df.index)\n",
    "for c in df.columns:\n",
    "    feat[c] = df[c]\n",
    "    feat[c+\"_z\"] = zscore(df[c])\n",
    "    feat[c+\"_vol4\"] = df[c].pct_change().rolling(4).std()\n",
    "\n",
    "weights = {c: spectral_entropy(df[c].values) for c in df.columns}\n",
    "w = pd.Series(weights).fillna(0.5)\n",
    "w = (w - w.min()) / (w.max() - w.min() + 1e-12)\n",
    "w = w.clip(0.05, 1.0)\n",
    "\n",
    "# === OVERREACH AWARENESS INDEX (OAI) ========================================\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df.values)\n",
    "W = np.diag(np.sqrt(w[df.columns].values))\n",
    "Xw = X.dot(W)\n",
    "\n",
    "pca = PCA(n_components=1, random_state=CFG[\"RNG_SEED\"])\n",
    "oai_raw = pca.fit_transform(Xw).ravel()\n",
    "OAI = pd.Series(logistic_scale(zscore(oai_raw)), index=df.index, name=\"OAI\")\n",
    "\n",
    "# === STATE-SPACE NOWCAST (Statsmodels 0.14+ patch) ===========================\n",
    "exo = feat.filter(regex=\"_z$|_vol4$\").fillna(0)\n",
    "\n",
    "try:\n",
    "    # Newer API prefers 'llevel' for a local level component\n",
    "    model = UnobservedComponents(endog=OAI, level='llevel', exog=exo)\n",
    "except Exception:\n",
    "    # Fallback for other builds (trend=True creates a stochastic local level)\n",
    "    model = UnobservedComponents(endog=OAI, trend=True, exog=exo)\n",
    "\n",
    "res = model.fit(disp=False)\n",
    "\n",
    "h = CFG[\"HORIZON_WEEKS\"]\n",
    "lastX = exo.iloc[-1:].values\n",
    "Xf = np.repeat(lastX, h, axis=0)\n",
    "fc = res.get_forecast(steps=h, exog=Xf)\n",
    "idxf = pd.date_range(OAI.index[-1] + pd.offsets.Week(1), periods=h, freq=\"W-MON\")\n",
    "OAI_fc = pd.Series(fc.predicted_mean.clip(0,1), index=idxf, name=\"OAI_fc\")\n",
    "\n",
    "# === CHANGE-POINTS (CUSUM) ===================================================\n",
    "resid = OAI - res.fittedvalues.reindex_like(OAI).fillna(method=\"bfill\")\n",
    "k_cusum = resid.std() * 0.25\n",
    "thr     = resid.std() * 3.0\n",
    "pos = neg = 0.0\n",
    "alarms = []\n",
    "for t, e in resid.items():\n",
    "    pos = max(0.0, pos + e - k_cusum)\n",
    "    neg = min(0.0, neg + e + k_cusum)\n",
    "    if pos > thr or abs(neg) > thr:\n",
    "        alarms.append(t); pos = neg = 0.0\n",
    "cp_dates = alarms[-5:]\n",
    "\n",
    "# === EVENT SIMULATION (sustained τ for k weeks) ==============================\n",
    "tau, kreq = CFG[\"THRESHOLD_TAU\"], CFG[\"RUN_LENGTH_K\"]\n",
    "sig = float(resid.std() or 0.05)\n",
    "n_sims = 1500\n",
    "paths = np.clip(OAI_fc.values + np.random.normal(0, sig, (n_sims, h)), 0, 1)\n",
    "\n",
    "def first_sustained(sim, tau, kreq):\n",
    "    run = 0\n",
    "    for i, a in enumerate(sim >= tau):\n",
    "        run = run + 1 if a else 0\n",
    "        if run >= kreq: return i\n",
    "    return None\n",
    "\n",
    "hits = [first_sustained(p, tau, kreq) for p in paths]\n",
    "hit_idxs = [x for x in hits if x is not None]\n",
    "\n",
    "if hit_idxs:\n",
    "    dates = [idxf[i] for i in hit_idxs]\n",
    "    med_date = pd.to_datetime(dates).sort_values().iloc[len(dates)//2]\n",
    "    pH = len(hit_idxs) / n_sims\n",
    "    d10 = pd.to_datetime(dates).sort_values().iloc[int(0.10*len(dates))]\n",
    "    d90 = pd.to_datetime(dates).sort_values().iloc[int(0.90*len(dates))]\n",
    "    d80 = (d10, d90)\n",
    "else:\n",
    "    med_date, pH, d80 = None, 0.0, (None, None)\n",
    "\n",
    "# === PLOTS ===================================================================\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(OAI.index, OAI, label=\"OAI\")\n",
    "plt.plot(res.fittedvalues.index, res.fittedvalues.clip(0,1), \"--\", label=\"State-space fit\")\n",
    "plt.axhline(tau, linestyle=\":\", label=f\"τ={tau}\")\n",
    "for d in cp_dates: plt.axvline(d, linestyle=\":\", alpha=0.5)\n",
    "plt.legend(); plt.title(\"Overreach Awareness Index (OAI)\")\n",
    "plt.tight_layout(); plt.savefig(os.path.join(CFG[\"SAVE_DIR\"], \"oai_fit.png\"), dpi=160); plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(OAI_fc.index, OAI_fc, label=\"Forecast mean\")\n",
    "plt.axhline(tau, linestyle=\":\", label=f\"τ={tau}\")\n",
    "plt.legend(); plt.title(\"OAI Forecast Horizon\")\n",
    "plt.tight_layout(); plt.savefig(os.path.join(CFG[\"SAVE_DIR\"], \"oai_fc.png\"), dpi=160); plt.close()\n",
    "\n",
    "prob_curve = [np.mean([(x is not None and x <= t) for x in hits]) for t in range(h)]\n",
    "pd.Series(prob_curve, index=idxf).plot(figsize=(10,3), ylim=(0,1), title=\"Probability of sustained crossing (by week t)\")\n",
    "plt.tight_layout(); plt.savefig(os.path.join(CFG[\"SAVE_DIR\"], \"oai_prob.png\"), dpi=160); plt.close()\n",
    "\n",
    "# === SUMMARY OUTPUT ==========================================================\n",
    "summary = dict(\n",
    "    generated_at = dt.datetime.utcnow().isoformat()+\"Z\",\n",
    "    horizon_weeks = h,\n",
    "    threshold_tau = float(tau),\n",
    "    run_length_k  = int(kreq),\n",
    "    last_week     = str(OAI.index[-1].date()),\n",
    "    change_points = [str(pd.Timestamp(d).date()) for d in cp_dates],\n",
    "    prob_within_horizon = round(float(pH), 3),\n",
    "    median_event_date   = (str(pd.Timestamp(med_date).date()) if med_date is not None else None),\n",
    "    event_window_80     = tuple(str(pd.Timestamp(d).date()) if d is not None else None for d in d80),\n",
    "    weights = {k: float(v) for k, v in w.to_dict().items()},\n",
    "    pca_var = float(pca.explained_variance_ratio_[0]),\n",
    "    sources = {\n",
    "        \"trends_cols\": list(trends.columns) if not trends.empty else [],\n",
    "        \"wiki_cols\":   list(wiki.columns)   if not wiki.empty   else [],\n",
    "        \"fallback_used\": bool(len(frames)==1 and \"trust\" in df.columns)\n",
    "    }\n",
    ")\n",
    "with open(os.path.join(CFG[\"SAVE_DIR\"], \"oai_summary.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(json.dumps(summary, indent=2))\n",
    "print(\"\\nFigures saved to:\", os.path.abspath(CFG[\"SAVE_DIR\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47de0f31-21e2-4b58-86c7-48505adcf7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] pytrends unavailable; skipping Google Trends.\n",
      "Fetching data...\n",
      "[FALLBACK] Using EWMA nowcast/forecast (series too short for UCM).\n",
      "{\n",
      "  \"generated_at\": \"2025-10-16T04:55:06.988587Z\",\n",
      "  \"horizon_weeks\": 52,\n",
      "  \"threshold_tau\": 0.65,\n",
      "  \"run_length_k\": 6,\n",
      "  \"last_week\": \"2025-10-20\",\n",
      "  \"change_points\": [],\n",
      "  \"prob_within_horizon\": 0.0,\n",
      "  \"median_event_date\": null,\n",
      "  \"event_window_80\": [\n",
      "    null,\n",
      "    null\n",
      "  ],\n",
      "  \"weights\": {\n",
      "    \"Mass_surveillance_in_the_United_States\": 0.05,\n",
      "    \"First_Amendment_to_the_United_States_Constitution\": 0.2658909185548501,\n",
      "    \"Censorship_in_the_United_States\": 0.2974816205736362,\n",
      "    \"Civil_liberties_in_the_United_States\": 0.9999999999979158\n",
      "  },\n",
      "  \"pca_var\": 0.7709565458878976,\n",
      "  \"sources\": {\n",
      "    \"trends_cols\": [],\n",
      "    \"wiki_cols\": [\n",
      "      \"Mass_surveillance_in_the_United_States\",\n",
      "      \"First_Amendment_to_the_United_States_Constitution\",\n",
      "      \"Censorship_in_the_United_States\",\n",
      "      \"Civil_liberties_in_the_United_States\"\n",
      "    ],\n",
      "    \"fallback_used\": false,\n",
      "    \"model_used\": \"EWMA\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Figures saved to: C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\artifacts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_32224\\617628626.py:264: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  generated_at = dt.datetime.utcnow().isoformat()+\"Z\",\n"
     ]
    }
   ],
   "source": [
    "# === CNT \"Wake-Up Field\" Mega-Cell — Ultra-Resilient =========================\n",
    "# Handles: no pytrends, wiki 403s, offline fallback, Statsmodels 0.14+,\n",
    "# and too-short series via EWMA fallback (no HP-filter dependency).\n",
    "# -----------------------------------------------------------------------------\n",
    "import os, sys, time, json, math, datetime as dt, warnings, requests\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Try importing UCM last (we might not use it if data is short)\n",
    "try:\n",
    "    from statsmodels.tsa.statespace.structural import UnobservedComponents\n",
    "    _HAS_UCM = True\n",
    "except Exception:\n",
    "    _HAS_UCM = False\n",
    "\n",
    "# === CONFIG ==================================================================\n",
    "CFG = dict(\n",
    "    ROOT = Path.cwd(),\n",
    "    REGION = \"US\",\n",
    "    HORIZON_WEEKS = 52,\n",
    "    THRESHOLD_TAU = 0.65,\n",
    "    RUN_LENGTH_K = 6,\n",
    "    BACKTEST_START = \"2012-01-01\",\n",
    "    TOPICS = [\"government surveillance\",\"censorship\",\"civil liberties\",\"free speech\"],\n",
    "    WIKI_PAGES = [\n",
    "        \"Mass_surveillance_in_the_United_States\",\n",
    "        \"First_Amendment_to_the_United_States_Constitution\",\n",
    "        \"Censorship_in_the_United_States\",\n",
    "        \"Civil_liberties_in_the_United_States\"\n",
    "    ],\n",
    "    SAVE_DIR = \"artifacts\",\n",
    "    RNG_SEED = 1337,\n",
    "    MIN_LEN_UCM = 12,    # require at least 12 weekly points to use UCM\n",
    "    EWMA_SPAN = 8        # smoothing window for fallback\n",
    ")\n",
    "os.makedirs(CFG[\"SAVE_DIR\"], exist_ok=True)\n",
    "np.random.seed(CFG[\"RNG_SEED\"])\n",
    "\n",
    "# === NETWORK SESSION (fix Wikimedia 403s) ====================================\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\n",
    "    \"User-Agent\": \"CNTLab/1.0 (https://cnt.local; contact: telos@cnt.local)\"\n",
    "})\n",
    "requests_get = SESSION.get\n",
    "\n",
    "# === OPTIONAL: Google Trends =================================================\n",
    "try:\n",
    "    from pytrends.request import TrendReq\n",
    "    _HAS_TRENDS = True\n",
    "except Exception:\n",
    "    print(\"[INFO] pytrends unavailable; skipping Google Trends.\")\n",
    "    TrendReq = None; _HAS_TRENDS = False\n",
    "\n",
    "# === UTILS ===================================================================\n",
    "def as_week_index(dts):\n",
    "    return pd.to_datetime(dts).to_period('W-MON').to_timestamp()\n",
    "\n",
    "def spectral_entropy(x, nseg=8, eps=1e-12):\n",
    "    x = pd.Series(x).dropna().values\n",
    "    if len(x) < 16: return np.nan\n",
    "    x = (x - x.mean()) / (x.std() + 1e-9)\n",
    "    seg = max(16, len(x)//nseg)\n",
    "    ps = []\n",
    "    for i in range(0, len(x)-seg+1, seg):\n",
    "        X = np.fft.rfft(x[i:i+seg]); P = (X*np.conj(X)).real; P /= (P.sum() + eps); ps.append(P)\n",
    "    Pm = np.mean(ps, axis=0)\n",
    "    H = -(Pm*np.log(Pm + eps)).sum(); Hmax = math.log(len(Pm))\n",
    "    return float(1 - H/Hmax)\n",
    "\n",
    "def zscore(s):\n",
    "    s = pd.Series(s)\n",
    "    sd = s.std()\n",
    "    return (s - s.mean()) / (sd + 1e-9)\n",
    "\n",
    "def logistic_scale(s):\n",
    "    s = pd.Series(s)\n",
    "    q1, q2, q3 = s.quantile([0.10, 0.50, 0.90])\n",
    "    scale = (q3 - q1)/2.0 if q3 > q1 else (s.std() or 1.0)\n",
    "    return 1.0 / (1.0 + np.exp(-(s - q2) / (scale + 1e-9)))\n",
    "\n",
    "# === FETCHERS ================================================================\n",
    "def fetch_trends(topics, geo=\"US\", since=\"2012-01-01\"):\n",
    "    if not _HAS_TRENDS: return pd.DataFrame()\n",
    "    pytrends = TrendReq(hl=\"en-US\", tz=360)\n",
    "    frames = []\n",
    "    for kw in topics:\n",
    "        try:\n",
    "            pytrends.build_payload([kw], timeframe=f\"{since} {dt.date.today():%Y-%m-%d}\", geo=geo)\n",
    "            df = pytrends.interest_over_time()\n",
    "            if df.empty: continue\n",
    "            s = df[kw].rename(kw); s.index = as_week_index(s.index); frames.append(s)\n",
    "            time.sleep(1.0)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] pytrends failed for {kw}: {e}\")\n",
    "    return pd.concat(frames, axis=1).sort_index() if frames else pd.DataFrame()\n",
    "\n",
    "def fetch_wikiviews(pages, since=\"2012-01-01\", project=\"en.wikipedia\", agent=\"user\"):\n",
    "    def one_page(title):\n",
    "        start = pd.Timestamp(since).strftime(\"%Y%m0100\")\n",
    "        end   = (pd.Timestamp(dt.date.today()) + pd.offsets.Day(0)).strftime(\"%Y%m%d00\")\n",
    "        url = (f\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/\"\n",
    "               f\"{project}/all-access/{agent}/{title}/daily/{start}/{end}\")\n",
    "        try:\n",
    "            r = requests_get(url, timeout=20); r.raise_for_status()\n",
    "            data = r.json().get(\"items\", [])\n",
    "            ts = {pd.to_datetime(i[\"timestamp\"][:8]): i[\"views\"] for i in data}\n",
    "            s  = pd.Series(ts, name=title).sort_index().resample(\"W-MON\").sum()\n",
    "            return s\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] wiki fail {title}: {e}\")\n",
    "            return pd.Series(dtype=float)\n",
    "    cols = []\n",
    "    for p in pages:\n",
    "        s = one_page(p.replace(\" \", \"_\"))\n",
    "        if s.shape[0]: cols.append(s)\n",
    "        time.sleep(0.3)\n",
    "    return pd.concat(cols, axis=1).sort_index() if cols else pd.DataFrame()\n",
    "\n",
    "# === INGEST (with resilient fallback) =======================================\n",
    "print(\"Fetching data...\")\n",
    "trends = fetch_trends(CFG[\"TOPICS\"], geo=CFG[\"REGION\"], since=CFG[\"BACKTEST_START\"])\n",
    "wiki   = fetch_wikiviews(CFG[\"WIKI_PAGES\"], since=CFG[\"BACKTEST_START\"])\n",
    "frames = [f for f in [trends, wiki] if not f.empty]\n",
    "\n",
    "if not frames:\n",
    "    print(\"[FALLBACK] Using static Pew dataset from OWID (trust in government).\")\n",
    "    try:\n",
    "        pew = pd.read_csv(\n",
    "            \"https://raw.githubusercontent.com/owid/owid-datasets/master/datasets/Trust%20in%20government%20(Pew)/Trust%20in%20government%20(Pew).csv\"\n",
    "        )\n",
    "        pew = pew.rename(columns={\"Year\":\"date\",\"Trust in government (Pew)\":\"trust\"}).dropna(subset=[\"date\",\"trust\"])\n",
    "        pew[\"date\"] = pd.to_datetime(pew[\"date\"], format=\"%Y\")\n",
    "        pew = pew.set_index(\"date\")[\"trust\"].resample(\"W-MON\").ffill().to_frame()\n",
    "        frames = [pew]\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"No data sources available and fallback failed: {e}\")\n",
    "\n",
    "df = pd.concat(frames, axis=1).sort_index()\n",
    "df = df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "\n",
    "# === FEATURES & FORECASTABILITY =============================================\n",
    "feat = pd.DataFrame(index=df.index)\n",
    "for c in df.columns:\n",
    "    feat[c] = df[c]\n",
    "    feat[c+\"_z\"] = zscore(df[c])\n",
    "    feat[c+\"_vol4\"] = df[c].pct_change().rolling(4).std()\n",
    "\n",
    "weights = {c: spectral_entropy(df[c].values) for c in df.columns}\n",
    "w = pd.Series(weights).fillna(0.5)\n",
    "w = (w - w.min()) / (w.max() - w.min() + 1e-12)\n",
    "w = w.clip(0.05, 1.0)\n",
    "\n",
    "# === OVERREACH AWARENESS INDEX (OAI) ========================================\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df.values)\n",
    "W = np.diag(np.sqrt(w[df.columns].values))\n",
    "Xw = X.dot(W)\n",
    "\n",
    "pca = PCA(n_components=1, random_state=CFG[\"RNG_SEED\"])\n",
    "oai_raw = pca.fit_transform(Xw).ravel()\n",
    "OAI = pd.Series(logistic_scale(zscore(oai_raw)), index=df.index, name=\"OAI\")\n",
    "\n",
    "# === MODEL SELECTION: UCM or EWMA FALLBACK ==================================\n",
    "use_ucm = _HAS_UCM and (len(OAI.dropna()) >= CFG[\"MIN_LEN_UCM\"])\n",
    "\n",
    "# prepare exogenous features for UCM branch\n",
    "exo = feat.filter(regex=\"_z$|_vol4$\").fillna(0)\n",
    "\n",
    "if use_ucm:\n",
    "    # Try UCM with modern parameter spelling; fall back to stochastic trend\n",
    "    try:\n",
    "        model = UnobservedComponents(endog=OAI, level='llevel', exog=exo)\n",
    "    except Exception:\n",
    "        model = UnobservedComponents(endog=OAI, trend=True, exog=exo)\n",
    "    res = model.fit(disp=False)\n",
    "\n",
    "    h = CFG[\"HORIZON_WEEKS\"]\n",
    "    lastX = exo.iloc[-1:].values\n",
    "    Xf = np.repeat(lastX, h, axis=0)\n",
    "    fc_obj = res.get_forecast(steps=h, exog=Xf)\n",
    "    idxf = pd.date_range(OAI.index[-1] + pd.offsets.Week(1), periods=h, freq=\"W-MON\")\n",
    "    OAI_fc = pd.Series(fc_obj.predicted_mean.clip(0,1), index=idxf, name=\"OAI_fc\")\n",
    "    resid = (OAI - res.fittedvalues.reindex_like(OAI).fillna(method=\"bfill\")).dropna()\n",
    "    model_used = \"UCM\"\n",
    "else:\n",
    "    # EWMA fallback: smooth + random-walk forecast with residual noise\n",
    "    print(\"[FALLBACK] Using EWMA nowcast/forecast (series too short for UCM).\")\n",
    "    h = CFG[\"HORIZON_WEEKS\"]\n",
    "    OAI_fit = OAI.ewm(span=CFG[\"EWMA_SPAN\"], adjust=False).mean()\n",
    "    last_val = float(OAI_fit.iloc[-1])\n",
    "    idxf = pd.date_range(OAI.index[-1] + pd.offsets.Week(1), periods=h, freq=\"W-MON\")\n",
    "    OAI_fc = pd.Series([last_val]*h, index=idxf, name=\"OAI_fc\")\n",
    "    resid = (OAI - OAI_fit).dropna()\n",
    "    model_used = \"EWMA\"\n",
    "\n",
    "# === CHANGE-POINTS (CUSUM) ===================================================\n",
    "k_cusum = resid.std() * 0.25\n",
    "thr     = resid.std() * 3.0\n",
    "pos = neg = 0.0\n",
    "alarms = []\n",
    "for t, e in resid.items():\n",
    "    pos = max(0.0, pos + e - k_cusum)\n",
    "    neg = min(0.0, neg + e + k_cusum)\n",
    "    if pos > thr or abs(neg) > thr:\n",
    "        alarms.append(t); pos = neg = 0.0\n",
    "cp_dates = alarms[-5:]\n",
    "\n",
    "# === EVENT SIMULATION (sustained τ for k weeks) ==============================\n",
    "tau, kreq = CFG[\"THRESHOLD_TAU\"], CFG[\"RUN_LENGTH_K\"]\n",
    "sig = float(resid.std() or 0.05)\n",
    "n_sims = 1500\n",
    "paths = np.clip(OAI_fc.values + np.random.normal(0, sig, (n_sims, h)), 0, 1)\n",
    "\n",
    "def first_sustained(sim, tau, kreq):\n",
    "    run = 0\n",
    "    for i, a in enumerate(sim >= tau):\n",
    "        run = run + 1 if a else 0\n",
    "        if run >= kreq: return i\n",
    "    return None\n",
    "\n",
    "hits = [first_sustained(p, tau, kreq) for p in paths]\n",
    "hit_idxs = [x for x in hits if x is not None]\n",
    "\n",
    "if hit_idxs:\n",
    "    dates = [idxf[i] for i in hit_idxs]\n",
    "    med_date = pd.to_datetime(dates).sort_values().iloc[len(dates)//2]\n",
    "    pH = len(hit_idxs) / n_sims\n",
    "    d10 = pd.to_datetime(dates).sort_values().iloc[int(0.10*len(dates))]\n",
    "    d90 = pd.to_datetime(dates).sort_values().iloc[int(0.90*len(dates))]\n",
    "    d80 = (d10, d90)\n",
    "else:\n",
    "    med_date, pH, d80 = None, 0.0, (None, None)\n",
    "\n",
    "# === PLOTS ===================================================================\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(OAI.index, OAI, label=\"OAI\")\n",
    "if use_ucm:\n",
    "    try:\n",
    "        # If UCM, plot fitted mean clipped\n",
    "        from pandas import Series\n",
    "        plt.plot(OAI.index, (OAI - resid).clip(0,1), \"--\", label=\"Fit\")\n",
    "    except Exception:\n",
    "        pass\n",
    "plt.axhline(tau, linestyle=\":\", label=f\"τ={tau}\")\n",
    "for d in cp_dates: plt.axvline(d, linestyle=\":\", alpha=0.5)\n",
    "plt.legend(); plt.title(f\"Overreach Awareness Index (OAI) — Model: {model_used}\")\n",
    "plt.tight_layout(); plt.savefig(os.path.join(CFG[\"SAVE_DIR\"], \"oai_fit.png\"), dpi=160); plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(OAI_fc.index, OAI_fc, label=\"Forecast mean\")\n",
    "plt.axhline(tau, linestyle=\":\", label=f\"τ={tau}\")\n",
    "plt.legend(); plt.title(\"OAI Forecast Horizon\")\n",
    "plt.tight_layout(); plt.savefig(os.path.join(CFG[\"SAVE_DIR\"], \"oai_fc.png\"), dpi=160); plt.close()\n",
    "\n",
    "prob_curve = [np.mean([(x is not None and x <= t) for x in hits]) for t in range(h)]\n",
    "pd.Series(prob_curve, index=idxf).plot(figsize=(10,3), ylim=(0,1), title=\"Probability of sustained crossing (by week t)\")\n",
    "plt.tight_layout(); plt.savefig(os.path.join(CFG[\"SAVE_DIR\"], \"oai_prob.png\"), dpi=160); plt.close()\n",
    "\n",
    "# === SUMMARY OUTPUT ==========================================================\n",
    "summary = dict(\n",
    "    generated_at = dt.datetime.utcnow().isoformat()+\"Z\",\n",
    "    horizon_weeks = h,\n",
    "    threshold_tau = float(tau),\n",
    "    run_length_k  = int(kreq),\n",
    "    last_week     = str(OAI.index[-1].date()),\n",
    "    change_points = [str(pd.Timestamp(d).date()) for d in cp_dates],\n",
    "    prob_within_horizon = round(float(pH), 3),\n",
    "    median_event_date   = (str(pd.Timestamp(med_date).date()) if med_date is not None else None),\n",
    "    event_window_80     = tuple(str(pd.Timestamp(d).date()) if d is not None else None for d in d80),\n",
    "    weights = {k: float(v) for k, v in w.to_dict().items()},\n",
    "    pca_var = float(pca.explained_variance_ratio_[0]),\n",
    "    sources = {\n",
    "        \"trends_cols\": list(trends.columns) if not trends.empty else [],\n",
    "        \"wiki_cols\":   list(wiki.columns)   if not wiki.empty   else [],\n",
    "        \"fallback_used\": (len(frames)==1 and \"trust\" in df.columns),\n",
    "        \"model_used\": model_used\n",
    "    }\n",
    ")\n",
    "with open(os.path.join(CFG[\"SAVE_DIR\"], \"oai_summary.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(json.dumps(summary, indent=2))\n",
    "print(\"\\nFigures saved to:\", os.path.abspath(CFG[\"SAVE_DIR\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cea79de9-a5ba-4ec9-ade5-943fedef0f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] pytrends unavailable; skipping Google Trends.\n",
      "Fetching data...\n",
      "[WARN] GDELT: HTTPSConnectionPool(host='api.gdeltproject.org', port=443): Read timed out. (read timeout=20)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: HTTPSConnectionPool(host='api.gdeltproject.org', port=443): Read timed out. (read timeout=20)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: Expecting value: line 1 column 1 (char 0)\n",
      "[WARN] GDELT: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
      "[FALLBACK] Using EWMA nowcast/forecast (series too short or UCM unavailable).\n",
      "{\n",
      "  \"generated_at\": \"2025-10-16T05:21:37.383023+00:00\",\n",
      "  \"horizon_weeks\": 104,\n",
      "  \"threshold_tau\": NaN,\n",
      "  \"run_length_k\": 6,\n",
      "  \"last_week\": \"2025-10-20\",\n",
      "  \"change_points\": [],\n",
      "  \"prob_within_horizon\": 0.0,\n",
      "  \"median_event_date\": null,\n",
      "  \"event_window_80\": [\n",
      "    null,\n",
      "    null\n",
      "  ],\n",
      "  \"weights\": {\n",
      "    \"Mass_surveillance_in_the_United_States\": 0.05,\n",
      "    \"First_Amendment_to_the_United_States_Constitution\": 0.2658909185548501,\n",
      "    \"Censorship_in_the_United_States\": 0.2974816205736362,\n",
      "    \"Civil_liberties_in_the_United_States\": 0.9999999999979158\n",
      "  },\n",
      "  \"pca_var\": 0.7709565458878976,\n",
      "  \"sources\": {\n",
      "    \"trends_cols\": [],\n",
      "    \"wiki_cols\": [\n",
      "      \"Mass_surveillance_in_the_United_States\",\n",
      "      \"First_Amendment_to_the_United_States_Constitution\",\n",
      "      \"Censorship_in_the_United_States\",\n",
      "      \"Civil_liberties_in_the_United_States\"\n",
      "    ],\n",
      "    \"gdelt_cols\": [],\n",
      "    \"model_used\": \"EWMA\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Figures saved to: C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\artifacts\n"
     ]
    }
   ],
   "source": [
    "# === CNT \"Wake-Up Field\" Mega-Cell — Upgraded, Cached, Calibrated ============\n",
    "# One cell: robust ingest (Wiki + GDELT + optional Trends), OAI build, nowcast,\n",
    "# change-points, sustained-threshold event forecast, caching, and clean output.\n",
    "# -----------------------------------------------------------------------------\n",
    "import os, sys, time, json, math, warnings, requests\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Try UCM, but we'll gracefully fallback if missing/too-short\n",
    "try:\n",
    "    from statsmodels.tsa.statespace.structural import UnobservedComponents\n",
    "    _HAS_UCM = True\n",
    "except Exception:\n",
    "    _HAS_UCM = False\n",
    "\n",
    "# === CONFIG ==================================================================\n",
    "CFG = dict(\n",
    "    ROOT = Path.cwd(),\n",
    "    REGION = \"US\",\n",
    "    HORIZON_WEEKS = 104,         # longer horizon gives nonzero event mass if plausible\n",
    "    RUN_LENGTH_K = 6,            # weeks required above threshold\n",
    "    BACKTEST_START = \"2012-01-01\",\n",
    "    # Signals\n",
    "    TOPICS = [\"government surveillance\",\"censorship\",\"civil liberties\",\"free speech\"],\n",
    "    WIKI_PAGES = [\n",
    "        \"Mass_surveillance_in_the_United_States\",\n",
    "        \"First_Amendment_to_the_United_States_Constitution\",\n",
    "        \"Censorship_in_the_United_States\",\n",
    "        \"Civil_liberties_in_the_United_States\"\n",
    "    ],\n",
    "    GDELT_QUERY = '(censorship OR surveillance OR \"civil liberties\") AND location:United States',\n",
    "    # Threshold policy\n",
    "    CALIBRATE_TAU = True,        # if True, τ := 85th percentile of OAI; else use FIXED_TAU\n",
    "    FIXED_TAU = 0.65,\n",
    "    # Model heuristics\n",
    "    MIN_LEN_UCM = 12,            # need at least this many weekly points for UCM\n",
    "    EWMA_SPAN = 8,               # smoothing for fallback\n",
    "    # IO\n",
    "    SAVE_DIR = \"artifacts\",\n",
    "    CACHE_DIR = \"artifacts/cache\",\n",
    "    RNG_SEED = 1337\n",
    ")\n",
    "np.random.seed(CFG[\"RNG_SEED\"])\n",
    "Path(CFG[\"SAVE_DIR\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(CFG[\"CACHE_DIR\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === NETWORK SESSION (fix Wikimedia 403s) ====================================\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\n",
    "    # Use a descriptive UA per Wikimedia policy; include a contact if you like\n",
    "    \"User-Agent\": \"CNTLab/1.1 (fieldwalker://local; contact: telos@cnt.local)\"\n",
    "})\n",
    "requests_get = SESSION.get\n",
    "\n",
    "# === OPTIONAL: Google Trends =================================================\n",
    "try:\n",
    "    from pytrends.request import TrendReq\n",
    "    _HAS_TRENDS = True\n",
    "except Exception:\n",
    "    print(\"[INFO] pytrends unavailable; skipping Google Trends.\")\n",
    "    TrendReq = None; _HAS_TRENDS = False\n",
    "\n",
    "# === UTILS ===================================================================\n",
    "def as_week_index(dts):\n",
    "    return pd.to_datetime(dts).to_period('W-MON').to_timestamp()\n",
    "\n",
    "def spectral_entropy(x, nseg=8, eps=1e-12):\n",
    "    x = pd.Series(x).dropna().values\n",
    "    if len(x) < 16: return np.nan\n",
    "    x = (x - x.mean()) / (x.std() + 1e-9)\n",
    "    seg = max(16, len(x)//nseg)\n",
    "    ps = []\n",
    "    for i in range(0, len(x)-seg+1, seg):\n",
    "        X = np.fft.rfft(x[i:i+seg]); P = (X*np.conj(X)).real; P /= (P.sum() + eps); ps.append(P)\n",
    "    Pm = np.mean(ps, axis=0)\n",
    "    H = -(Pm*np.log(Pm + eps)).sum(); Hmax = math.log(len(Pm))\n",
    "    return float(1 - H/Hmax)\n",
    "\n",
    "def zscore(s):\n",
    "    s = pd.Series(s)\n",
    "    return (s - s.mean()) / (s.std() + 1e-9)\n",
    "\n",
    "def logistic_scale(s):\n",
    "    s = pd.Series(s)\n",
    "    q1, q2, q3 = s.quantile([0.10, 0.50, 0.90])\n",
    "    scale = (q3 - q1)/2.0 if q3 > q1 else (s.std() or 1.0)\n",
    "    return 1.0 / (1.0 + np.exp(-(s - q2) / (scale + 1e-9)))\n",
    "\n",
    "# === FETCHERS ================================================================\n",
    "def fetch_trends(topics, geo=\"US\", since=\"2012-01-01\"):\n",
    "    if not _HAS_TRENDS: return pd.DataFrame()\n",
    "    pytrends = TrendReq(hl=\"en-US\", tz=360)\n",
    "    frames = []\n",
    "    for kw in topics:\n",
    "        try:\n",
    "            pytrends.build_payload([kw], timeframe=f\"{since} {pd.Timestamp.today():%Y-%m-%d}\", geo=geo)\n",
    "            df = pytrends.interest_over_time()\n",
    "            if df.empty: continue\n",
    "            s = df[kw].rename(kw); s.index = as_week_index(s.index); frames.append(s)\n",
    "            time.sleep(1.0)  # be polite\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] pytrends failed for {kw}: {e}\")\n",
    "    return pd.concat(frames, axis=1).sort_index() if frames else pd.DataFrame()\n",
    "\n",
    "def fetch_wikiviews(pages, since=\"2012-01-01\", project=\"en.wikipedia\", agent=\"user\"):\n",
    "    \"\"\"Wikimedia REST pageviews; weekly; merges with on-disk cache to grow series.\"\"\"\n",
    "    cache_path = Path(CFG[\"CACHE_DIR\"]) / \"wiki_views.csv\"\n",
    "    def one_page(title):\n",
    "        start = pd.Timestamp(since).strftime(\"%Y%m0100\")\n",
    "        end   = pd.Timestamp.today().strftime(\"%Y%m%d00\")\n",
    "        url = (f\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/\"\n",
    "               f\"{project}/all-access/{agent}/{title}/daily/{start}/{end}\")\n",
    "        try:\n",
    "            r = requests_get(url, timeout=20); r.raise_for_status()\n",
    "            data = r.json().get(\"items\", [])\n",
    "            ts = {pd.to_datetime(i[\"timestamp\"][:8]): i[\"views\"] for i in data}\n",
    "            s  = pd.Series(ts, name=title).sort_index().resample(\"W-MON\").sum()\n",
    "            return s\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] wiki fail {title}: {e}\")\n",
    "            return pd.Series(dtype=float)\n",
    "    cols = []\n",
    "    for p in pages:\n",
    "        s = one_page(p.replace(\" \", \"_\"))\n",
    "        if s.shape[0]: cols.append(s)\n",
    "        time.sleep(0.3)\n",
    "    wiki = pd.concat(cols, axis=1).sort_index() if cols else pd.DataFrame()\n",
    "    # merge with cache\n",
    "    if cache_path.exists():\n",
    "        old = pd.read_csv(cache_path, parse_dates=[\"date\"]).set_index(\"date\")\n",
    "        wiki = old.combine_first(wiki) if not wiki.empty else old\n",
    "    if not wiki.empty:\n",
    "        wiki.to_csv(cache_path, index_label=\"date\")\n",
    "    return wiki\n",
    "\n",
    "def fetch_gdelt_counts(query, since=\"2012-01-01\"):\n",
    "    \"\"\"GDELT Doc API monthly timeline → weekly sum; no extra packages.\"\"\"\n",
    "    start = pd.Timestamp(since).to_period(\"M\").to_timestamp()\n",
    "    end   = pd.Timestamp.today().to_period(\"M\").to_timestamp()\n",
    "    months = pd.period_range(start, end, freq=\"M\").to_timestamp()\n",
    "    rows = []\n",
    "    for m in months:\n",
    "        url = (\"https://api.gdeltproject.org/api/v2/doc/doc?\"\n",
    "               f\"query={requests.utils.quote(query)}&mode=TimelineVol&format=json\"\n",
    "               f\"&startdatetime={m:%Y%m%d000000}&enddatetime={(m+pd.offsets.MonthEnd(0)):%Y%m%d235959}\")\n",
    "        try:\n",
    "            js = requests_get(url, timeout=20).json()\n",
    "            for pt in js.get(\"timelines\", [{}])[0].get(\"data\", []):\n",
    "                rows.append((pd.to_datetime(pt[\"date\"]), int(pt[\"value\"])))\n",
    "            time.sleep(0.3)\n",
    "        except Exception as e:\n",
    "            print(\"[WARN] GDELT:\", e)\n",
    "    if not rows: return pd.DataFrame()\n",
    "    s = pd.Series({d:v for d,v in rows}).sort_index().resample(\"W-MON\").sum().rename(\"gdelt_volume\")\n",
    "    # cache\n",
    "    cache_path = Path(CFG[\"CACHE_DIR\"]) / \"gdelt_volume.csv\"\n",
    "    if cache_path.exists():\n",
    "        old = pd.read_csv(cache_path, parse_dates=[\"date\"]).set_index(\"date\")[\"gdelt_volume\"]\n",
    "        s = old.combine_first(s)\n",
    "    s.to_frame().to_csv(cache_path, index_label=\"date\")\n",
    "    return s.to_frame()\n",
    "\n",
    "# === INGEST ==================================================================\n",
    "print(\"Fetching data...\")\n",
    "frames = []\n",
    "\n",
    "trends = fetch_trends(CFG[\"TOPICS\"], geo=CFG[\"REGION\"], since=CFG[\"BACKTEST_START\"])\n",
    "if not trends.empty: frames.append(trends)\n",
    "\n",
    "wiki   = fetch_wikiviews(CFG[\"WIKI_PAGES\"], since=CFG[\"BACKTEST_START\"])\n",
    "if not wiki.empty: frames.append(wiki)\n",
    "\n",
    "gdelt  = fetch_gdelt_counts(CFG[\"GDELT_QUERY\"], since=CFG[\"BACKTEST_START\"])\n",
    "if isinstance(gdelt, pd.DataFrame) and not gdelt.empty: frames.append(gdelt)\n",
    "\n",
    "if not frames:\n",
    "    print(\"[FALLBACK] Using static Pew dataset (OWID).\")\n",
    "    try:\n",
    "        pew = pd.read_csv(\n",
    "            \"https://raw.githubusercontent.com/owid/owid-datasets/master/datasets/Trust%20in%20government%20(Pew)/Trust%20in%20government%20(Pew).csv\"\n",
    "        )\n",
    "        pew = pew.rename(columns={\"Year\":\"date\",\"Trust in government (Pew)\":\"trust\"}).dropna(subset=[\"date\",\"trust\"])\n",
    "        pew[\"date\"] = pd.to_datetime(pew[\"date\"], format=\"%Y\")\n",
    "        pew = pew.set_index(\"date\")[\"trust\"].resample(\"W-MON\").ffill().to_frame()\n",
    "        frames = [pew]\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"No data sources available and fallback failed: {e}\")\n",
    "\n",
    "df = pd.concat(frames, axis=1).sort_index()\n",
    "df = df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "\n",
    "# === FEATURES & FORECASTABILITY =============================================\n",
    "feat = pd.DataFrame(index=df.index)\n",
    "for c in df.columns:\n",
    "    feat[c] = df[c]\n",
    "    feat[c+\"_z\"]   = zscore(df[c])\n",
    "    feat[c+\"_vol4\"] = df[c].pct_change().rolling(4).std()\n",
    "\n",
    "weights = {c: spectral_entropy(df[c].values) for c in df.columns}\n",
    "w = pd.Series(weights).fillna(0.5)\n",
    "# cap extremes so a single column can't dominate\n",
    "w = w.clip(0.15, 0.85)\n",
    "w = (w - w.min()) / (w.max() - w.min() + 1e-12)\n",
    "w = w.clip(0.05, 1.0)\n",
    "\n",
    "# === OAI (Overreach Awareness Index) =========================================\n",
    "scaler = StandardScaler()\n",
    "X  = scaler.fit_transform(df.values)\n",
    "Wm = np.diag(np.sqrt(w[df.columns].values))\n",
    "Xw = X.dot(Wm)\n",
    "pca = PCA(n_components=1, random_state=CFG[\"RNG_SEED\"])\n",
    "oai_raw = pca.fit_transform(Xw).ravel()\n",
    "OAI = pd.Series(logistic_scale(zscore(oai_raw)), index=df.index, name=\"OAI\")\n",
    "\n",
    "# === Threshold policy (calibrate if requested) ===============================\n",
    "if CFG[\"CALIBRATE_TAU\"]:\n",
    "    TAU = float(pd.Series(OAI).quantile(0.85))   # 85th percentile\n",
    "else:\n",
    "    TAU = float(CFG[\"FIXED_TAU\"])\n",
    "\n",
    "# === NOWCAST / FORECAST ======================================================\n",
    "use_ucm = _HAS_UCM and (len(OAI.dropna()) >= CFG[\"MIN_LEN_UCM\"])\n",
    "exo = feat.filter(regex=\"_z$|_vol4$\").fillna(0)\n",
    "\n",
    "if use_ucm:\n",
    "    try:\n",
    "        model = UnobservedComponents(endog=OAI, level='llevel', exog=exo)\n",
    "    except Exception:\n",
    "        model = UnobservedComponents(endog=OAI, trend=True, exog=exo)\n",
    "    res = model.fit(disp=False)\n",
    "\n",
    "    h = int(CFG[\"HORIZON_WEEKS\"])\n",
    "    lastX = exo.iloc[-1:].values\n",
    "    Xf = np.repeat(lastX, h, axis=0)\n",
    "    fc = res.get_forecast(steps=h, exog=Xf)\n",
    "    idxf = pd.date_range(OAI.index[-1] + pd.offsets.Week(1), periods=h, freq=\"W-MON\")\n",
    "    OAI_fc = pd.Series(fc.predicted_mean.clip(0,1), index=idxf, name=\"OAI_fc\")\n",
    "    resid  = (OAI - res.fittedvalues.reindex_like(OAI).fillna(method=\"bfill\")).dropna()\n",
    "    model_used = \"UCM\"\n",
    "else:\n",
    "    print(\"[FALLBACK] Using EWMA nowcast/forecast (series too short or UCM unavailable).\")\n",
    "    h = int(CFG[\"HORIZON_WEEKS\"])\n",
    "    OAI_fit  = OAI.ewm(span=int(CFG[\"EWMA_SPAN\"]), adjust=False).mean()\n",
    "    last_val = float(OAI_fit.iloc[-1])\n",
    "    idxf = pd.date_range(OAI.index[-1] + pd.offsets.Week(1), periods=h, freq=\"W-MON\")\n",
    "    OAI_fc = pd.Series([last_val]*h, index=idxf, name=\"OAI_fc\")\n",
    "    resid  = (OAI - OAI_fit).dropna()\n",
    "    model_used = \"EWMA\"\n",
    "\n",
    "# === CHANGE-POINTS (CUSUM on residuals) =====================================\n",
    "k_cusum = resid.std() * 0.25\n",
    "thr     = resid.std() * 3.0\n",
    "pos = neg = 0.0\n",
    "alarms = []\n",
    "for t, e in resid.items():\n",
    "    pos = max(0.0, pos + e - k_cusum)\n",
    "    neg = min(0.0, neg + e + k_cusum)\n",
    "    if pos > thr or abs(neg) > thr:\n",
    "        alarms.append(t); pos = neg = 0.0\n",
    "cp_dates = [pd.Timestamp(d) for d in alarms[-5:]]\n",
    "\n",
    "# === TIME-TO-EVENT (sustained τ for k consecutive weeks) =====================\n",
    "sig = float(resid.std() or 0.05)\n",
    "n_sims = 1500\n",
    "paths = np.clip(OAI_fc.values + np.random.normal(0, sig, (n_sims, h)), 0, 1)\n",
    "\n",
    "def first_sustained(sim, tau, kreq):\n",
    "    run = 0\n",
    "    for i, a in enumerate(sim >= tau):\n",
    "        run = run + 1 if a else 0\n",
    "        if run >= kreq: return i\n",
    "    return None\n",
    "\n",
    "hits = [first_sustained(p, TAU, CFG[\"RUN_LENGTH_K\"]) for p in paths]\n",
    "hit_idxs = [x for x in hits if x is not None]\n",
    "if hit_idxs:\n",
    "    dates = pd.to_datetime([idxf[i] for i in hit_idxs]).sort_values()\n",
    "    med_date = dates.iloc[len(dates)//2]\n",
    "    pH = len(hit_idxs)/n_sims\n",
    "    d80 = (dates.iloc[int(0.10*len(dates))], dates.iloc[int(0.90*len(dates))])\n",
    "else:\n",
    "    med_date, pH, d80 = None, 0.0, (None, None)\n",
    "\n",
    "# === PLOTS ===================================================================\n",
    "outdir = Path(CFG[\"SAVE_DIR\"])\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(OAI.index, OAI, label=\"OAI\")\n",
    "if use_ucm:\n",
    "    try:\n",
    "        plt.plot((OAI - resid).clip(0,1).index, (OAI - resid).clip(0,1).values, \"--\", label=\"Fit\")\n",
    "    except Exception:\n",
    "        pass\n",
    "plt.axhline(TAU, linestyle=\":\", label=f\"τ={TAU:.3f}\")\n",
    "for d in cp_dates: plt.axvline(d, linestyle=\":\", alpha=0.5)\n",
    "plt.legend(); plt.title(f\"OAI — Model: {model_used}\")\n",
    "plt.tight_layout(); plt.savefig(outdir/\"oai_fit.png\", dpi=160); plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(OAI_fc.index, OAI_fc.values, label=\"Forecast mean\")\n",
    "plt.axhline(TAU, linestyle=\":\", label=f\"τ={TAU:.3f}\")\n",
    "plt.legend(); plt.title(\"OAI Forecast Horizon\")\n",
    "plt.tight_layout(); plt.savefig(outdir/\"oai_fc.png\", dpi=160); plt.close()\n",
    "\n",
    "prob_curve = [np.mean([(x is not None and x <= t) for x in hits]) for t in range(h)]\n",
    "pd.Series(prob_curve, index=idxf).plot(figsize=(10,3), ylim=(0,1), title=\"Pr(sustained crossing by week t)\")\n",
    "plt.tight_layout(); plt.savefig(outdir/\"oai_prob.png\", dpi=160); plt.close()\n",
    "\n",
    "# === SUMMARY ================================================================\n",
    "summary = dict(\n",
    "    generated_at = datetime.now(timezone.utc).isoformat(),\n",
    "    horizon_weeks = h,\n",
    "    threshold_tau = float(TAU),\n",
    "    run_length_k  = int(CFG[\"RUN_LENGTH_K\"]),\n",
    "    last_week     = str(pd.Timestamp(OAI.index[-1]).date()),\n",
    "    change_points = [str(pd.Timestamp(d).date()) for d in cp_dates],\n",
    "    prob_within_horizon = round(float(pH), 3),\n",
    "    median_event_date   = (str(pd.Timestamp(med_date).date()) if med_date is not None else None),\n",
    "    event_window_80     = tuple(str(pd.Timestamp(d).date()) if d is not None else None for d in d80),\n",
    "    weights = {k: float(v) for k, v in w.to_dict().items()},\n",
    "    pca_var = float(pca.explained_variance_ratio_[0]),\n",
    "    sources = {\n",
    "        \"trends_cols\": list(trends.columns) if isinstance(trends, pd.DataFrame) and not trends.empty else [],\n",
    "        \"wiki_cols\":   list(wiki.columns)   if isinstance(wiki,   pd.DataFrame) and not wiki.empty   else [],\n",
    "        \"gdelt_cols\":  list(gdelt.columns)  if isinstance(gdelt,  pd.DataFrame) and not gdelt.empty  else [],\n",
    "        \"model_used\":  model_used\n",
    "    }\n",
    ")\n",
    "with open(outdir/\"oai_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(json.dumps(summary, indent=2))\n",
    "print(\"\\nFigures saved to:\", str(outdir.resolve()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab0290b9-10cb-417e-a3a6-d1768ecd0ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] pytrends unavailable; skipping Google Trends.\n",
      "Fetching data...\n",
      "[GDELT] Too many errors in a row (6); stopping early.\n",
      "[TAU] Not enough points (0) for calibrated τ; using fallback=0.65.\n",
      "[FALLBACK] Using EWMA nowcast/forecast (series too short or UCM unavailable).\n",
      "{\n",
      "  \"generated_at\": \"2025-10-16T05:42:07.560263+00:00\",\n",
      "  \"horizon_weeks\": 104,\n",
      "  \"threshold_tau\": 0.65,\n",
      "  \"run_length_k\": 6,\n",
      "  \"last_week\": \"2025-10-20\",\n",
      "  \"change_points\": [],\n",
      "  \"prob_within_horizon\": 0.0,\n",
      "  \"median_event_date\": null,\n",
      "  \"event_window_80\": [\n",
      "    null,\n",
      "    null\n",
      "  ],\n",
      "  \"weights\": {\n",
      "    \"Mass_surveillance_in_the_United_States\": 0.05,\n",
      "    \"First_Amendment_to_the_United_States_Constitution\": 0.2658909185548501,\n",
      "    \"Censorship_in_the_United_States\": 0.2974816205736362,\n",
      "    \"Civil_liberties_in_the_United_States\": 0.9999999999979158\n",
      "  },\n",
      "  \"pca_var\": 0.7709565458878976,\n",
      "  \"sources\": {\n",
      "    \"trends_cols\": [],\n",
      "    \"wiki_cols\": [\n",
      "      \"Mass_surveillance_in_the_United_States\",\n",
      "      \"First_Amendment_to_the_United_States_Constitution\",\n",
      "      \"Censorship_in_the_United_States\",\n",
      "      \"Civil_liberties_in_the_United_States\"\n",
      "    ],\n",
      "    \"gdelt_cols\": [],\n",
      "    \"model_used\": \"EWMA\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Figures saved to: C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\artifacts\n"
     ]
    }
   ],
   "source": [
    "# === CNT \"Wake-Up Field\" — Single Ultra-Resilient Mega Cell ===================\n",
    "# One cell: ingest (Wiki + safe GDELT + optional Trends), cache, build OAI,\n",
    "# nowcast/forecast, regime shifts, sustained-threshold event date, robust τ.\n",
    "# -----------------------------------------------------------------------------\n",
    "import os, sys, json, math, time, warnings, requests\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Try UCM; we’ll fallback to EWMA if unavailable/too short\n",
    "try:\n",
    "    from statsmodels.tsa.statespace.structural import UnobservedComponents\n",
    "    _HAS_UCM = True\n",
    "except Exception:\n",
    "    _HAS_UCM = False\n",
    "\n",
    "# ============================== CONFIG =======================================\n",
    "CFG = dict(\n",
    "    ROOT = Path.cwd(),\n",
    "    REGION = \"US\",\n",
    "    HORIZON_WEEKS = 104,          # forecast horizon\n",
    "    RUN_LENGTH_K  = 6,            # sustain weeks ≥ τ\n",
    "    BACKTEST_START = \"2012-01-01\",\n",
    "    # Signals\n",
    "    TOPICS = [\"government surveillance\",\"censorship\",\"civil liberties\",\"free speech\"],\n",
    "    WIKI_PAGES = [\n",
    "        \"Mass_surveillance_in_the_United_States\",\n",
    "        \"First_Amendment_to_the_United_States_Constitution\",\n",
    "        \"Censorship_in_the_United_States\",\n",
    "        \"Civil_liberties_in_the_United_States\"\n",
    "    ],\n",
    "    GDELT_QUERY = '(censorship OR surveillance OR \"civil liberties\") AND location: United States',\n",
    "    # Threshold policy\n",
    "    CALIBRATE_TAU = True,         # if False uses FIXED_TAU\n",
    "    FIXED_TAU = 0.65,\n",
    "    # Modeling heuristics\n",
    "    MIN_LEN_UCM = 12,             # min weekly points to enable UCM\n",
    "    EWMA_SPAN   = 8,              # EWMA smoothing when UCM not used\n",
    "    # IO / caching\n",
    "    SAVE_DIR  = \"artifacts\",\n",
    "    CACHE_DIR = \"artifacts/cache\",\n",
    "    RNG_SEED = 1337\n",
    ")\n",
    "np.random.seed(CFG[\"RNG_SEED\"])\n",
    "Path(CFG[\"SAVE_DIR\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(CFG[\"CACHE_DIR\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========================== NETWORK SESSION ==================================\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\n",
    "    \"User-Agent\": \"CNTLab/1.2 (fieldwalker://local; contact: telos@cnt.local)\"\n",
    "})\n",
    "requests_get = SESSION.get\n",
    "\n",
    "# Optional: Google Trends (skips gracefully if not present)\n",
    "try:\n",
    "    from pytrends.request import TrendReq\n",
    "    _HAS_TRENDS = True\n",
    "except Exception:\n",
    "    print(\"[INFO] pytrends unavailable; skipping Google Trends.\")\n",
    "    TrendReq = None; _HAS_TRENDS = False\n",
    "\n",
    "# =============================== UTILS =======================================\n",
    "def as_week_index(dts):\n",
    "    return pd.to_datetime(dts).to_period('W-MON').to_timestamp()\n",
    "\n",
    "def spectral_entropy(x, nseg=8, eps=1e-12):\n",
    "    x = pd.Series(x).dropna().values\n",
    "    if len(x) < 16: return np.nan\n",
    "    x = (x - x.mean()) / (x.std() + 1e-9)\n",
    "    seg = max(16, len(x)//nseg)\n",
    "    ps = []\n",
    "    for i in range(0, len(x)-seg+1, seg):\n",
    "        X = np.fft.rfft(x[i:i+seg]); P = (X*np.conj(X)).real; P /= (P.sum()+eps); ps.append(P)\n",
    "    Pm = np.mean(ps, axis=0); H = -(Pm*np.log(Pm+eps)).sum(); Hmax = math.log(len(Pm))\n",
    "    return float(1 - H/Hmax)\n",
    "\n",
    "def zscore(s):\n",
    "    s = pd.Series(s); return (s - s.mean()) / (s.std() + 1e-9)\n",
    "\n",
    "def logistic_scale(s):\n",
    "    s = pd.Series(s); q1,q2,q3 = s.quantile([.1,.5,.9])\n",
    "    scale = (q3-q1)/2.0 if q3>q1 else (s.std() or 1.0)\n",
    "    return 1/(1+np.exp(-(s-q2)/(scale+1e-9)))\n",
    "\n",
    "def robust_tau(oai_series, q=0.85, fallback=0.65, min_points=10):\n",
    "    s = pd.Series(oai_series).astype(float).dropna()\n",
    "    if len(s) < min_points:\n",
    "        print(f\"[TAU] Not enough points ({len(s)}) for calibrated τ; using fallback={fallback}.\")\n",
    "        return float(fallback)\n",
    "    try:\n",
    "        tau = float(np.nanquantile(s.values, q))\n",
    "        if not np.isfinite(tau): raise ValueError(\"non-finite τ\")\n",
    "        return tau\n",
    "    except Exception:\n",
    "        print(f\"[TAU] Calibration produced NaN/inf; using fallback={fallback}.\")\n",
    "        return float(fallback)\n",
    "\n",
    "# ============================== FETCHERS =====================================\n",
    "def fetch_trends(topics, geo=\"US\", since=\"2012-01-01\"):\n",
    "    if not _HAS_TRENDS: return pd.DataFrame()\n",
    "    pytrends = TrendReq(hl=\"en-US\", tz=360)\n",
    "    frames = []\n",
    "    for kw in topics:\n",
    "        try:\n",
    "            pytrends.build_payload([kw], timeframe=f\"{since} {pd.Timestamp.today():%Y-%m-%d}\", geo=geo)\n",
    "            df = pytrends.interest_over_time()\n",
    "            if df.empty: continue\n",
    "            s = df[kw].rename(kw); s.index = as_week_index(s.index); frames.append(s)\n",
    "            time.sleep(1.0)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] pytrends failed for {kw}: {e}\")\n",
    "    return pd.concat(frames, axis=1).sort_index() if frames else pd.DataFrame()\n",
    "\n",
    "def fetch_wikiviews(pages, since=\"2012-01-01\", project=\"en.wikipedia\", agent=\"user\"):\n",
    "    cache_path = Path(CFG[\"CACHE_DIR\"]) / \"wiki_views.csv\"\n",
    "    def one_page(title):\n",
    "        start = pd.Timestamp(since).strftime(\"%Y%m0100\")\n",
    "        end   = pd.Timestamp.today().strftime(\"%Y%m%d00\")\n",
    "        url = (f\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/\"\n",
    "               f\"{project}/all-access/{agent}/{title}/daily/{start}/{end}\")\n",
    "        try:\n",
    "            r = requests_get(url, timeout=20); r.raise_for_status()\n",
    "            data = r.json().get(\"items\", [])\n",
    "            ts = {pd.to_datetime(i[\"timestamp\"][:8]): i[\"views\"] for i in data}\n",
    "            return pd.Series(ts, name=title).sort_index().resample(\"W-MON\").sum()\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] wiki fail {title}: {e}\"); return pd.Series(dtype=float)\n",
    "    cols=[]\n",
    "    for p in pages:\n",
    "        s = one_page(p.replace(\" \",\"_\"))\n",
    "        if s.shape[0]: cols.append(s)\n",
    "        time.sleep(0.3)\n",
    "    wiki = pd.concat(cols, axis=1).sort_index() if cols else pd.DataFrame()\n",
    "    if cache_path.exists():\n",
    "        old = pd.read_csv(cache_path, parse_dates=[\"date\"]).set_index(\"date\")\n",
    "        wiki = old.combine_first(wiki) if not wiki.empty else old\n",
    "    if not wiki.empty: wiki.to_csv(cache_path, index_label=\"date\")\n",
    "    return wiki\n",
    "\n",
    "def fetch_gdelt_counts_safe(query, since=\"2018-01-01\", timeout=8, max_consec_err=6, max_months=60):\n",
    "    start = pd.Timestamp(since).to_period(\"M\").to_timestamp()\n",
    "    end   = pd.Timestamp.today().to_period(\"M\").to_timestamp()\n",
    "    months = pd.period_range(start, end, freq=\"M\").to_timestamp()[-max_months:]\n",
    "    rows, consec = [], 0\n",
    "    for m in months:\n",
    "        url = (\"https://api.gdeltproject.org/api/v2/doc/doc?\"\n",
    "               f\"query={requests.utils.quote(query)}&mode=TimelineVol&format=json\"\n",
    "               f\"&startdatetime={m:%Y%m%d000000}&enddatetime={(m+pd.offsets.MonthEnd(0)):%Y%m%d235959}\")\n",
    "        try:\n",
    "            js = requests_get(url, timeout=timeout).json()\n",
    "            data = js.get(\"timelines\", [{}])[0].get(\"data\", [])\n",
    "            for pt in data: rows.append((pd.to_datetime(pt[\"date\"]), int(pt[\"value\"])))\n",
    "            consec = 0; time.sleep(0.25)\n",
    "        except Exception:\n",
    "            consec += 1\n",
    "            if consec >= max_consec_err:\n",
    "                print(f\"[GDELT] Too many errors in a row ({consec}); stopping early.\"); break\n",
    "    if not rows: return pd.DataFrame()\n",
    "    s = pd.Series({d:v for d,v in rows}).sort_index().resample(\"W-MON\").sum().rename(\"gdelt_volume\")\n",
    "    # cache\n",
    "    cache_path = Path(CFG[\"CACHE_DIR\"]) / \"gdelt_volume.csv\"\n",
    "    if cache_path.exists():\n",
    "        old = pd.read_csv(cache_path, parse_dates=[\"date\"]).set_index(\"date\")[\"gdelt_volume\"]\n",
    "        s = old.combine_first(s)\n",
    "    s.to_frame().to_csv(cache_path, index_label=\"date\")\n",
    "    return s.to_frame()\n",
    "\n",
    "# ============================== INGEST =======================================\n",
    "print(\"Fetching data...\")\n",
    "frames = []\n",
    "trends = fetch_trends(CFG[\"TOPICS\"], geo=CFG[\"REGION\"], since=CFG[\"BACKTEST_START\"])\n",
    "if not trends.empty: frames.append(trends)\n",
    "\n",
    "wiki   = fetch_wikiviews(CFG[\"WIKI_PAGES\"], since=CFG[\"BACKTEST_START\"])\n",
    "if not wiki.empty: frames.append(wiki)\n",
    "\n",
    "gdelt  = fetch_gdelt_counts_safe(CFG[\"GDELT_QUERY\"], since=CFG[\"BACKTEST_START\"])\n",
    "if isinstance(gdelt, pd.DataFrame) and not gdelt.empty: frames.append(gdelt)\n",
    "\n",
    "if not frames:\n",
    "    print(\"[FALLBACK] Using static Pew dataset (OWID).\")\n",
    "    pew = pd.read_csv(\n",
    "        \"https://raw.githubusercontent.com/owid/owid-datasets/master/datasets/Trust%20in%20government%20(Pew)/Trust%20in%20government%20(Pew).csv\"\n",
    "    )\n",
    "    pew = pew.rename(columns={\"Year\":\"date\",\"Trust in government (Pew)\":\"trust\"}).dropna(subset=[\"date\",\"trust\"])\n",
    "    pew[\"date\"] = pd.to_datetime(pew[\"date\"], format=\"%Y\")\n",
    "    pew = pew.set_index(\"date\")[\"trust\"].resample(\"W-MON\").ffill().to_frame()\n",
    "    frames = [pew]\n",
    "\n",
    "df = pd.concat(frames, axis=1).sort_index().ffill().bfill()\n",
    "\n",
    "# ===================== FEATURES & FORECASTABILITY ============================\n",
    "feat = pd.DataFrame(index=df.index)\n",
    "for c in df.columns:\n",
    "    feat[c] = df[c]\n",
    "    feat[c+\"_z\"] = zscore(df[c])\n",
    "    feat[c+\"_vol4\"] = df[c].pct_change().rolling(4).std()\n",
    "\n",
    "weights = {c: spectral_entropy(df[c].values) for c in df.columns}\n",
    "w = pd.Series(weights).fillna(0.5)\n",
    "w = w.clip(0.15, 0.85)                      # avoid domination\n",
    "w = (w - w.min()) / (w.max() - w.min() + 1e-12)\n",
    "w = w.clip(0.05, 1.0)\n",
    "\n",
    "# ========================= OVERREACH AWARENESS INDEX =========================\n",
    "scaler = StandardScaler()\n",
    "X  = scaler.fit_transform(df.values)\n",
    "Wm = np.diag(np.sqrt(w[df.columns].values))\n",
    "Xw = X.dot(Wm)\n",
    "pca = PCA(n_components=1, random_state=CFG[\"RNG_SEED\"])\n",
    "oai_raw = pca.fit_transform(Xw).ravel()\n",
    "OAI = pd.Series(logistic_scale(zscore(oai_raw)), index=df.index, name=\"OAI\")\n",
    "\n",
    "# ========================= THRESHOLD τ (robust) ==============================\n",
    "TAU = robust_tau(OAI, q=0.85, fallback=CFG[\"FIXED_TAU\"], min_points=10) if CFG[\"CALIBRATE_TAU\"] else float(CFG[\"FIXED_TAU\"])\n",
    "\n",
    "# ========================= NOWCAST / FORECAST ================================\n",
    "use_ucm = _HAS_UCM and (len(OAI.dropna()) >= CFG[\"MIN_LEN_UCM\"])\n",
    "exo = feat.filter(regex=\"_z$|_vol4$\").fillna(0)\n",
    "\n",
    "if use_ucm:\n",
    "    try:\n",
    "        model = UnobservedComponents(endog=OAI, level='llevel', exog=exo)\n",
    "    except Exception:\n",
    "        model = UnobservedComponents(endog=OAI, trend=True, exog=exo)\n",
    "    res = model.fit(disp=False)\n",
    "    h = int(CFG[\"HORIZON_WEEKS\"])\n",
    "    lastX = exo.iloc[-1:].values; Xf = np.repeat(lastX, h, axis=0)\n",
    "    fc = res.get_forecast(steps=h, exog=Xf)\n",
    "    idxf = pd.date_range(OAI.index[-1] + pd.offsets.Week(1), periods=h, freq=\"W-MON\")\n",
    "    OAI_fc = pd.Series(fc.predicted_mean.clip(0,1), index=idxf, name=\"OAI_fc\")\n",
    "    resid  = (OAI - res.fittedvalues.reindex_like(OAI).bfill()).dropna()\n",
    "    model_used = \"UCM\"\n",
    "else:\n",
    "    print(\"[FALLBACK] Using EWMA nowcast/forecast (series too short or UCM unavailable).\")\n",
    "    h = int(CFG[\"HORIZON_WEEKS\"])\n",
    "    OAI_fit  = OAI.ewm(span=int(CFG[\"EWMA_SPAN\"]), adjust=False).mean()\n",
    "    last_val = float(OAI_fit.iloc[-1])\n",
    "    idxf = pd.date_range(OAI.index[-1] + pd.offsets.Week(1), periods=h, freq=\"W-MON\")\n",
    "    OAI_fc = pd.Series([last_val]*h, index=idxf, name=\"OAI_fc\")\n",
    "    resid  = (OAI - OAI_fit).dropna()\n",
    "    model_used = \"EWMA\"\n",
    "\n",
    "# ========================= CHANGE-POINTS (CUSUM) =============================\n",
    "k_cusum = resid.std() * 0.25\n",
    "thr     = resid.std() * 3.0\n",
    "pos = neg = 0.0; alarms = []\n",
    "for t, e in resid.items():\n",
    "    pos = max(0.0, pos + e - k_cusum)\n",
    "    neg = min(0.0, neg + e + k_cusum)\n",
    "    if pos > thr or abs(neg) > thr:\n",
    "        alarms.append(t); pos = neg = 0.0\n",
    "cp_dates = [pd.Timestamp(d) for d in alarms[-5:]]\n",
    "\n",
    "# =================== TIME-TO-EVENT (sustained τ for k weeks) =================\n",
    "sig = float(resid.std() or 0.05)\n",
    "n_sims = 1500\n",
    "paths = np.clip(OAI_fc.values + np.random.normal(0, sig, (n_sims, h)), 0, 1)\n",
    "\n",
    "def first_sustained(sim, tau, kreq):\n",
    "    run = 0\n",
    "    for i, a in enumerate(sim >= tau):\n",
    "        run = run + 1 if a else 0\n",
    "        if run >= kreq: return i\n",
    "    return None\n",
    "\n",
    "hits = [first_sustained(p, TAU, CFG[\"RUN_LENGTH_K\"]) for p in paths]\n",
    "hit_idxs = [x for x in hits if x is not None]\n",
    "if hit_idxs:\n",
    "    dates = pd.to_datetime([idxf[i] for i in hit_idxs]).sort_values()\n",
    "    med_date = dates.iloc[len(dates)//2]\n",
    "    pH = len(hit_idxs)/n_sims\n",
    "    d80 = (dates.iloc[int(0.10*len(dates))], dates.iloc[int(0.90*len(dates))])\n",
    "else:\n",
    "    med_date, pH, d80 = None, 0.0, (None, None)\n",
    "\n",
    "# =============================== PLOTS =======================================\n",
    "outdir = Path(CFG[\"SAVE_DIR\"])\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(OAI.index, OAI, label=\"OAI\")\n",
    "if model_used == \"UCM\":\n",
    "    try:\n",
    "        plt.plot((OAI - resid).clip(0,1).index, (OAI - resid).clip(0,1).values, \"--\", label=\"Fit\")\n",
    "    except Exception:\n",
    "        pass\n",
    "plt.axhline(TAU, linestyle=\":\", label=f\"τ={TAU:.3f}\")\n",
    "for d in cp_dates: plt.axvline(d, linestyle=\":\", alpha=0.5)\n",
    "plt.legend(); plt.title(f\"OAI — Model: {model_used}\")\n",
    "plt.tight_layout(); plt.savefig(outdir/\"oai_fit.png\", dpi=160); plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(OAI_fc.index, OAI_fc.values, label=\"Forecast mean\")\n",
    "plt.axhline(TAU, linestyle=\":\", label=f\"τ={TAU:.3f}\")\n",
    "plt.legend(); plt.title(\"OAI Forecast Horizon\")\n",
    "plt.tight_layout(); plt.savefig(outdir/\"oai_fc.png\", dpi=160); plt.close()\n",
    "\n",
    "prob_curve = [np.mean([(x is not None and x <= t) for x in hits]) for t in range(h)]\n",
    "pd.Series(prob_curve, index=idxf).plot(figsize=(10,3), ylim=(0,1), title=\"Pr(sustained crossing by week t)\")\n",
    "plt.tight_layout(); plt.savefig(outdir/\"oai_prob.png\", dpi=160); plt.close()\n",
    "\n",
    "# ============================== SUMMARY ======================================\n",
    "summary = dict(\n",
    "    generated_at = datetime.now(timezone.utc).isoformat(),\n",
    "    horizon_weeks = h,\n",
    "    threshold_tau = float(TAU),\n",
    "    run_length_k  = int(CFG[\"RUN_LENGTH_K\"]),\n",
    "    last_week     = str(pd.Timestamp(OAI.index[-1]).date()),\n",
    "    change_points = [str(pd.Timestamp(d).date()) for d in cp_dates],\n",
    "    prob_within_horizon = round(float(pH), 3),\n",
    "    median_event_date   = (str(pd.Timestamp(med_date).date()) if med_date is not None else None),\n",
    "    event_window_80     = tuple(str(pd.Timestamp(d).date()) if d is not None else None for d in d80),\n",
    "    weights = {k: float(v) for k, v in w.to_dict().items()},\n",
    "    pca_var = float(pca.explained_variance_ratio_[0]),\n",
    "    sources = {\n",
    "        \"trends_cols\": list(trends.columns) if isinstance(trends, pd.DataFrame) and not trends.empty else [],\n",
    "        \"wiki_cols\":   list(wiki.columns)   if isinstance(wiki,   pd.DataFrame) and not wiki.empty   else [],\n",
    "        \"gdelt_cols\":  list(gdelt.columns)  if isinstance(gdelt,  pd.DataFrame) and not gdelt.empty  else [],\n",
    "        \"model_used\":  model_used\n",
    "    }\n",
    ")\n",
    "with open(outdir/\"oai_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(json.dumps(summary, indent=2))\n",
    "print(\"\\nFigures saved to:\", str(outdir.resolve()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31a5cab1-d8ec-4308-a0b1-876797f301fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[τ] Rolling τ (last 3y, q=0.85) = 0.650\n",
      "[Forecast] Pr(sustained ≥ τ_rolling) within 104w = 0.000\n",
      "[Forecast] median date = None, 80% window = (None, None)\n"
     ]
    }
   ],
   "source": [
    "# === CNT Quick Patch: rolling τ + momentum-aware forecast ====================\n",
    "import numpy as _np, pandas as _pd\n",
    "\n",
    "# 1) Rolling τ (local baseline): 3-year window 85th percentile with safe fallback\n",
    "def _rolling_tau(oai, window_weeks=156, q=0.85, fallback=0.65):\n",
    "    oai = _pd.Series(oai).astype(float)\n",
    "    def _q(x):\n",
    "        x = _np.asarray(x, float)\n",
    "        x = x[_np.isfinite(x)]\n",
    "        return _np.nanquantile(x, q) if x.size >= 12 else fallback\n",
    "    rtau = oai.rolling(window_weeks, min_periods=12).apply(_q, raw=False)\n",
    "    rtau = rtau.bfill().fillna(fallback)\n",
    "    return float(rtau.iloc[-1]), rtau\n",
    "\n",
    "TAU_rolling, TAU_series = _rolling_tau(OAI, window_weeks=156, q=0.85, fallback=CFG.get(\"FIXED_TAU\", 0.65))\n",
    "print(f\"[τ] Rolling τ (last 3y, q=0.85) = {TAU_rolling:.3f}\")\n",
    "\n",
    "# 2) Momentum-aware forecast: add a gentle drift term based on recent OAI slope\n",
    "#    (keeps honesty: just extrapolates recent velocity; noise stays the same)\n",
    "lookback = 26  # weeks\n",
    "if len(OAI) >= lookback+2:\n",
    "    y_tail = _pd.Series(OAI.iloc[-lookback:])\n",
    "    # simple slope per week\n",
    "    x = _np.arange(len(y_tail))\n",
    "    m = _np.polyfit(x, y_tail.values, 1)[0]\n",
    "else:\n",
    "    m = 0.0\n",
    "\n",
    "drift_per_week = float(m)  # small if OAI is flat; can be negative\n",
    "OAI_fc_mom = (_pd.Series(OAI_fc, index=OAI_fc.index) +\n",
    "              _np.arange(len(OAI_fc))*drift_per_week).clip(0,1)\n",
    "\n",
    "# 3) Re-run sustained-crossing simulation with rolling τ and momentum forecast\n",
    "sig = float((OAI - OAI.ewm(span=int(CFG.get(\"EWMA_SPAN\", 8)), adjust=False).mean()).std() or 0.05)\n",
    "h = int(CFG.get(\"HORIZON_WEEKS\", 104))\n",
    "n_sims = 1500\n",
    "paths = _np.clip(OAI_fc_mom.values + _np.random.normal(0, sig, (n_sims, h)), 0, 1)\n",
    "\n",
    "def _first_sustained(sim, tau, kreq):\n",
    "    run = 0\n",
    "    for i, a in enumerate(sim >= tau):\n",
    "        run = run + 1 if a else 0\n",
    "        if run >= kreq: return i\n",
    "    return None\n",
    "\n",
    "hits = [_first_sustained(p, TAU_rolling, int(CFG.get(\"RUN_LENGTH_K\", 6))) for p in paths]\n",
    "hit_idxs = [x for x in hits if x is not None]\n",
    "if hit_idxs:\n",
    "    idxf = OAI_fc.index\n",
    "    dates = _pd.to_datetime([idxf[i] for i in hit_idxs]).sort_values()\n",
    "    med_date = dates.iloc[len(dates)//2]\n",
    "    pH = len(hit_idxs)/n_sims\n",
    "    d80 = (dates.iloc[int(0.10*len(dates))], dates.iloc[int(0.90*len(dates))])\n",
    "else:\n",
    "    med_date, pH, d80 = None, 0.0, (None, None)\n",
    "\n",
    "print(f\"[Forecast] Pr(sustained ≥ τ_rolling) within {h}w = {pH:.3f}\")\n",
    "print(f\"[Forecast] median date = {str(med_date.date()) if med_date is not None else None}, 80% window = {tuple(str(d.date()) if d is not None else None for d in d80)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61369876-7076-4c92-b648-7e9758aec1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Shift-Event] z_tau=1.0, m_min=0.00150, k=6  ->  triggered=False\n",
      "[Drift] Need Δ ≥ 0.000000 per week to sustain ≥ τ within 104w (start in ~0w).  x0=nan, τ_eff=nan, σ=nan\n",
      "[What-if @ Δ*] Pr(sustain in 104w) ≈ 0.000\n",
      "[Summary] Shift-trigger=False | Δ*=0.000000/wk | τ=0.650 (τ_eff=nan) | k=6 | H=104\n"
     ]
    }
   ],
   "source": [
    "# === CNT Patch Cell: Shift-Event + Minimum-Drift Sensitivity ==================\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "assert \"OAI\" in globals() and \"OAI_fc\" in globals(), \"Run the mega cell first.\"\n",
    "\n",
    "# ---------- A) SHIFT-BASED EVENT (z-score & slope sustain) -------------------\n",
    "# Define a \"wake-up\" as a *structural lift*, not just a high level:\n",
    "#   condition1: rolling z-score of OAI >= z_tau\n",
    "#   condition2: rolling slope (per week) >= m_min\n",
    "#   both must hold for k weeks.\n",
    "z_window = 26          # ~6 months\n",
    "z_tau    = 1.0         # ~1σ above local mean\n",
    "m_min    = 0.0015      # ~0.15 points per 100 weeks; tweak to taste\n",
    "k_shift  = int(CFG.get(\"RUN_LENGTH_K\", 6))\n",
    "\n",
    "oai = pd.Series(OAI, index=OAI.index).astype(float)\n",
    "mu  = oai.rolling(z_window, min_periods=max(8, z_window//3)).mean()\n",
    "sd  = oai.rolling(z_window, min_periods=max(8, z_window//3)).std()\n",
    "zsc = (oai - mu) / (sd.replace(0, np.nan) + 1e-9)\n",
    "\n",
    "# rolling slope via simple linear fit on a sliding window\n",
    "def rolling_slope(y, w):\n",
    "    y = pd.Series(y).astype(float)\n",
    "    if len(y) < w: return pd.Series(index=y.index, dtype=float)\n",
    "    X = np.arange(w)\n",
    "    out = [np.nan]*(w-1)\n",
    "    for i in range(w, len(y)+1):\n",
    "        yi = y.iloc[i-w:i].values\n",
    "        m = np.polyfit(X, yi, 1)[0]\n",
    "        out.append(m)\n",
    "    return pd.Series(out, index=y.index, dtype=float)\n",
    "\n",
    "slope = rolling_slope(oai, z_window)\n",
    "\n",
    "cond = (zsc >= z_tau) & (slope >= m_min)\n",
    "run  = (cond.groupby((~cond).cumsum()).cumcount()+1)*cond  # run-lengths\n",
    "shift_triggered = bool((run >= k_shift).any())\n",
    "\n",
    "print(f\"[Shift-Event] z_tau={z_tau}, m_min={m_min:.5f}, k={k_shift}  ->  triggered={shift_triggered}\")\n",
    "\n",
    "# ---------- B) DRIFT-TO-CROSS (how much weekly lift is needed?) --------------\n",
    "# Given current OAI_fc mean path, what constant drift per week (Δ) is needed\n",
    "# to achieve a sustained crossing >= TAU for k weeks within H?\n",
    "H = int(CFG.get(\"HORIZON_WEEKS\", 104))\n",
    "k = int(CFG.get(\"RUN_LENGTH_K\", 6))\n",
    "tau = float(globals().get(\"TAU\", CFG.get(\"FIXED_TAU\", 0.65)))\n",
    "\n",
    "fc = pd.Series(OAI_fc, index=OAI_fc.index).astype(float)\n",
    "x0 = float(oai.iloc[-1])         # last observed OAI\n",
    "sig = float((oai - oai.ewm(span=int(CFG.get(\"EWMA_SPAN\", 8)), adjust=False).mean()).std() or 0.05)\n",
    "\n",
    "# Safety margin: require mean - z*sig >= τ (e.g., z=0.5 ~ 69% one-sided)\n",
    "z_safety = 0.5\n",
    "tau_eff  = tau + z_safety*sig\n",
    "\n",
    "# Minimal drift if we allow the crossing at the very end (latest start t0 = H-k)\n",
    "# x0 + Δ*(t0) >= tau_eff  ->  Δ >= (tau_eff - x0) / t0\n",
    "def min_drift_required(x0, tau_eff, H, k):\n",
    "    if H <= k: return np.inf, None\n",
    "    candidates = []\n",
    "    for t0 in range(0, H - k + 1):\n",
    "        t_end = t0 + (k-1)\n",
    "        # ensure all k weeks are above: check at start is sufficient for linear increase\n",
    "        t_use = max(1, t0)\n",
    "        d = (tau_eff - x0) / t_use if t_use > 0 else (tau_eff - x0)\n",
    "        candidates.append((t0, d))\n",
    "    # choose feasible minimal non-negative drift\n",
    "    feas = [(t0, d) for (t0, d) in candidates if d >= 0]\n",
    "    if not feas:\n",
    "        return 0.0, 0\n",
    "    t0_star, d_star = min(feas, key=lambda x: x[1])\n",
    "    return float(d_star), int(t0_star)\n",
    "\n",
    "d_star, t0_star = min_drift_required(x0, tau_eff, H, k)\n",
    "weeks_until_start = t0_star if t0_star is not None else None\n",
    "print(f\"[Drift] Need Δ ≥ {d_star:.6f} per week to sustain ≥ τ within {H}w \"\n",
    "      f\"(start in ~{weeks_until_start}w).  x0={x0:.3f}, τ_eff={tau_eff:.3f}, σ={sig:.3f}\")\n",
    "\n",
    "# ---------- Optional: simulate with that drift to see the borderline ---------\n",
    "n_sims = 1500\n",
    "t = np.arange(H)\n",
    "path_mean = np.clip(fc.values + d_star*t, 0, 1)\n",
    "paths = np.clip(path_mean + np.random.normal(0, sig, (n_sims, H)), 0, 1)\n",
    "\n",
    "def first_sustained(sim, tau, kreq):\n",
    "    r = 0\n",
    "    for i, a in enumerate(sim >= tau):\n",
    "        r = r+1 if a else 0\n",
    "        if r >= kreq: return i\n",
    "    return None\n",
    "\n",
    "hits = [first_sustained(p, tau, k) for p in paths]\n",
    "pH = np.mean([h is not None for h in hits])\n",
    "print(f\"[What-if @ Δ*] Pr(sustain in {H}w) ≈ {pH:.3f}\")\n",
    "\n",
    "# ---------- Summary line you can log -----------------------------------------\n",
    "print(f\"[Summary] Shift-trigger={shift_triggered} | Δ*={d_star:.6f}/wk | \"\n",
    "      f\"τ={tau:.3f} (τ_eff={tau_eff:.3f}) | k={k} | H={H}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f865f39-8048-43bf-b8b7-f8c38ec47771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SANITY] OAI empty → building a minimal synthetic spine from current df.\n",
      "[SANITY] OAI_fc empty → flat forecast from last OAI.\n",
      "[τ] Using τ = 0.695\n",
      "[Clean Prob] σ≈0.059, slope≈0.003036/wk → Pr(sustain ≥ τ in 104w) = 0.000\n",
      "[Clean Dates] median = None, 80% = (None, None)\n"
     ]
    }
   ],
   "source": [
    "# === CNT NaN Hardener Patch: sanitize series, robust noise, safe targets =====\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "def _safe_series(x):\n",
    "    s = pd.Series(x).astype(float).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    return s\n",
    "\n",
    "def _mad(x):\n",
    "    s = _safe_series(x)\n",
    "    if len(s) == 0: return np.nan\n",
    "    med = np.median(s)\n",
    "    return 1.4826 * np.median(np.abs(s - med))  # robust σ\n",
    "\n",
    "# 0) Ensure OAI & OAI_fc exist\n",
    "assert \"OAI\" in globals(), \"Run mega cell first (creates OAI).\"\n",
    "assert \"OAI_fc\" in globals(), \"Run mega cell first (creates OAI_fc).\"\n",
    "\n",
    "# 1) Sanitize to finite values\n",
    "OAI_s   = _safe_series(OAI).copy()\n",
    "OAI_fcS = _safe_series(OAI_fc).copy()\n",
    "\n",
    "# If either is empty, synthesize a minimal steady spine (so logic can proceed)\n",
    "if OAI_s.empty:\n",
    "    print(\"[SANITY] OAI empty → building a minimal synthetic spine from current df.\")\n",
    "    base = df.copy() if 'df' in globals() else pd.DataFrame(index=pd.date_range(\"2020-01-06\", periods=32, freq=\"W-MON\"))\n",
    "    if base.empty:\n",
    "        base = pd.DataFrame(index=pd.date_range(\"2020-01-06\", periods=32, freq=\"W-MON\"))\n",
    "        base[\"synthetic\"] = np.linspace(0.45, 0.55, len(base))\n",
    "    else:\n",
    "        # mean-normalize available columns and average\n",
    "        tmp = base.apply(lambda col: (col - np.nanmean(col)) / (np.nanstd(col) + 1e-9))\n",
    "        base[\"synthetic\"] = tmp.mean(axis=1).fillna(0).clip(-3, 3)\n",
    "        base[\"synthetic\"] = 1/(1+np.exp(-base[\"synthetic\"]))  # squash to [0,1]\n",
    "    OAI_s = _safe_series(base[\"synthetic\"])\n",
    "\n",
    "if OAI_fcS.empty:\n",
    "    print(\"[SANITY] OAI_fc empty → flat forecast from last OAI.\")\n",
    "    last = float(OAI_s.iloc[-1]) if len(OAI_s) else 0.5\n",
    "    horizon = int(CFG.get(\"HORIZON_WEEKS\", 104))\n",
    "    OAI_fcS = pd.Series(np.full(horizon, last),\n",
    "                        index=pd.date_range(OAI_s.index[-1] + pd.offsets.Week(1), periods=horizon, freq=\"W-MON\"))\n",
    "\n",
    "# 2) Recompute τ robustly if needed\n",
    "def _robust_tau(oai_series, q=0.85, fallback=0.65, min_points=10):\n",
    "    s = _safe_series(oai_series)\n",
    "    if len(s) < min_points:\n",
    "        print(f\"[τ] Not enough points ({len(s)}) for calibrated τ→ fallback={fallback}\")\n",
    "        return float(fallback)\n",
    "    try:\n",
    "        t = float(np.nanquantile(s.values, q))\n",
    "        return t if np.isfinite(t) else float(fallback)\n",
    "    except Exception:\n",
    "        return float(fallback)\n",
    "\n",
    "if CFG.get(\"CALIBRATE_TAU\", True):\n",
    "    TAU = _robust_tau(OAI_s, q=0.85, fallback=CFG.get(\"FIXED_TAU\", 0.65), min_points=10)\n",
    "else:\n",
    "    TAU = float(CFG.get(\"FIXED_TAU\", 0.65))\n",
    "print(f\"[τ] Using τ = {TAU:.3f}\")\n",
    "\n",
    "# 3) Robust noise & momentum\n",
    "sigma = _mad(OAI_s - OAI_s.ewm(span=int(CFG.get(\"EWMA_SPAN\", 8)), adjust=False).mean())\n",
    "if not np.isfinite(sigma) or sigma == 0:\n",
    "    sigma = 0.05\n",
    "lookback = min(26, len(OAI_s))\n",
    "slope = 0.0\n",
    "if lookback >= 8:\n",
    "    x = np.arange(lookback)\n",
    "    y = OAI_s.iloc[-lookback:].values\n",
    "    slope = np.polyfit(x, y, 1)[0]\n",
    "\n",
    "# 4) Momentum-aware mean path (gentle, honest)\n",
    "h = int(CFG.get(\"HORIZON_WEEKS\", 104))\n",
    "t = np.arange(h)\n",
    "mean_path = np.clip(OAI_fcS.values + slope * t, 0, 1)\n",
    "\n",
    "# 5) Recompute event probability cleanly\n",
    "def _first_sustained(sim, tau, kreq):\n",
    "    r = 0\n",
    "    for i, a in enumerate(sim >= tau):\n",
    "        r = r + 1 if a else 0\n",
    "        if r >= kreq:\n",
    "            return i\n",
    "    return None\n",
    "\n",
    "kreq = int(CFG.get(\"RUN_LENGTH_K\", 6))\n",
    "n_sims = 2000\n",
    "paths = np.clip(mean_path + np.random.normal(0, sigma, (n_sims, h)), 0, 1)\n",
    "hits = [_first_sustained(p, TAU, kreq) for p in paths]\n",
    "pH = np.mean([x is not None for x in hits])\n",
    "med = None\n",
    "w80 = (None, None)\n",
    "if any(x is not None for x in hits):\n",
    "    idxf = OAI_fcS.index\n",
    "    dates = pd.to_datetime([idxf[i] for i in hits if i is not None]).sort_values()\n",
    "    med = dates.iloc[len(dates)//2]\n",
    "    w80 = (dates.iloc[int(0.10*len(dates))], dates.iloc[int(0.90*len(dates))])\n",
    "\n",
    "print(f\"[Clean Prob] σ≈{sigma:.3f}, slope≈{slope:.6f}/wk → Pr(sustain ≥ τ in {h}w) = {pH:.3f}\")\n",
    "print(f\"[Clean Dates] median = {str(med.date()) if med is not None else None}, 80% = {tuple(str(d.date()) if d is not None else None for d in w80)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04ca796e-067d-4886-b171-65de18799411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] pytrends unavailable; skipping Google Trends.\n",
      "Fetching data...\n",
      "[GDELT] Too many errors in a row (6); stopping early.\n",
      "[SANITY] OAI empty → building a minimal synthetic spine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\CNT_Lab\\.venv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency W-MON will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "C:\\Users\\caleb\\CNT_Lab\\.venv\\Lib\\site-packages\\statsmodels\\tsa\\filters\\hp_filter.py:100: SparseEfficiencyWarning: spsolve requires A be CSC or CSR matrix format\n",
      "  trend = spsolve(I+lamb*K.T.dot(K), x, use_umfpack=use_umfpack)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"generated_at\": \"2025-10-16T06:15:15.894105+00:00\",\n",
      "  \"horizon_weeks\": 104,\n",
      "  \"threshold_tau\": 0.6954909408971236,\n",
      "  \"run_length_k\": 6,\n",
      "  \"last_week\": \"2025-10-20\",\n",
      "  \"change_points\": [\n",
      "    \"2020-04-06\",\n",
      "    \"2020-05-11\",\n",
      "    \"2021-01-11\",\n",
      "    \"2021-01-18\",\n",
      "    \"2021-01-25\"\n",
      "  ],\n",
      "  \"prob_within_horizon\": 0.0,\n",
      "  \"median_event_date\": null,\n",
      "  \"event_window_80\": [\n",
      "    null,\n",
      "    null\n",
      "  ],\n",
      "  \"shift_event_triggered\": true,\n",
      "  \"weights\": {\n",
      "    \"Mass_surveillance_in_the_United_States\": 0.05,\n",
      "    \"First_Amendment_to_the_United_States_Constitution\": 0.2658909185548501,\n",
      "    \"Censorship_in_the_United_States\": 0.2974816205736362,\n",
      "    \"Civil_liberties_in_the_United_States\": 0.9999999999979158\n",
      "  },\n",
      "  \"pca_var\": 0.7709565458878976,\n",
      "  \"sources\": {\n",
      "    \"trends_cols\": [],\n",
      "    \"wiki_cols\": [\n",
      "      \"Mass_surveillance_in_the_United_States\",\n",
      "      \"First_Amendment_to_the_United_States_Constitution\",\n",
      "      \"Censorship_in_the_United_States\",\n",
      "      \"Civil_liberties_in_the_United_States\"\n",
      "    ],\n",
      "    \"gdelt_cols\": [],\n",
      "    \"local_series\": [],\n",
      "    \"model_used\": \"UCM\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Figures saved to: C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\artifacts\n"
     ]
    }
   ],
   "source": [
    "# === CNT \"Wake-Up Field\" — Fused Ultra-Resilient Single Mega Cell ============\n",
    "# Ingest (Wiki + safe GDELT + optional Trends + local CSVs), cache, build OAI,\n",
    "# UCM/EWMA nowcast, momentum-aware forecast, shift-event detector, robust τ,\n",
    "# sustained-threshold event simulation, plots + summary JSON.\n",
    "# -----------------------------------------------------------------------------\n",
    "import os, sys, json, math, time, glob, warnings, requests\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# Try UCM; fallback to EWMA if unavailable/too short\n",
    "try:\n",
    "    from statsmodels.tsa.statespace.structural import UnobservedComponents\n",
    "    _HAS_UCM = True\n",
    "except Exception:\n",
    "    _HAS_UCM = False\n",
    "\n",
    "# ============================== CONFIG =======================================\n",
    "CFG = dict(\n",
    "    ROOT = Path.cwd(),\n",
    "    REGION = \"US\",\n",
    "    HORIZON_WEEKS = 104,          # forecast horizon\n",
    "    RUN_LENGTH_K  = 6,            # require k consecutive weeks ≥ τ\n",
    "    BACKTEST_START = \"2012-01-01\",\n",
    "    # Signals\n",
    "    TOPICS = [\"government surveillance\",\"censorship\",\"civil liberties\",\"free speech\"],\n",
    "    WIKI_PAGES = [\n",
    "        \"Mass_surveillance_in_the_United_States\",\n",
    "        \"First_Amendment_to_the_United_States_Constitution\",\n",
    "        \"Censorship_in_the_United_States\",\n",
    "        \"Civil_liberties_in_the_United_States\"\n",
    "    ],\n",
    "    GDELT_QUERY = '(censorship OR surveillance OR \"civil liberties\") AND location: United States',\n",
    "    # Local CSV backbone (zero-network): put date,value CSVs in CNT_Lab/data/\n",
    "    LOCAL_DATA_DIR = Path(\"CNT_Lab\")/\"data\",\n",
    "    # Threshold policy\n",
    "    CALIBRATE_TAU = True,         # if False uses FIXED_TAU\n",
    "    TAU_QUANTILE  = 0.85,         # 0.80 is looser, 0.85 stricter\n",
    "    FIXED_TAU     = 0.65,\n",
    "    # Modeling heuristics\n",
    "    MIN_LEN_UCM = 12,             # min weekly points to enable UCM\n",
    "    EWMA_SPAN   = 8,              # EWMA smoothing when UCM not used\n",
    "    MOMENTUM_LOOKBACK = 26,       # weeks for slope\n",
    "    # Shift-event detector (optional)\n",
    "    SHIFT_Z_WINDOW = 26,          # window for z-score & slope\n",
    "    SHIFT_Z_TAU    = 1.0,         # ~1σ above local mean\n",
    "    SHIFT_MIN_SLOPE= 0.0015,      # per-week slope gate\n",
    "    # IO / caching\n",
    "    SAVE_DIR  = \"artifacts\",\n",
    "    CACHE_DIR = \"artifacts/cache\",\n",
    "    RNG_SEED = 1337\n",
    ")\n",
    "np.random.seed(CFG[\"RNG_SEED\"])\n",
    "Path(CFG[\"SAVE_DIR\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(CFG[\"CACHE_DIR\"]).mkdir(parents=True, exist_ok=True)\n",
    "Path(CFG[\"LOCAL_DATA_DIR\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========================== NETWORK SESSION ==================================\n",
    "SESSION = requests.Session()\n",
    "SESSION.headers.update({\n",
    "    \"User-Agent\": \"CNTLab/1.3 (fieldwalker://local; contact: telos@cnt.local)\"\n",
    "})\n",
    "requests_get = SESSION.get\n",
    "\n",
    "# Optional: Google Trends (skips gracefully if not present)\n",
    "try:\n",
    "    from pytrends.request import TrendReq\n",
    "    _HAS_TRENDS = True\n",
    "except Exception:\n",
    "    print(\"[INFO] pytrends unavailable; skipping Google Trends.\")\n",
    "    TrendReq = None; _HAS_TRENDS = False\n",
    "\n",
    "# =============================== UTILS =======================================\n",
    "def as_week_index(dts):\n",
    "    return pd.to_datetime(dts).to_period('W-MON').to_timestamp()\n",
    "\n",
    "def spectral_entropy(x, nseg=8, eps=1e-12):\n",
    "    x = pd.Series(x).dropna().values\n",
    "    if len(x) < 16: return np.nan\n",
    "    x = (x - x.mean()) / (x.std() + 1e-9)\n",
    "    seg = max(16, len(x)//nseg)\n",
    "    ps = []\n",
    "    for i in range(0, len(x)-seg+1, seg):\n",
    "        X = np.fft.rfft(x[i:i+seg]); P = (X*np.conj(X)).real; P /= (P.sum()+eps); ps.append(P)\n",
    "    Pm = np.mean(ps, axis=0); H = -(Pm*np.log(Pm+eps)).sum(); Hmax = math.log(len(Pm))\n",
    "    return float(1 - H/Hmax)\n",
    "\n",
    "def zscore(s): s = pd.Series(s); return (s - s.mean()) / (s.std() + 1e-9)\n",
    "\n",
    "def logistic_scale(s):\n",
    "    s = pd.Series(s); q1,q2,q3 = s.quantile([.1,.5,.9])\n",
    "    scale = (q3-q1)/2.0 if q3>q1 else (s.std() or 1.0)\n",
    "    return 1/(1+np.exp(-(s-q2)/(scale+1e-9)))\n",
    "\n",
    "def robust_tau(oai_series, q=0.85, fallback=0.65, min_points=10):\n",
    "    s = pd.Series(oai_series).astype(float).replace([np.inf,-np.inf],np.nan).dropna()\n",
    "    if len(s) < min_points:\n",
    "        print(f\"[τ] Not enough points ({len(s)}) for calibrated τ; using fallback={fallback}.\")\n",
    "        return float(fallback)\n",
    "    try:\n",
    "        tau = float(np.nanquantile(s.values, q))\n",
    "        return tau if np.isfinite(tau) else float(fallback)\n",
    "    except Exception:\n",
    "        return float(fallback)\n",
    "\n",
    "def mad_sigma(x):\n",
    "    s = pd.Series(x).astype(float).replace([np.inf,-np.inf],np.nan).dropna()\n",
    "    if s.empty: return np.nan\n",
    "    med = float(np.median(s)); return 1.4826*float(np.median(np.abs(s - med)))\n",
    "\n",
    "# ============================== FETCHERS =====================================\n",
    "def fetch_trends(topics, geo=\"US\", since=\"2012-01-01\"):\n",
    "    if not _HAS_TRENDS: return pd.DataFrame()\n",
    "    pytrends = TrendReq(hl=\"en-US\", tz=360)\n",
    "    frames = []\n",
    "    for kw in topics:\n",
    "        try:\n",
    "            pytrends.build_payload([kw], timeframe=f\"{since} {pd.Timestamp.today():%Y-%m-%d}\", geo=geo)\n",
    "            df = pytrends.interest_over_time()\n",
    "            if df.empty: continue\n",
    "            s = df[kw].rename(kw); s.index = as_week_index(s.index); frames.append(s); time.sleep(1.0)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] pytrends failed for {kw}: {e}\")\n",
    "    return pd.concat(frames, axis=1).sort_index() if frames else pd.DataFrame()\n",
    "\n",
    "def fetch_wikiviews(pages, since=\"2012-01-01\", project=\"en.wikipedia\", agent=\"user\"):\n",
    "    cache_path = Path(CFG[\"CACHE_DIR\"]) / \"wiki_views.csv\"\n",
    "    def one_page(title):\n",
    "        start = pd.Timestamp(since).strftime(\"%Y%m0100\")\n",
    "        end   = pd.Timestamp.today().strftime(\"%Y%m%d00\")\n",
    "        url = (f\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/\"\n",
    "               f\"{project}/all-access/{agent}/{title}/daily/{start}/{end}\")\n",
    "        try:\n",
    "            r = requests_get(url, timeout=20); r.raise_for_status()\n",
    "            data = r.json().get(\"items\", [])\n",
    "            ts = {pd.to_datetime(i[\"timestamp\"][:8]): i[\"views\"] for i in data}\n",
    "            return pd.Series(ts, name=title).sort_index().resample(\"W-MON\").sum()\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] wiki fail {title}: {e}\"); return pd.Series(dtype=float)\n",
    "    cols=[]\n",
    "    for p in pages:\n",
    "        s = one_page(p.replace(\" \",\"_\"))\n",
    "        if s.shape[0]: cols.append(s)\n",
    "        time.sleep(0.3)\n",
    "    wiki = pd.concat(cols, axis=1).sort_index() if cols else pd.DataFrame()\n",
    "    if cache_path.exists():\n",
    "        old = pd.read_csv(cache_path, parse_dates=[\"date\"]).set_index(\"date\")\n",
    "        wiki = old.combine_first(wiki) if not wiki.empty else old\n",
    "    if not wiki.empty: wiki.to_csv(cache_path, index_label=\"date\")\n",
    "    return wiki\n",
    "\n",
    "def fetch_gdelt_counts_safe(query, since=\"2018-01-01\", timeout=8, max_consec_err=6, max_months=60):\n",
    "    start = pd.Timestamp(since).to_period(\"M\").to_timestamp()\n",
    "    end   = pd.Timestamp.today().to_period(\"M\").to_timestamp()\n",
    "    months = pd.period_range(start, end, freq=\"M\").to_timestamp()[-max_months:]\n",
    "    rows, consec = [], 0\n",
    "    for m in months:\n",
    "        url = (\"https://api.gdeltproject.org/api/v2/doc/doc?\"\n",
    "               f\"query={requests.utils.quote(query)}&mode=TimelineVol&format=json\"\n",
    "               f\"&startdatetime={m:%Y%m%d000000}&enddatetime={(m+pd.offsets.MonthEnd(0)):%Y%m%d235959}\")\n",
    "        try:\n",
    "            js = requests_get(url, timeout=timeout).json()\n",
    "            data = js.get(\"timelines\", [{}])[0].get(\"data\", [])\n",
    "            for pt in data: rows.append((pd.to_datetime(pt[\"date\"]), int(pt[\"value\"])))\n",
    "            consec = 0; time.sleep(0.25)\n",
    "        except Exception:\n",
    "            consec += 1\n",
    "            if consec >= max_consec_err:\n",
    "                print(f\"[GDELT] Too many errors in a row ({consec}); stopping early.\"); break\n",
    "    if not rows: return pd.DataFrame()\n",
    "    s = pd.Series({d:v for d,v in rows}).sort_index().resample(\"W-MON\").sum().rename(\"gdelt_volume\")\n",
    "    cache_path = Path(CFG[\"CACHE_DIR\"]) / \"gdelt_volume.csv\"\n",
    "    if cache_path.exists():\n",
    "        old = pd.read_csv(cache_path, parse_dates=[\"date\"]).set_index(\"date\")[\"gdelt_volume\"]\n",
    "        s = old.combine_first(s)\n",
    "    s.to_frame().to_csv(cache_path, index_label=\"date\")\n",
    "    return s.to_frame()\n",
    "\n",
    "# ============================== INGEST =======================================\n",
    "print(\"Fetching data...\")\n",
    "frames = []\n",
    "trends = fetch_trends(CFG[\"TOPICS\"], geo=CFG[\"REGION\"], since=CFG[\"BACKTEST_START\"])\n",
    "if not trends.empty: frames.append(trends)\n",
    "\n",
    "wiki   = fetch_wikiviews(CFG[\"WIKI_PAGES\"], since=CFG[\"BACKTEST_START\"])\n",
    "if not wiki.empty: frames.append(wiki)\n",
    "\n",
    "gdelt  = fetch_gdelt_counts_safe(CFG[\"GDELT_QUERY\"], since=CFG[\"BACKTEST_START\"])\n",
    "if isinstance(gdelt, pd.DataFrame) and not gdelt.empty: frames.append(gdelt)\n",
    "\n",
    "# Local CSV backbone (zero network): any date,value CSV in CNT_Lab/data\n",
    "local_list = []\n",
    "for p in glob.glob(str(CFG[\"LOCAL_DATA_DIR\"]/\"*.csv\")):\n",
    "    try:\n",
    "        dfc = pd.read_csv(p)\n",
    "        dcol = [c for c in dfc.columns if \"date\" in c.lower()][0]\n",
    "        vcol = [c for c in dfc.columns if c != dcol][0]\n",
    "        s = pd.Series(dfc[vcol].values, index=pd.to_datetime(dfc[dcol]), name=Path(p).stem)\n",
    "        s = s.resample(\"W-MON\").mean()\n",
    "        if s.dropna().shape[0] >= 8:\n",
    "            local_list.append(s)\n",
    "            print(f\"[LOCAL] using {Path(p).name} ({s.dropna().shape[0]} pts)\")\n",
    "    except Exception as e:\n",
    "        print(f\"[LOCAL] skip {p}: {e}\")\n",
    "\n",
    "if local_list:\n",
    "    local_df = pd.concat(local_list, axis=1).sort_index().ffill().bfill()\n",
    "    frames.append(local_df)\n",
    "\n",
    "if not frames:\n",
    "    # as a last resort, attempt OWID Pew trust (may require network)\n",
    "    print(\"[FALLBACK] No sources found; attempting OWID Pew dataset.\")\n",
    "    pew = pd.read_csv(\n",
    "        \"https://raw.githubusercontent.com/owid/owid-datasets/master/datasets/Trust%20in%20government%20(Pew)/Trust%20in%20government%20(Pew).csv\"\n",
    "    )\n",
    "    pew = pew.rename(columns={\"Year\":\"date\",\"Trust in government (Pew)\":\"trust\"}).dropna(subset=[\"date\",\"trust\"])\n",
    "    pew[\"date\"] = pd.to_datetime(pew[\"date\"], format=\"%Y\")\n",
    "    pew = pew.set_index(\"date\")[\"trust\"].resample(\"W-MON\").ffill().to_frame()\n",
    "    frames = [pew]\n",
    "\n",
    "df = pd.concat(frames, axis=1).sort_index().ffill().bfill()\n",
    "\n",
    "# ===================== FEATURES & FORECASTABILITY ============================\n",
    "feat = pd.DataFrame(index=df.index)\n",
    "for c in df.columns:\n",
    "    feat[c] = df[c]; feat[c+\"_z\"] = zscore(df[c]); feat[c+\"_vol4\"] = df[c].pct_change().rolling(4).std()\n",
    "\n",
    "weights = {c: spectral_entropy(df[c].values) for c in df.columns}\n",
    "w = pd.Series(weights).fillna(0.5).clip(0.15, 0.85)\n",
    "w = (w - w.min()) / (w.max() - w.min() + 1e-12)\n",
    "w = w.clip(0.05, 1.0)\n",
    "\n",
    "# ========================= OVERREACH AWARENESS INDEX =========================\n",
    "scaler = StandardScaler(); X = scaler.fit_transform(df.values)\n",
    "Wm = np.diag(np.sqrt(w[df.columns].values))\n",
    "Xw = X.dot(Wm)\n",
    "pca = PCA(n_components=1, random_state=CFG[\"RNG_SEED\"])\n",
    "oai_raw = pca.fit_transform(Xw).ravel()\n",
    "OAI = pd.Series(logistic_scale(zscore(oai_raw)), index=df.index, name=\"OAI\")\n",
    "\n",
    "# ========== NaN hardener (ensure non-empty OAI + baseline forecast) ==========\n",
    "def _safe_series(x):\n",
    "    return pd.Series(x).astype(float).replace([np.inf,-np.inf],np.nan).dropna()\n",
    "\n",
    "OAI_s = _safe_series(OAI)\n",
    "if OAI_s.empty:\n",
    "    print(\"[SANITY] OAI empty → building a minimal synthetic spine.\")\n",
    "    base = df.copy()\n",
    "    if base.empty:\n",
    "        base = pd.DataFrame(index=pd.date_range(\"2020-01-06\", periods=32, freq=\"W-MON\"))\n",
    "        base[\"synthetic\"] = np.linspace(0.45, 0.55, len(base))\n",
    "    else:\n",
    "        tmp = base.apply(lambda col: (col - np.nanmean(col)) / (np.nanstd(col) + 1e-9))\n",
    "        base[\"synthetic\"] = 1/(1+np.exp(-tmp.mean(axis=1).fillna(0).clip(-3,3)))\n",
    "    OAI_s = _safe_series(base[\"synthetic\"])\n",
    "    OAI = OAI_s.copy()\n",
    "\n",
    "# ========================= THRESHOLD τ (robust) ==============================\n",
    "TAU = robust_tau(OAI, q=CFG[\"TAU_QUANTILE\"], fallback=CFG[\"FIXED_TAU\"], min_points=10) \\\n",
    "      if CFG[\"CALIBRATE_TAU\"] else float(CFG[\"FIXED_TAU\"])\n",
    "\n",
    "# ========================= NOWCAST / FORECAST ================================\n",
    "use_ucm = _HAS_UCM and (len(OAI.dropna()) >= CFG[\"MIN_LEN_UCM\"])\n",
    "exo = feat.filter(regex=\"_z$|_vol4$\").fillna(0)\n",
    "\n",
    "if use_ucm:\n",
    "    try:\n",
    "        model = UnobservedComponents(endog=OAI, level='llevel', exog=exo)\n",
    "    except Exception:\n",
    "        model = UnobservedComponents(endog=OAI, trend=True, exog=exo)\n",
    "    res = model.fit(disp=False)\n",
    "    h = int(CFG[\"HORIZON_WEEKS\"])\n",
    "    lastX = exo.iloc[-1:].values; Xf = np.repeat(lastX, h, axis=0)\n",
    "    fc = res.get_forecast(steps=h, exog=Xf)\n",
    "    idxf = pd.date_range(OAI.index[-1] + pd.offsets.Week(1), periods=h, freq=\"W-MON\")\n",
    "    OAI_fc = pd.Series(fc.predicted_mean.clip(0,1), index=idxf, name=\"OAI_fc\")\n",
    "    resid  = (OAI - res.fittedvalues.reindex_like(OAI).bfill()).dropna()\n",
    "    model_used = \"UCM\"\n",
    "else:\n",
    "    print(\"[FALLBACK] Using EWMA nowcast/forecast (series too short or UCM unavailable).\")\n",
    "    h = int(CFG[\"HORIZON_WEEKS\"])\n",
    "    OAI_fit  = OAI.ewm(span=int(CFG[\"EWMA_SPAN\"]), adjust=False).mean()\n",
    "    last_val = float(OAI_fit.iloc[-1])\n",
    "    idxf = pd.date_range(OAI.index[-1] + pd.offsets.Week(1), periods=h, freq=\"W-MON\")\n",
    "    OAI_fc = pd.Series([last_val]*h, index=idxf, name=\"OAI_fc\")\n",
    "    resid  = (OAI - OAI_fit).dropna()\n",
    "    model_used = \"EWMA\"\n",
    "\n",
    "# Momentum-aware projection (gentle, honest)\n",
    "lookback = min(int(CFG[\"MOMENTUM_LOOKBACK\"]), len(OAI))\n",
    "slope = 0.0\n",
    "if lookback >= 12:\n",
    "    x = np.arange(lookback); y = pd.Series(OAI.iloc[-lookback:]).values\n",
    "    slope = np.polyfit(x, y, 1)[0]\n",
    "mean_path = pd.Series(OAI_fc.values + np.arange(h)*slope, index=OAI_fc.index).clip(0,1)\n",
    "\n",
    "# Robust noise\n",
    "sigma = mad_sigma(OAI - OAI.ewm(span=int(CFG[\"EWMA_SPAN\"]), adjust=False).mean())\n",
    "if not np.isfinite(sigma) or sigma == 0: sigma = 0.05\n",
    "\n",
    "# ========================= CHANGE-POINTS (CUSUM) =============================\n",
    "k_cusum = float(resid.std() or sigma) * 0.25\n",
    "thr     = float(resid.std() or sigma) * 3.0\n",
    "pos = neg = 0.0; alarms = []\n",
    "for t, e in (OAI - (OAI - resid.reindex_like(OAI).fillna(0))).fillna(0).items():\n",
    "    pos = max(0.0, pos + e - k_cusum); neg = min(0.0, neg + e + k_cusum)\n",
    "    if pos > thr or abs(neg) > thr: alarms.append(t); pos = neg = 0.0\n",
    "cp_dates = [pd.Timestamp(d) for d in alarms[-5:]]\n",
    "\n",
    "# =================== TIME-TO-EVENT (sustained τ for k weeks) =================\n",
    "def first_sustained(sim, tau, kreq):\n",
    "    r = 0\n",
    "    for i, a in enumerate(sim >= tau):\n",
    "        r = r+1 if a else 0\n",
    "        if r >= kreq: return i\n",
    "    return None\n",
    "\n",
    "n_sims = 2000\n",
    "paths  = np.clip(mean_path.values + np.random.normal(0, sigma, (n_sims, h)), 0, 1)\n",
    "hits   = [first_sustained(p, TAU, CFG[\"RUN_LENGTH_K\"]) for p in paths]\n",
    "hit_idxs = [x for x in hits if x is not None]\n",
    "if hit_idxs:\n",
    "    dates = pd.to_datetime([idxf[i] for i in hit_idxs]).sort_values()\n",
    "    med_date = dates.iloc[len(dates)//2]\n",
    "    pH = len(hit_idxs)/n_sims\n",
    "    d80 = (dates.iloc[int(0.10*len(dates))], dates.iloc[int(0.90*len(dates))])\n",
    "else:\n",
    "    med_date, pH, d80 = None, 0.0, (None, None)\n",
    "\n",
    "# ===================== OPTIONAL: SHIFT-EVENT DETECTOR ========================\n",
    "# z-score & slope must both exceed gates for k weeks\n",
    "def rolling_slope(series, w):\n",
    "    y = pd.Series(series).astype(float)\n",
    "    if len(y) < w: return pd.Series(index=y.index, dtype=float)\n",
    "    X = np.arange(w); out = [np.nan]*(w-1)\n",
    "    for i in range(w, len(y)+1):\n",
    "        yi = y.iloc[i-w:i].values; m = np.polyfit(X, yi, 1)[0]; out.append(m)\n",
    "    return pd.Series(out, index=y.index, dtype=float)\n",
    "\n",
    "ZW = int(CFG[\"SHIFT_Z_WINDOW\"]); ZT = float(CFG[\"SHIFT_Z_TAU\"]); MMIN = float(CFG[\"SHIFT_MIN_SLOPE\"])\n",
    "oai = pd.Series(OAI, index=OAI.index).astype(float)\n",
    "mu  = oai.rolling(ZW, min_periods=max(8, ZW//3)).mean()\n",
    "sd  = oai.rolling(ZW, min_periods=max(8, ZW//3)).std()\n",
    "zsc = (oai - mu) / (sd.replace(0, np.nan) + 1e-9)\n",
    "slo = rolling_slope(oai, ZW)\n",
    "cond = (zsc >= ZT) & (slo >= MMIN)\n",
    "run  = (cond.groupby((~cond).cumsum()).cumcount()+1)*cond\n",
    "shift_triggered = bool((run >= CFG[\"RUN_LENGTH_K\"]).any())\n",
    "\n",
    "# =============================== PLOTS =======================================\n",
    "outdir = Path(CFG[\"SAVE_DIR\"])\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(OAI.index, OAI, label=\"OAI\")\n",
    "if model_used == \"UCM\":\n",
    "    try:\n",
    "        plt.plot((OAI - resid).clip(0,1).index, (OAI - resid).clip(0,1).values, \"--\", label=\"Fit\")\n",
    "    except Exception:\n",
    "        pass\n",
    "plt.axhline(TAU, linestyle=\":\", label=f\"τ={TAU:.3f}\")\n",
    "for d in cp_dates: plt.axvline(d, linestyle=\":\", alpha=0.5)\n",
    "plt.legend(); plt.title(f\"OAI — Model: {model_used} | Shift-event: {shift_triggered}\")\n",
    "plt.tight_layout(); plt.savefig(outdir/\"oai_fit.png\", dpi=160); plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(mean_path.index, mean_path.values, label=\"Forecast (momentum-aware)\")\n",
    "plt.axhline(TAU, linestyle=\":\", label=f\"τ={TAU:.3f}\")\n",
    "plt.legend(); plt.title(\"OAI Forecast Horizon\")\n",
    "plt.tight_layout(); plt.savefig(outdir/\"oai_fc.png\", dpi=160); plt.close()\n",
    "\n",
    "prob_curve = [np.mean([(x is not None and x <= t) for x in hits]) for t in range(h)]\n",
    "pd.Series(prob_curve, index=idxf).plot(figsize=(10,3), ylim=(0,1), title=\"Pr(sustained crossing by week t)\")\n",
    "plt.tight_layout(); plt.savefig(outdir/\"oai_prob.png\", dpi=160); plt.close()\n",
    "\n",
    "# ============================== SUMMARY ======================================\n",
    "summary = dict(\n",
    "    generated_at = datetime.now(timezone.utc).isoformat(),\n",
    "    horizon_weeks = h,\n",
    "    threshold_tau = float(TAU),\n",
    "    run_length_k  = int(CFG[\"RUN_LENGTH_K\"]),\n",
    "    last_week     = str(pd.Timestamp(OAI.index[-1]).date()),\n",
    "    change_points = [str(pd.Timestamp(d).date()) for d in cp_dates],\n",
    "    prob_within_horizon = round(float(pH), 3),\n",
    "    median_event_date   = (str(pd.Timestamp(med_date).date()) if med_date is not None else None),\n",
    "    event_window_80     = tuple(str(pd.Timestamp(d).date()) if d is not None else None for d in d80),\n",
    "    shift_event_triggered = bool(shift_triggered),\n",
    "    weights = {k: float(v) for k, v in w.to_dict().items()},\n",
    "    pca_var = float(pca.explained_variance_ratio_[0]),\n",
    "    sources = {\n",
    "        \"trends_cols\": list(trends.columns) if isinstance(trends, pd.DataFrame) and not trends.empty else [],\n",
    "        \"wiki_cols\":   list(wiki.columns)   if isinstance(wiki,   pd.DataFrame) and not wiki.empty   else [],\n",
    "        \"gdelt_cols\":  list(gdelt.columns)  if isinstance(gdelt,  pd.DataFrame) and not gdelt.empty  else [],\n",
    "        \"local_series\": [Path(p).name for p in glob.glob(str(CFG[\"LOCAL_DATA_DIR\"]/\"*.csv\"))],\n",
    "        \"model_used\":  model_used\n",
    "    }\n",
    ")\n",
    "with open(outdir/\"oai_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(json.dumps(summary, indent=2))\n",
    "print(\"\\nFigures saved to:\", str(outdir.resolve()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e8af0e0-a6ee-4df3-8be3-a0b3171c3dae",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'mad'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_32224\\3091281099.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      8\u001b[39m TAU = float(pd.Series(OAI).quantile(\u001b[32m0.80\u001b[39m))  \u001b[38;5;66;03m# calibrated to history (80th pct)\u001b[39;00m\n\u001b[32m      9\u001b[39m \n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Robust noise (MAD→σ)\u001b[39;00m\n\u001b[32m     11\u001b[39m OAI_fit = pd.Series(OAI).ewm(span=int(CFG.get(\u001b[33m\"EWMA_SPAN\"\u001b[39m, \u001b[32m8\u001b[39m)), adjust=\u001b[38;5;28;01mFalse\u001b[39;00m).mean()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m sigma = float((pd.Series(OAI) - OAI_fit).mad() * \u001b[32m1.4826\u001b[39m) \u001b[38;5;28;01mor\u001b[39;00m \u001b[32m0.05\u001b[39m\n\u001b[32m     13\u001b[39m \n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Re-sim sustained crossing with momentum-aware mean_path from the mega cell\u001b[39;00m\n\u001b[32m     15\u001b[39m h = int(CFG.get(\u001b[33m\"HORIZON_WEEKS\"\u001b[39m, \u001b[32m104\u001b[39m))\n",
      "\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   6317\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m name \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01min\u001b[39;00m self._accessors\n\u001b[32m   6318\u001b[39m             \u001b[38;5;28;01mand\u001b[39;00m self._info_axis._can_hold_identifiers_and_holds_name(name)\n\u001b[32m   6319\u001b[39m         ):\n\u001b[32m   6320\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m self[name]\n\u001b[32m-> \u001b[39m\u001b[32m6321\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m object.__getattribute__(self, name)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Series' object has no attribute 'mad'"
     ]
    }
   ],
   "source": [
    "# === Gate Tuner: k=4 + τ@80th + re-sim (no refetch) =========================\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "assert \"OAI\" in globals() and \"OAI_fc\" in globals() and \"mean_path\" in globals(), \"Run mega cell first.\"\n",
    "\n",
    "# Soften but stay honest\n",
    "CFG[\"RUN_LENGTH_K\"] = 4\n",
    "TAU = float(pd.Series(OAI).quantile(0.80))  # calibrated to history (80th pct)\n",
    "\n",
    "# Robust noise (MAD→σ)\n",
    "OAI_fit = pd.Series(OAI).ewm(span=int(CFG.get(\"EWMA_SPAN\", 8)), adjust=False).mean()\n",
    "sigma = float((pd.Series(OAI) - OAI_fit).mad() * 1.4826) or 0.05\n",
    "\n",
    "# Re-sim sustained crossing with momentum-aware mean_path from the mega cell\n",
    "h = int(CFG.get(\"HORIZON_WEEKS\", 104))\n",
    "idxf = mean_path.index\n",
    "\n",
    "def _first_sustained(sim, tau, kreq):\n",
    "    r = 0\n",
    "    for i, a in enumerate(sim >= tau):\n",
    "        r = r + 1 if a else 0\n",
    "        if r >= kreq: return i\n",
    "    return None\n",
    "\n",
    "n_sims = 3000\n",
    "paths = np.clip(mean_path.values + np.random.normal(0, sigma, (n_sims, h)), 0, 1)\n",
    "hits = [_first_sustained(p, TAU, CFG[\"RUN_LENGTH_K\"]) for p in paths]\n",
    "pH = np.mean([h is not None for h in hits])\n",
    "\n",
    "med = None; w80 = (None, None)\n",
    "if any(x is not None for x in hits):\n",
    "    dates = pd.to_datetime([idxf[i] for i in hits if i is not None]).sort_values()\n",
    "    med = dates.iloc[len(dates)//2]\n",
    "    w80 = (dates.iloc[int(0.10*len(dates))], dates.iloc[int(0.90*len(dates))])\n",
    "\n",
    "print(f\"[Gate Tuner] τ@80th={TAU:.3f}, k={CFG['RUN_LENGTH_K']}, σ≈{sigma:.3f}\")\n",
    "print(f\"[Gate Tuner] Pr(sustain ≥ τ in {h}w) = {pH:.3f}\")\n",
    "print(f\"[Gate Tuner] median={str(med.date()) if med is not None else None}, 80%={tuple(str(d.date()) if d is not None else None for d in w80)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15020787-baa5-4f96-8f9b-827a8cbc6d22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
