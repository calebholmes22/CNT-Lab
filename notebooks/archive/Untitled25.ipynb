{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba694375-d44d-46b9-bcbd-3f99708e8b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-29 01:34:36] 3I Atlas Check-In starting…\n",
      "  Run dir: E:\\CNT\\notebooks\\archive\\cnt_runs\\3i_atlas_checkin\\20251029-053436Z\n",
      "  Searching under: C:\\Users\\caleb\\CNT_Lab\n",
      "  Pack: C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a_vector_embedding_vector_embedding\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "No CSVs found under C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a_vector_embedding_vector_embedding.",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m No CSVs found under C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a_vector_embedding_vector_embedding.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\CNT_Lab\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# === CNT \"3I Atlas\" — Mega Check-In (single cell) ============================\n",
    "# Purpose: Reconnect with your 3I Atlas pack, recompute quick health stats,\n",
    "#          produce deltas vs last check-in, and emit a compact report bundle.\n",
    "# Usage:   Paste this cell into JupyterLab and run. Edit PACK_DIR or ROOT_HINTS\n",
    "#          if auto-discovery fails.\n",
    "#\n",
    "# Outputs (under CNT_Lab\\notebooks\\archive\\cnt_runs\\3i_atlas_checkin\\<STAMP>\\):\n",
    "#   - report.md (+ lightweight PDF if fpdf available)\n",
    "#   - summary_stats.csv, top_gini_genes.csv, top_entropy_genes.csv\n",
    "#   - delta_summary.json (if previous snapshot found)\n",
    "#   - snapshot.json (for the next run)\n",
    "#   - plots: gini_hist.png, entropy_hist.png, top_gini_bar.png, pca_scatter.png\n",
    "#\n",
    "# Notes:\n",
    "#  - Tries scikit-learn (PCA) & umap-learn if installed; gracefully degrades if not.\n",
    "#  - Accepts both wide (genes x samples) and long (tidy) formats; does best-effort inference.\n",
    "#  - No internet use. Safe to run offline.\n",
    "# ============================================================================\n",
    "\n",
    "import os, re, sys, json, glob, math, time, uuid, platform, textwrap\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Prefer pandas; fall back to polars if desired by setting USE_POLARS=True\n",
    "USE_POLARS = False\n",
    "try:\n",
    "    import pandas as pd\n",
    "except Exception as e:\n",
    "    pd = None\n",
    "\n",
    "if USE_POLARS:\n",
    "    try:\n",
    "        import polars as pl\n",
    "    except Exception:\n",
    "        USE_POLARS = False\n",
    "\n",
    "# Optional libs\n",
    "try:\n",
    "    from sklearn.decomposition import PCA\n",
    "except Exception:\n",
    "    PCA = None\n",
    "\n",
    "try:\n",
    "    import umap\n",
    "except Exception:\n",
    "    umap = None\n",
    "\n",
    "# Optional PDF\n",
    "try:\n",
    "    from fpdf import FPDF\n",
    "except Exception:\n",
    "    FPDF = None\n",
    "\n",
    "\n",
    "# ----------------------------- Helpers --------------------------------------\n",
    "\n",
    "def ts_utc():\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y%m%d-%H%M%SZ\")\n",
    "\n",
    "def ts_local():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def first_existing(paths):\n",
    "    for p in paths:\n",
    "        if Path(p).exists():\n",
    "            return Path(p)\n",
    "    return None\n",
    "\n",
    "def scan_for_pack(root: Path):\n",
    "    \"\"\"\n",
    "    Heuristic pack discovery: look for dirs/files with '3i' and 'atlas' in name;\n",
    "    prefer vector_embedding packs.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    patterns = [\n",
    "        \"**/*3i*atlas*vector*embed*\",\n",
    "        \"**/*3i*atlas*embed*\",\n",
    "        \"**/*3i*atlas*\",\n",
    "        \"**/cnt_3i_atlas*\",\n",
    "        \"**/*3i*atlas*.csv\",\n",
    "    ]\n",
    "    for pat in patterns:\n",
    "        for hit in root.glob(pat):\n",
    "            if \".ipynb_checkpoints\" in str(hit):\n",
    "                continue\n",
    "            candidates.append(hit)\n",
    "\n",
    "    # Rank: prefer directories named like '...vector_embedding'\n",
    "    def score(p: Path):\n",
    "        s = str(p).lower()\n",
    "        sc = 0\n",
    "        if p.is_dir(): sc += 3\n",
    "        if \"vector\" in s and \"embed\" in s: sc += 5\n",
    "        if \"cnt_3i_atlas_all\" in s: sc += 3\n",
    "        if s.endswith(\".csv\"): sc -= 1  # file, not pack dir\n",
    "        # more recent mtime gets a bump\n",
    "        try:\n",
    "            sc += int(p.stat().st_mtime // 3600) % 10  # coarse bump\n",
    "        except Exception:\n",
    "            pass\n",
    "        return sc\n",
    "\n",
    "    candidates.sort(key=score, reverse=True)\n",
    "    for c in candidates:\n",
    "        # Prefer directories with 'out' or 'data' children\n",
    "        if c.is_dir():\n",
    "            return c\n",
    "        # else if CSV, return parent dir\n",
    "        if c.is_file() and c.suffix.lower()==\".csv\":\n",
    "            return c.parent\n",
    "    return None\n",
    "\n",
    "def list_csvs(pack: Path):\n",
    "    pats = [\n",
    "        \"out/*.csv\",\n",
    "        \"out/**/**/*.csv\",\n",
    "        \"data/*.csv\",\n",
    "        \"data/**/**/*.csv\",\n",
    "        \"*.csv\",\n",
    "    ]\n",
    "    hits = []\n",
    "    for pat in pats:\n",
    "        hits += list(pack.glob(pat))\n",
    "    # de-dup, keep only files\n",
    "    hits = [h for h in hits if h.is_file() and h.suffix.lower()==\".csv\"]\n",
    "    # prefer larger files first\n",
    "    hits.sort(key=lambda p: p.stat().st_size if p.exists() else 0, reverse=True)\n",
    "    return hits\n",
    "\n",
    "def read_table_any(path: Path, max_rows=None):\n",
    "    if USE_POLARS:\n",
    "        df = pl.read_csv(str(path))\n",
    "        if max_rows is not None:\n",
    "            df = df.head(max_rows)\n",
    "        return df\n",
    "    else:\n",
    "        assert pd is not None, \"pandas not available; install pandas or set USE_POLARS=True\"\n",
    "        try:\n",
    "            return pd.read_csv(path, nrows=max_rows)\n",
    "        except Exception:\n",
    "            try:\n",
    "                return pd.read_csv(path, sep=\"\\t\", nrows=max_rows)\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to read {path}: {e}\")\n",
    "\n",
    "def to_pandas(df):\n",
    "    if pd is None:\n",
    "        raise RuntimeError(\"pandas not available\")\n",
    "    if USE_POLARS:\n",
    "        return df.to_pandas()\n",
    "    return df\n",
    "\n",
    "def gini_coefficient(x: np.ndarray, eps=1e-12):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.size == 0:\n",
    "        return np.nan\n",
    "    # shift to non-negative\n",
    "    mn = np.nanmin(x)\n",
    "    if mn < 0:\n",
    "        x = x - mn\n",
    "    x = np.nan_to_num(x, nan=0.0)\n",
    "    mu = x.mean() + eps\n",
    "    # mean absolute difference formulation\n",
    "    diff_sum = np.abs(x[:, None] - x[None, :]).mean()\n",
    "    return 0.5 * diff_sum / mu\n",
    "\n",
    "def shannon_entropy(p: np.ndarray, eps=1e-12):\n",
    "    p = np.clip(p, eps, None)\n",
    "    p = p / p.sum()\n",
    "    return float(-(p * np.log(p)).sum())\n",
    "\n",
    "def infer_matrix(df: 'pd.DataFrame'):\n",
    "    \"\"\"\n",
    "    Try to infer a (genes x samples) numeric matrix and a 'gene' name/index.\n",
    "    Accepts either:\n",
    "      - wide form: first col gene identifier, other cols samples\n",
    "      - tidy form: columns include ['gene','tissue','value'] or similar\n",
    "    Returns: (E, gene_names, sample_names, meta) where\n",
    "             E is (n_genes, n_samples) numpy array\n",
    "    \"\"\"\n",
    "    meta = {\"format\": None, \"value_col\": None, \"gene_col\": None, \"tissue_col\": None}\n",
    "    cols = [c.lower() for c in df.columns]\n",
    "\n",
    "    # candidate name columns\n",
    "    gene_cols = [c for c in df.columns if c.lower() in (\"gene\",\"gene_id\",\"gene_name\",\"symbol\",\"ensembl\",\"ensembl_id\")]\n",
    "    tissue_cols = [c for c in df.columns if c.lower() in (\"tissue\",\"organ\",\"celltype\",\"cell_type\",\"sample\",\"sample_id\")]\n",
    "\n",
    "    # Tidy form?\n",
    "    # look for a 'value' column\n",
    "    value_cols = [c for c in df.columns if c.lower() in (\"value\",\"expression\",\"expr\",\"count\",\"tpms\",\"fpkm\",\"reads\")]\n",
    "    if gene_cols and tissue_cols and value_cols:\n",
    "        meta.update({\"format\":\"long/tidy\",\"gene_col\":gene_cols[0],\"tissue_col\":tissue_cols[0],\"value_col\":value_cols[0]})\n",
    "        g = meta[\"gene_col\"]; t = meta[\"tissue_col\"]; v = meta[\"value_col\"]\n",
    "        # pivot to wide\n",
    "        pivot = df.pivot_table(index=g, columns=t, values=v, aggfunc=\"mean\")\n",
    "        pivot = pivot.sort_index()\n",
    "        E = pivot.to_numpy(dtype=float)\n",
    "        gene_names = pivot.index.astype(str).to_list()\n",
    "        sample_names = [str(c) for c in pivot.columns.to_list()]\n",
    "        return E, gene_names, sample_names, meta\n",
    "\n",
    "    # Wide form: assume first non-numeric is gene id\n",
    "    if gene_cols:\n",
    "        g = gene_cols[0]\n",
    "        sub = df.copy()\n",
    "        sub = sub.drop_duplicates(subset=[g])\n",
    "        sub = sub.set_index(g)\n",
    "        # keep only numeric columns\n",
    "        num = sub.select_dtypes(include=[np.number])\n",
    "        # if none numeric, try to coerce\n",
    "        if num.shape[1]==0:\n",
    "            num = sub.apply(pd.to_numeric, errors=\"coerce\")\n",
    "        num = num.dropna(how=\"all\", axis=1)\n",
    "        E = num.to_numpy(dtype=float)\n",
    "        gene_names = [str(i) for i in num.index.to_list()]\n",
    "        sample_names = [str(c) for c in num.columns.to_list()]\n",
    "        meta.update({\"format\":\"wide\",\"gene_col\":g})\n",
    "        return E, gene_names, sample_names, meta\n",
    "\n",
    "    # Fallback: assume first column is gene id, rest numeric\n",
    "    sub = df.copy()\n",
    "    sub = sub.dropna(how=\"all\", axis=1)\n",
    "    if sub.shape[1] < 2:\n",
    "        raise RuntimeError(\"Table has <2 columns; can't infer matrix.\")\n",
    "    g = sub.columns[0]\n",
    "    sub = sub.drop_duplicates(subset=[g])\n",
    "    sub = sub.set_index(g)\n",
    "    num = sub.select_dtypes(include=[np.number])\n",
    "    if num.shape[1]==0:\n",
    "        num = sub.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    num = num.dropna(how=\"all\", axis=1)\n",
    "    E = num.to_numpy(dtype=float)\n",
    "    gene_names = [str(i) for i in num.index.to_list()]\n",
    "    sample_names = [str(c) for c in num.columns.to_list()]\n",
    "    meta.update({\"format\":\"wide/fallback\",\"gene_col\":str(g)})\n",
    "    return E, gene_names, sample_names, meta\n",
    "\n",
    "def summarize_matrix(E: np.ndarray, gene_names, sample_names, k_top=25):\n",
    "    n_genes, n_samp = E.shape\n",
    "    # zero-floor for stats\n",
    "    X = E.copy()\n",
    "    if np.nanmin(X) < 0:\n",
    "        X = X - np.nanmin(X)\n",
    "    X = np.nan_to_num(X, nan=0.0)\n",
    "    # per-gene\n",
    "    var = np.nanvar(X, axis=1)\n",
    "    mean = np.nanmean(X, axis=1) + 1e-12\n",
    "    cv = np.sqrt(var) / mean\n",
    "    # gini and entropy\n",
    "    gini = np.array([gini_coefficient(row) for row in X])\n",
    "    H = np.array([shannon_entropy(row) for row in X])\n",
    "    H_norm = H / (np.log(X.shape[1]) if X.shape[1] > 1 else 1.0)  # 0..1\n",
    "\n",
    "    # top lists\n",
    "    idx_gini = np.argsort(-gini)[:k_top]\n",
    "    idx_entropy_low = np.argsort(H_norm)[:k_top]    # \"specialized\"\n",
    "    idx_entropy_high = np.argsort(-H_norm)[:k_top]  # \"ubiquitous\"\n",
    "\n",
    "    def take(idx):\n",
    "        return [(gene_names[i], float(gini[i]), float(H_norm[i]), float(cv[i]), float(mean[i])) for i in idx]\n",
    "\n",
    "    top_gini = take(idx_gini)\n",
    "    top_spec = take(idx_entropy_low)\n",
    "    top_house = take(idx_entropy_high)\n",
    "\n",
    "    summary = {\n",
    "        \"n_genes\": int(n_genes),\n",
    "        \"n_samples\": int(n_samp),\n",
    "        \"gini_mean\": float(np.nanmean(gini)),\n",
    "        \"gini_median\": float(np.nanmedian(gini)),\n",
    "        \"entropy_mean\": float(np.nanmean(H_norm)),\n",
    "        \"entropy_median\": float(np.nanmedian(H_norm)),\n",
    "        \"cv_mean\": float(np.nanmean(cv)),\n",
    "    }\n",
    "    per_gene = {\n",
    "        \"var\": var.tolist(),\n",
    "        \"mean\": mean.tolist(),\n",
    "        \"cv\": cv.tolist(),\n",
    "        \"gini\": gini.tolist(),\n",
    "        \"H_norm\": H_norm.tolist(),\n",
    "    }\n",
    "    tops = {\n",
    "        \"top_gini\": top_gini,\n",
    "        \"top_specialized_low_entropy\": top_spec,\n",
    "        \"top_housekeeping_high_entropy\": top_house,\n",
    "    }\n",
    "    return summary, per_gene, tops\n",
    "\n",
    "def to_csv(path: Path, rows, header):\n",
    "    ensure_dir(path.parent)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\",\".join(header) + \"\\n\")\n",
    "        for r in rows:\n",
    "            f.write(\",\".join(map(lambda x: str(x).replace(\",\",\";\"), r)) + \"\\n\")\n",
    "\n",
    "def try_pca(E: np.ndarray, n=2, random_state=42):\n",
    "    if PCA is None:\n",
    "        return None, None\n",
    "    X = np.nan_to_num(E, nan=0.0)\n",
    "    # center genes across samples\n",
    "    X = X - X.mean(axis=1, keepdims=True)\n",
    "    # compute PCA on sample axis: transpose to samples x genes\n",
    "    pca = PCA(n_components=min(n, min(X.shape)-1), random_state=random_state)\n",
    "    try:\n",
    "        Y = pca.fit_transform(X.T)  # (n_samples, n)\n",
    "        return Y, pca.explained_variance_ratio_.tolist()\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "def try_umap(E: np.ndarray, n=2, random_state=42):\n",
    "    if umap is None:\n",
    "        return None\n",
    "    X = np.nan_to_num(E, nan=0.0)\n",
    "    X = X - X.mean(axis=1, keepdims=True)\n",
    "    try:\n",
    "        Y = umap.UMAP(n_components=n, random_state=random_state).fit_transform(X.T)\n",
    "        return Y\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def plot_hist(arr, path: Path, title, xlabel):\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    ensure_dir(path.parent)\n",
    "    plt.figure()\n",
    "    plt.hist([a for a in arr if not np.isnan(a)], bins=50)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel); plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def plot_bar(items, path: Path, title, ylabel):\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    ensure_dir(path.parent)\n",
    "    labels = [i[0] for i in items]\n",
    "    vals = [i[1] for i in items]\n",
    "    plt.figure(figsize=(10, max(3, 0.3*len(items))))\n",
    "    y = np.arange(len(items))\n",
    "    plt.barh(y, vals)\n",
    "    plt.yticks(y, labels)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(ylabel); plt.ylabel(\"Gene\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_scatter(Y, path: Path, title, xlabel=\"Dim 1\", ylabel=\"Dim 2\"):\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    ensure_dir(path.parent)\n",
    "    plt.figure()\n",
    "    plt.scatter(Y[:,0], Y[:,1], s=12, alpha=0.8)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel); plt.ylabel(ylabel)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def write_pdf(report_md_path: Path, images, out_pdf: Path, title=\"3I Atlas Check-In\"):\n",
    "    if FPDF is None:\n",
    "        return False\n",
    "    pdf = FPDF(orientation=\"P\", unit=\"mm\", format=\"A4\")\n",
    "    pdf.set_auto_page_break(auto=True, margin=12)\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", \"B\", 16)\n",
    "    pdf.cell(0, 10, title, ln=1)\n",
    "    pdf.set_font(\"Arial\", \"\", 10)\n",
    "    with open(report_md_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip().startswith(\"!\"):  # skip md image lines here\n",
    "                continue\n",
    "            pdf.multi_cell(0, 5, line.rstrip())\n",
    "    for img in images:\n",
    "        if Path(img).exists():\n",
    "            pdf.add_page()\n",
    "            pdf.image(str(img), x=10, y=20, w=180)  # scale to page width\n",
    "            pdf.ln(5)\n",
    "            pdf.set_font(\"Arial\", \"I\", 9)\n",
    "            pdf.cell(0, 6, str(Path(img).name), ln=1, align=\"C\")\n",
    "    ensure_dir(out_pdf.parent)\n",
    "    pdf.output(str(out_pdf))\n",
    "    return True\n",
    "\n",
    "def read_json(path: Path):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def write_json(path: Path, obj):\n",
    "    ensure_dir(path.parent)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def last_snapshot(dir_base: Path):\n",
    "    # Snapshots are under dir_base/*/snapshot.json\n",
    "    pattern = str(dir_base / \"*\" / \"snapshot.json\")\n",
    "    files = glob.glob(pattern)\n",
    "    if not files:\n",
    "        return None, None\n",
    "    files.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "    path = Path(files[0])\n",
    "    try:\n",
    "        return path, read_json(path)\n",
    "    except Exception:\n",
    "        return path, None\n",
    "\n",
    "def write_report_md(path: Path, info):\n",
    "    \"\"\"\n",
    "    info: dict with keys:\n",
    "      - meta: dict\n",
    "      - summary: dict\n",
    "      - tops: dict\n",
    "      - deltas: dict or None\n",
    "      - plots: dict of image paths\n",
    "    \"\"\"\n",
    "    ensure_dir(path.parent)\n",
    "    import textwrap\n",
    "    wrap = lambda s: textwrap.fill(s, width=100)\n",
    "    lines = []\n",
    "    lines.append(f\"# 3I Atlas Check-In — {info['meta']['stamp_local']}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"- **Pack**: `{info['meta']['pack']}`\")\n",
    "    lines.append(f\"- **Run dir**: `{info['meta']['run_dir']}`\")\n",
    "    lines.append(f\"- **Rows (genes)**: **{info['summary']['n_genes']}**, **Samples**: **{info['summary']['n_samples']}**\")\n",
    "    lines.append(f\"- Gini (mean/median): **{info['summary']['gini_mean']:.4f} / {info['summary']['gini_median']:.4f}**\")\n",
    "    lines.append(f\"- Entropyₙ (mean/median): **{info['summary']['entropy_mean']:.4f} / {info['summary']['entropy_median']:.4f}**\")\n",
    "    lines.append(f\"- CV (mean): **{info['summary']['cv_mean']:.4f}**\")\n",
    "    lines.append(\"\")\n",
    "    # Plots inline (as markdown)\n",
    "    if info[\"plots\"].get(\"gini_hist\"):\n",
    "        lines.append(f\"![Gini distribution]({Path(info['plots']['gini_hist']).name})\")\n",
    "    if info[\"plots\"].get(\"entropy_hist\"):\n",
    "        lines.append(f\"![Normalized entropy distribution]({Path(info['plots']['entropy_hist']).name})\")\n",
    "    if info[\"plots\"].get(\"top_gini_bar\"):\n",
    "        lines.append(f\"![Top specialized genes (Gini)]({Path(info['plots']['top_gini_bar']).name})\")\n",
    "    if info[\"plots\"].get(\"pca_scatter\"):\n",
    "        lines.append(f\"![Sample PCA scatter]({Path(info['plots']['pca_scatter']).name})\")\n",
    "    lines.append(\"\")\n",
    "    # Top tables (brief)\n",
    "    tg = info[\"tops\"][\"top_gini\"][:10]\n",
    "    lines.append(\"## Top specialized (by Gini) — preview\")\n",
    "    for (name,g,h,cv,mu) in tg:\n",
    "        lines.append(f\"- {name}: Gini={g:.4f}, Hₙ={h:.4f}, CV={cv:.3f}, mean={mu:.3g}\")\n",
    "    lines.append(\"\")\n",
    "    th = info[\"tops\"][\"top_housekeeping_high_entropy\"][:10]\n",
    "    lines.append(\"## Top housekeeping (high normalized entropy) — preview\")\n",
    "    for (name,g,h,cv,mu) in th:\n",
    "        lines.append(f\"- {name}: Hₙ={h:.4f}, Gini={g:.4f}, CV={cv:.3f}, mean={mu:.3g}\")\n",
    "    lines.append(\"\")\n",
    "    # Deltas\n",
    "    if info.get(\"deltas\"):\n",
    "        d = info[\"deltas\"]\n",
    "        lines.append(\"## Delta vs last snapshot\")\n",
    "        lines.append(f\"- Genes: **{d.get('n_genes_delta',0):+d}**, Samples: **{d.get('n_samples_delta',0):+d}**\")\n",
    "        if \"gini_mean_delta\" in d:\n",
    "            lines.append(f\"- Δ Gini mean: **{d['gini_mean_delta']:+.4f}**, Δ Entropyₙ mean: **{d.get('entropy_mean_delta',0):+.4f}**\")\n",
    "        if d.get(\"changed_samples\"):\n",
    "            lines.append(f\"- Changed sample set: +{len(d['added_samples'])} / -{len(d['removed_samples'])}\")\n",
    "        lines.append(\"\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "\n",
    "# ----------------------------- Main -----------------------------------------\n",
    "\n",
    "# === Config ===\n",
    "PACK_DIR = None  # set to a specific folder if you want to bypass auto-discovery\n",
    "ROOT_HINTS = [\n",
    "    r\"C:\\Users\\caleb\\CNT_Lab\",\n",
    "    r\"E:\\CNT\",\n",
    "    r\"D:\\CNT\",\n",
    "    r\"C:\\CNT\",\n",
    "    str(Path.cwd()),\n",
    "]\n",
    "\n",
    "RUN_BASE = r\"C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_runs\\3i_atlas_checkin\"\n",
    "if not Path(RUN_BASE).exists():\n",
    "    # fallback inside current working dir if CNT_Lab not available on this machine\n",
    "    RUN_BASE = str(Path.cwd() / \"cnt_runs\" / \"3i_atlas_checkin\")\n",
    "\n",
    "STAMP = ts_utc()\n",
    "RUN_DIR = ensure_dir(Path(RUN_BASE) / STAMP)\n",
    "\n",
    "print(f\"[{ts_local()}] 3I Atlas Check-In starting…\")\n",
    "print(f\"  Run dir: {RUN_DIR}\")\n",
    "\n",
    "# Discover pack\n",
    "if PACK_DIR:\n",
    "    pack = Path(PACK_DIR)\n",
    "else:\n",
    "    root = first_existing(ROOT_HINTS)\n",
    "    if root is None:\n",
    "        raise SystemExit(\"No CNT root found. Set PACK_DIR or adjust ROOT_HINTS.\")\n",
    "    print(f\"  Searching under: {root}\")\n",
    "    pack = scan_for_pack(root)\n",
    "    if pack is None:\n",
    "        raise SystemExit(\"Could not find a 3I Atlas pack. Set PACK_DIR manually.\")\n",
    "\n",
    "print(f\"  Pack: {pack}\")\n",
    "\n",
    "# Load a representative CSV (or stitch multiple if tidy)\n",
    "csvs = list_csvs(pack)\n",
    "if not csvs:\n",
    "    raise SystemExit(f\"No CSVs found under {pack}.\")\n",
    "\n",
    "# Heuristic: prefer 'all' or 'atlas' CSVs first\n",
    "csvs.sort(key=lambda p: ((\"all\" in p.name.lower()) or (\"atlas\" in p.name.lower()), p.stat().st_size), reverse=True)\n",
    "\n",
    "# Try reading first, if tidy with (gene,tissue,value) we may be done;\n",
    "# otherwise, if multiple CSVs appear to be shards, we can try concat with outer join on columns.\n",
    "df = None\n",
    "errors = []\n",
    "for c in csvs[:6]:  # don't scan too many\n",
    "    try:\n",
    "        df_c = read_table_any(c, max_rows=None)\n",
    "        df = to_pandas(df_c)\n",
    "        if df.shape[1] >= 2 and df.shape[0] >= 10:\n",
    "            print(f\"  Using: {c.name}  (shape={df.shape})\")\n",
    "            break\n",
    "    except Exception as e:\n",
    "        errors.append((c, str(e)))\n",
    "if df is None:\n",
    "    raise SystemExit(f\"Failed to read candidate CSVs: {errors[:2]}\")\n",
    "\n",
    "# Infer matrix\n",
    "E, gene_names, sample_names, meta = infer_matrix(df)\n",
    "print(f\"  Inferred matrix: genes={len(gene_names)}, samples={len(sample_names)}  format={meta['format']}\")\n",
    "\n",
    "# Summarize\n",
    "summary, per_gene, tops = summarize_matrix(E, gene_names, sample_names, k_top=25)\n",
    "\n",
    "# Write top tables\n",
    "to_csv(Path(RUN_DIR/\"top_gini_genes.csv\"), tops[\"top_gini\"], [\"gene\",\"gini\",\"H_norm\",\"cv\",\"mean\"])\n",
    "to_csv(Path(RUN_DIR/\"top_specialized_low_entropy.csv\"), tops[\"top_specialized_low_entropy\"], [\"gene\",\"gini\",\"H_norm\",\"cv\",\"mean\"])\n",
    "to_csv(Path(RUN_DIR/\"top_housekeeping_high_entropy.csv\"), tops[\"top_housekeeping_high_entropy\"], [\"gene\",\"gini\",\"H_norm\",\"cv\",\"mean\"])\n",
    "\n",
    "# Summary CSV\n",
    "summary_rows = [[k, v] for k, v in summary.items()]\n",
    "to_csv(Path(RUN_DIR/\"summary_stats.csv\"), summary_rows, [\"metric\",\"value\"])\n",
    "\n",
    "# Plots\n",
    "plots = {}\n",
    "plot_hist(per_gene[\"gini\"], Path(RUN_DIR/\"plots/gini_hist.png\"), \"Gini distribution (gene specialization)\", \"Gini\")\n",
    "plots[\"gini_hist\"] = str(Path(RUN_DIR/\"plots/gini_hist.png\"))\n",
    "plot_hist(per_gene[\"H_norm\"], Path(RUN_DIR/\"plots/entropy_hist.png\"), \"Normalized entropy across samples\", \"H_norm\")\n",
    "plots[\"entropy_hist\"] = str(Path(RUN_DIR/\"plots/entropy_hist.png\"))\n",
    "plot_bar(tops[\"top_gini\"], Path(RUN_DIR/\"plots/top_gini_bar.png\"), \"Top specialized genes (by Gini)\", \"Gini\")\n",
    "plots[\"top_gini_bar\"] = str(Path(RUN_DIR/\"plots/top_gini_bar.png\"))\n",
    "\n",
    "# Embeddings (optional)\n",
    "pca_pts, pca_var = try_pca(E, n=2, random_state=42)\n",
    "if pca_pts is not None:\n",
    "    plot_scatter(pca_pts, Path(RUN_DIR/\"plots/pca_scatter.png\"),\n",
    "                 f\"PCA on samples (var={sum(pca_var):.2%})\", \"PC1\", \"PC2\")\n",
    "    plots[\"pca_scatter\"] = str(Path(RUN_DIR/\"plots/pca_scatter.png\"))\n",
    "else:\n",
    "    print(\"  PCA not available or failed; skipping PCA plot.\")\n",
    "\n",
    "umap_pts = try_umap(E, n=2, random_state=42)\n",
    "if umap_pts is not None:\n",
    "    plot_scatter(umap_pts, Path(RUN_DIR/\"plots/umap_scatter.png\"),\n",
    "                 \"UMAP on samples\", \"UMAP-1\", \"UMAP-2\")\n",
    "    plots[\"umap_scatter\"] = str(Path(RUN_DIR/\"plots/umap_scatter.png\"))\n",
    "\n",
    "# Snapshot & delta\n",
    "SNAPSHOT_PATH = Path(RUN_DIR/\"snapshot.json\")\n",
    "prev_path, prev = last_snapshot(Path(RUN_BASE))\n",
    "deltas = None\n",
    "if prev:\n",
    "    # Compare scalar stats and sample sets\n",
    "    deltas = {\n",
    "        \"n_genes_delta\": summary[\"n_genes\"] - int(prev.get(\"summary\",{}).get(\"n_genes\", 0)),\n",
    "        \"n_samples_delta\": summary[\"n_samples\"] - int(prev.get(\"summary\",{}).get(\"n_samples\", 0)),\n",
    "        \"gini_mean_delta\": summary[\"gini_mean\"] - float(prev.get(\"summary\",{}).get(\"gini_mean\", 0.0)),\n",
    "        \"entropy_mean_delta\": summary[\"entropy_mean\"] - float(prev.get(\"summary\",{}).get(\"entropy_mean\", 0.0)),\n",
    "        \"cv_mean_delta\": summary[\"cv_mean\"] - float(prev.get(\"summary\",{}).get(\"cv_mean\", 0.0)),\n",
    "        \"changed_samples\": False,\n",
    "        \"added_samples\": [],\n",
    "        \"removed_samples\": [],\n",
    "    }\n",
    "    try:\n",
    "        prev_samples = set(prev.get(\"sample_names\", []))\n",
    "        cur_samples = set(sample_names)\n",
    "        add = sorted(cur_samples - prev_samples)\n",
    "        rem = sorted(prev_samples - cur_samples)\n",
    "        if add or rem:\n",
    "            deltas[\"changed_samples\"] = True\n",
    "            deltas[\"added_samples\"] = add\n",
    "            deltas[\"removed_samples\"] = rem\n",
    "    except Exception:\n",
    "        pass\n",
    "    write_json(Path(RUN_DIR/\"delta_summary.json\"), deltas)\n",
    "    print(f\"  Δ written: {Path(RUN_DIR/'delta_summary.json')}\")\n",
    "else:\n",
    "    print(\"  No prior snapshot found; this will serve as the baseline.\")\n",
    "\n",
    "snapshot = {\n",
    "    \"meta\": {\n",
    "        \"stamp_utc\": STAMP,\n",
    "        \"stamp_local\": ts_local(),\n",
    "        \"host\": platform.node(),\n",
    "        \"python\": sys.version.split()[0],\n",
    "        \"pack_dir\": str(pack),\n",
    "        \"csv_used\": csvs[0].name if csvs else None,\n",
    "    },\n",
    "    \"summary\": summary,\n",
    "    \"sample_names\": sample_names[:5000],  # avoid huge JSON; trim if very large\n",
    "    \"top_gini\": tops[\"top_gini\"],\n",
    "    \"top_housekeeping_high_entropy\": tops[\"top_housekeeping_high_entropy\"],\n",
    "}\n",
    "write_json(SNAPSHOT_PATH, snapshot)\n",
    "\n",
    "# Report\n",
    "info = {\n",
    "    \"meta\": {\n",
    "        \"stamp_local\": ts_local(),\n",
    "        \"pack\": str(pack),\n",
    "        \"run_dir\": str(RUN_DIR),\n",
    "    },\n",
    "    \"summary\": summary,\n",
    "    \"tops\": tops,\n",
    "    \"deltas\": deltas,\n",
    "    \"plots\": plots,\n",
    "}\n",
    "REPORT_MD = Path(RUN_DIR/\"report.md\")\n",
    "write_report_md(REPORT_MD, info)\n",
    "print(f\"  Wrote: {REPORT_MD}\")\n",
    "\n",
    "# Try a lightweight PDF\n",
    "REPORT_PDF = Path(RUN_DIR/\"report.pdf\")\n",
    "ok_pdf = write_pdf(REPORT_MD, images=[plots.get(\"gini_hist\"), plots.get(\"entropy_hist\"),\n",
    "                                      plots.get(\"top_gini_bar\"), plots.get(\"pca_scatter\")],\n",
    "                   out_pdf=REPORT_PDF, title=\"3I Atlas Check-In\")\n",
    "if ok_pdf:\n",
    "    print(f\"  PDF:   {REPORT_PDF}\")\n",
    "else:\n",
    "    print(\"  PDF:   (skipped; fpdf missing)\")\n",
    "\n",
    "print(f\"[{ts_local()}] Done. — Keep the field humming.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966b90f2-1554-4119-bcd5-24ed3c3e4d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
