{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6b47b9d-2b8b-4bac-944e-c6a64e6baa72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-29 01:43:56] 3I Atlas Check‑In v2 starting…\n",
      "  Run dir: E:\\CNT\\notebooks\\archive\\cnt_runs\\3i_atlas_checkin\\20251029-054356Z\n",
      "  Scanning: C:\\Users\\caleb\\CNT_Lab\n",
      "  Scanning: E:\\CNT\n",
      "  Scanning: E:\\CNT\\notebooks\\archive\n",
      "  Candidate pack: C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\n",
      "  Using data file: C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\data\\noaa_mag_3d.csv (0.25 MiB)\n",
      "  Inferred matrix: genes=4300, samples=6  format=wide/fallback\n",
      "  No prior snapshot found; this will serve as the baseline.\n",
      "  Wrote: E:\\CNT\\notebooks\\archive\\cnt_runs\\3i_atlas_checkin\\20251029-054356Z\\report.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2236\\1993366039.py:619: DeprecationWarning: Substituting font arial by core font helvetica - This is deprecated since v2.7.8, and will soon be removed\n",
      "  pdf.set_font(\"Arial\", \"B\", 16)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2236\\1993366039.py:620: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=1 use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
      "  pdf.cell(0, 10, title, ln=1)\n"
     ]
    },
    {
     "ename": "FPDFUnicodeEncodingException",
     "evalue": "Character \"‑\" at index 14 in text is outside the range of characters supported by the font used: \"helveticaB\". Please consider using a Unicode font.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeEncodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\fpdf\\fpdf.py:4910\u001b[39m, in \u001b[36mFPDF.normalize_text\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m   4909\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4910\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcore_fonts_encoding\u001b[49m\u001b[43m)\u001b[49m.decode(\u001b[33m\"\u001b[39m\u001b[33mlatin-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4911\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeEncodeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[31mUnicodeEncodeError\u001b[39m: 'latin-1' codec can't encode character '\\u2011' in position 14: ordinal not in range(256)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mFPDFUnicodeEncodingException\u001b[39m              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 738\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;66;03m# Lightweight PDF\u001b[39;00m\n\u001b[32m    737\u001b[39m REPORT_PDF = Path(REPORT_DIR/\u001b[33m\"\u001b[39m\u001b[33mreport.pdf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m738\u001b[39m ok_pdf = \u001b[43mwrite_pdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mREPORT_MD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mplots\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgini_hist\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplots\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mentropy_hist\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    739\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mplots\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_gini_bar\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplots\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpca_scatter\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplots\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mumap_scatter\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    740\u001b[39m \u001b[43m                   \u001b[49m\u001b[43mout_pdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mREPORT_PDF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m3I Atlas Check‑In\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    741\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ok_pdf:\n\u001b[32m    742\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  PDF:   \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mREPORT_PDF\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 620\u001b[39m, in \u001b[36mwrite_pdf\u001b[39m\u001b[34m(report_md_path, images, out_pdf, title)\u001b[39m\n\u001b[32m    618\u001b[39m pdf.add_page()\n\u001b[32m    619\u001b[39m pdf.set_font(\u001b[33m\"\u001b[39m\u001b[33mArial\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mB\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m16\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m \u001b[43mpdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mln\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    621\u001b[39m pdf.set_font(\u001b[33m\"\u001b[39m\u001b[33mArial\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m10\u001b[39m)\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(report_md_path, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\fpdf\\fpdf.py:221\u001b[39m, in \u001b[36mcheck_page.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.page \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mdry_run\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33msplit_only\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m FPDFException(\u001b[33m\"\u001b[39m\u001b[33mNo page open, you need to call add_page() first\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\fpdf\\deprecation.py:32\u001b[39m, in \u001b[36msupport_deprecated_txt_arg.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     26\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m] = txt_value\n\u001b[32m     27\u001b[39m     warnings.warn(\n\u001b[32m     28\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mThe parameter \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtxt\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m has been renamed to \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m in 2.7.6\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     29\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m     30\u001b[39m         stacklevel=get_stack_level(),\n\u001b[32m     31\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\fpdf\\fpdf.py:3253\u001b[39m, in \u001b[36mFPDF.cell\u001b[39m\u001b[34m(self, w, h, text, border, ln, align, fill, link, center, markdown, new_x, new_y)\u001b[39m\n\u001b[32m   3244\u001b[39m     warnings.warn(\n\u001b[32m   3245\u001b[39m         (\n\u001b[32m   3246\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mThe parameter \u001b[39m\u001b[33m\"\u001b[39m\u001b[33mln\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m is deprecated since v2.5.2.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3250\u001b[39m         stacklevel=get_stack_level(),\n\u001b[32m   3251\u001b[39m     )\n\u001b[32m   3252\u001b[39m \u001b[38;5;66;03m# Font styles preloading must be performed before any call to FPDF.get_string_width:\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3253\u001b[39m text = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnormalize_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3254\u001b[39m styled_txt_frags = (\n\u001b[32m   3255\u001b[39m     \u001b[38;5;28mself\u001b[39m._preload_bidirectional_text(text, markdown)\n\u001b[32m   3256\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.text_shaping\n\u001b[32m   3257\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._preload_font_styles(text, markdown)\n\u001b[32m   3258\u001b[39m )\n\u001b[32m   3259\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._render_styled_text_line(\n\u001b[32m   3260\u001b[39m     TextLine(\n\u001b[32m   3261\u001b[39m         styled_txt_frags,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3276\u001b[39m     prevent_font_change=markdown,\n\u001b[32m   3277\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\fpdf\\fpdf.py:4912\u001b[39m, in \u001b[36mFPDF.normalize_text\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m   4910\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m text.encode(\u001b[38;5;28mself\u001b[39m.core_fonts_encoding).decode(\u001b[33m\"\u001b[39m\u001b[33mlatin-1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   4911\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeEncodeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m-> \u001b[39m\u001b[32m4912\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m FPDFUnicodeEncodingException(\n\u001b[32m   4913\u001b[39m             text_index=error.start,\n\u001b[32m   4914\u001b[39m             character=text[error.start],\n\u001b[32m   4915\u001b[39m             font_name=\u001b[38;5;28mself\u001b[39m.font_family + \u001b[38;5;28mself\u001b[39m.font_style,\n\u001b[32m   4916\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m\n\u001b[32m   4917\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "\u001b[31mFPDFUnicodeEncodingException\u001b[39m: Character \"‑\" at index 14 in text is outside the range of characters supported by the font used: \"helveticaB\". Please consider using a Unicode font."
     ]
    }
   ],
   "source": [
    "# === CNT \"3I Atlas\" — Mega Check‑In (single cell, v2 resilient) =============\n",
    "# Fixes:\n",
    "#  - Search ALL roots; don't stop at the first existing one.\n",
    "#  - Normalize duplicate suffix dirs (…\\vector_embedding\\vector_embedding).\n",
    "#  - Accept CSV/TSV/Parquet/Feather/NPZ/NPY, not just CSV.\n",
    "#  - If target dir has no tables, try its parent once.\n",
    "#  - Better candidate scoring: prefer dirs with actual data files.\n",
    "# ============================================================================\n",
    "\n",
    "import os, re, sys, json, glob, math, time, uuid, platform, textwrap\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Prefer pandas; fall back to polars by toggling USE_POLARS=True\n",
    "USE_POLARS = False\n",
    "try:\n",
    "    import pandas as pd\n",
    "except Exception as e:\n",
    "    pd = None\n",
    "\n",
    "if USE_POLARS:\n",
    "    try:\n",
    "        import polars as pl\n",
    "    except Exception:\n",
    "        USE_POLARS = False\n",
    "\n",
    "# Optional libs\n",
    "try:\n",
    "    from sklearn.decomposition import PCA\n",
    "except Exception:\n",
    "    PCA = None\n",
    "\n",
    "try:\n",
    "    import umap\n",
    "except Exception:\n",
    "    umap = None\n",
    "\n",
    "# Optional PDF\n",
    "try:\n",
    "    from fpdf import FPDF\n",
    "except Exception:\n",
    "    FPDF = None\n",
    "\n",
    "\n",
    "# ----------------------------- Helpers --------------------------------------\n",
    "\n",
    "def ts_utc():\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y%m%d-%H%M%SZ\")\n",
    "\n",
    "def ts_local():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def normalize_pack_dir(p: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Collapse duplicate 'vector_embedding' suffixes and strip trailing empty segments.\n",
    "    e.g., .../vector_embedding/vector_embedding -> .../vector_embedding\n",
    "    \"\"\"\n",
    "    parts = list(p.parts)\n",
    "    if len(parts) >= 2 and parts[-1].lower() == \"vector_embedding\" and parts[-2].lower() == \"vector_embedding\":\n",
    "        return Path(*parts[:-1])\n",
    "    # Sometimes the duplication appears in a single folder name as \"..._vector_embedding_vector_embedding\"\n",
    "    name = p.name.lower()\n",
    "    if name.endswith(\"_vector_embedding_vector_embedding\"):\n",
    "        return p.with_name(p.name[: -len(\"_vector_embedding\")])\n",
    "    return p\n",
    "\n",
    "def first_existing(paths):\n",
    "    return [Path(p) for p in paths if Path(p).exists()]\n",
    "\n",
    "def list_datafiles(root: Path):\n",
    "    \"\"\"\n",
    "    Return a list of candidate data files under root with supported suffixes.\n",
    "    We search typical subfolders: out/, data/, current dir.\n",
    "    \"\"\"\n",
    "    patterns = []\n",
    "    for base in (\"out\", \"data\", \"\"):\n",
    "        basep = (root / base) if base else root\n",
    "        patterns += [\n",
    "            str(basep / \"**/*.csv\"),\n",
    "            str(basep / \"**/*.tsv\"),\n",
    "            str(basep / \"**/*.parquet\"),\n",
    "            str(basep / \"**/*.feather\"),\n",
    "            str(basep / \"**/*.npz\"),\n",
    "            str(basep / \"**/*.npy\"),\n",
    "        ]\n",
    "    hits = []\n",
    "    for pat in patterns:\n",
    "        hits.extend([Path(p) for p in glob.glob(pat, recursive=True)])\n",
    "    # files only\n",
    "    hits = [h for h in hits if h.is_file()]\n",
    "    # prefer larger files first\n",
    "    hits.sort(key=lambda p: p.stat().st_size if p.exists() else 0, reverse=True)\n",
    "    return hits\n",
    "\n",
    "def read_table_any(path: Path, max_rows=None):\n",
    "    suff = path.suffix.lower()\n",
    "    if USE_POLARS:\n",
    "        if suff in (\".csv\", \".tsv\"):\n",
    "            sep = \",\" if suff == \".csv\" else \"\\t\"\n",
    "            df = pl.read_csv(str(path), separator=sep)\n",
    "            return df if max_rows is None else df.head(max_rows)\n",
    "        elif suff == \".parquet\":\n",
    "            df = pl.read_parquet(str(path))\n",
    "            return df if max_rows is None else df.head(max_rows)\n",
    "        elif suff == \".feather\":\n",
    "            df = pl.read_ipc(str(path))\n",
    "            return df if max_rows is None else df.head(max_rows)\n",
    "        elif suff in (\".npz\", \".npy\"):\n",
    "            arr = np.load(str(path))\n",
    "            if isinstance(arr, np.lib.npyio.NpzFile):\n",
    "                # choose first array-like\n",
    "                key = next(iter(arr.files))\n",
    "                arr = arr[key]\n",
    "            if arr.ndim == 2:\n",
    "                # synthesize a DataFrame-like table with index + numbered columns\n",
    "                df = pl.DataFrame(arr)\n",
    "                df = df.with_columns(pl.Series(\"gene\", [f\"g{i}\" for i in range(arr.shape[0])]))\n",
    "                df = df.select([\"gene\"] + [c for c in df.columns if c != \"gene\"])\n",
    "                return df if max_rows is None else df.head(max_rows)\n",
    "            raise RuntimeError(f\"Unsupported NPZ/NPY shape in {path}: {arr.shape}\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unsupported file type: {suff}\")\n",
    "    else:\n",
    "        if pd is None:\n",
    "            raise RuntimeError(\"pandas not available; install pandas or set USE_POLARS=True\")\n",
    "        if suff in (\".csv\", \".tsv\"):\n",
    "            sep = \",\" if suff == \".csv\" else \"\\t\"\n",
    "            try:\n",
    "                return pd.read_csv(path, nrows=max_rows, sep=sep)\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to read {path}: {e}\")\n",
    "        elif suff == \".parquet\":\n",
    "            return pd.read_parquet(path)\n",
    "        elif suff == \".feather\":\n",
    "            return pd.read_feather(path)\n",
    "        elif suff in (\".npz\", \".npy\"):\n",
    "            arr = np.load(str(path))\n",
    "            if isinstance(arr, np.lib.npyio.NpzFile):\n",
    "                key = next(iter(arr.files))\n",
    "                arr = arr[key]\n",
    "            if arr.ndim == 2:\n",
    "                # build a DataFrame with 'gene' + col_*\n",
    "                cols = [f\"col_{j}\" for j in range(arr.shape[1])]\n",
    "                df = pd.DataFrame(arr, columns=cols)\n",
    "                df.insert(0, \"gene\", [f\"g{i}\" for i in range(arr.shape[0])])\n",
    "                return df if max_rows is None else df.head(max_rows)\n",
    "            raise RuntimeError(f\"Unsupported NPZ/NPY shape in {path}: {arr.shape}\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unsupported file type: {suff}\")\n",
    "\n",
    "def to_pandas(df):\n",
    "    if pd is None:\n",
    "        raise RuntimeError(\"pandas not available\")\n",
    "    if USE_POLARS:\n",
    "        return df.to_pandas()\n",
    "    return df\n",
    "\n",
    "def infer_matrix(df: 'pd.DataFrame'):\n",
    "    \"\"\"\n",
    "    Infer a (genes x samples) numeric matrix from common 3I Atlas shapes.\n",
    "    \"\"\"\n",
    "    meta = {\"format\": None, \"value_col\": None, \"gene_col\": None, \"tissue_col\": None}\n",
    "    cols = [str(c).lower() for c in df.columns]\n",
    "\n",
    "    # candidate id columns\n",
    "    gene_cols = [c for c in df.columns if str(c).lower() in (\"gene\",\"gene_id\",\"gene_name\",\"symbol\",\"ensembl\",\"ensembl_id\",\"id\")]\n",
    "    tissue_cols = [c for c in df.columns if str(c).lower() in (\"tissue\",\"organ\",\"celltype\",\"cell_type\",\"sample\",\"sample_id\")]\n",
    "\n",
    "    # likely value columns\n",
    "    val_keys = (\"value\",\"expression\",\"expr\",\"count\",\"tpms\",\"fpkm\",\"reads\",\"abundance\",\"intensity\")\n",
    "    value_cols = [c for c in df.columns if str(c).lower() in val_keys]\n",
    "\n",
    "    # Embedding-shaped (e.g., embedding_0, embedding_1, …)\n",
    "    emb_like = [c for c in df.columns if re.match(r\"(emb(ed(ding)?)?_?\\d+)$\", str(c).lower())]\n",
    "\n",
    "    # Tidy form?\n",
    "    if gene_cols and tissue_cols and (value_cols or emb_like):\n",
    "        g = gene_cols[0]; t = tissue_cols[0]\n",
    "        v = (value_cols[0] if value_cols else emb_like[0])\n",
    "        meta.update({\"format\":\"long/tidy\",\"gene_col\":g,\"tissue_col\":t,\"value_col\":v})\n",
    "        pivot = df.pivot_table(index=g, columns=t, values=v, aggfunc=\"mean\")\n",
    "        pivot = pivot.sort_index()\n",
    "        E = pivot.to_numpy(dtype=float)\n",
    "        gene_names = pivot.index.astype(str).to_list()\n",
    "        sample_names = [str(c) for c in pivot.columns.to_list()]\n",
    "        return E, gene_names, sample_names, meta\n",
    "\n",
    "    # Wide form with known gene column\n",
    "    if gene_cols:\n",
    "        g = gene_cols[0]\n",
    "        sub = df.copy().drop_duplicates(subset=[g]).set_index(g)\n",
    "        num = sub.select_dtypes(include=[np.number])\n",
    "        if num.shape[1]==0:\n",
    "            num = sub.apply(pd.to_numeric, errors=\"coerce\")\n",
    "        num = num.dropna(how=\"all\", axis=1)\n",
    "        E = num.to_numpy(dtype=float)\n",
    "        gene_names = [str(i) for i in num.index.to_list()]\n",
    "        sample_names = [str(c) for c in num.columns.to_list()]\n",
    "        meta.update({\"format\":\"wide\",\"gene_col\":g})\n",
    "        return E, gene_names, sample_names, meta\n",
    "\n",
    "    # Fallback: first column id, rest numeric\n",
    "    sub = df.copy().dropna(how=\"all\", axis=1)\n",
    "    if sub.shape[1] < 2:\n",
    "        raise RuntimeError(\"Table has <2 columns; can't infer matrix.\")\n",
    "    g = sub.columns[0]\n",
    "    sub = sub.drop_duplicates(subset=[g]).set_index(g)\n",
    "    num = sub.select_dtypes(include=[np.number])\n",
    "    if num.shape[1]==0:\n",
    "        num = sub.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    num = num.dropna(how=\"all\", axis=1)\n",
    "    E = num.to_numpy(dtype=float)\n",
    "    gene_names = [str(i) for i in num.index.to_list()]\n",
    "    sample_names = [str(c) for c in num.columns.to_list()]\n",
    "    meta.update({\"format\":\"wide/fallback\",\"gene_col\":str(g)})\n",
    "    return E, gene_names, sample_names, meta\n",
    "\n",
    "def summarize_matrix(E: np.ndarray, gene_names, sample_names, k_top=25):\n",
    "    n_genes, n_samp = E.shape\n",
    "    # zero-floor for stats\n",
    "    X = E.copy()\n",
    "    if np.nanmin(X) < 0:\n",
    "        X = X - np.nanmin(X)\n",
    "    X = np.nan_to_num(X, nan=0.0)\n",
    "    # per-gene\n",
    "    var = np.nanvar(X, axis=1)\n",
    "    mean = np.nanmean(X, axis=1) + 1e-12\n",
    "    cv = np.sqrt(var) / mean\n",
    "    # gini and entropy\n",
    "    def gini_coefficient(row, eps=1e-12):\n",
    "        r = np.asarray(row, dtype=float)\n",
    "        mn = np.nanmin(r)\n",
    "        if mn < 0:\n",
    "            r = r - mn\n",
    "        r = np.nan_to_num(r, nan=0.0)\n",
    "        mu = r.mean() + eps\n",
    "        diff_sum = np.abs(r[:, None] - r[None, :]).mean()\n",
    "        return 0.5 * diff_sum / mu\n",
    "    def shannon_entropy(p, eps=1e-12):\n",
    "        p = np.clip(p, eps, None)\n",
    "        p = p / p.sum()\n",
    "        return float(-(p * np.log(p)).sum())\n",
    "    gini = np.array([gini_coefficient(row) for row in X])\n",
    "    H = np.array([shannon_entropy(row) for row in X])\n",
    "    H_norm = H / (np.log(X.shape[1]) if X.shape[1] > 1 else 1.0)  # 0..1\n",
    "\n",
    "    idx_gini = np.argsort(-gini)[:k_top]\n",
    "    idx_entropy_low = np.argsort(H_norm)[:k_top]\n",
    "    idx_entropy_high = np.argsort(-H_norm)[:k_top]\n",
    "\n",
    "    def take(idx):\n",
    "        return [(gene_names[i], float(gini[i]), float(H_norm[i]), float(cv[i]), float(mean[i])) for i in idx]\n",
    "\n",
    "    top_gini = take(idx_gini)\n",
    "    top_spec = take(idx_entropy_low)\n",
    "    top_house = take(idx_entropy_high)\n",
    "\n",
    "    summary = {\n",
    "        \"n_genes\": int(n_genes),\n",
    "        \"n_samples\": int(n_samp),\n",
    "        \"gini_mean\": float(np.nanmean(gini)),\n",
    "        \"gini_median\": float(np.nanmedian(gini)),\n",
    "        \"entropy_mean\": float(np.nanmean(H_norm)),\n",
    "        \"entropy_median\": float(np.nanmedian(H_norm)),\n",
    "        \"cv_mean\": float(np.nanmean(cv)),\n",
    "    }\n",
    "    per_gene = {\n",
    "        \"var\": var.tolist(),\n",
    "        \"mean\": mean.tolist(),\n",
    "        \"cv\": cv.tolist(),\n",
    "        \"gini\": gini.tolist(),\n",
    "        \"H_norm\": H_norm.tolist(),\n",
    "    }\n",
    "    tops = {\n",
    "        \"top_gini\": top_gini,\n",
    "        \"top_specialized_low_entropy\": top_spec,\n",
    "        \"top_housekeeping_high_entropy\": top_house,\n",
    "    }\n",
    "    return summary, per_gene, tops\n",
    "\n",
    "def to_csv(path: Path, rows, header):\n",
    "    ensure_dir(path.parent)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\",\".join(header) + \"\\n\")\n",
    "        for r in rows:\n",
    "            f.write(\",\".join(map(lambda x: str(x).replace(\",\",\";\"), r)) + \"\\n\")\n",
    "\n",
    "def try_pca(E: np.ndarray, n=2, random_state=42):\n",
    "    if PCA is None:\n",
    "        return None, None\n",
    "    X = np.nan_to_num(E, nan=0.0)\n",
    "    X = X - X.mean(axis=1, keepdims=True)\n",
    "    pca = PCA(n_components=min(n, min(X.shape)-1), random_state=random_state)\n",
    "    try:\n",
    "        Y = pca.fit_transform(X.T)\n",
    "        return Y, pca.explained_variance_ratio_.tolist()\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "def try_umap(E: np.ndarray, n=2, random_state=42):\n",
    "    if umap is None:\n",
    "        return None\n",
    "    X = np.nan_to_num(E, nan=0.0)\n",
    "    X = X - X.mean(axis=1, keepdims=True)\n",
    "    try:\n",
    "        Y = umap.UMAP(n_components=n, random_state=random_state).fit_transform(X.T)\n",
    "        return Y\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def plot_hist(arr, path: Path, title, xlabel):\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    ensure_dir(path.parent)\n",
    "    plt.figure()\n",
    "    plt.hist([a for a in arr if not np.isnan(a)], bins=50)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel); plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def plot_bar(items, path: Path, title, ylabel):\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    ensure_dir(path.parent)\n",
    "    labels = [i[0] for i in items]\n",
    "    vals = [i[1] for i in items]\n",
    "    plt.figure(figsize=(10, max(3, 0.3*len(items))))\n",
    "    y = np.arange(len(items))\n",
    "    plt.barh(y, vals)\n",
    "    plt.yticks(y, labels)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(ylabel); plt.ylabel(\"Gene\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_scatter(Y, path: Path, title, xlabel=\"Dim 1\", ylabel=\"Dim 2\"):\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    ensure_dir(path.parent)\n",
    "    plt.figure()\n",
    "    plt.scatter(Y[:,0], Y[:,1], s=12, alpha=0.8)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel); plt.ylabel(ylabel)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def write_pdf(report_md_path: Path, images, out_pdf: Path, title=\"3I Atlas Check‑In\"):\n",
    "    if FPDF is None:\n",
    "        return False\n",
    "    pdf = FPDF(orientation=\"P\", unit=\"mm\", format=\"A4\")\n",
    "    pdf.set_auto_page_break(auto=True, margin=12)\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", \"B\", 16)\n",
    "    pdf.cell(0, 10, title, ln=1)\n",
    "    pdf.set_font(\"Arial\", \"\", 10)\n",
    "    with open(report_md_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip().startswith(\"!\"):\n",
    "                continue\n",
    "            pdf.multi_cell(0, 5, line.rstrip())\n",
    "    for img in images:\n",
    "        if img and Path(img).exists():\n",
    "            pdf.add_page()\n",
    "            pdf.image(str(img), x=10, y=20, w=180)\n",
    "            pdf.ln(5)\n",
    "            pdf.set_font(\"Arial\", \"I\", 9)\n",
    "            pdf.cell(0, 6, str(Path(img).name), ln=1, align=\"C\")\n",
    "    ensure_dir(out_pdf.parent)\n",
    "    pdf.output(str(out_pdf))\n",
    "    return True\n",
    "\n",
    "def read_json(path: Path):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def write_json(path: Path, obj):\n",
    "    ensure_dir(path.parent)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def last_snapshot(dir_base: Path):\n",
    "    files = glob.glob(str(dir_base / \"*\" / \"snapshot.json\"))\n",
    "    if not files:\n",
    "        return None, None\n",
    "    files.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "    path = Path(files[0])\n",
    "    try:\n",
    "        return path, read_json(path)\n",
    "    except Exception:\n",
    "        return path, None\n",
    "\n",
    "def write_report_md(path: Path, info):\n",
    "    ensure_dir(path.parent)\n",
    "    lines = []\n",
    "    lines.append(f\"# 3I Atlas Check‑In — {info['meta']['stamp_local']}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"- **Pack**: `{info['meta']['pack']}`\")\n",
    "    lines.append(f\"- **Run dir**: `{info['meta']['run_dir']}`\")\n",
    "    lines.append(f\"- **Rows (genes)**: **{info['summary']['n_genes']}**, **Samples**: **{info['summary']['n_samples']}**\")\n",
    "    lines.append(f\"- Gini (mean/median): **{info['summary']['gini_mean']:.4f} / {info['summary']['gini_median']:.4f}**\")\n",
    "    lines.append(f\"- Entropyₙ (mean/median): **{info['summary']['entropy_mean']:.4f} / {info['summary']['entropy_median']:.4f}**\")\n",
    "    lines.append(f\"- CV (mean): **{info['summary']['cv_mean']:.4f}**\")\n",
    "    lines.append(\"\")\n",
    "    for key in (\"gini_hist\",\"entropy_hist\",\"top_gini_bar\",\"pca_scatter\",\"umap_scatter\"):\n",
    "        p = info[\"plots\"].get(key)\n",
    "        if p:\n",
    "            lines.append(f\"![{key}]({Path(p).name})\")\n",
    "    lines.append(\"\")\n",
    "    tg = info[\"tops\"][\"top_gini\"][:10]\n",
    "    lines.append(\"## Top specialized (by Gini) — preview\")\n",
    "    for (name,g,h,cv,mu) in tg:\n",
    "        lines.append(f\"- {name}: Gini={g:.4f}, Hₙ={h:.4f}, CV={cv:.3f}, mean={mu:.3g}\")\n",
    "    lines.append(\"\")\n",
    "    th = info[\"tops\"][\"top_housekeeping_high_entropy\"][:10]\n",
    "    lines.append(\"## Top housekeeping (high normalized entropy) — preview\")\n",
    "    for (name,g,h,cv,mu) in th:\n",
    "        lines.append(f\"- {name}: Hₙ={h:.4f}, Gini={g:.4f}, CV={cv:.3f}, mean={mu:.3g}\")\n",
    "    lines.append(\"\")\n",
    "    if info.get(\"deltas\"):\n",
    "        d = info[\"deltas\"]\n",
    "        lines.append(\"## Delta vs last snapshot\")\n",
    "        lines.append(f\"- Genes: **{d.get('n_genes_delta',0):+d}**, Samples: **{d.get('n_samples_delta',0):+d}**\")\n",
    "        if \"gini_mean_delta\" in d:\n",
    "            lines.append(f\"- Δ Gini mean: **{d['gini_mean_delta']:+.4f}**, Δ Entropyₙ mean: **{d.get('entropy_mean_delta',0):+.4f}**\")\n",
    "        if d.get(\"changed_samples\"):\n",
    "            lines.append(f\"- Changed sample set: +{len(d['added_samples'])} / -{len(d['removed_samples'])}\")\n",
    "        lines.append(\"\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "\n",
    "# ----------------------------- Main -----------------------------------------\n",
    "\n",
    "# === Config ===\n",
    "PACK_DIR = None  # Optionally set this to the exact pack folder to skip discovery.\n",
    "ROOT_HINTS = [\n",
    "    r\"C:\\Users\\caleb\\CNT_Lab\",\n",
    "    r\"E:\\CNT\",\n",
    "    r\"D:\\CNT\",\n",
    "    r\"C:\\CNT\",\n",
    "    str(Path.cwd()),\n",
    "]\n",
    "\n",
    "RUN_BASE = r\"C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_runs\\3i_atlas_checkin\"\n",
    "if not Path(RUN_BASE).exists():\n",
    "    RUN_BASE = str(Path.cwd() / \"cnt_runs\" / \"3i_atlas_checkin\")\n",
    "\n",
    "STAMP = ts_utc()\n",
    "RUN_DIR = ensure_dir(Path(RUN_BASE) / STAMP)\n",
    "\n",
    "print(f\"[{ts_local()}] 3I Atlas Check‑In v2 starting…\")\n",
    "print(f\"  Run dir: {RUN_DIR}\")\n",
    "\n",
    "# ---- Discover pack across ALL roots\n",
    "candidates = []\n",
    "\n",
    "def score_candidate(path: Path) -> int:\n",
    "    s = str(path).lower()\n",
    "    sc = 0\n",
    "    if path.is_dir(): sc += 3\n",
    "    if \"vector\" in s and \"embed\" in s: sc += 5\n",
    "    if \"cnt_3i_atlas_all\" in s: sc += 3\n",
    "    if s.endswith(\".csv\") or s.endswith(\".tsv\") or s.endswith(\".parquet\") or s.endswith(\".feather\"): sc += 1\n",
    "    if \"vector_embedding_vector_embedding\" in s: sc -= 4  # penalize duplicate suffix\n",
    "    try:\n",
    "        dcount = len(list_datafiles(path)) if path.is_dir() else 1\n",
    "        sc += min(6, dcount)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        sc += int(path.stat().st_mtime // 3600) % 10\n",
    "    except Exception:\n",
    "        pass\n",
    "    return sc\n",
    "\n",
    "def gather_candidates(root: Path):\n",
    "    pats = [\n",
    "        \"**/*3i*atlas*vector*embed*\",\n",
    "        \"**/*3i*atlas*embed*\",\n",
    "        \"**/*3i*atlas*\",\n",
    "        \"**/cnt_3i_atlas*\",\n",
    "        \"**/*3i*atlas*.csv\",\n",
    "    ]\n",
    "    for pat in pats:\n",
    "        for hit in root.glob(pat):\n",
    "            if \".ipynb_checkpoints\" in str(hit):\n",
    "                continue\n",
    "            candidates.append(hit)\n",
    "\n",
    "roots = first_existing(ROOT_HINTS)\n",
    "if PACK_DIR:\n",
    "    pack = normalize_pack_dir(Path(PACK_DIR))\n",
    "    print(f\"  PACK_DIR override: {pack}\")\n",
    "else:\n",
    "    for r in roots:\n",
    "        print(f\"  Scanning: {r}\")\n",
    "        gather_candidates(r)\n",
    "    if not candidates:\n",
    "        raise SystemExit(\"No 3I Atlas candidates found under configured roots. Set PACK_DIR manually.\")\n",
    "    candidates = [normalize_pack_dir(c) for c in candidates]\n",
    "    uniq = []\n",
    "    seen = set()\n",
    "    for c in candidates:\n",
    "        key = str(c).lower()\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            uniq.append(c)\n",
    "    candidates = uniq\n",
    "    candidates.sort(key=score_candidate, reverse=True)\n",
    "    pack = candidates[0]\n",
    "\n",
    "print(f\"  Candidate pack: {pack}\")\n",
    "\n",
    "# ---- Find data files; if none, try parent once\n",
    "def choose_data_root(p: Path) -> Path:\n",
    "    files = list_datafiles(p)\n",
    "    if files:\n",
    "        return p, files\n",
    "    par = p.parent\n",
    "    if par and par.exists():\n",
    "        files = list_datafiles(par)\n",
    "        if files:\n",
    "            print(f\"  Recovery: using parent of candidate ({par})\")\n",
    "            return par, files\n",
    "    return p, []\n",
    "\n",
    "pack, data_files = choose_data_root(pack)\n",
    "if not data_files:\n",
    "    raise SystemExit(f\"No supported data files under {pack}. Set PACK_DIR to the pack root that contains out/ or data/.\")\n",
    "\n",
    "# Prefer CSV/TSV first, then parquet/feather, then NPZ/NPY\n",
    "def file_rank(p: Path):\n",
    "    ext = p.suffix.lower()\n",
    "    order = {\".csv\":3, \".tsv\":3, \".parquet\":2, \".feather\":2, \".npz\":1, \".npy\":1}\n",
    "    return (order.get(ext,0), p.stat().st_size)\n",
    "\n",
    "data_files.sort(key=file_rank, reverse=True)\n",
    "chosen = data_files[0]\n",
    "print(f\"  Using data file: {chosen} ({chosen.stat().st_size/1_048_576:.2f} MiB)\")\n",
    "\n",
    "# ---- Load and infer\n",
    "df_any = read_table_any(chosen, max_rows=None)\n",
    "df = to_pandas(df_any)\n",
    "E, gene_names, sample_names, meta = infer_matrix(df)\n",
    "print(f\"  Inferred matrix: genes={len(gene_names)}, samples={len(sample_names)}  format={meta['format']}\")\n",
    "\n",
    "# ---- Summarize\n",
    "summary, per_gene, tops = summarize_matrix(E, gene_names, sample_names, k_top=25)\n",
    "\n",
    "# ---- Outputs\n",
    "REPORT_DIR = Path(RUN_DIR)\n",
    "plots = {}\n",
    "\n",
    "def plot_hist(arr, path: Path, title, xlabel):\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    ensure_dir(path.parent)\n",
    "    plt.figure()\n",
    "    plt.hist([a for a in arr if not np.isnan(a)], bins=50)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel); plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def plot_bar(items, path: Path, title, ylabel):\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    ensure_dir(path.parent)\n",
    "    labels = [i[0] for i in items]\n",
    "    vals = [i[1] for i in items]\n",
    "    plt.figure(figsize=(10, max(3, 0.3*len(items))))\n",
    "    y = np.arange(len(items))\n",
    "    plt.barh(y, vals)\n",
    "    plt.yticks(y, labels)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(ylabel); plt.ylabel(\"Gene\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_scatter(Y, path: Path, title, xlabel=\"Dim 1\", ylabel=\"Dim 2\"):\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    ensure_dir(path.parent)\n",
    "    plt.figure()\n",
    "    plt.scatter(Y[:,0], Y[:,1], s=12, alpha=0.8)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel); plt.ylabel(ylabel)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def write_pdf(report_md_path: Path, images, out_pdf: Path, title=\"3I Atlas Check‑In\"):\n",
    "    if FPDF is None:\n",
    "        return False\n",
    "    pdf = FPDF(orientation=\"P\", unit=\"mm\", format=\"A4\")\n",
    "    pdf.set_auto_page_break(auto=True, margin=12)\n",
    "    pdf.add_page()\n",
    "    pdf.set_font(\"Arial\", \"B\", 16)\n",
    "    pdf.cell(0, 10, title, ln=1)\n",
    "    pdf.set_font(\"Arial\", \"\", 10)\n",
    "    with open(report_md_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip().startswith(\"!\"):\n",
    "                continue\n",
    "            pdf.multi_cell(0, 5, line.rstrip())\n",
    "    for img in images:\n",
    "        if img and Path(img).exists():\n",
    "            pdf.add_page()\n",
    "            pdf.image(str(img), x=10, y=20, w=180)\n",
    "            pdf.ln(5)\n",
    "            pdf.set_font(\"Arial\", \"I\", 9)\n",
    "            pdf.cell(0, 6, str(Path(img).name), ln=1, align=\"C\")\n",
    "    ensure_dir(out_pdf.parent)\n",
    "    pdf.output(str(out_pdf))\n",
    "    return True\n",
    "\n",
    "def to_csv(path: Path, rows, header):\n",
    "    ensure_dir(path.parent)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\",\".join(header) + \"\\n\")\n",
    "        for r in rows:\n",
    "            f.write(\",\".join(map(lambda x: str(x).replace(\",\",\";\"), r)) + \"\\n\")\n",
    "\n",
    "# CSVs\n",
    "to_csv(Path(REPORT_DIR/\"top_gini_genes.csv\"), tops[\"top_gini\"], [\"gene\",\"gini\",\"H_norm\",\"cv\",\"mean\"])\n",
    "to_csv(Path(REPORT_DIR/\"top_specialized_low_entropy.csv\"), tops[\"top_specialized_low_entropy\"], [\"gene\",\"gini\",\"H_norm\",\"cv\",\"mean\"])\n",
    "to_csv(Path(REPORT_DIR/\"top_housekeeping_high_entropy.csv\"), tops[\"top_housekeeping_high_entropy\"], [\"gene\",\"gini\",\"H_norm\",\"cv\",\"mean\"])\n",
    "to_csv(Path(REPORT_DIR/\"summary_stats.csv\"), [[k, v] for k, v in summary.items()], [\"metric\",\"value\"])\n",
    "\n",
    "# Plots\n",
    "plot_hist(per_gene[\"gini\"], Path(REPORT_DIR/\"plots/gini_hist.png\"), \"Gini distribution (gene specialization)\", \"Gini\")\n",
    "plots[\"gini_hist\"] = str(Path(REPORT_DIR/\"plots/gini_hist.png\"))\n",
    "plot_hist(per_gene[\"H_norm\"], Path(REPORT_DIR/\"plots/entropy_hist.png\"), \"Normalized entropy across samples\", \"H_norm\")\n",
    "plots[\"entropy_hist\"] = str(Path(REPORT_DIR/\"plots/entropy_hist.png\"))\n",
    "plot_bar(tops[\"top_gini\"], Path(REPORT_DIR/\"plots/top_gini_bar.png\"), \"Top specialized genes (by Gini)\", \"Gini\")\n",
    "plots[\"top_gini_bar\"] = str(Path(REPORT_DIR/\"plots/top_gini_bar.png\"))\n",
    "\n",
    "# Embeddings\n",
    "pca_pts, pca_var = try_pca(E, n=2, random_state=42)\n",
    "if pca_pts is not None:\n",
    "    plot_scatter(pca_pts, Path(REPORT_DIR/\"plots/pca_scatter.png\"),\n",
    "                 f\"PCA on samples (var={sum(pca_var):.2%})\", \"PC1\", \"PC2\")\n",
    "    plots[\"pca_scatter\"] = str(Path(REPORT_DIR/\"plots/pca_scatter.png\"))\n",
    "else:\n",
    "    print(\"  PCA not available or failed; skipping PCA plot.\")\n",
    "umap_pts = try_umap(E, n=2, random_state=42)\n",
    "if umap_pts is not None:\n",
    "    plot_scatter(umap_pts, Path(REPORT_DIR/\"plots/umap_scatter.png\"),\n",
    "                 \"UMAP on samples\", \"UMAP-1\", \"UMAP-2\")\n",
    "    plots[\"umap_scatter\"] = str(Path(REPORT_DIR/\"plots/umap_scatter.png\"))\n",
    "\n",
    "# Snapshot & delta\n",
    "SNAPSHOT_PATH = Path(REPORT_DIR/\"snapshot.json\")\n",
    "prev_path, prev = last_snapshot(Path(RUN_BASE))\n",
    "deltas = None\n",
    "if prev:\n",
    "    deltas = {\n",
    "        \"n_genes_delta\": summary[\"n_genes\"] - int(prev.get(\"summary\",{}).get(\"n_genes\", 0)),\n",
    "        \"n_samples_delta\": summary[\"n_samples\"] - int(prev.get(\"summary\",{}).get(\"n_samples\", 0)),\n",
    "        \"gini_mean_delta\": summary[\"gini_mean\"] - float(prev.get(\"summary\",{}).get(\"gini_mean\", 0.0)),\n",
    "        \"entropy_mean_delta\": summary[\"entropy_mean\"] - float(prev.get(\"summary\",{}).get(\"entropy_mean\", 0.0)),\n",
    "        \"cv_mean_delta\": summary[\"cv_mean\"] - float(prev.get(\"summary\",{}).get(\"cv_mean\", 0.0)),\n",
    "        \"changed_samples\": False,\n",
    "        \"added_samples\": [],\n",
    "        \"removed_samples\": [],\n",
    "    }\n",
    "    try:\n",
    "        prev_samples = set(prev.get(\"sample_names\", []))\n",
    "        cur_samples = set(sample_names)\n",
    "        add = sorted(cur_samples - prev_samples)\n",
    "        rem = sorted(prev_samples - cur_samples)\n",
    "        if add or rem:\n",
    "            deltas[\"changed_samples\"] = True\n",
    "            deltas[\"added_samples\"] = add\n",
    "            deltas[\"removed_samples\"] = rem\n",
    "    except Exception:\n",
    "        pass\n",
    "    write_json(Path(REPORT_DIR/\"delta_summary.json\"), deltas)\n",
    "    print(f\"  Δ written: {Path(REPORT_DIR/'delta_summary.json')}\")\n",
    "else:\n",
    "    print(\"  No prior snapshot found; this will serve as the baseline.\")\n",
    "\n",
    "snapshot = {\n",
    "    \"meta\": {\n",
    "        \"stamp_utc\": ts_utc(),\n",
    "        \"stamp_local\": ts_local(),\n",
    "        \"host\": platform.node(),\n",
    "        \"python\": sys.version.split()[0],\n",
    "        \"pack_dir\": str(pack),\n",
    "        \"data_file\": str(chosen),\n",
    "    },\n",
    "    \"summary\": summary,\n",
    "    \"sample_names\": sample_names[:5000],\n",
    "    \"top_gini\": tops[\"top_gini\"],\n",
    "    \"top_housekeeping_high_entropy\": tops[\"top_housekeeping_high_entropy\"],\n",
    "}\n",
    "write_json(SNAPSHOT_PATH, snapshot)\n",
    "\n",
    "# Report\n",
    "info = {\n",
    "    \"meta\": {\n",
    "        \"stamp_local\": ts_local(),\n",
    "        \"pack\": str(pack),\n",
    "        \"run_dir\": str(REPORT_DIR),\n",
    "    },\n",
    "    \"summary\": summary,\n",
    "    \"tops\": tops,\n",
    "    \"deltas\": deltas,\n",
    "    \"plots\": plots,\n",
    "}\n",
    "REPORT_MD = Path(REPORT_DIR/\"report.md\")\n",
    "write_report_md(REPORT_MD, info)\n",
    "print(f\"  Wrote: {REPORT_MD}\")\n",
    "\n",
    "# Lightweight PDF\n",
    "REPORT_PDF = Path(REPORT_DIR/\"report.pdf\")\n",
    "ok_pdf = write_pdf(REPORT_MD, images=[plots.get(\"gini_hist\"), plots.get(\"entropy_hist\"),\n",
    "                                      plots.get(\"top_gini_bar\"), plots.get(\"pca_scatter\"), plots.get(\"umap_scatter\")],\n",
    "                   out_pdf=REPORT_PDF, title=\"3I Atlas Check‑In\")\n",
    "if ok_pdf:\n",
    "    print(f\"  PDF:   {REPORT_PDF}\")\n",
    "else:\n",
    "    print(\"  PDF:   (skipped; fpdf missing)\")\n",
    "\n",
    "print(f\"[{ts_local()}] Done. — The field answers when you listen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "344221cf-8312-4262-a643-ca2b4ff9615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HOTFIX: Unicode-safe PDF writer (drop-in replacement) ---\n",
    "def write_pdf(report_md_path, images, out_pdf, title=\"3I Atlas Check-In\"):\n",
    "    from pathlib import Path\n",
    "    from fpdf import FPDF\n",
    "    try:\n",
    "        from fpdf.enums import XPos, YPos\n",
    "        HAVE_ENUMS = True\n",
    "    except Exception:\n",
    "        HAVE_ENUMS = False\n",
    "\n",
    "    # Replace curly quotes/dashes & NB hyphen if we fall back to core fonts\n",
    "    REPL = {\n",
    "        \"\\u2011\": \"-\",  # non-breaking hyphen\n",
    "        \"\\u2013\": \"-\",  # en dash\n",
    "        \"\\u2014\": \"-\",  # em dash\n",
    "        \"\\u2018\": \"'\", \"\\u2019\": \"'\",  # single quotes\n",
    "        \"\\u201c\": '\"', \"\\u201d\": '\"',  # double quotes\n",
    "        \"\\u2026\": \"...\"               # ellipsis\n",
    "    }\n",
    "    def ascii_fallback(s: str) -> str:\n",
    "        for k,v in REPL.items():\n",
    "            s = s.replace(k, v)\n",
    "        return s\n",
    "\n",
    "    # Try to use a real Unicode font from Windows; otherwise sanitize\n",
    "    ttf_candidates = [\n",
    "        r\"C:\\Windows\\Fonts\\arial.ttf\",\n",
    "        r\"C:\\Windows\\Fonts\\DejaVuSans.ttf\",\n",
    "        r\"C:\\Windows\\Fonts\\Calibri.ttf\",\n",
    "        r\"C:\\Windows\\Fonts\\segoeui.ttf\",\n",
    "    ]\n",
    "\n",
    "    pdf = FPDF(orientation=\"P\", unit=\"mm\", format=\"A4\")\n",
    "    pdf.set_auto_page_break(auto=True, margin=12)\n",
    "    pdf.add_page()\n",
    "\n",
    "    used_unicode = False\n",
    "    for ttf in ttf_candidates:\n",
    "        if Path(ttf).exists():\n",
    "            try:\n",
    "                # fpdf2 ≥ 2.5: uni arg no longer needed, keep for compatibility if present\n",
    "                try:\n",
    "                    pdf.add_font(\"U\", \"\", ttf, uni=True)\n",
    "                except TypeError:\n",
    "                    pdf.add_font(\"U\", \"\", ttf)\n",
    "                pdf.set_font(\"U\", \"\", 16)     # use regular weight to avoid needing arialbd.ttf\n",
    "                used_unicode = True\n",
    "                break\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    if not used_unicode:\n",
    "        pdf.set_font(\"helvetica\", \"\", 16)\n",
    "\n",
    "    # Title\n",
    "    safe_title = title if used_unicode else ascii_fallback(title)\n",
    "    if HAVE_ENUMS:\n",
    "        pdf.cell(0, 10, safe_title, new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
    "    else:\n",
    "        pdf.cell(0, 10, safe_title, ln=1)\n",
    "\n",
    "    # Body\n",
    "    pdf.set_font(\"U\" if used_unicode else \"helvetica\", \"\", 10)\n",
    "    with open(report_md_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip().startswith(\"!\"):  # skip markdown image lines\n",
    "                continue\n",
    "            pdf.multi_cell(0, 5, line if used_unicode else ascii_fallback(line))\n",
    "\n",
    "    # Images\n",
    "    for img in images:\n",
    "        if img and Path(img).exists():\n",
    "            pdf.add_page()\n",
    "            pdf.image(str(img), x=10, y=20, w=180)\n",
    "            if HAVE_ENUMS:\n",
    "                pdf.cell(0, 6, Path(img).name, new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
    "            else:\n",
    "                pdf.ln(6)\n",
    "\n",
    "    Path(out_pdf).parent.mkdir(parents=True, exist_ok=True)\n",
    "    pdf.output(str(out_pdf))\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a797d606-3888-446c-ba89-23a1468050ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-29 02:07:37] 3I Atlas Check-In starting…\n",
      "  Run dir: E:\\CNT\\notebooks\\archive\\cnt_runs\\3i_atlas_checkin\\20251029-060737Z\n",
      "  Scanning: C:\\Users\\caleb\\CNT_Lab\n",
      "  Scanning: E:\\CNT\n",
      "  Scanning: E:\\CNT\\notebooks\\archive\n",
      "  Candidate pack: C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\n",
      "  Using data file: C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\data\\noaa_mag_3d.csv (0.25 MiB)\n",
      "  Inferred matrix: genes=4300, samples=6  format=wide/fallback\n",
      "  Δ written: E:\\CNT\\notebooks\\archive\\cnt_runs\\3i_atlas_checkin\\20251029-060737Z\\delta_summary.json\n",
      "  Wrote: E:\\CNT\\notebooks\\archive\\cnt_runs\\3i_atlas_checkin\\20251029-060737Z\\report.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2236\\2283439131.py:398: DeprecationWarning: \"uni\" parameter is deprecated since v2.5.1, unused and will soon be removed\n",
      "  pdf.add_font(\"U\", \"\", ttf, uni=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PDF:   E:\\CNT\\notebooks\\archive\\cnt_runs\\3i_atlas_checkin\\20251029-060737Z\\report.pdf\n",
      "[2025-10-29 02:08:02] Done. Keep the field humming.\n"
     ]
    }
   ],
   "source": [
    "# === CNT \"3I Atlas\" — Mega Check-In (single cell, resilient) =================\n",
    "# What this does (one run, one cell):\n",
    "#   1) Scans your CNT roots for the freshest 3I Atlas pack (robust patterns).\n",
    "#   2) Auto-fixes duplicate suffix dirs (...\\vector_embedding\\vector_embedding).\n",
    "#   3) Accepts CSV/TSV/Parquet/Feather/NPZ/NPY (not just CSV).\n",
    "#   4) Infers a genes x samples matrix from wide or tidy tables.\n",
    "#   5) Computes Gini, normalized entropy, CV; renders histograms + top-Gini bar.\n",
    "#   6) PCA/UMAP on samples (if scikit-learn/umap-learn are installed).\n",
    "#   7) Emits report.md (+ PDF with Unicode-safe fallback), plots, CSVs, snapshot.json.\n",
    "#   8) Diffs against your last snapshot and writes delta_summary.json.\n",
    "#\n",
    "# Quick tweak: set PACK_DIR below to skip discovery if you know the exact pack:\n",
    "# PACK_DIR = r\"C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a_vector_embedding\"\n",
    "# =============================================================================\n",
    "\n",
    "import os, re, sys, json, glob, math, platform\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Optional dataframes: prefer pandas; you can flip to Polars by setting USE_POLARS=True\n",
    "USE_POLARS = False\n",
    "try:\n",
    "    import pandas as pd\n",
    "except Exception:\n",
    "    pd = None\n",
    "\n",
    "if USE_POLARS:\n",
    "    try:\n",
    "        import polars as pl\n",
    "    except Exception:\n",
    "        USE_POLARS = False\n",
    "\n",
    "# Optional algorithms\n",
    "try:\n",
    "    from sklearn.decomposition import PCA\n",
    "except Exception:\n",
    "    PCA = None\n",
    "\n",
    "try:\n",
    "    import umap\n",
    "except Exception:\n",
    "    umap = None\n",
    "\n",
    "# Optional PDF\n",
    "try:\n",
    "    from fpdf import FPDF  # fpdf2\n",
    "except Exception:\n",
    "    FPDF = None\n",
    "\n",
    "\n",
    "# ----------------------------- Utilities -------------------------------------\n",
    "\n",
    "def ts_utc():\n",
    "    return datetime.now(timezone.utc).strftime(\"%Y%m%d-%H%M%SZ\")\n",
    "\n",
    "def ts_local():\n",
    "    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def ensure_dir(p: Path):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def normalize_pack_dir(p: Path) -> Path:\n",
    "    \"\"\"\n",
    "    Collapse duplicate 'vector_embedding' suffixes and strip trailing empties.\n",
    "    Example: .../vector_embedding/vector_embedding -> .../vector_embedding\n",
    "             .../_vector_embedding_vector_embedding -> .../_vector_embedding\n",
    "    \"\"\"\n",
    "    parts = list(p.parts)\n",
    "    if len(parts) >= 2 and parts[-1].lower() == \"vector_embedding\" and parts[-2].lower() == \"vector_embedding\":\n",
    "        return Path(*parts[:-1])\n",
    "    name = p.name.lower()\n",
    "    if name.endswith(\"_vector_embedding_vector_embedding\"):\n",
    "        return p.with_name(p.name[: -len(\"_vector_embedding\")])\n",
    "    return p\n",
    "\n",
    "def all_existing(paths):\n",
    "    return [Path(p) for p in paths if Path(p).exists()]\n",
    "\n",
    "def list_datafiles(root: Path):\n",
    "    \"\"\"\n",
    "    Return candidate data files under root with supported suffixes.\n",
    "    Searches out/, data/, and the root recursively.\n",
    "    \"\"\"\n",
    "    patterns = []\n",
    "    for base in (\"out\", \"data\", \"\"):\n",
    "        basep = (root / base) if base else root\n",
    "        patterns += [\n",
    "            str(basep / \"**/*.csv\"),\n",
    "            str(basep / \"**/*.tsv\"),\n",
    "            str(basep / \"**/*.parquet\"),\n",
    "            str(basep / \"**/*.feather\"),\n",
    "            str(basep / \"**/*.npz\"),\n",
    "            str(basep / \"**/*.npy\"),\n",
    "        ]\n",
    "    hits = []\n",
    "    for pat in patterns:\n",
    "        hits.extend([Path(p) for p in glob.glob(pat, recursive=True)])\n",
    "    hits = [h for h in hits if h.is_file()]\n",
    "    hits.sort(key=lambda p: p.stat().st_size if p.exists() else 0, reverse=True)\n",
    "    return hits\n",
    "\n",
    "def read_table_any(path: Path, max_rows=None):\n",
    "    suff = path.suffix.lower()\n",
    "    if USE_POLARS:\n",
    "        if 'pl' not in globals():\n",
    "            raise RuntimeError(\"Polars not available; set USE_POLARS=False or install polars\")\n",
    "        if suff in (\".csv\", \".tsv\"):\n",
    "            sep = \",\" if suff == \".csv\" else \"\\t\"\n",
    "            df = pl.read_csv(str(path), separator=sep)\n",
    "            return df if max_rows is None else df.head(max_rows)\n",
    "        elif suff == \".parquet\":\n",
    "            df = pl.read_parquet(str(path))\n",
    "            return df if max_rows is None else df.head(max_rows)\n",
    "        elif suff == \".feather\":\n",
    "            df = pl.read_ipc(str(path))\n",
    "            return df if max_rows is None else df.head(max_rows)\n",
    "        elif suff in (\".npz\", \".npy\"):\n",
    "            arr = np.load(str(path))\n",
    "            if isinstance(arr, np.lib.npyio.NpzFile):\n",
    "                key = next(iter(arr.files))\n",
    "                arr = arr[key]\n",
    "            if arr.ndim == 2:\n",
    "                df = pl.DataFrame(arr)\n",
    "                df = df.with_columns(pl.Series(\"gene\", [f\"g{i}\" for i in range(arr.shape[0])]))\n",
    "                df = df.select([\"gene\"] + [c for c in df.columns if c != \"gene\"])\n",
    "                return df if max_rows is None else df.head(max_rows)\n",
    "            raise RuntimeError(f\"Unsupported NPZ/NPY shape in {path}: {arr.shape}\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unsupported file type: {suff}\")\n",
    "    else:\n",
    "        if pd is None:\n",
    "            raise RuntimeError(\"pandas not available; install pandas or set USE_POLARS=True\")\n",
    "        if suff in (\".csv\", \".tsv\"):\n",
    "            sep = \",\" if suff == \".csv\" else \"\\t\"\n",
    "            return pd.read_csv(path, nrows=max_rows, sep=sep)\n",
    "        elif suff == \".parquet\":\n",
    "            return pd.read_parquet(path)\n",
    "        elif suff == \".feather\":\n",
    "            return pd.read_feather(path)\n",
    "        elif suff in (\".npz\", \".npy\"):\n",
    "            arr = np.load(str(path))\n",
    "            if isinstance(arr, np.lib.npyio.NpzFile):\n",
    "                key = next(iter(arr.files))\n",
    "                arr = arr[key]\n",
    "            if arr.ndim == 2:\n",
    "                cols = [f\"col_{j}\" for j in range(arr.shape[1])]\n",
    "                df = pd.DataFrame(arr, columns=cols)\n",
    "                df.insert(0, \"gene\", [f\"g{i}\" for i in range(arr.shape[0])])\n",
    "                return df if max_rows is None else df.head(max_rows)\n",
    "            raise RuntimeError(f\"Unsupported NPZ/NPY shape in {path}: {arr.shape}\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unsupported file type: {suff}\")\n",
    "\n",
    "def to_pandas(df):\n",
    "    if pd is None:\n",
    "        raise RuntimeError(\"pandas not available\")\n",
    "    if USE_POLARS:\n",
    "        return df.to_pandas()\n",
    "    return df\n",
    "\n",
    "def infer_matrix(df: 'pd.DataFrame'):\n",
    "    \"\"\"\n",
    "    Infer a (genes x samples) numeric matrix from common 3I Atlas shapes.\n",
    "    Returns: E (n_genes x n_samples), gene_names, sample_names, meta\n",
    "    \"\"\"\n",
    "    meta = {\"format\": None, \"value_col\": None, \"gene_col\": None, \"tissue_col\": None}\n",
    "    cols_l = [str(c).lower() for c in df.columns]\n",
    "\n",
    "    gene_cols = [c for c in df.columns if str(c).lower() in\n",
    "                 (\"gene\",\"gene_id\",\"gene_name\",\"symbol\",\"ensembl\",\"ensembl_id\",\"id\")]\n",
    "    tissue_cols = [c for c in df.columns if str(c).lower() in\n",
    "                   (\"tissue\",\"organ\",\"celltype\",\"cell_type\",\"sample\",\"sample_id\")]\n",
    "    val_keys = (\"value\",\"expression\",\"expr\",\"count\",\"tpms\",\"fpkm\",\"reads\",\"abundance\",\"intensity\")\n",
    "    value_cols = [c for c in df.columns if str(c).lower() in val_keys]\n",
    "    emb_like = [c for c in df.columns if re.match(r\"(emb(ed(ding)?)?_?\\d+)$\", str(c).lower())]\n",
    "\n",
    "    # Tidy form: (gene, tissue, value) or embeddings under tidy\n",
    "    if gene_cols and tissue_cols and (value_cols or emb_like):\n",
    "        g = gene_cols[0]; t = tissue_cols[0]\n",
    "        v = (value_cols[0] if value_cols else emb_like[0])\n",
    "        pivot = df.pivot_table(index=g, columns=t, values=v, aggfunc=\"mean\").sort_index()\n",
    "        E = pivot.to_numpy(dtype=float)\n",
    "        gene_names = pivot.index.astype(str).to_list()\n",
    "        sample_names = [str(c) for c in pivot.columns.to_list()]\n",
    "        meta.update({\"format\":\"long/tidy\",\"gene_col\":g,\"tissue_col\":t,\"value_col\":v})\n",
    "        return E, gene_names, sample_names, meta\n",
    "\n",
    "    # Wide form with explicit gene column\n",
    "    if gene_cols:\n",
    "        g = gene_cols[0]\n",
    "        sub = df.copy().drop_duplicates(subset=[g]).set_index(g)\n",
    "        num = sub.select_dtypes(include=[np.number])\n",
    "        if num.shape[1]==0:\n",
    "            num = sub.apply(pd.to_numeric, errors=\"coerce\")\n",
    "        num = num.dropna(how=\"all\", axis=1)\n",
    "        E = num.to_numpy(dtype=float)\n",
    "        gene_names = [str(i) for i in num.index.to_list()]\n",
    "        sample_names = [str(c) for c in num.columns.to_list()]\n",
    "        meta.update({\"format\":\"wide\",\"gene_col\":g})\n",
    "        return E, gene_names, sample_names, meta\n",
    "\n",
    "    # Fallback: first column is id, rest numeric\n",
    "    sub = df.copy().dropna(how=\"all\", axis=1)\n",
    "    if sub.shape[1] < 2:\n",
    "        raise RuntimeError(\"Table has <2 columns; can't infer matrix.\")\n",
    "    g = sub.columns[0]\n",
    "    sub = sub.drop_duplicates(subset=[g]).set_index(g)\n",
    "    num = sub.select_dtypes(include=[np.number])\n",
    "    if num.shape[1]==0:\n",
    "        num = sub.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    num = num.dropna(how=\"all\", axis=1)\n",
    "    E = num.to_numpy(dtype=float)\n",
    "    gene_names = [str(i) for i in num.index.to_list()]\n",
    "    sample_names = [str(c) for c in num.columns.to_list()]\n",
    "    meta.update({\"format\":\"wide/fallback\",\"gene_col\":str(g)})\n",
    "    return E, gene_names, sample_names, meta\n",
    "\n",
    "def summarize_matrix(E: np.ndarray, gene_names, sample_names, k_top=25):\n",
    "    n_genes, n_samp = E.shape\n",
    "    X = E.copy()\n",
    "    if np.nanmin(X) < 0:\n",
    "        X = X - np.nanmin(X)\n",
    "    X = np.nan_to_num(X, nan=0.0)\n",
    "\n",
    "    var = np.nanvar(X, axis=1)\n",
    "    mean = np.nanmean(X, axis=1) + 1e-12\n",
    "    cv = np.sqrt(var) / mean\n",
    "\n",
    "    def gini_coefficient(row, eps=1e-12):\n",
    "        r = np.asarray(row, dtype=float)\n",
    "        mn = np.nanmin(r)\n",
    "        if mn < 0:\n",
    "            r = r - mn\n",
    "        r = np.nan_to_num(r, nan=0.0)\n",
    "        mu = r.mean() + eps\n",
    "        diff_sum = np.abs(r[:, None] - r[None, :]).mean()\n",
    "        return 0.5 * diff_sum / mu\n",
    "\n",
    "    def shannon_entropy(p, eps=1e-12):\n",
    "        p = np.clip(p, eps, None)\n",
    "        p = p / p.sum()\n",
    "        return float(-(p * np.log(p)).sum())\n",
    "\n",
    "    gini = np.array([gini_coefficient(row) for row in X])\n",
    "    H = np.array([shannon_entropy(row) for row in X])\n",
    "    H_norm = H / (np.log(X.shape[1]) if X.shape[1] > 1 else 1.0)\n",
    "\n",
    "    idx_gini = np.argsort(-gini)[:k_top]\n",
    "    idx_entropy_low  = np.argsort(H_norm)[:k_top]     # specialized\n",
    "    idx_entropy_high = np.argsort(-H_norm)[:k_top]    # housekeeping\n",
    "\n",
    "    def take(idx):\n",
    "        return [(gene_names[i], float(gini[i]), float(H_norm[i]),\n",
    "                 float(cv[i]), float(mean[i])) for i in idx]\n",
    "\n",
    "    top_gini  = take(idx_gini)\n",
    "    top_spec  = take(idx_entropy_low)\n",
    "    top_house = take(idx_entropy_high)\n",
    "\n",
    "    summary = {\n",
    "        \"n_genes\": int(n_genes),\n",
    "        \"n_samples\": int(n_samp),\n",
    "        \"gini_mean\": float(np.nanmean(gini)),\n",
    "        \"gini_median\": float(np.nanmedian(gini)),\n",
    "        \"entropy_mean\": float(np.nanmean(H_norm)),\n",
    "        \"entropy_median\": float(np.nanmedian(H_norm)),\n",
    "        \"cv_mean\": float(np.nanmean(cv)),\n",
    "    }\n",
    "    per_gene = {\n",
    "        \"var\": var.tolist(),\n",
    "        \"mean\": mean.tolist(),\n",
    "        \"cv\": cv.tolist(),\n",
    "        \"gini\": gini.tolist(),\n",
    "        \"H_norm\": H_norm.tolist(),\n",
    "    }\n",
    "    tops = {\n",
    "        \"top_gini\": top_gini,\n",
    "        \"top_specialized_low_entropy\": top_spec,\n",
    "        \"top_housekeeping_high_entropy\": top_house,\n",
    "    }\n",
    "    return summary, per_gene, tops\n",
    "\n",
    "def to_csv(path: Path, rows, header):\n",
    "    ensure_dir(path.parent)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\",\".join(header) + \"\\n\")\n",
    "        for r in rows:\n",
    "            f.write(\",\".join(map(lambda x: str(x).replace(\",\",\";\"), r)) + \"\\n\")\n",
    "\n",
    "def try_pca(E: np.ndarray, n=2, random_state=42):\n",
    "    if PCA is None:\n",
    "        return None, None\n",
    "    X = np.nan_to_num(E, nan=0.0)\n",
    "    X = X - X.mean(axis=1, keepdims=True)\n",
    "    pca = PCA(n_components=min(n, min(X.shape)-1), random_state=random_state)\n",
    "    try:\n",
    "        Y = pca.fit_transform(X.T)  # samples x n\n",
    "        return Y, pca.explained_variance_ratio_.tolist()\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "def try_umap(E: np.ndarray, n=2, random_state=42):\n",
    "    if umap is None:\n",
    "        return None\n",
    "    X = np.nan_to_num(E, nan=0.0)\n",
    "    X = X - X.mean(axis=1, keepdims=True)\n",
    "    try:\n",
    "        Y = umap.UMAP(n_components=n, random_state=random_state).fit_transform(X.T)\n",
    "        return Y\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def plot_hist(arr, path: Path, title, xlabel):\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    ensure_dir(path.parent)\n",
    "    plt.figure()\n",
    "    plt.hist([a for a in arr if not np.isnan(a)], bins=50)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel); plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def plot_bar(items, path: Path, title, ylabel):\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    ensure_dir(path.parent)\n",
    "    labels = [i[0] for i in items]\n",
    "    vals   = [i[1] for i in items]\n",
    "    plt.figure(figsize=(10, max(3, 0.3*len(items))))\n",
    "    y = np.arange(len(items))\n",
    "    plt.barh(y, vals)\n",
    "    plt.yticks(y, labels)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(ylabel); plt.ylabel(\"Gene\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_scatter(Y, path: Path, title, xlabel=\"Dim 1\", ylabel=\"Dim 2\"):\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    ensure_dir(path.parent)\n",
    "    plt.figure()\n",
    "    plt.scatter(Y[:,0], Y[:,1], s=12, alpha=0.8)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel); plt.ylabel(ylabel)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# Unicode-safe PDF writer (embeds a TTF if available; otherwise sanitizes punctuation)\n",
    "def write_pdf(report_md_path: Path, images, out_pdf: Path, title=\"3I Atlas Check-In\"):\n",
    "    if FPDF is None:\n",
    "        return False\n",
    "    try:\n",
    "        from fpdf.enums import XPos, YPos\n",
    "        HAVE_ENUMS = True\n",
    "    except Exception:\n",
    "        HAVE_ENUMS = False\n",
    "\n",
    "    REPL = {\n",
    "        \"\\u2011\": \"-\",  # NB hyphen\n",
    "        \"\\u2013\": \"-\",  # en dash\n",
    "        \"\\u2014\": \"-\",  # em dash\n",
    "        \"\\u2018\": \"'\",  \"\\u2019\": \"'\",  # curly single\n",
    "        \"\\u201c\": '\"',  \"\\u201d\": '\"',  # curly double\n",
    "        \"\\u2026\": \"...\",\n",
    "    }\n",
    "    def ascii_fallback(s: str) -> str:\n",
    "        for k,v in REPL.items():\n",
    "            s = s.replace(k, v)\n",
    "        return s\n",
    "\n",
    "    ttf_candidates = [\n",
    "        r\"C:\\Windows\\Fonts\\arial.ttf\",\n",
    "        r\"C:\\Windows\\Fonts\\DejaVuSans.ttf\",\n",
    "        r\"C:\\Windows\\Fonts\\Calibri.ttf\",\n",
    "        r\"C:\\Windows\\Fonts\\segoeui.ttf\",\n",
    "    ]\n",
    "\n",
    "    pdf = FPDF(orientation=\"P\", unit=\"mm\", format=\"A4\")\n",
    "    pdf.set_auto_page_break(auto=True, margin=12)\n",
    "    pdf.add_page()\n",
    "\n",
    "    used_unicode = False\n",
    "    for ttf in ttf_candidates:\n",
    "        if Path(ttf).exists():\n",
    "            try:\n",
    "                try:\n",
    "                    pdf.add_font(\"U\", \"\", ttf, uni=True)\n",
    "                except TypeError:\n",
    "                    pdf.add_font(\"U\", \"\", ttf)\n",
    "                pdf.set_font(\"U\", \"\", 16)\n",
    "                used_unicode = True\n",
    "                break\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    if not used_unicode:\n",
    "        pdf.set_font(\"helvetica\", \"\", 16)\n",
    "\n",
    "    safe_title = title if used_unicode else ascii_fallback(title)\n",
    "    if HAVE_ENUMS:\n",
    "        pdf.cell(0, 10, safe_title, new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
    "    else:\n",
    "        pdf.cell(0, 10, safe_title, ln=1)\n",
    "\n",
    "    pdf.set_font(\"U\" if used_unicode else \"helvetica\", \"\", 10)\n",
    "    with open(report_md_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip().startswith(\"!\"):  # skip md image lines\n",
    "                continue\n",
    "            pdf.multi_cell(0, 5, line if used_unicode else ascii_fallback(line))\n",
    "\n",
    "    for img in images:\n",
    "        if img and Path(img).exists():\n",
    "            pdf.add_page()\n",
    "            pdf.image(str(img), x=10, y=20, w=180)\n",
    "            if HAVE_ENUMS:\n",
    "                pdf.cell(0, 6, Path(img).name, new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
    "            else:\n",
    "                pdf.ln(6)\n",
    "\n",
    "    ensure_dir(Path(out_pdf).parent)\n",
    "    pdf.output(str(out_pdf))\n",
    "    return True\n",
    "\n",
    "\n",
    "# ----------------------------- Main ------------------------------------------\n",
    "\n",
    "# Optional hard override (set this to skip discovery):\n",
    "PACK_DIR = None\n",
    "\n",
    "ROOT_HINTS = [\n",
    "    r\"C:\\Users\\caleb\\CNT_Lab\",\n",
    "    r\"E:\\CNT\",\n",
    "    r\"D:\\CNT\",\n",
    "    r\"C:\\CNT\",\n",
    "    str(Path.cwd()),\n",
    "]\n",
    "\n",
    "RUN_BASE = r\"C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_runs\\3i_atlas_checkin\"\n",
    "if not Path(RUN_BASE).exists():\n",
    "    # fall back near your current notebook (e.g., E:\\CNT\\notebooks\\archive\\...)\n",
    "    RUN_BASE = str(Path.cwd() / \"cnt_runs\" / \"3i_atlas_checkin\")\n",
    "\n",
    "STAMP = ts_utc()\n",
    "RUN_DIR = ensure_dir(Path(RUN_BASE) / STAMP)\n",
    "\n",
    "print(f\"[{ts_local()}] 3I Atlas Check-In starting…\")\n",
    "print(f\"  Run dir: {RUN_DIR}\")\n",
    "\n",
    "# ---- Discover pack across ALL roots\n",
    "candidates = []\n",
    "\n",
    "def score_candidate(path: Path) -> int:\n",
    "    s = str(path).lower()\n",
    "    sc = 0\n",
    "    if path.is_dir(): sc += 3\n",
    "    if \"vector\" in s and \"embed\" in s: sc += 5\n",
    "    if \"cnt_3i_atlas_all\" in s: sc += 3\n",
    "    if s.endswith(\".csv\") or s.endswith(\".tsv\") or s.endswith(\".parquet\") or s.endswith(\".feather\"): sc += 1\n",
    "    if \"vector_embedding_vector_embedding\" in s: sc -= 4\n",
    "    try:\n",
    "        dcount = len(list_datafiles(path)) if path.is_dir() else 1\n",
    "        sc += min(6, dcount)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        sc += int(path.stat().st_mtime // 3600) % 10\n",
    "    except Exception:\n",
    "        pass\n",
    "    return sc\n",
    "\n",
    "def gather_candidates(root: Path):\n",
    "    pats = [\n",
    "        \"**/*3i*atlas*vector*embed*\",\n",
    "        \"**/*3i*atlas*embed*\",\n",
    "        \"**/*3i*atlas*\",\n",
    "        \"**/cnt_3i_atlas*\",\n",
    "        \"**/*3i*atlas*.csv\",\n",
    "    ]\n",
    "    for pat in pats:\n",
    "        for hit in root.glob(pat):\n",
    "            if \".ipynb_checkpoints\" in str(hit):\n",
    "                continue\n",
    "            candidates.append(hit)\n",
    "\n",
    "if PACK_DIR:\n",
    "    pack = normalize_pack_dir(Path(PACK_DIR))\n",
    "    print(f\"  PACK_DIR override: {pack}\")\n",
    "else:\n",
    "    for r in all_existing(ROOT_HINTS):\n",
    "        print(f\"  Scanning: {r}\")\n",
    "        gather_candidates(r)\n",
    "    if not candidates:\n",
    "        raise SystemExit(\"No 3I Atlas candidates found. Set PACK_DIR to the pack root.\")\n",
    "    candidates = [normalize_pack_dir(c) for c in candidates]\n",
    "    uniq = []\n",
    "    seen = set()\n",
    "    for c in candidates:\n",
    "        k = str(c).lower()\n",
    "        if k not in seen:\n",
    "            seen.add(k)\n",
    "            uniq.append(c)\n",
    "    candidates = uniq\n",
    "    candidates.sort(key=score_candidate, reverse=True)\n",
    "    pack = candidates[0]\n",
    "\n",
    "print(f\"  Candidate pack: {pack}\")\n",
    "\n",
    "# ---- Pick a data file; if none under pack, try its parent once\n",
    "def choose_data_root(p: Path):\n",
    "    files = list_datafiles(p)\n",
    "    if files:\n",
    "        return p, files\n",
    "    par = p.parent\n",
    "    if par and par.exists():\n",
    "        files = list_datafiles(par)\n",
    "        if files:\n",
    "            print(f\"  Recovery: using parent of candidate ({par})\")\n",
    "            return par, files\n",
    "    return p, []\n",
    "\n",
    "pack, data_files = choose_data_root(pack)\n",
    "if not data_files:\n",
    "    raise SystemExit(f\"No supported data files under {pack}. Set PACK_DIR to the pack root with out/ or data/.\")\n",
    "\n",
    "def file_rank(p: Path):\n",
    "    ext = p.suffix.lower()\n",
    "    order = {\".csv\":3, \".tsv\":3, \".parquet\":2, \".feather\":2, \".npz\":1, \".npy\":1}\n",
    "    return (order.get(ext,0), p.stat().st_size)\n",
    "\n",
    "data_files.sort(key=file_rank, reverse=True)\n",
    "chosen = data_files[0]\n",
    "print(f\"  Using data file: {chosen} ({chosen.stat().st_size/1_048_576:.2f} MiB)\")\n",
    "\n",
    "# ---- Load, infer, summarize\n",
    "df_any = read_table_any(chosen, max_rows=None)\n",
    "df = to_pandas(df_any)\n",
    "E, gene_names, sample_names, meta = infer_matrix(df)\n",
    "print(f\"  Inferred matrix: genes={len(gene_names)}, samples={len(sample_names)}  format={meta['format']}\")\n",
    "\n",
    "summary, per_gene, tops = summarize_matrix(E, gene_names, sample_names, k_top=25)\n",
    "\n",
    "# ---- Outputs\n",
    "plots = {}\n",
    "to_csv(Path(RUN_DIR/\"top_gini_genes.csv\"), tops[\"top_gini\"], [\"gene\",\"gini\",\"H_norm\",\"cv\",\"mean\"])\n",
    "to_csv(Path(RUN_DIR/\"top_specialized_low_entropy.csv\"), tops[\"top_specialized_low_entropy\"], [\"gene\",\"gini\",\"H_norm\",\"cv\",\"mean\"])\n",
    "to_csv(Path(RUN_DIR/\"top_housekeeping_high_entropy.csv\"), tops[\"top_housekeeping_high_entropy\"], [\"gene\",\"gini\",\"H_norm\",\"cv\",\"mean\"])\n",
    "to_csv(Path(RUN_DIR/\"summary_stats.csv\"), [[k, v] for k, v in summary.items()], [\"metric\",\"value\"])\n",
    "\n",
    "plot_hist(per_gene[\"gini\"], Path(RUN_DIR/\"plots/gini_hist.png\"), \"Gini distribution (gene specialization)\", \"Gini\")\n",
    "plots[\"gini_hist\"] = str(Path(RUN_DIR/\"plots/gini_hist.png\"))\n",
    "plot_hist(per_gene[\"H_norm\"], Path(RUN_DIR/\"plots/entropy_hist.png\"), \"Normalized entropy across samples\", \"H_norm\")\n",
    "plots[\"entropy_hist\"] = str(Path(RUN_DIR/\"plots/entropy_hist.png\"))\n",
    "plot_bar(tops[\"top_gini\"], Path(RUN_DIR/\"plots/top_gini_bar.png\"), \"Top specialized genes (by Gini)\", \"Gini\")\n",
    "plots[\"top_gini_bar\"] = str(Path(RUN_DIR/\"plots/top_gini_bar.png\"))\n",
    "\n",
    "pca_pts, pca_var = try_pca(E, n=2, random_state=42)\n",
    "if pca_pts is not None:\n",
    "    plot_scatter(pca_pts, Path(RUN_DIR/\"plots/pca_scatter.png\"),\n",
    "                 f\"PCA on samples (var={sum(pca_var):.2%})\", \"PC1\", \"PC2\")\n",
    "    plots[\"pca_scatter\"] = str(Path(RUN_DIR/\"plots/pca_scatter.png\"))\n",
    "else:\n",
    "    print(\"  PCA not available or failed; skipping PCA plot.\")\n",
    "\n",
    "umap_pts = try_umap(E, n=2, random_state=42)\n",
    "if umap_pts is not None:\n",
    "    plot_scatter(umap_pts, Path(RUN_DIR/\"plots/umap_scatter.png\"),\n",
    "                 \"UMAP on samples\", \"UMAP-1\", \"UMAP-2\")\n",
    "    plots[\"umap_scatter\"] = str(Path(RUN_DIR/\"plots/umap_scatter.png\"))\n",
    "\n",
    "# ---- Snapshot & delta\n",
    "def read_json(path: Path):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def write_json(path: Path, obj):\n",
    "    ensure_dir(path.parent)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def last_snapshot(dir_base: Path):\n",
    "    files = glob.glob(str(dir_base / \"*\" / \"snapshot.json\"))\n",
    "    if not files:\n",
    "        return None, None\n",
    "    files.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "    path = Path(files[0])\n",
    "    try:\n",
    "        return path, read_json(path)\n",
    "    except Exception:\n",
    "        return path, None\n",
    "\n",
    "SNAPSHOT_PATH = Path(RUN_DIR/\"snapshot.json\")\n",
    "prev_path, prev = last_snapshot(Path(RUN_BASE))\n",
    "deltas = None\n",
    "if prev:\n",
    "    deltas = {\n",
    "        \"n_genes_delta\": summary[\"n_genes\"] - int(prev.get(\"summary\",{}).get(\"n_genes\", 0)),\n",
    "        \"n_samples_delta\": summary[\"n_samples\"] - int(prev.get(\"summary\",{}).get(\"n_samples\", 0)),\n",
    "        \"gini_mean_delta\": summary[\"gini_mean\"] - float(prev.get(\"summary\",{}).get(\"gini_mean\", 0.0)),\n",
    "        \"entropy_mean_delta\": summary[\"entropy_mean\"] - float(prev.get(\"summary\",{}).get(\"entropy_mean\", 0.0)),\n",
    "        \"cv_mean_delta\": summary[\"cv_mean\"] - float(prev.get(\"summary\",{}).get(\"cv_mean\", 0.0)),\n",
    "        \"changed_samples\": False,\n",
    "        \"added_samples\": [],\n",
    "        \"removed_samples\": [],\n",
    "    }\n",
    "    try:\n",
    "        prev_samples = set(prev.get(\"sample_names\", []))\n",
    "        cur_samples  = set(sample_names)\n",
    "        add = sorted(cur_samples - prev_samples)\n",
    "        rem = sorted(prev_samples - cur_samples)\n",
    "        if add or rem:\n",
    "            deltas[\"changed_samples\"] = True\n",
    "            deltas[\"added_samples\"] = add\n",
    "            deltas[\"removed_samples\"] = rem\n",
    "    except Exception:\n",
    "        pass\n",
    "    write_json(Path(RUN_DIR/\"delta_summary.json\"), deltas)\n",
    "    print(f\"  Δ written: {Path(RUN_DIR/'delta_summary.json')}\")\n",
    "else:\n",
    "    print(\"  No prior snapshot found; this will serve as the baseline.\")\n",
    "\n",
    "snapshot = {\n",
    "    \"meta\": {\n",
    "        \"stamp_utc\": ts_utc(),\n",
    "        \"stamp_local\": ts_local(),\n",
    "        \"host\": platform.node(),\n",
    "        \"python\": sys.version.split()[0],\n",
    "        \"pack_dir\": str(pack),\n",
    "        \"data_file\": str(chosen),\n",
    "    },\n",
    "    \"summary\": summary,\n",
    "    \"sample_names\": sample_names[:5000],\n",
    "    \"top_gini\": tops[\"top_gini\"],\n",
    "    \"top_housekeeping_high_entropy\": tops[\"top_housekeeping_high_entropy\"],\n",
    "}\n",
    "write_json(SNAPSHOT_PATH, snapshot)\n",
    "\n",
    "# ---- Markdown report (embed relative image refs so PDF writer can include images)\n",
    "def write_report_md(path: Path, info):\n",
    "    ensure_dir(path.parent)\n",
    "    lines = []\n",
    "    lines.append(f\"# 3I Atlas Check-In — {info['meta']['stamp_local']}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"- **Pack**: `{info['meta']['pack']}`\")\n",
    "    lines.append(f\"- **Run dir**: `{info['meta']['run_dir']}`\")\n",
    "    lines.append(f\"- **Rows (genes)**: **{info['summary']['n_genes']}**, **Samples**: **{info['summary']['n_samples']}**\")\n",
    "    lines.append(f\"- Gini (mean/median): **{info['summary']['gini_mean']:.4f} / {info['summary']['gini_median']:.4f}**\")\n",
    "    lines.append(f\"- Entropy_n (mean/median): **{info['summary']['entropy_mean']:.4f} / {info['summary']['entropy_median']:.4f}**\")\n",
    "    lines.append(f\"- CV (mean): **{info['summary']['cv_mean']:.4f}**\")\n",
    "    lines.append(\"\")\n",
    "    for key in (\"gini_hist\",\"entropy_hist\",\"top_gini_bar\",\"pca_scatter\",\"umap_scatter\"):\n",
    "        p = info[\"plots\"].get(key)\n",
    "        if p:\n",
    "            lines.append(f\"![{key}]({Path(p).name})\")\n",
    "    lines.append(\"\")\n",
    "    tg = info[\"tops\"][\"top_gini\"][:10]\n",
    "    lines.append(\"## Top specialized (by Gini) — preview\")\n",
    "    for (name,g,h,cv,mu) in tg:\n",
    "        lines.append(f\"- {name}: Gini={g:.4f}, H_n={h:.4f}, CV={cv:.3f}, mean={mu:.3g}\")\n",
    "    lines.append(\"\")\n",
    "    th = info[\"tops\"][\"top_housekeeping_high_entropy\"][:10]\n",
    "    lines.append(\"## Top housekeeping (high normalized entropy) — preview\")\n",
    "    for (name,g,h,cv,mu) in th:\n",
    "        lines.append(f\"- {name}: H_n={h:.4f}, Gini={g:.4f}, CV={cv:.3f}, mean={mu:.3g}\")\n",
    "    lines.append(\"\")\n",
    "    if info.get(\"deltas\"):\n",
    "        d = info[\"deltas\"]\n",
    "        lines.append(\"## Delta vs last snapshot\")\n",
    "        lines.append(f\"- Genes: **{d.get('n_genes_delta',0):+d}**, Samples: **{d.get('n_samples_delta',0):+d}**\")\n",
    "        if \"gini_mean_delta\" in d:\n",
    "            lines.append(f\"- Δ Gini mean: **{d['gini_mean_delta']:+.4f}**, Δ Entropy_n mean: **{d.get('entropy_mean_delta',0):+.4f}**\")\n",
    "        if d.get(\"changed_samples\"):\n",
    "            lines.append(f\"- Changed sample set: +{len(d['added_samples'])} / -{len(d['removed_samples'])}\")\n",
    "        lines.append(\"\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "info = {\n",
    "    \"meta\":  {\"stamp_local\": ts_local(), \"pack\": str(pack), \"run_dir\": str(RUN_DIR)},\n",
    "    \"summary\": summary,\n",
    "    \"tops\":    tops,\n",
    "    \"deltas\":  deltas,\n",
    "    \"plots\":   plots,\n",
    "}\n",
    "REPORT_MD  = Path(RUN_DIR/\"report.md\")\n",
    "write_report_md(REPORT_MD, info)\n",
    "print(f\"  Wrote: {REPORT_MD}\")\n",
    "\n",
    "# ---- PDF export (Unicode-safe)\n",
    "REPORT_PDF = Path(RUN_DIR/\"report.pdf\")\n",
    "ok_pdf = write_pdf(REPORT_MD,\n",
    "                   images=[plots.get(\"gini_hist\"), plots.get(\"entropy_hist\"),\n",
    "                           plots.get(\"top_gini_bar\"), plots.get(\"pca_scatter\"), plots.get(\"umap_scatter\")],\n",
    "                   out_pdf=REPORT_PDF,\n",
    "                   title=\"3I Atlas Check-In\")\n",
    "print(\"  PDF:   {}\".format(REPORT_PDF if ok_pdf else \"(skipped; fpdf missing)\"))\n",
    "\n",
    "print(f\"[{ts_local()}] Done. Keep the field humming.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fe359a9-617b-4437-bf1e-9955f3fe9b05",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4289025274.py, line 327)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 327\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mp = info[\"plots\"].get(key); if p: L.append(f\"![{key}]({Path(p).name})\")\u001b[39m\n                                ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# === CNT \"3I Atlas\" — Mega Check‑In (single cell, v3) ========================\n",
    "# (See prior cell content for full documentation; this is the complete code.)\n",
    "import os, re, sys, json, glob, platform\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "USE_POLARS = False\n",
    "try:\n",
    "    import pandas as pd\n",
    "except Exception:\n",
    "    pd = None\n",
    "if USE_POLARS:\n",
    "    try:\n",
    "        import polars as pl\n",
    "    except Exception:\n",
    "        USE_POLARS = False\n",
    "try:\n",
    "    from sklearn.decomposition import PCA\n",
    "except Exception:\n",
    "    PCA = None\n",
    "try:\n",
    "    import umap\n",
    "except Exception:\n",
    "    umap = None\n",
    "try:\n",
    "    from fpdf import FPDF\n",
    "except Exception:\n",
    "    FPDF = None\n",
    "def ts_utc(): return datetime.now(timezone.utc).strftime(\"%Y%m%d-%H%M%SZ\")\n",
    "def ts_local(): return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "def ensure_dir(p: Path): p.mkdir(parents=True, exist_ok=True); return p\n",
    "def normalize_pack_dir(p: Path) -> Path:\n",
    "    parts = list(p.parts)\n",
    "    if len(parts) >= 2 and parts[-1].lower() == \"vector_embedding\" and parts[-2].lower() == \"vector_embedding\":\n",
    "        return Path(*parts[:-1])\n",
    "    name = p.name.lower()\n",
    "    if name.endswith(\"_vector_embedding_vector_embedding\"):\n",
    "        return p.with_name(p.name[: -len(\"_vector_embedding\")])\n",
    "    return p\n",
    "def list_datafiles(root: Path):\n",
    "    patterns = []\n",
    "    for base in (\"out\", \"data\", \"\"):\n",
    "        basep = (root / base) if base else root\n",
    "        patterns += [str(basep / \"**/*.csv\"), str(basep / \"**/*.tsv\"),\n",
    "                     str(basep / \"**/*.parquet\"), str(basep / \"**/*.feather\"),\n",
    "                     str(basep / \"**/*.npz\"), str(basep / \"**/*.npy\")]\n",
    "    hits = []\n",
    "    for pat in patterns: hits.extend([Path(p) for p in glob.glob(pat, recursive=True)])\n",
    "    hits = [h for h in hits if h.is_file()]\n",
    "    hits.sort(key=lambda p: p.stat().st_size if p.exists() else 0, reverse=True)\n",
    "    return hits\n",
    "def read_table_any(path: Path, max_rows=None):\n",
    "    suff = path.suffix.lower()\n",
    "    if USE_POLARS:\n",
    "        if 'pl' not in globals(): raise RuntimeError(\"Polars not available\")\n",
    "        if suff in (\".csv\", \".tsv\"):\n",
    "            sep = \",\" if suff == \".csv\" else \"\\t\"; df = pl.read_csv(str(path), separator=sep)\n",
    "            return df if max_rows is None else df.head(max_rows)\n",
    "        elif suff == \".parquet\": df = pl.read_parquet(str(path)); return df if max_rows is None else df.head(max_rows)\n",
    "        elif suff == \".feather\": df = pl.read_ipc(str(path)); return df if max_rows is None else df.head(max_rows)\n",
    "        elif suff in (\".npz\", \".npy\"):\n",
    "            arr = np.load(str(path)); \n",
    "            if isinstance(arr, np.lib.npyio.NpzFile): key = next(iter(arr.files)); arr = arr[key]\n",
    "            if arr.ndim == 2:\n",
    "                df = pl.DataFrame(arr); df = df.with_columns(pl.Series(\"gene\", [f\"g{i}\" for i in range(arr.shape[0])]))\n",
    "                df = df.select([\"gene\"] + [c for c in df.columns if c != \"gene\"]); return df if max_rows is None else df.head(max_rows)\n",
    "            raise RuntimeError(f\"Unsupported NPZ/NPY shape in {path}: {arr.shape}\")\n",
    "        else: raise RuntimeError(f\"Unsupported file type: {suff}\")\n",
    "    else:\n",
    "        if pd is None: raise RuntimeError(\"pandas not available\")\n",
    "        if suff in (\".csv\", \".tsv\"):\n",
    "            sep = \",\" if suff == \".csv\" else \"\\t\"; return pd.read_csv(path, nrows=max_rows, sep=sep)\n",
    "        elif suff == \".parquet\": return pd.read_parquet(path)\n",
    "        elif suff == \".feather\": return pd.read_feather(path)\n",
    "        elif suff in (\".npz\", \".npy\"):\n",
    "            arr = np.load(str(path))\n",
    "            if isinstance(arr, np.lib.npyio.NpzFile): key = next(iter(arr.files)); arr = arr[key]\n",
    "            if arr.ndim == 2:\n",
    "                cols = [f\"col_{j}\" for j in range(arr.shape[1])]; df = pd.DataFrame(arr, columns=cols)\n",
    "                df.insert(0, \"gene\", [f\"g{i}\" for i in range(arr.shape[0])]); return df if max_rows is None else df.head(max_rows)\n",
    "            raise RuntimeError(f\"Unsupported NPZ/NPY shape in {path}: {arr.shape}\")\n",
    "        else: raise RuntimeError(f\"Unsupported file type: {suff}\")\n",
    "def to_pandas(df):\n",
    "    if pd is None: raise RuntimeError(\"pandas not available\")\n",
    "    if USE_POLARS: return df.to_pandas()\n",
    "    return df\n",
    "def infer_matrix(df):\n",
    "    meta = {\"format\": None, \"value_col\": None, \"gene_col\": None, \"tissue_col\": None}\n",
    "    gene_cols = [c for c in df.columns if str(c).lower() in (\"gene\",\"gene_id\",\"gene_name\",\"symbol\",\"ensembl\",\"ensembl_id\",\"id\")]\n",
    "    tissue_cols = [c for c in df.columns if str(c).lower() in (\"tissue\",\"organ\",\"celltype\",\"cell_type\",\"sample\",\"sample_id\")]\n",
    "    val_keys = (\"value\",\"expression\",\"expr\",\"count\",\"tpms\",\"fpkm\",\"reads\",\"abundance\",\"intensity\")\n",
    "    value_cols = [c for c in df.columns if str(c).lower() in val_keys]\n",
    "    emb_like = [c for c in df.columns if re.match(r\"(emb(ed(ding)?)?_?\\d+)$\", str(c).lower())]\n",
    "    if gene_cols and tissue_cols and (value_cols or emb_like):\n",
    "        g = gene_cols[0]; t = tissue_cols[0]; v = (value_cols[0] if value_cols else emb_like[0])\n",
    "        pivot = df.pivot_table(index=g, columns=t, values=v, aggfunc=\"mean\").sort_index()\n",
    "        E = pivot.to_numpy(dtype=float); gene_names = pivot.index.astype(str).to_list(); sample_names = [str(c) for c in pivot.columns.to_list()]\n",
    "        meta.update({\"format\":\"long/tidy\",\"gene_col\":g,\"tissue_col\":t,\"value_col\":v}); return E, gene_names, sample_names, meta\n",
    "    if gene_cols:\n",
    "        g = gene_cols[0]; sub = df.copy().drop_duplicates(subset=[g]).set_index(g)\n",
    "        num = sub.select_dtypes(include=[np.number]); \n",
    "        if num.shape[1]==0: num = sub.apply(pd.to_numeric, errors=\"coerce\")\n",
    "        num = num.dropna(how=\"all\", axis=1)\n",
    "        E = num.to_numpy(dtype=float); gene_names = [str(i) for i in num.index.to_list()]; sample_names = [str(c) for c in num.columns.to_list()]\n",
    "        meta.update({\"format\":\"wide\",\"gene_col\":g}); return E, gene_names, sample_names, meta\n",
    "    sub = df.copy().dropna(how=\"all\", axis=1)\n",
    "    if sub.shape[1] < 2: raise RuntimeError(\"Table has <2 columns; can't infer matrix.\")\n",
    "    g = sub.columns[0]; sub = sub.drop_duplicates(subset=[g]).set_index(g)\n",
    "    num = sub.select_dtypes(include=[np.number]); \n",
    "    if num.shape[1]==0: num = sub.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    num = num.dropna(how=\"all\", axis=1)\n",
    "    E = num.to_numpy(dtype=float); gene_names = [str(i) for i in num.index.to_list()]; sample_names = [str(c) for c in num.columns.to_list()]\n",
    "    meta.update({\"format\":\"wide/fallback\",\"gene_col\":str(g)}); return E, gene_names, sample_names, meta\n",
    "def summarize_matrix(E, gene_names, sample_names, k_top=25):\n",
    "    X = E.copy(); \n",
    "    if np.nanmin(X) < 0: X = X - np.nanmin(X)\n",
    "    X = np.nan_to_num(X, nan=0.0)\n",
    "    var  = np.nanvar(X, axis=1); mean = np.nanmean(X, axis=1) + 1e-12; cv = np.sqrt(var) / mean\n",
    "    def gini(row, eps=1e-12):\n",
    "        r = np.asarray(row, dtype=float); mn = np.nanmin(r)\n",
    "        if mn < 0: r = r - mn\n",
    "        r = np.nan_to_num(r, nan=0.0); mu = r.mean() + eps\n",
    "        diff_sum = np.abs(r[:, None] - r[None, :]).mean(); return 0.5 * diff_sum / mu\n",
    "    def Hn_row(p, eps=1e-12):\n",
    "        p = np.clip(p, eps, None); p = p / p.sum(); H = float(-(p * np.log(p)).sum())\n",
    "        return H / (np.log(X.shape[1]) if X.shape[1] > 1 else 1.0)\n",
    "    gini_v = np.array([gini(row) for row in X]); Hn = np.array([Hn_row(row) for row in X])\n",
    "    idx_g = np.argsort(-gini_v)[:k_top]; idx_lo = np.argsort(Hn)[:k_top]; idx_hi = np.argsort(-Hn)[:k_top]\n",
    "    def take(idx): return [(gene_names[i], float(gini_v[i]), float(Hn[i]), float(cv[i]), float(mean[i])) for i in idx]\n",
    "    tops = {\"top_gini\": take(idx_g), \"top_specialized_low_entropy\": take(idx_lo), \"top_housekeeping_high_entropy\": take(idx_hi)}\n",
    "    summary = {\"n_genes\": int(X.shape[0]), \"n_samples\": int(X.shape[1]),\n",
    "               \"gini_mean\": float(np.nanmean(gini_v)), \"gini_median\": float(np.nanmedian(gini_v)),\n",
    "               \"entropy_mean\": float(np.nanmean(Hn)), \"entropy_median\": float(np.nanmedian(Hn)), \"cv_mean\": float(np.nanmean(cv))}\n",
    "    per_gene = {\"var\": var.tolist(), \"mean\": mean.tolist(), \"cv\": cv.tolist(), \"gini\": gini_v.tolist(), \"H_norm\": Hn.tolist()}\n",
    "    return summary, per_gene, tops\n",
    "def to_csv(path, rows, header):\n",
    "    ensure_dir(path.parent)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\",\".join(header) + \"\\n\")\n",
    "        for r in rows: f.write(\",\".join(map(lambda x: str(x).replace(\",\",\";\"), r)) + \"\\n\")\n",
    "def try_pca(E, n=2, random_state=42):\n",
    "    if PCA is None: return None, None\n",
    "    X = np.nan_to_num(E, nan=0.0); X = X - X.mean(axis=1, keepdims=True)\n",
    "    p = PCA(n_components=min(n, min(X.shape)-1), random_state=random_state)\n",
    "    try: Y = p.fit_transform(X.T); return Y, p.explained_variance_ratio_.tolist()\n",
    "    except Exception: return None, None\n",
    "def try_umap(E, n=2, random_state=42):\n",
    "    if umap is None: return None\n",
    "    X = np.nan_to_num(E, nan=0.0); X = X - X.mean(axis=1, keepdims=True)\n",
    "    try: return umap.UMAP(n_components=n, random_state=random_state).fit_transform(X.T)\n",
    "    except Exception: return None\n",
    "def plot_hist(arr, path, title, xlabel):\n",
    "    import matplotlib; matplotlib.use(\"Agg\"); import matplotlib.pyplot as plt\n",
    "    ensure_dir(Path(path).parent); plt.figure(); plt.hist([a for a in arr if not np.isnan(a)], bins=50)\n",
    "    plt.title(title); plt.xlabel(xlabel); plt.ylabel(\"Count\"); plt.tight_layout(); plt.savefig(path, dpi=150); plt.close()\n",
    "def plot_bar(items, path, title, ylabel):\n",
    "    import matplotlib; matplotlib.use(\"Agg\"); import matplotlib.pyplot as plt, numpy as _np\n",
    "    ensure_dir(Path(path).parent); labels = [i[0] for i in items]; vals = [i[1] for i in items]\n",
    "    plt.figure(figsize=(10, max(3, 0.3*len(items)))); y = _np.arange(len(items)); plt.barh(y, vals); plt.yticks(y, labels)\n",
    "    plt.title(title); plt.xlabel(ylabel); plt.ylabel(\"Gene\"); plt.tight_layout(); plt.savefig(path, dpi=150, bbox_inches=\"tight\"); plt.close()\n",
    "def plot_scatter(Y, path, title, xlabel=\"Dim 1\", ylabel=\"Dim 2\"):\n",
    "    import matplotlib; matplotlib.use(\"Agg\"); import matplotlib.pyplot as plt\n",
    "    ensure_dir(Path(path).parent); plt.figure(); plt.scatter(Y[:,0], Y[:,1], s=12, alpha=0.8)\n",
    "    plt.title(title); plt.xlabel(xlabel); plt.ylabel(ylabel); plt.tight_layout(); plt.savefig(path, dpi=150); plt.close()\n",
    "def write_pdf(report_md_path: Path, images, out_pdf: Path, title=\"3I Atlas Check-In\"):\n",
    "    if FPDF is None: return False\n",
    "    try:\n",
    "        from fpdf.enums import XPos, YPos; HAVE_ENUMS = True\n",
    "    except Exception:\n",
    "        HAVE_ENUMS = False\n",
    "    REPL = {\"\\u2011\":\"-\",\"\\u2013\":\"-\",\"\\u2014\":\"-\",\"\\u2018\":\"'\",\"\\u2019\":\"'\",\"\\u201c\":'\"',\"\\u201d\":'\"',\"\\u2026\":\"...\"}\n",
    "    def ascii_fallback(s: str):\n",
    "        for k,v in REPL.items(): s = s.replace(k, v)\n",
    "        return s\n",
    "    ttf_candidates = [r\"C:\\Windows\\Fonts\\arial.ttf\", r\"C:\\Windows\\Fonts\\DejaVuSans.ttf\",\n",
    "                      r\"C:\\Windows\\Fonts\\Calibri.ttf\", r\"C:\\Windows\\Fonts\\segoeui.ttf\"]\n",
    "    pdf = FPDF(orientation=\"P\", unit=\"mm\", format=\"A4\"); pdf.set_auto_page_break(auto=True, margin=12); pdf.add_page()\n",
    "    used_unicode = False\n",
    "    for ttf in ttf_candidates:\n",
    "        if Path(ttf).exists():\n",
    "            try:\n",
    "                try: pdf.add_font(\"U\", \"\", ttf, uni=True)\n",
    "                except TypeError: pdf.add_font(\"U\", \"\", ttf)\n",
    "                pdf.set_font(\"U\", \"\", 16); used_unicode = True; break\n",
    "            except Exception: pass\n",
    "    if not used_unicode: pdf.set_font(\"helvetica\", \"\", 16)\n",
    "    safe_title = title if used_unicode else ascii_fallback(title)\n",
    "    if HAVE_ENUMS: pdf.cell(0, 10, safe_title, new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
    "    else:          pdf.cell(0, 10, safe_title, ln=1)\n",
    "    pdf.set_font(\"U\" if used_unicode else \"helvetica\", \"\", 10)\n",
    "    with open(report_md_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip().startswith(\"!\"): continue\n",
    "            pdf.multi_cell(0, 5, line if used_unicode else ascii_fallback(line))\n",
    "    for img in images:\n",
    "        if img and Path(img).exists():\n",
    "            pdf.add_page(); pdf.image(str(img), x=10, y=20, w=180)\n",
    "            if HAVE_ENUMS: pdf.cell(0, 6, Path(img).name, new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
    "            else:          pdf.ln(6)\n",
    "    ensure_dir(Path(out_pdf).parent); pdf.output(str(out_pdf)); return True\n",
    "# ---- Main\n",
    "PACK_DIR = None\n",
    "ROOT_HINTS = [r\"C:\\Users\\caleb\\CNT_Lab\", r\"E:\\CNT\", r\"E:\\CNT\\notebooks\\archive\", r\"D:\\CNT\", r\"C:\\CNT\", str(Path.cwd())]\n",
    "RUN_BASE = r\"C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_runs\\3i_atlas_checkin\"\n",
    "if not Path(RUN_BASE).exists(): RUN_BASE = str(Path.cwd() / \"cnt_runs\" / \"3i_atlas_checkin\")\n",
    "STAMP = ts_utc(); RUN_DIR = ensure_dir(Path(RUN_BASE) / STAMP)\n",
    "print(f\"[{ts_local()}] 3I Atlas Check-In v3 starting…\"); print(f\"  Run dir: {RUN_DIR}\")\n",
    "candidates = []\n",
    "def score_candidate(path: Path) -> int:\n",
    "    s = str(path).lower(); sc = 0\n",
    "    if path.is_dir(): sc += 3\n",
    "    if \"vector\" in s and \"embed\" in s: sc += 5\n",
    "    if \"cnt_3i_atlas_all\" in s: sc += 3\n",
    "    if s.endswith((\".csv\",\".tsv\",\".parquet\",\".feather\")): sc += 1\n",
    "    if \"vector_embedding_vector_embedding\" in s: sc -= 4\n",
    "    try: sc += min(6, len(list_datafiles(path)) if path.is_dir() else 1)\n",
    "    except Exception: pass\n",
    "    try: sc += int(path.stat().st_mtime // 3600) % 10\n",
    "    except Exception: pass\n",
    "    return sc\n",
    "def gather_candidates(root: Path):\n",
    "    pats = [\"**/*3i*atlas*vector*embed*\",\"**/*3i*atlas*embed*\",\"**/*3i*atlas*\",\"**/cnt_3i_atlas*\",\"**/*3i*atlas*.csv\"]\n",
    "    for pat in pats:\n",
    "        for hit in root.glob(pat):\n",
    "            if \".ipynb_checkpoints\" in str(hit): continue\n",
    "            candidates.append(hit)\n",
    "def all_roots(): return [Path(p) for p in ROOT_HINTS if Path(p).exists()]\n",
    "if PACK_DIR:\n",
    "    pack = normalize_pack_dir(Path(PACK_DIR)); print(f\"  PACK_DIR override: {pack}\")\n",
    "else:\n",
    "    for r in all_roots(): print(f\"  Scanning: {r}\"); gather_candidates(r)\n",
    "    if not candidates: raise SystemExit(\"No 3I Atlas candidates found. Set PACK_DIR to the pack root.\")\n",
    "    candidates = [normalize_pack_dir(c) for c in candidates]\n",
    "    uniq, seen = [], set()\n",
    "    for c in candidates:\n",
    "        k = str(c).lower()\n",
    "        if k not in seen: seen.add(k); uniq.append(c)\n",
    "    candidates = uniq; candidates.sort(key=score_candidate, reverse=True); pack = candidates[0]\n",
    "print(f\"  Candidate pack: {pack}\")\n",
    "INCLUDE_PATTERNS = [\"atlas\",\"gene\",\"expr\",\"tpm\",\"fpkm\",\"counts\"]; EXCLUDE_PATTERNS = [\"noaa\",\"mag\",\"weather\",\"test\",\"debug\"]\n",
    "def choose_data_root(p: Path):\n",
    "    files = list_datafiles(p)\n",
    "    if files: return p, files\n",
    "    par = p.parent\n",
    "    if par and par.exists():\n",
    "        files = list_datafiles(par)\n",
    "        if files: print(f\"  Recovery: using parent of candidate ({par})\"); return par, files\n",
    "    return p, []\n",
    "pack, data_files = choose_data_root(pack)\n",
    "if not data_files: raise SystemExit(f\"No supported data files under {pack}. Set PACK_DIR to the pack root with out/ or data/.\")\n",
    "def file_rank(p: Path):\n",
    "    ext = p.suffix.lower(); base = p.name.lower()\n",
    "    order = {\".csv\":3, \".tsv\":3, \".parquet\":2, \".feather\":2, \".npz\":1, \".npy\":1}\n",
    "    bonus = sum(1 for w in INCLUDE_PATTERNS if w in base) - sum(1 for w in EXCLUDE_PATTERNS if w in base)\n",
    "    sniff = 0\n",
    "    try:\n",
    "        tmp = read_table_any(p, max_rows=32); tmp_pd = to_pandas(tmp)\n",
    "        cols = [str(c).lower() for c in tmp_pd.columns]\n",
    "        if any(c in cols for c in [\"gene\",\"gene_id\",\"gene_name\",\"symbol\",\"ensembl\",\"ensembl_id\"]): sniff += 4\n",
    "        if tmp_pd.shape[1] >= 20: sniff += 1\n",
    "    except Exception: pass\n",
    "    return (order.get(ext,0), bonus, sniff, p.stat().st_size)\n",
    "data_files.sort(key=file_rank, reverse=True); chosen = data_files[0]\n",
    "print(f\"  Using data file: {chosen} ({chosen.stat().st_size/1_048_576:.2f} MiB)\")\n",
    "df_any = read_table_any(chosen, max_rows=None); df = to_pandas(df_any)\n",
    "E, gene_names, sample_names, meta = infer_matrix(df)\n",
    "print(f\"  Inferred matrix: genes={len(gene_names)}, samples={len(sample_names)}  format={meta['format']}\")\n",
    "summary, per_gene, tops = summarize_matrix(E, gene_names, sample_names, k_top=25)\n",
    "plots = {}\n",
    "to_csv(Path(RUN_DIR/\"top_gini_genes.csv\"), tops[\"top_gini\"], [\"gene\",\"gini\",\"H_norm\",\"cv\",\"mean\"])\n",
    "to_csv(Path(RUN_DIR/\"top_specialized_low_entropy.csv\"), tops[\"top_specialized_low_entropy\"], [\"gene\",\"gini\",\"H_norm\",\"cv\",\"mean\"])\n",
    "to_csv(Path(RUN_DIR/\"top_housekeeping_high_entropy.csv\"), tops[\"top_housekeeping_high_entropy\"], [\"gene\",\"gini\",\"H_norm\",\"cv\",\"mean\"])\n",
    "to_csv(Path(RUN_DIR/\"summary_stats.csv\"), [[k, v] for k, v in summary.items()], [\"metric\",\"value\"])\n",
    "def plot_all():\n",
    "    global plots\n",
    "    plot_hist(per_gene[\"gini\"], Path(RUN_DIR/\"plots/gini_hist.png\"), \"Gini distribution (gene specialization)\", \"Gini\"); plots[\"gini_hist\"] = str(Path(RUN_DIR/\"plots/gini_hist.png\"))\n",
    "    plot_hist(per_gene[\"H_norm\"], Path(RUN_DIR/\"plots/entropy_hist.png\"), \"Normalized entropy across samples\", \"H_norm\"); plots[\"entropy_hist\"] = str(Path(RUN_DIR/\"plots/entropy_hist.png\"))\n",
    "    plot_bar(tops[\"top_gini\"], Path(RUN_DIR/\"plots/top_gini_bar.png\"), \"Top specialized genes (by Gini)\", \"Gini\"); plots[\"top_gini_bar\"] = str(Path(RUN_DIR/\"plots/top_gini_bar.png\"))\n",
    "    pca_pts, pca_var = try_pca(E, n=2, random_state=42)\n",
    "    if pca_pts is not None:\n",
    "        plot_scatter(pca_pts, Path(RUN_DIR/\"plots/pca_scatter.png\"), f\"PCA on samples (var={sum(pca_var):.2%})\", \"PC1\", \"PC2\"); plots[\"pca_scatter\"] = str(Path(RUN_DIR/\"plots/pca_scatter.png\"))\n",
    "    else: print(\"  PCA not available or failed; skipping PCA plot.\")\n",
    "    umap_pts = try_umap(E, n=2, random_state=42)\n",
    "    if umap_pts is not None:\n",
    "        plot_scatter(umap_pts, Path(RUN_DIR/\"plots/umap_scatter.png\"), \"UMAP on samples\", \"UMAP-1\", \"UMAP-2\"); plots[\"umap_scatter\"] = str(Path(RUN_DIR/\"plots/umap_scatter.png\"))\n",
    "plot_all()\n",
    "def read_json(path: Path):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f: return json.load(f)\n",
    "    except Exception: return None\n",
    "def write_json(path: Path, obj):\n",
    "    ensure_dir(path.parent)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f: json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "def last_snapshot(dir_base: Path):\n",
    "    files = glob.glob(str(dir_base / \"*\" / \"snapshot.json\"))\n",
    "    if not files: return None, None\n",
    "    files.sort(key=lambda p: os.path.getmtime(p), reverse=True); path = Path(files[0]); return path, read_json(path)\n",
    "SNAPSHOT_PATH = Path(RUN_DIR/\"snapshot.json\"); prev_path, prev = last_snapshot(Path(RUN_BASE)); deltas = None\n",
    "if prev:\n",
    "    deltas = {\"n_genes_delta\": summary[\"n_genes\"] - int(prev.get(\"summary\",{}).get(\"n_genes\", 0)),\n",
    "              \"n_samples_delta\": summary[\"n_samples\"] - int(prev.get(\"summary\",{}).get(\"n_samples\", 0)),\n",
    "              \"gini_mean_delta\": summary[\"gini_mean\"] - float(prev.get(\"summary\",{}).get(\"gini_mean\", 0.0)),\n",
    "              \"entropy_mean_delta\": summary[\"entropy_mean\"] - float(prev.get(\"summary\",{}).get(\"entropy_mean\", 0.0)),\n",
    "              \"cv_mean_delta\": summary[\"cv_mean\"] - float(prev.get(\"summary\",{}).get(\"cv_mean\", 0.0)),\n",
    "              \"changed_samples\": False, \"added_samples\": [], \"removed_samples\": []}\n",
    "    try:\n",
    "        prev_samples = set(prev.get(\"sample_names\", [])); cur_samples = set(sample_names)\n",
    "        add = sorted(cur_samples - prev_samples); rem = sorted(prev_samples - cur_samples)\n",
    "        if add or rem: deltas[\"changed_samples\"] = True; deltas[\"added_samples\"] = add; deltas[\"removed_samples\"] = rem\n",
    "    except Exception: pass\n",
    "    write_json(Path(RUN_DIR/\"delta_summary.json\"), deltas); print(f\"  Δ written: {Path(RUN_DIR/'delta_summary.json')}\")\n",
    "else: print(\"  No prior snapshot found; this will serve as the baseline.\")\n",
    "snapshot = {\"meta\": {\"stamp_utc\": ts_utc(), \"stamp_local\": ts_local(), \"host\": platform.node(),\n",
    "                     \"python\": sys.version.split()[0], \"pack_dir\": str(pack), \"data_file\": str(chosen)},\n",
    "            \"summary\": summary, \"sample_names\": sample_names[:5000],\n",
    "            \"top_gini\": tops[\"top_gini\"], \"top_housekeeping_high_entropy\": tops[\"top_housekeeping_high_entropy\"]}\n",
    "write_json(SNAPSHOT_PATH, snapshot)\n",
    "def write_report_md(path: Path, info):\n",
    "    ensure_dir(path.parent); L = []\n",
    "    L.append(f\"# 3I Atlas Check-In — {info['meta']['stamp_local']}\"); L.append(\"\")\n",
    "    L.append(f\"- **Pack**: `{info['meta']['pack']}`\"); L.append(f\"- **Run dir**: `{info['meta']['run_dir']}`\")\n",
    "    L.append(f\"- **Rows (genes)**: **{info['summary']['n_genes']}**, **Samples**: **{info['summary']['n_samples']}**\")\n",
    "    L.append(f\"- Gini (mean/median): **{info['summary']['gini_mean']:.4f} / {info['summary']['gini_median']:.4f}**\")\n",
    "    L.append(f\"- Entropy_n (mean/median): **{info['summary']['entropy_mean']:.4f} / {info['summary']['entropy_median']:.4f}**\")\n",
    "    L.append(f\"- CV (mean): **{info['summary']['cv_mean']:.4f}**\"); L.append(\"\")\n",
    "    for key in (\"gini_hist\",\"entropy_hist\",\"top_gini_bar\",\"pca_scatter\",\"umap_scatter\"):\n",
    "        p = info[\"plots\"].get(key); if p: L.append(f\"![{key}]({Path(p).name})\")\n",
    "    L.append(\"\"); L.append(\"## Top specialized (by Gini) — preview\")\n",
    "    for (name,g,h,cv,mu) in info[\"tops\"][\"top_gini\"][:10]:\n",
    "        L.append(f\"- {name}: Gini={g:.4f}, H_n={h:.4f}, CV={cv:.3f}, mean={mu:.3g}\")\n",
    "    L.append(\"\"); L.append(\"## Top housekeeping (high normalized entropy) — preview\")\n",
    "    for (name,g,h,cv,mu) in info[\"tops\"][\"top_housekeeping_high_entropy\"][:10]:\n",
    "        L.append(f\"- {name}: H_n={h:.4f}, Gini={g:.4f}, CV={cv:.3f}, mean={mu:.3g}\")\n",
    "    if info.get(\"deltas\"):\n",
    "        d = info[\"deltas\"]; L.append(\"\"); L.append(\"## Delta vs last snapshot\")\n",
    "        L.append(f\"- Genes: **{d.get('n_genes_delta',0):+d}**, Samples: **{d.get('n_samples_delta',0):+d}**\")\n",
    "        if \"gini_mean_delta\" in d: L.append(f\"- Δ Gini mean: **{d['gini_mean_delta']:+.4f}**, Δ Entropy_n mean: **{d.get('entropy_mean_delta',0):+.4f}**\")\n",
    "        if d.get(\"changed_samples\"): L.append(f\"- Changed sample set: +{len(d['added_samples'])} / -{len(d['removed_samples'])}\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f: f.write(\"\\n\".join(L))\n",
    "info = {\"meta\": {\"stamp_local\": ts_local(), \"pack\": str(pack), \"run_dir\": str(RUN_DIR)},\n",
    "        \"summary\": summary, \"tops\": tops, \"deltas\": deltas, \"plots\": plots}\n",
    "REPORT_MD = Path(RUN_DIR/\"report.md\"); write_report_md(REPORT_MD, info); print(f\"  Wrote: {REPORT_MD}\")\n",
    "REPORT_PDF = Path(RUN_DIR/\"report.pdf\")\n",
    "ok_pdf = write_pdf(REPORT_MD,\n",
    "                   images=[plots.get(\"gini_hist\"), plots.get(\"entropy_hist\"), plots.get(\"top_gini_bar\"),\n",
    "                           plots.get(\"pca_scatter\"), plots.get(\"umap_scatter\")],\n",
    "                   out_pdf=REPORT_PDF, title=\"3I Atlas Check-In\")\n",
    "print(\"  PDF:   {}\".format(REPORT_PDF if ok_pdf else \"(skipped; fpdf missing)\"))\n",
    "print(f\"[{ts_local()}] Done. Keep the field humming.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b291156-9368-46e7-8204-0ada78c5325a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-29 02:44:14] 3I Atlas Check-In (ATLAS-ONLY v4) starting…\n",
      "  Run dir: C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_runs\\3i_atlas_checkin\\20251029-064414Z\n",
      "  Scanning: C:\\Users\\caleb\\CNT_Lab\n",
      "  Scanning: E:\\CNT\n",
      "  Scanning: E:\\CNT\\notebooks\\archive\n",
      "  Scanning: E:\\CNT\\notebooks\\archive\n",
      "  Candidate pack: C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "Atlas-only selection failed (no file met thresholds).\nDiagnostics:\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\data\\spectrum_B.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\data\\spectrum_B.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\data\\spectrum_A.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\data\\spectrum_A.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\lightcurve_theta.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\lightcurve_theta.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\plasma_phase_72h.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\plasma_phase_72h.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\data\\lightcurve.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\data\\lightcurve.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\plasma_resonance_72h.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\plasma_resonance_72h.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\gra_trials_A.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\gra_trials_A.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\gra_trials_B.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\gra_trials_B.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\overview.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\overview.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\gra_summary.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\gra_summary.csv",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m Atlas-only selection failed (no file met thresholds).\nDiagnostics:\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\data\\spectrum_B.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\data\\spectrum_B.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\data\\spectrum_A.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\data\\spectrum_A.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\lightcurve_theta.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\lightcurve_theta.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\plasma_phase_72h.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\plasma_phase_72h.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\data\\lightcurve.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\data\\lightcurve.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\plasma_resonance_72h.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\plasma_resonance_72h.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\gra_trials_A.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\gra_trials_A.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\gra_trials_B.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\gra_trials_B.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\overview.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\overview.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\gra_summary.csv\n   -9998  no_gene_col                     C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\out\\tables\\gra_summary.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\CNT_Lab\\.venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3707: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# === CNT \"3I Atlas\" — ATLAS-ONLY Check-In (single cell, v4) ==================\n",
    "# Guarantees: selects a true gene-atlas table and outputs NEW facts.\n",
    "# Key tactics:\n",
    "#   • Hard exclude NOAA/geomag/etc. by name and by content.\n",
    "#   • Require a gene id column (gene/gene_id/gene_name/symbol/ensembl*).\n",
    "#   • Require sufficient sample breadth (≥ MIN_SAMPLES or tidy pivot ≥ MIN_SAMPLES).\n",
    "#   • Prefer files with explicit atlas/gene/expr/tpm/fpkm/counts cues + content sniff.\n",
    "# Outputs under <RUN_BASE>\\<STAMP>\\ :\n",
    "#   report.md, report.pdf (Unicode-safe), atlas_facts.md\n",
    "#   summary_stats.csv, top_gini_genes.csv, top_specialized_low_entropy.csv, top_housekeeping_high_entropy.csv\n",
    "#   delta_summary.json (if prior), snapshot.json, plots/*.png\n",
    "# =============================================================================\n",
    "\n",
    "import os, re, sys, json, glob, platform\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# ---------- Config (tweak if needed) -----------------------------------------\n",
    "PACK_DIR = None  # ← set to exact pack dir to skip discovery (recommended if you know it)\n",
    "# Example: r\"C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a_vector_embedding\"\n",
    "\n",
    "ROOT_HINTS = [\n",
    "    r\"C:\\Users\\caleb\\CNT_Lab\",\n",
    "    r\"E:\\CNT\",\n",
    "    r\"E:\\CNT\\notebooks\\archive\",\n",
    "    r\"D:\\CNT\",\n",
    "    r\"C:\\CNT\",\n",
    "    str(Path.cwd()),\n",
    "]\n",
    "\n",
    "RUN_BASE_HINTS = [\n",
    "    r\"C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_runs\\3i_atlas_checkin\",\n",
    "    r\"E:\\CNT\\notebooks\\archive\\cnt_runs\\3i_atlas_checkin\",\n",
    "    str(Path.cwd() / \"cnt_runs\" / \"3i_atlas_checkin\"),\n",
    "]\n",
    "\n",
    "STRICT_ATLAS_ONLY = True\n",
    "MIN_SAMPLES = 10          # reject candidates with fewer than this many samples (or tidy categories)\n",
    "MIN_GENES   = 1000        # reject candidates with fewer than this many gene rows (estimated/sniffed)\n",
    "SNIFF_ROWS  = 2000        # how many rows to read for content sniff (header always fully read)\n",
    "\n",
    "INCLUDE_PATTERNS = [\"atlas\", \"gene\", \"expr\", \"tpm\", \"fpkm\", \"counts\", \"matrix\"]\n",
    "EXCLUDE_PATTERNS = [\"noaa\", \"mag\", \"geomag\", \"weather\", \"storm\", \"wind\", \"met\", \"debug\", \"test\"]\n",
    "\n",
    "# ---------- Optional deps -----------------------------------------------------\n",
    "USE_POLARS = False\n",
    "try:\n",
    "    import pandas as pd\n",
    "except Exception:\n",
    "    pd = None\n",
    "\n",
    "if USE_POLARS:\n",
    "    try:\n",
    "        import polars as pl\n",
    "    except Exception:\n",
    "        USE_POLARS = False\n",
    "\n",
    "try:\n",
    "    from sklearn.decomposition import PCA\n",
    "except Exception:\n",
    "    PCA = None\n",
    "\n",
    "try:\n",
    "    import umap\n",
    "except Exception:\n",
    "    umap = None\n",
    "\n",
    "try:\n",
    "    from fpdf import FPDF\n",
    "except Exception:\n",
    "    FPDF = None\n",
    "\n",
    "# ---------- Utils -------------------------------------------------------------\n",
    "def ts_utc(): return datetime.now(timezone.utc).strftime(\"%Y%m%d-%H%M%SZ\")\n",
    "def ts_local(): return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "def ensure_dir(p: Path): p.mkdir(parents=True, exist_ok=True); return p\n",
    "\n",
    "def pick_run_base():\n",
    "    for p in RUN_BASE_HINTS:\n",
    "        path = Path(p)\n",
    "        try:\n",
    "            path.mkdir(parents=True, exist_ok=True)\n",
    "            return str(path)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return str(Path.cwd() / \"cnt_runs\" / \"3i_atlas_checkin\")\n",
    "\n",
    "def normalize_pack_dir(p: Path) -> Path:\n",
    "    parts = list(p.parts)\n",
    "    if len(parts) >= 2 and parts[-1].lower() == \"vector_embedding\" and parts[-2].lower() == \"vector_embedding\":\n",
    "        return Path(*parts[:-1])\n",
    "    name = p.name.lower()\n",
    "    if name.endswith(\"_vector_embedding_vector_embedding\"):\n",
    "        return p.with_name(p.name[: -len(\"_vector_embedding\")])\n",
    "    return p\n",
    "\n",
    "def all_existing(paths): return [Path(p) for p in paths if Path(p).exists()]\n",
    "\n",
    "def list_datafiles(root: Path):\n",
    "    patterns = []\n",
    "    for base in (\"out\", \"data\", \"\"):\n",
    "        basep = (root / base) if base else root\n",
    "        patterns += [\n",
    "            str(basep / \"**/*.csv\"),\n",
    "            str(basep / \"**/*.tsv\"),\n",
    "            str(basep / \"**/*.parquet\"),\n",
    "            str(basep / \"**/*.feather\"),\n",
    "            str(basep / \"**/*.npz\"),\n",
    "            str(basep / \"**/*.npy\"),\n",
    "        ]\n",
    "    hits = []\n",
    "    for pat in patterns:\n",
    "        hits.extend([Path(p) for p in glob.glob(pat, recursive=True)])\n",
    "    hits = [h for h in hits if h.is_file()]\n",
    "    hits.sort(key=lambda p: p.stat().st_size if p.exists() else 0, reverse=True)\n",
    "    return hits\n",
    "\n",
    "def read_table_any(path: Path, nrows=None):\n",
    "    suff = path.suffix.lower()\n",
    "    if USE_POLARS:\n",
    "        if 'pl' not in globals():\n",
    "            raise RuntimeError(\"Polars not available; set USE_POLARS=False or install polars\")\n",
    "        if suff in (\".csv\", \".tsv\"):\n",
    "            sep = \",\" if suff == \".csv\" else \"\\t\"\n",
    "            df = pl.read_csv(str(path), separator=sep)\n",
    "            return df if nrows is None else df.head(nrows)\n",
    "        elif suff == \".parquet\":\n",
    "            df = pl.read_parquet(str(path))\n",
    "            return df if nrows is None else df.head(nrows)\n",
    "        elif suff == \".feather\":\n",
    "            df = pl.read_ipc(str(path))\n",
    "            return df if nrows is None else df.head(nrows)\n",
    "        elif suff in (\".npz\", \".npy\"):\n",
    "            arr = np.load(str(path))\n",
    "            if isinstance(arr, np.lib.npyio.NpzFile):\n",
    "                key = next(iter(arr.files)); arr = arr[key]\n",
    "            if arr.ndim == 2:\n",
    "                df = pl.DataFrame(arr)\n",
    "                df = df.with_columns(pl.Series(\"gene\", [f\"g{i}\" for i in range(arr.shape[0])]))\n",
    "                df = df.select([\"gene\"] + [c for c in df.columns if c != \"gene\"])\n",
    "                return df if nrows is None else df.head(nrows)\n",
    "            raise RuntimeError(f\"Unsupported NPZ/NPY shape in {path}: {arr.shape}\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unsupported file type: {suff}\")\n",
    "    else:\n",
    "        if pd is None:\n",
    "            raise RuntimeError(\"pandas not available; install pandas or set USE_POLARS=True\")\n",
    "        if suff in (\".csv\", \".tsv\"):\n",
    "            sep = \",\" if suff == \".csv\" else \"\\t\"\n",
    "            return pd.read_csv(path, nrows=nrows, sep=sep)\n",
    "        elif suff == \".parquet\":\n",
    "            try:\n",
    "                return pd.read_parquet(path)\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to read parquet {path}: {e}\")\n",
    "        elif suff == \".feather\":\n",
    "            return pd.read_feather(path)\n",
    "        elif suff in (\".npz\", \".npy\"):\n",
    "            arr = np.load(str(path))\n",
    "            if isinstance(arr, np.lib.npyio.NpzFile):\n",
    "                key = next(iter(arr.files)); arr = arr[key]\n",
    "            if arr.ndim == 2:\n",
    "                cols = [f\"col_{j}\" for j in range(arr.shape[1])]\n",
    "                df = pd.DataFrame(arr, columns=cols)\n",
    "                df.insert(0, \"gene\", [f\"g{i}\" for i in range(arr.shape[0])])\n",
    "                return df if nrows is None else df.head(nrows)\n",
    "            raise RuntimeError(f\"Unsupported NPZ/NPY shape in {path}: {arr.shape}\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unsupported file type: {suff}\")\n",
    "\n",
    "def to_pandas(df):\n",
    "    if pd is None: raise RuntimeError(\"pandas not available\")\n",
    "    if USE_POLARS: return df.to_pandas()\n",
    "    return df\n",
    "\n",
    "# ---------- Candidate scoring (ATLAS-ONLY) -----------------------------------\n",
    "GENE_COLS = {\"gene\",\"gene_id\",\"gene_name\",\"symbol\",\"ensembl\",\"ensembl_id\",\"id\"}\n",
    "SAMPLE_COLS = {\"tissue\",\"organ\",\"celltype\",\"cell_type\",\"sample\",\"sample_id\"}\n",
    "\n",
    "def sniff_file(path: Path):\n",
    "    \"\"\"Return (has_gene_col, has_sample_col, n_samples_est, n_genes_est, tidy_possible, cols_lower)\"\"\"\n",
    "    try:\n",
    "        df = to_pandas(read_table_any(path, nrows=SNIFF_ROWS))\n",
    "    except Exception:\n",
    "        return False, False, 0, 0, False, []\n",
    "\n",
    "    cols = [str(c).lower() for c in df.columns]\n",
    "    has_gene = any(c in GENE_COLS for c in cols)\n",
    "    has_sample_col = any(c in SAMPLE_COLS for c in cols)\n",
    "\n",
    "    n_samples_est = 0\n",
    "    n_genes_est = 0\n",
    "    tidy_possible = False\n",
    "\n",
    "    if has_gene and has_sample_col:\n",
    "        # tidy candidate\n",
    "        tidy_possible = True\n",
    "        gcol = next(c for c in df.columns if str(c).lower() in GENE_COLS)\n",
    "        scol = next(c for c in df.columns if str(c).lower() in SAMPLE_COLS)\n",
    "        n_samples_est = int(df[scol].nunique())\n",
    "        n_genes_est = int(df[gcol].nunique())\n",
    "    elif has_gene:\n",
    "        # wide candidate: #numeric columns ≈ samples, #rows ≈ genes\n",
    "        sub = df.copy().set_index(next(c for c in df.columns if str(c).lower() in GENE_COLS))\n",
    "        num = sub.select_dtypes(include=[np.number])\n",
    "        if num.shape[1] == 0:\n",
    "            num = sub.apply(pd.to_numeric, errors=\"coerce\")\n",
    "        num = num.dropna(how=\"all\", axis=1)\n",
    "        n_samples_est = int(num.shape[1])\n",
    "        n_genes_est = int(num.shape[0])\n",
    "    else:\n",
    "        # no gene column → treat as non-atlas\n",
    "        return False, has_sample_col, 0, 0, False, cols\n",
    "\n",
    "    return has_gene, has_sample_col, n_samples_est, n_genes_est, tidy_possible, cols\n",
    "\n",
    "def file_rank_atlas_only(p: Path):\n",
    "    base = p.name.lower()\n",
    "    ext  = p.suffix.lower()\n",
    "    # Hard block by name\n",
    "    if any(w in base for w in EXCLUDE_PATTERNS):\n",
    "        return (-9999, \"blocked_by_name\")\n",
    "\n",
    "    # Include cues\n",
    "    inc_bonus = sum(1 for w in INCLUDE_PATTERNS if w in base)\n",
    "\n",
    "    # Content sniff\n",
    "    has_gene, has_sample_col, ns, ng, tidy_ok, cols = sniff_file(p)\n",
    "\n",
    "    if STRICT_ATLAS_ONLY and not has_gene:\n",
    "        return (-9998, \"no_gene_col\")\n",
    "\n",
    "    # Enforce thresholds\n",
    "    if ns < MIN_SAMPLES or ng < MIN_GENES:\n",
    "        return (-9997 + inc_bonus, f\"too_small(ns={ns},ng={ng})\")\n",
    "\n",
    "    # Prefer tidy (gene,tissue,value) or wide with many samples\n",
    "    tidy_bonus = 2 if tidy_ok else 0\n",
    "\n",
    "    # Prefer common formats\n",
    "    fmt_score = {\".csv\": 3, \".tsv\": 3, \".parquet\": 2, \".feather\": 2, \".npz\": 1, \".npy\": 1}.get(ext, 0)\n",
    "\n",
    "    size_score = min(6, int(p.stat().st_size / 1_000_000))  # up to +6 for size (in MB)\n",
    "\n",
    "    total = (10 * int(has_gene)) + (5 * int(has_sample_col)) + inc_bonus + tidy_bonus + fmt_score + size_score + ns // 5 + ng // 500\n",
    "    return (total, f\"ok(ns={ns}, ng={ng}, tidy={tidy_ok})\")\n",
    "\n",
    "# ---------- Matrix inference & stats -----------------------------------------\n",
    "def infer_matrix(df):\n",
    "    meta = {\"format\": None, \"value_col\": None, \"gene_col\": None, \"tissue_col\": None}\n",
    "    cols_l = [str(c).lower() for c in df.columns]\n",
    "    gene_cols = [c for c in df.columns if str(c).lower() in GENE_COLS]\n",
    "    sample_cols = [c for c in df.columns if str(c).lower() in SAMPLE_COLS]\n",
    "    val_keys = (\"value\",\"expression\",\"expr\",\"count\",\"tpms\",\"tpm\",\"fpkm\",\"reads\",\"abundance\",\"intensity\")\n",
    "    value_cols = [c for c in df.columns if str(c).lower() in val_keys]\n",
    "    emb_like = [c for c in df.columns if re.match(r\"(emb(ed(ding)?)?_?\\d+)$\", str(c).lower())]\n",
    "\n",
    "    if gene_cols and sample_cols and (value_cols or emb_like):\n",
    "        g = gene_cols[0]; t = sample_cols[0]; v = (value_cols[0] if value_cols else emb_like[0])\n",
    "        pivot = df.pivot_table(index=g, columns=t, values=v, aggfunc=\"mean\").sort_index()\n",
    "        E = pivot.to_numpy(dtype=float)\n",
    "        gene_names = pivot.index.astype(str).to_list()\n",
    "        sample_names = [str(c) for c in pivot.columns.to_list()]\n",
    "        meta.update({\"format\":\"long/tidy\",\"gene_col\":g,\"tissue_col\":t,\"value_col\":v})\n",
    "        return E, gene_names, sample_names, meta\n",
    "\n",
    "    if gene_cols:\n",
    "        g = gene_cols[0]\n",
    "        sub = df.copy().drop_duplicates(subset=[g]).set_index(g)\n",
    "        num = sub.select_dtypes(include=[np.number])\n",
    "        if num.shape[1]==0:\n",
    "            num = sub.apply(pd.to_numeric, errors=\"coerce\")\n",
    "        num = num.dropna(how=\"all\", axis=1)\n",
    "        E = num.to_numpy(dtype=float)\n",
    "        gene_names = [str(i) for i in num.index.to_list()]\n",
    "        sample_names = [str(c) for c in num.columns.to_list()]\n",
    "        meta.update({\"format\":\"wide\",\"gene_col\":g})\n",
    "        return E, gene_names, sample_names, meta\n",
    "\n",
    "    raise RuntimeError(\"No gene column — this should not happen in ATLAS-ONLY mode.\")\n",
    "\n",
    "def summarize_matrix(E, gene_names, sample_names, k_top=25):\n",
    "    X = E.copy()\n",
    "    if np.nanmin(X) < 0:\n",
    "        X = X - np.nanmin(X)\n",
    "    X = np.nan_to_num(X, nan=0.0)\n",
    "    var  = np.nanvar(X, axis=1)\n",
    "    mean = np.nanmean(X, axis=1) + 1e-12\n",
    "    cv   = np.sqrt(var) / mean\n",
    "\n",
    "    def gini(row, eps=1e-12):\n",
    "        r = np.asarray(row, dtype=float)\n",
    "        mn = np.nanmin(r)\n",
    "        if mn < 0: r = r - mn\n",
    "        r = np.nan_to_num(r, nan=0.0)\n",
    "        mu = r.mean() + eps\n",
    "        diff_sum = np.abs(r[:, None] - r[None, :]).mean()\n",
    "        return 0.5 * diff_sum / mu\n",
    "\n",
    "    def Hn_row(p, eps=1e-12):\n",
    "        p = np.clip(p, eps, None)\n",
    "        p = p / p.sum()\n",
    "        H = float(-(p * np.log(p)).sum())\n",
    "        return H / (np.log(X.shape[1]) if X.shape[1] > 1 else 1.0)\n",
    "\n",
    "    gini_v = np.array([gini(row) for row in X])\n",
    "    Hn     = np.array([Hn_row(row) for row in X])\n",
    "\n",
    "    idx_g = np.argsort(-gini_v)[:k_top]\n",
    "    idx_lo = np.argsort(Hn)[:k_top]\n",
    "    idx_hi = np.argsort(-Hn)[:k_top]\n",
    "\n",
    "    def take(idx): return [(gene_names[i], float(gini_v[i]), float(Hn[i]), float(cv[i]), float(mean[i])) for i in idx]\n",
    "\n",
    "    tops = {\n",
    "        \"top_gini\": take(idx_g),\n",
    "        \"top_specialized_low_entropy\": take(idx_lo),\n",
    "        \"top_housekeeping_high_entropy\": take(idx_hi),\n",
    "    }\n",
    "    summary = {\n",
    "        \"n_genes\": int(X.shape[0]),\n",
    "        \"n_samples\": int(X.shape[1]),\n",
    "        \"gini_mean\": float(np.nanmean(gini_v)),\n",
    "        \"gini_median\": float(np.nanmedian(gini_v)),\n",
    "        \"entropy_mean\": float(np.nanmean(Hn)),\n",
    "        \"entropy_median\": float(np.nanmedian(Hn)),\n",
    "        \"cv_mean\": float(np.nanmean(cv)),\n",
    "    }\n",
    "    per_gene = {\"var\": var.tolist(), \"mean\": mean.tolist(), \"cv\": cv.tolist(), \"gini\": gini_v.tolist(), \"H_norm\": Hn.tolist()}\n",
    "    return summary, per_gene, tops\n",
    "\n",
    "def to_csv(path, rows, header):\n",
    "    ensure_dir(Path(path).parent)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\",\".join(header) + \"\\n\")\n",
    "        for r in rows:\n",
    "            f.write(\",\".join(map(lambda x: str(x).replace(\",\",\";\"), r)) + \"\\n\")\n",
    "\n",
    "# ---------- Plots ------------------------------------------------------------\n",
    "def plot_hist(arr, path, title, xlabel):\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    ensure_dir(Path(path).parent)\n",
    "    plt.figure()\n",
    "    plt.hist([a for a in arr if not np.isnan(a)], bins=50)\n",
    "    plt.title(title); plt.xlabel(xlabel); plt.ylabel(\"Count\")\n",
    "    plt.tight_layout(); plt.savefig(path, dpi=150); plt.close()\n",
    "\n",
    "def plot_bar(items, path, title, ylabel):\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as _np\n",
    "    ensure_dir(Path(path).parent)\n",
    "    labels = [i[0] for i in items]; vals = [i[1] for i in items]\n",
    "    plt.figure(figsize=(10, max(3, 0.3*len(items))))\n",
    "    y = _np.arange(len(items))\n",
    "    plt.barh(y, vals); plt.yticks(y, labels)\n",
    "    plt.title(title); plt.xlabel(ylabel); plt.ylabel(\"Gene\")\n",
    "    plt.tight_layout(); plt.savefig(path, dpi=150, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "def plot_scatter(Y, path, title, xlabel=\"Dim 1\", ylabel=\"Dim 2\"):\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    ensure_dir(Path(path).parent)\n",
    "    plt.figure()\n",
    "    plt.scatter(Y[:,0], Y[:,1], s=12, alpha=0.8)\n",
    "    plt.title(title); plt.xlabel(xlabel); plt.ylabel(ylabel)\n",
    "    plt.tight_layout(); plt.savefig(path, dpi=150); plt.close()\n",
    "\n",
    "# ---------- PDF (Unicode-safe) -----------------------------------------------\n",
    "def write_pdf(report_md_path: Path, images, out_pdf: Path, title=\"3I Atlas Check-In\"):\n",
    "    if FPDF is None:\n",
    "        return False\n",
    "    try:\n",
    "        from fpdf.enums import XPos, YPos\n",
    "        HAVE_ENUMS = True\n",
    "    except Exception:\n",
    "        HAVE_ENUMS = False\n",
    "    REPL = {\"\\u2011\":\"-\",\"\\u2013\":\"-\",\"\\u2014\":\"-\",\"\\u2018\":\"'\",\"\\u2019\":\"'\",\"\\u201c\":'\"',\"\\u201d\":'\"',\"\\u2026\":\"...\"}\n",
    "    def ascii_fallback(s: str):\n",
    "        for k,v in REPL.items(): s = s.replace(k, v)\n",
    "        return s\n",
    "    ttf_candidates = [\n",
    "        r\"C:\\Windows\\Fonts\\arial.ttf\",\n",
    "        r\"C:\\Windows\\Fonts\\DejaVuSans.ttf\",\n",
    "        r\"C:\\Windows\\Fonts\\Calibri.ttf\",\n",
    "        r\"C:\\Windows\\Fonts\\segoeui.ttf\",\n",
    "    ]\n",
    "    pdf = FPDF(orientation=\"P\", unit=\"mm\", format=\"A4\")\n",
    "    pdf.set_auto_page_break(auto=True, margin=12); pdf.add_page()\n",
    "    used_unicode = False\n",
    "    for ttf in ttf_candidates:\n",
    "        if Path(ttf).exists():\n",
    "            try:\n",
    "                try: pdf.add_font(\"U\", \"\", ttf, uni=True)\n",
    "                except TypeError: pdf.add_font(\"U\", \"\", ttf)\n",
    "                pdf.set_font(\"U\", \"\", 16); used_unicode = True; break\n",
    "            except Exception:\n",
    "                pass\n",
    "    if not used_unicode: pdf.set_font(\"helvetica\", \"\", 16)\n",
    "    safe_title = title if used_unicode else ascii_fallback(title)\n",
    "    if HAVE_ENUMS: pdf.cell(0, 10, safe_title, new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
    "    else:          pdf.cell(0, 10, safe_title, ln=1)\n",
    "    pdf.set_font(\"U\" if used_unicode else \"helvetica\", \"\", 10)\n",
    "    with open(report_md_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip().startswith(\"!\"): continue\n",
    "            pdf.multi_cell(0, 5, line if used_unicode else ascii_fallback(line))\n",
    "    for img in images:\n",
    "        if img and Path(img).exists():\n",
    "            pdf.add_page(); pdf.image(str(img), x=10, y=20, w=180)\n",
    "            if HAVE_ENUMS: pdf.cell(0, 6, Path(img).name, new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
    "            else:          pdf.ln(6)\n",
    "    ensure_dir(Path(out_pdf).parent); pdf.output(str(out_pdf)); return True\n",
    "\n",
    "# ---------- Main --------------------------------------------------------------\n",
    "RUN_BASE = pick_run_base()\n",
    "STAMP = ts_utc(); RUN_DIR = ensure_dir(Path(RUN_BASE) / STAMP)\n",
    "\n",
    "print(f\"[{ts_local()}] 3I Atlas Check-In (ATLAS-ONLY v4) starting…\")\n",
    "print(f\"  Run dir: {RUN_DIR}\")\n",
    "\n",
    "# Discover candidate pack\n",
    "if PACK_DIR:\n",
    "    pack = normalize_pack_dir(Path(PACK_DIR))\n",
    "    print(f\"  PACK_DIR override: {pack}\")\n",
    "else:\n",
    "    candidates = []\n",
    "    def gather_candidates(root: Path):\n",
    "        pats = [\n",
    "            \"**/*3i*atlas*vector*embed*\",\n",
    "            \"**/*3i*atlas*embed*\",\n",
    "            \"**/*3i*atlas*\",\n",
    "            \"**/cnt_3i_atlas*\",\n",
    "            \"**/*3i*atlas*.csv\",\n",
    "        ]\n",
    "        for pat in pats:\n",
    "            for hit in root.glob(pat):\n",
    "                if \".ipynb_checkpoints\" in str(hit): continue\n",
    "                candidates.append(hit)\n",
    "    for r in all_existing(ROOT_HINTS):\n",
    "        print(f\"  Scanning: {r}\"); gather_candidates(r)\n",
    "    if not candidates:\n",
    "        raise SystemExit(\"No 3I Atlas candidates found. Set PACK_DIR to the pack root.\")\n",
    "    # de-dupe & normalize\n",
    "    uniq, seen = [], set()\n",
    "    for c in candidates:\n",
    "        c = normalize_pack_dir(c)\n",
    "        k = str(c).lower()\n",
    "        if k not in seen:\n",
    "            seen.add(k); uniq.append(c)\n",
    "    candidates = uniq\n",
    "    # choose the best candidate dir by presence of *data files*\n",
    "    def score_pack_dir(p: Path):\n",
    "        sc = 0\n",
    "        if p.is_dir(): sc += 3\n",
    "        s = str(p).lower()\n",
    "        if \"vector\" in s and \"embed\" in s: sc += 2\n",
    "        if \"cnt_3i_atlas_all\" in s: sc += 3\n",
    "        try: sc += min(6, len(list_datafiles(p)))\n",
    "        except Exception: pass\n",
    "        try: sc += int(p.stat().st_mtime // 3600) % 10\n",
    "        except Exception: pass\n",
    "        return sc\n",
    "    candidates.sort(key=score_pack_dir, reverse=True)\n",
    "    pack = candidates[0]\n",
    "print(f\"  Candidate pack: {pack}\")\n",
    "\n",
    "# Choose data file from pack (or its parent) with ATLAS-ONLY rank\n",
    "def choose_data_root(p: Path):\n",
    "    files = list_datafiles(p)\n",
    "    if files: return p, files\n",
    "    if p.parent and p.parent.exists():\n",
    "        files = list_datafiles(p.parent)\n",
    "        if files:\n",
    "            print(f\"  Recovery: using parent of candidate ({p.parent})\")\n",
    "            return p.parent, files\n",
    "    return p, []\n",
    "\n",
    "pack, data_files = choose_data_root(pack)\n",
    "if not data_files:\n",
    "    raise SystemExit(f\"No supported data files under {pack}. Set PACK_DIR to the pack root with out/ or data/.\")\n",
    "\n",
    "ranked = []\n",
    "for f in data_files:\n",
    "    score, note = file_rank_atlas_only(f)\n",
    "    ranked.append((score, note, f))\n",
    "ranked.sort(key=lambda x: x[0], reverse=True)\n",
    "chosen_score, chosen_note, chosen = ranked[0]\n",
    "if chosen_score < -9000:\n",
    "    # Fail explicitly with diagnostics\n",
    "    diag = \"\\n\".join([f\"  {sc:>6}  {nt:30}  {fp}\" for sc, nt, fp in ranked[:20]])\n",
    "    raise SystemExit(\"Atlas-only selection failed (no file met thresholds).\\nDiagnostics:\\n\" + diag)\n",
    "\n",
    "print(f\"  Using data file: {chosen}  [{chosen_note}, score={chosen_score}]\")\n",
    "\n",
    "# Load full table and infer matrix\n",
    "df_any = read_table_any(chosen, nrows=None)\n",
    "df = to_pandas(df_any)\n",
    "E, gene_names, sample_names, meta = infer_matrix(df)\n",
    "print(f\"  Inferred matrix: genes={len(gene_names)}, samples={len(sample_names)}  format={meta['format']}\")\n",
    "\n",
    "# Summaries & tops\n",
    "summary, per_gene, tops = summarize_matrix(E, gene_names, sample_names, k_top=25)\n",
    "\n",
    "# Outputs: CSVs\n",
    "to_csv(Path(RUN_DIR/\"top_gini_genes.csv\"), tops[\"top_gini\"], [\"gene\",\"gini\",\"H_norm\",\"cv\",\"mean\"])\n",
    "to_csv(Path(RUN_DIR/\"top_specialized_low_entropy.csv\"), tops[\"top_specialized_low_entropy\"], [\"gene\",\"gini\",\"H_norm\",\"cv\",\"mean\"])\n",
    "to_csv(Path(RUN_DIR/\"top_housekeeping_high_entropy.csv\"), tops[\"top_housekeeping_high_entropy\"], [\"gene\",\"gini\",\"H_norm\",\"cv\",\"mean\"])\n",
    "to_csv(Path(RUN_DIR/\"summary_stats.csv\"), [[k, v] for k, v in summary.items()], [\"metric\",\"value\"])\n",
    "\n",
    "# Plots\n",
    "plots = {}\n",
    "plot_hist(per_gene[\"gini\"], Path(RUN_DIR/\"plots/gini_hist.png\"), \"Gini distribution (gene specialization)\", \"Gini\")\n",
    "plots[\"gini_hist\"] = str(Path(RUN_DIR/\"plots/gini_hist.png\"))\n",
    "plot_hist(per_gene[\"H_norm\"], Path(RUN_DIR/\"plots/entropy_hist.png\"), \"Normalized entropy across samples\", \"H_norm\")\n",
    "plots[\"entropy_hist\"] = str(Path(RUN_DIR/\"plots/entropy_hist.png\"))\n",
    "plot_bar(tops[\"top_gini\"], Path(RUN_DIR/\"plots/top_gini_bar.png\"), \"Top specialized genes (by Gini)\", \"Gini\")\n",
    "plots[\"top_gini_bar\"] = str(Path(RUN_DIR/\"plots/top_gini_bar.png\"))\n",
    "\n",
    "# Embeddings (optional)\n",
    "def try_pca(E, n=2, random_state=42):\n",
    "    if PCA is None: return None, None\n",
    "    X = np.nan_to_num(E, nan=0.0)\n",
    "    X = X - X.mean(axis=1, keepdims=True)\n",
    "    pca = PCA(n_components=min(n, min(X.shape)-1), random_state=random_state)\n",
    "    try:\n",
    "        Y = pca.fit_transform(X.T)\n",
    "        return Y, pca.explained_variance_ratio_.tolist()\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "def try_umap(E, n=2, random_state=42):\n",
    "    if umap is None: return None\n",
    "    X = np.nan_to_num(E, nan=0.0); X = X - X.mean(axis=1, keepdims=True)\n",
    "    try: return umap.UMAP(n_components=n, random_state=random_state).fit_transform(X.T)\n",
    "    except Exception: return None\n",
    "\n",
    "pca_pts, pca_var = try_pca(E, n=2, random_state=42)\n",
    "if pca_pts is not None:\n",
    "    plot_scatter(pca_pts, Path(RUN_DIR/\"plots/pca_scatter.png\"),\n",
    "                 f\"PCA on samples (var={sum(pca_var):.2%})\", \"PC1\", \"PC2\")\n",
    "    plots[\"pca_scatter\"] = str(Path(RUN_DIR/\"plots/pca_scatter.png\"))\n",
    "else:\n",
    "    print(\"  PCA not available or failed; skipping PCA plot.\")\n",
    "\n",
    "umap_pts = try_umap(E, n=2, random_state=42)\n",
    "if umap_pts is not None:\n",
    "    plot_scatter(umap_pts, Path(RUN_DIR/\"plots/umap_scatter.png\"),\n",
    "                 \"UMAP on samples\", \"UMAP-1\", \"UMAP-2\")\n",
    "    plots[\"umap_scatter\"] = str(Path(RUN_DIR/\"plots/umap_scatter.png\"))\n",
    "\n",
    "# Snapshot & deltas\n",
    "def read_json(path: Path):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f: return json.load(f)\n",
    "    except Exception: return None\n",
    "\n",
    "def write_json(path: Path, obj):\n",
    "    ensure_dir(Path(path).parent)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f: json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def last_snapshot(dir_base: Path):\n",
    "    files = glob.glob(str(dir_base / \"*\" / \"snapshot.json\"))\n",
    "    if not files: return None, None\n",
    "    files.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "    path = Path(files[0]); return path, read_json(path)\n",
    "\n",
    "SNAPSHOT_PATH = Path(RUN_DIR/\"snapshot.json\")\n",
    "prev_path, prev = last_snapshot(Path(RUN_BASE))\n",
    "deltas = None\n",
    "if prev:\n",
    "    deltas = {\n",
    "        \"n_genes_delta\": summary[\"n_genes\"] - int(prev.get(\"summary\",{}).get(\"n_genes\", 0)),\n",
    "        \"n_samples_delta\": summary[\"n_samples\"] - int(prev.get(\"summary\",{}).get(\"n_samples\", 0)),\n",
    "        \"gini_mean_delta\": summary[\"gini_mean\"] - float(prev.get(\"summary\",{}).get(\"gini_mean\", 0.0)),\n",
    "        \"entropy_mean_delta\": summary[\"entropy_mean\"] - float(prev.get(\"summary\",{}).get(\"entropy_mean\", 0.0)),\n",
    "        \"cv_mean_delta\": summary[\"cv_mean\"] - float(prev.get(\"summary\",{}).get(\"cv_mean\", 0.0)),\n",
    "        \"changed_samples\": False, \"added_samples\": [], \"removed_samples\": [],\n",
    "    }\n",
    "    try:\n",
    "        prev_samples = set(prev.get(\"sample_names\", [])); cur_samples = set(sample_names)\n",
    "        add = sorted(cur_samples - prev_samples); rem = sorted(prev_samples - cur_samples)\n",
    "        if add or rem:\n",
    "            deltas[\"changed_samples\"] = True\n",
    "            deltas[\"added_samples\"] = add; deltas[\"removed_samples\"] = rem\n",
    "    except Exception: pass\n",
    "    write_json(Path(RUN_DIR/\"delta_summary.json\"), deltas)\n",
    "    print(f\"  Δ written: {Path(RUN_DIR/'delta_summary.json')}\")\n",
    "else:\n",
    "    print(\"  No prior snapshot found; this will serve as the baseline.\")\n",
    "\n",
    "snapshot = {\n",
    "    \"meta\": {\n",
    "        \"stamp_utc\": ts_utc(), \"stamp_local\": ts_local(),\n",
    "        \"host\": platform.node(), \"python\": sys.version.split()[0],\n",
    "        \"pack_dir\": str(pack), \"data_file\": str(chosen),\n",
    "    },\n",
    "    \"summary\": summary, \"sample_names\": sample_names[:5000],\n",
    "    \"top_gini\": tops[\"top_gini\"],\n",
    "    \"top_housekeeping_high_entropy\": tops[\"top_housekeeping_high_entropy\"],\n",
    "}\n",
    "write_json(SNAPSHOT_PATH, snapshot)\n",
    "\n",
    "# Markdown report\n",
    "def write_report_md(path: Path, info):\n",
    "    ensure_dir(Path(path).parent)\n",
    "    L = []\n",
    "    L.append(f\"# 3I Atlas Check-In — {info['meta']['stamp_local']}\"); L.append(\"\")\n",
    "    L.append(f\"- **Pack**: `{info['meta']['pack']}`\")\n",
    "    L.append(f\"- **Run dir**: `{info['meta']['run_dir']}`\")\n",
    "    L.append(f\"- **Rows (genes)**: **{info['summary']['n_genes']}**, **Samples**: **{info['summary']['n_samples']}**\")\n",
    "    L.append(f\"- Gini (mean/median): **{info['summary']['gini_mean']:.4f} / {info['summary']['gini_median']:.4f}**\")\n",
    "    L.append(f\"- Entropy_n (mean/median): **{info['summary']['entropy_mean']:.4f} / {info['summary']['entropy_median']:.4f}**\")\n",
    "    L.append(f\"- CV (mean): **{info['summary']['cv_mean']:.4f}**\")\n",
    "    L.append(\"\")\n",
    "    for key in (\"gini_hist\",\"entropy_hist\",\"top_gini_bar\",\"pca_scatter\",\"umap_scatter\"):\n",
    "        p = info[\"plots\"].get(key)\n",
    "        if p:\n",
    "            L.append(f\"![{key}]({Path(p).name})\")\n",
    "    L.append(\"\")\n",
    "    L.append(\"## Top specialized (by Gini) — preview\")\n",
    "    for (name,g,h,cv,mu) in info[\"tops\"][\"top_gini\"][:10]:\n",
    "        L.append(f\"- {name}: Gini={g:.4f}, H_n={h:.4f}, CV={cv:.3f}, mean={mu:.3g}\")\n",
    "    L.append(\"\")\n",
    "    L.append(\"## Top housekeeping (high normalized entropy) — preview\")\n",
    "    for (name,g,h,cv,mu) in info[\"tops\"][\"top_housekeeping_high_entropy\"][:10]:\n",
    "        L.append(f\"- {name}: H_n={h:.4f}, Gini={g:.4f}, CV={cv:.3f}, mean={mu:.3g}\")\n",
    "    L.append(\"\")\n",
    "    if info.get(\"deltas\"):\n",
    "        d = info[\"deltas\"]\n",
    "        L.append(\"## Delta vs last snapshot\")\n",
    "        L.append(f\"- Genes: **{d.get('n_genes_delta',0):+d}**, Samples: **{d.get('n_samples_delta',0):+d}**\")\n",
    "        if \"gini_mean_delta\" in d:\n",
    "            L.append(f\"- Δ Gini mean: **{d['gini_mean_delta']:+.4f}**, Δ Entropy_n mean: **{d.get('entropy_mean_delta',0):+.4f}**\")\n",
    "        if d.get(\"changed_samples\"):\n",
    "            L.append(f\"- Changed sample set: +{len(d['added_samples'])} / -{len(d['removed_samples'])}\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(L))\n",
    "\n",
    "info = {\"meta\": {\"stamp_local\": ts_local(), \"pack\": str(pack), \"run_dir\": str(RUN_DIR)},\n",
    "        \"summary\": summary, \"tops\": tops, \"deltas\": deltas, \"plots\": plots}\n",
    "REPORT_MD = Path(RUN_DIR/\"report.md\"); write_report_md(REPORT_MD, info); print(f\"  Wrote: {REPORT_MD}\")\n",
    "\n",
    "# Atlas facts (concise, human-readable NEW information)\n",
    "def write_atlas_facts(path: Path, summary, tops, sample_names):\n",
    "    ensure_dir(Path(path).parent)\n",
    "    lines = []\n",
    "    lines.append(\"# Atlas Facts (new)\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"- Samples detected: **{len(sample_names)}** → {', '.join(map(str, sample_names[:12]))}{' …' if len(sample_names)>12 else ''}\")\n",
    "    lines.append(f\"- Genes: **{summary['n_genes']}**\")\n",
    "    lines.append(f\"- Specialization: mean Gini **{summary['gini_mean']:.4f}** (median **{summary['gini_median']:.4f}**)\")\n",
    "    lines.append(f\"- Ubiquity: mean normalized entropy **{summary['entropy_mean']:.4f}** (median **{summary['entropy_median']:.4f}**)\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Top 10 specialized genes (highest Gini)\")\n",
    "    for (name,g,h,cv,mu) in tops[\"top_gini\"][:10]:\n",
    "        lines.append(f\"- {name}: Gini={g:.4f}, H_n={h:.4f}, CV={cv:.3f}, mean={mu:.3g}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Top 10 housekeeping genes (highest H_n)\")\n",
    "    for (name,g,h,cv,mu) in tops[\"top_housekeeping_high_entropy\"][:10]:\n",
    "        lines.append(f\"- {name}: H_n={h:.4f}, Gini={g:.4f}, CV={cv:.3f}, mean={mu:.3g}\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "write_atlas_facts(Path(RUN_DIR/\"atlas_facts.md\"), summary, tops, sample_names)\n",
    "print(f\"  Wrote: {RUN_DIR/'atlas_facts.md'}\")\n",
    "\n",
    "# PDF\n",
    "REPORT_PDF = Path(RUN_DIR/\"report.pdf\")\n",
    "ok_pdf = write_pdf(REPORT_MD,\n",
    "                   images=[plots.get(\"gini_hist\"), plots.get(\"entropy_hist\"),\n",
    "                           plots.get(\"top_gini_bar\"), plots.get(\"pca_scatter\"), plots.get(\"umap_scatter\")],\n",
    "                   out_pdf=REPORT_PDF, title=\"3I Atlas Check-In\")\n",
    "print(f\"  PDF:   {REPORT_PDF if ok_pdf else '(skipped; fpdf missing)'}\")\n",
    "\n",
    "print(f\"[{ts_local()}] Done. — ATLAS-ONLY facts produced.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89f02f63-2492-45f8-8fb7-ee855c6b5b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-29 02:48:17] 3I Atlas Check-In (ATLAS SEEKER v5) starting…\n",
      "  Run dir: C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_runs\\3i_atlas_checkin\\20251029-064817Z\n",
      "  Scanning: C:\\Users\\caleb\\CNT_Lab\n",
      "  Scanning: E:\\CNT\n",
      "  Scanning: E:\\CNT\\notebooks\\archive\n",
      "  Scanning: E:\\CNT\\notebooks\\archive\n",
      "  Strict pass found no qualifying atlas. Relaxing thresholds…\n",
      "  Using data file: C:\\Users\\caleb\\CNT_Lab\\artifacts\\tables\\genome3d__atlas__thread_edges_v1s_full__20251008-232719.parquet  [ok(ns=7,ng=78666,tidy=False), score=176]\n",
      "  Inferred matrix: genes=78666, samples=7  format=wide\n",
      "  No prior snapshot found; this will serve as the baseline.\n",
      "  Wrote: C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_runs\\3i_atlas_checkin\\20251029-064817Z\\report.md\n",
      "  Wrote: C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_runs\\3i_atlas_checkin\\20251029-064817Z\\atlas_facts.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2236\\4271284165.py:426: DeprecationWarning: \"uni\" parameter is deprecated since v2.5.1, unused and will soon be removed\n",
      "  try: pdf.add_font(\"U\", \"\", ttf, uni=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  PDF:   C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_runs\\3i_atlas_checkin\\20251029-064817Z\\report.pdf\n",
      "[2025-10-29 02:48:49] Done. — ATLAS facts produced (v5).\n"
     ]
    }
   ],
   "source": [
    "# === CNT \"3I Atlas\" — ATLAS SEEKER (single cell, v5) =========================\n",
    "# Goal: Finally obtain NEW gene-atlas facts by finding a real gene matrix\n",
    "#       anywhere under your CNT roots — not NOAA, not lightcurves.\n",
    "# Outputs (<RUN_BASE>\\<STAMP>\\):\n",
    "#   report.md, report.pdf (Unicode-safe), atlas_facts.md\n",
    "#   summary_stats.csv, top_gini_genes.csv, top_specialized_low_entropy.csv, top_housekeeping_high_entropy.csv\n",
    "#   delta_summary.json (if prior), snapshot.json, plots/*.png\n",
    "# =============================================================================\n",
    "\n",
    "import os, re, sys, json, glob, platform\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# ---------------- Config ------------------------------------------------------\n",
    "# Optional: hard-lock if you know the exact atlas pack dir\n",
    "PACK_DIR = None\n",
    "# Example:\n",
    "# PACK_DIR = r\"C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a_vector_embedding\"\n",
    "\n",
    "ROOT_HINTS = [\n",
    "    r\"C:\\Users\\caleb\\CNT_Lab\",\n",
    "    r\"E:\\CNT\",\n",
    "    r\"E:\\CNT\\notebooks\\archive\",\n",
    "    r\"D:\\CNT\",\n",
    "    r\"C:\\CNT\",\n",
    "    str(Path.cwd()),\n",
    "]\n",
    "\n",
    "RUN_BASE_HINTS = [\n",
    "    r\"C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_runs\\3i_atlas_checkin\",\n",
    "    r\"E:\\CNT\\notebooks\\archive\\cnt_runs\\3i_atlas_checkin\",\n",
    "    str(Path.cwd() / \"cnt_runs\" / \"3i_atlas_checkin\"),\n",
    "]\n",
    "\n",
    "# Selection thresholds (relax pass will soften these if needed)\n",
    "MIN_SAMPLES_STRICT = 10\n",
    "MIN_GENES_STRICT   = 1000\n",
    "MIN_SAMPLES_RELAX  = 3\n",
    "MIN_GENES_RELAX    = 300\n",
    "SNIFF_ROWS         = 2000\n",
    "\n",
    "# Filename cues\n",
    "INCLUDE_PATTERNS = [\"atlas\", \"gene\", \"expr\", \"tpm\", \"fpkm\", \"counts\", \"matrix\"]\n",
    "EXCLUDE_PATTERNS = [\"noaa\", \"mag\", \"geomag\", \"weather\", \"storm\", \"wind\", \"met\", \"lightcurve\", \"plasma\", \"gra_\"]\n",
    "\n",
    "# ---------------- Optional deps ----------------------------------------------\n",
    "USE_POLARS = False\n",
    "try:\n",
    "    import pandas as pd\n",
    "except Exception:\n",
    "    pd = None\n",
    "\n",
    "if USE_POLARS:\n",
    "    try:\n",
    "        import polars as pl\n",
    "    except Exception:\n",
    "        USE_POLARS = False\n",
    "\n",
    "try:\n",
    "    from sklearn.decomposition import PCA\n",
    "except Exception:\n",
    "    PCA = None\n",
    "\n",
    "try:\n",
    "    import umap\n",
    "except Exception:\n",
    "    umap = None\n",
    "\n",
    "try:\n",
    "    from fpdf import FPDF\n",
    "except Exception:\n",
    "    FPDF = None\n",
    "\n",
    "# ---------------- Utilities ---------------------------------------------------\n",
    "def ts_utc(): return datetime.now(timezone.utc).strftime(\"%Y%m%d-%H%M%SZ\")\n",
    "def ts_local(): return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "def ensure_dir(p: Path): p.mkdir(parents=True, exist_ok=True); return p\n",
    "\n",
    "def pick_run_base():\n",
    "    for p in RUN_BASE_HINTS:\n",
    "        path = Path(p)\n",
    "        try:\n",
    "            path.mkdir(parents=True, exist_ok=True)\n",
    "            return str(path)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return str(Path.cwd() / \"cnt_runs\" / \"3i_atlas_checkin\")\n",
    "\n",
    "def normalize_pack_dir(p: Path) -> Path:\n",
    "    parts = list(p.parts)\n",
    "    if len(parts) >= 2 and parts[-1].lower() == \"vector_embedding\" and parts[-2].lower() == \"vector_embedding\":\n",
    "        return Path(*parts[:-1])\n",
    "    name = p.name.lower()\n",
    "    if name.endswith(\"_vector_embedding_vector_embedding\"):\n",
    "        return p.with_name(p.name[: -len(\"_vector_embedding\")])\n",
    "    return p\n",
    "\n",
    "def all_existing(paths): return [Path(p) for p in paths if Path(p).exists()]\n",
    "\n",
    "ALLOWED_EXT = [\".csv\", \".tsv\", \".parquet\", \".feather\", \".npz\", \".npy\"]\n",
    "\n",
    "def list_datafiles(root: Path):\n",
    "    \"\"\"Return candidate data files under root (recursive, limited by ALLOWED_EXT).\"\"\"\n",
    "    hits = []\n",
    "    for ext in ALLOWED_EXT:\n",
    "        for key in (INCLUDE_PATTERNS + [\"cnt_3i_atlas\"]):\n",
    "            pat = str(root / \"**\" / f\"*{key}*{ext}\")\n",
    "            hits.extend([Path(p) for p in glob.glob(pat, recursive=True)])\n",
    "    # Dedup & filter files only\n",
    "    z = []\n",
    "    seen = set()\n",
    "    for h in hits:\n",
    "        if h.is_file():\n",
    "            k = str(h.resolve()).lower()\n",
    "            if k not in seen:\n",
    "                seen.add(k)\n",
    "                z.append(h)\n",
    "    # Prefer larger files\n",
    "    z.sort(key=lambda p: p.stat().st_size if p.exists() else 0, reverse=True)\n",
    "    return z\n",
    "\n",
    "def read_table_any(path: Path, nrows=None):\n",
    "    suff = path.suffix.lower()\n",
    "    if USE_POLARS:\n",
    "        if 'pl' not in globals():\n",
    "            raise RuntimeError(\"Polars not available; set USE_POLARS=False or install polars\")\n",
    "        if suff in (\".csv\", \".tsv\"):\n",
    "            sep = \",\" if suff == \".csv\" else \"\\t\"\n",
    "            df = pl.read_csv(str(path), separator=sep)\n",
    "            return df if nrows is None else df.head(nrows)\n",
    "        elif suff == \".parquet\":\n",
    "            df = pl.read_parquet(str(path))\n",
    "            return df if nrows is None else df.head(nrows)\n",
    "        elif suff == \".feather\":\n",
    "            df = pl.read_ipc(str(path))\n",
    "            return df if nrows is None else df.head(nrows)\n",
    "        elif suff in (\".npz\", \".npy\"):\n",
    "            arr = np.load(str(path))\n",
    "            if isinstance(arr, np.lib.npyio.NpzFile):\n",
    "                key = next(iter(arr.files)); arr = arr[key]\n",
    "            if arr.ndim == 2:\n",
    "                df = pl.DataFrame(arr)\n",
    "                df = df.with_columns(pl.Series(\"gene\", [f\"g{i}\" for i in range(arr.shape[0])]))\n",
    "                df = df.select([\"gene\"] + [c for c in df.columns if c != \"gene\"])\n",
    "                return df if nrows is None else df.head(nrows)\n",
    "            raise RuntimeError(f\"Unsupported NPZ/NPY shape in {path}: {arr.shape}\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unsupported file type: {suff}\")\n",
    "    else:\n",
    "        if pd is None:\n",
    "            raise RuntimeError(\"pandas not available; install pandas or set USE_POLARS=True\")\n",
    "        if suff in (\".csv\", \".tsv\"):\n",
    "            sep = \",\" if suff == \".csv\" else \"\\t\"\n",
    "            return pd.read_csv(path, nrows=nrows, sep=sep)\n",
    "        elif suff == \".parquet\":\n",
    "            return pd.read_parquet(path)\n",
    "        elif suff == \".feather\":\n",
    "            return pd.read_feather(path)\n",
    "        elif suff in (\".npz\", \".npy\"):\n",
    "            arr = np.load(str(path))\n",
    "            if isinstance(arr, np.lib.npyio.NpzFile):\n",
    "                key = next(iter(arr.files)); arr = arr[key]\n",
    "            if arr.ndim == 2:\n",
    "                cols = [f\"col_{j}\" for j in range(arr.shape[1])]\n",
    "                df = pd.DataFrame(arr, columns=cols)\n",
    "                df.insert(0, \"gene\", [f\"g{i}\" for i in range(arr.shape[0])])\n",
    "                return df if nrows is None else df.head(nrows)\n",
    "            raise RuntimeError(f\"Unsupported NPZ/NPY shape in {path}: {arr.shape}\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"Unsupported file type: {suff}\")\n",
    "\n",
    "def to_pandas(df):\n",
    "    if pd is None: raise RuntimeError(\"pandas not available\")\n",
    "    if USE_POLARS: return df.to_pandas()\n",
    "    return df\n",
    "\n",
    "# ---------------- Candidate sniffing -----------------------------------------\n",
    "GENE_COLS    = {\"gene\",\"gene_id\",\"gene_name\",\"symbol\",\"ensembl\",\"ensembl_id\",\"id\"}\n",
    "SAMPLE_COLS  = {\"tissue\",\"organ\",\"celltype\",\"cell_type\",\"sample\",\"sample_id\"}\n",
    "TIMEY_NAMES  = {\"time\",\"timestamp\",\"date\",\"datetime\",\"freq\",\"frequency\",\"wavelength\",\"phase\",\"index\"}\n",
    "\n",
    "def looks_like_gene_index(series):\n",
    "    \"\"\"Heuristic: first column is mostly non-numeric, contains letters, and isn't a 'timey' field.\"\"\"\n",
    "    name = str(series.name).lower()\n",
    "    if name in TIMEY_NAMES:\n",
    "        return False\n",
    "    s = series.astype(str).head(200)\n",
    "    if s.empty:\n",
    "        return False\n",
    "    has_letters = s.str.contains(r\"[A-Za-z]\", regex=True, na=False).mean()\n",
    "    is_numeric  = s.str.fullmatch(r\"\\s*[+-]?\\d+(\\.\\d+)?\\s*\", na=False).mean()\n",
    "    return (has_letters >= 0.5) and (is_numeric <= 0.3)\n",
    "\n",
    "def sniff_file(path: Path):\n",
    "    \"\"\"Return (has_gene, has_sample_col, ns_est, ng_est, tidy_ok, info_note)\"\"\"\n",
    "    base = path.name.lower()\n",
    "    if any(w in base for w in EXCLUDE_PATTERNS):\n",
    "        return False, False, 0, 0, False, \"blocked_by_name\"\n",
    "\n",
    "    try:\n",
    "        df = to_pandas(read_table_any(path, nrows=SNIFF_ROWS))\n",
    "    except Exception as e:\n",
    "        return False, False, 0, 0, False, f\"read_fail:{e.__class__.__name__}\"\n",
    "\n",
    "    cols = [str(c).lower() for c in df.columns]\n",
    "    has_gene_explicit  = any(c in GENE_COLS for c in cols)\n",
    "    has_sample_explicit= any(c in SAMPLE_COLS for c in cols)\n",
    "\n",
    "    has_gene = has_gene_explicit\n",
    "    tidy_ok  = False\n",
    "    ns_est   = 0\n",
    "    ng_est   = 0\n",
    "\n",
    "    if has_gene_explicit and has_sample_explicit:\n",
    "        tidy_ok = True\n",
    "        gcol = next(c for c in df.columns if str(c).lower() in GENE_COLS)\n",
    "        scol = next(c for c in df.columns if str(c).lower() in SAMPLE_COLS)\n",
    "        try:\n",
    "            ns_est = int(df[scol].nunique())\n",
    "            ng_est = int(df[gcol].nunique())\n",
    "        except Exception:\n",
    "            ns_est = 0; ng_est = 0\n",
    "\n",
    "    elif has_gene_explicit:\n",
    "        gcol = next(c for c in df.columns if str(c).lower() in GENE_COLS)\n",
    "        sub = df.copy().drop_duplicates(subset=[gcol]).set_index(gcol)\n",
    "        num = sub.select_dtypes(include=[np.number])\n",
    "        if num.shape[1] == 0:\n",
    "            num = sub.apply(pd.to_numeric, errors=\"coerce\")\n",
    "        num = num.dropna(how=\"all\", axis=1)\n",
    "        ns_est = int(num.shape[1])\n",
    "        ng_est = int(num.shape[0])\n",
    "\n",
    "    else:\n",
    "        # Fallback: treat first column as gene index if it looks like gene IDs\n",
    "        first = df.columns[0]\n",
    "        if looks_like_gene_index(df[first]):\n",
    "            has_gene = True\n",
    "            sub = df.copy().drop_duplicates(subset=[first]).set_index(first)\n",
    "            num = sub.select_dtypes(include=[np.number])\n",
    "            if num.shape[1] == 0:\n",
    "                num = sub.apply(pd.to_numeric, errors=\"coerce\")\n",
    "            num = num.dropna(how=\"all\", axis=1)\n",
    "            ns_est = int(num.shape[1])\n",
    "            ng_est = int(num.shape[0])\n",
    "        else:\n",
    "            return False, False, 0, 0, False, \"no_gene_signal\"\n",
    "\n",
    "    return has_gene, has_sample_explicit, ns_est, ng_est, tidy_ok, \"ok\"\n",
    "\n",
    "def rank_file_for_atlas(path: Path, min_samples: int, min_genes: int):\n",
    "    has_gene, has_sample_col, ns, ng, tidy_ok, note = sniff_file(path)\n",
    "    if not has_gene:\n",
    "        return -9999, f\"reject:{note}\"\n",
    "    if ns < min_samples or ng < min_genes:\n",
    "        return -9000 + min(5, ns//2) + min(5, ng//200), f\"too_small(ns={ns},ng={ng})\"\n",
    "    inc_bonus = sum(1 for w in INCLUDE_PATTERNS if w in path.name.lower())\n",
    "    tidy_bonus = 2 if tidy_ok else 0\n",
    "    fmt_score = {\".csv\":3, \".tsv\":3, \".parquet\":2, \".feather\":2, \".npz\":1, \".npy\":1}.get(path.suffix.lower(), 0)\n",
    "    size_score = min(6, int(path.stat().st_size/1_000_000))\n",
    "    return 10 + inc_bonus + tidy_bonus + fmt_score + size_score + ns//5 + ng//500, f\"ok(ns={ns},ng={ng},tidy={tidy_ok})\"\n",
    "\n",
    "# ---------------- Matrix inference & stats -----------------------------------\n",
    "def infer_matrix(df):\n",
    "    meta = {\"format\": None, \"value_col\": None, \"gene_col\": None, \"tissue_col\": None}\n",
    "    cols_l = [str(c).lower() for c in df.columns]\n",
    "    gene_cols   = [c for c in df.columns if str(c).lower() in GENE_COLS]\n",
    "    sample_cols = [c for c in df.columns if str(c).lower() in SAMPLE_COLS]\n",
    "    val_keys    = (\"value\",\"expression\",\"expr\",\"count\",\"tpms\",\"tpm\",\"fpkm\",\"reads\",\"abundance\",\"intensity\")\n",
    "    value_cols  = [c for c in df.columns if str(c).lower() in val_keys]\n",
    "    emb_like    = [c for c in df.columns if re.match(r\"(emb(ed(ding)?)?_?\\d+)$\", str(c).lower())]\n",
    "\n",
    "    if gene_cols and sample_cols and (value_cols or emb_like):\n",
    "        g = gene_cols[0]; t = sample_cols[0]; v = (value_cols[0] if value_cols else emb_like[0])\n",
    "        pivot = df.pivot_table(index=g, columns=t, values=v, aggfunc=\"mean\").sort_index()\n",
    "        E = pivot.to_numpy(dtype=float)\n",
    "        genes = pivot.index.astype(str).to_list()\n",
    "        samples = [str(c) for c in pivot.columns.to_list()]\n",
    "        meta.update({\"format\":\"long/tidy\",\"gene_col\":g,\"tissue_col\":t,\"value_col\":v})\n",
    "        return E, genes, samples, meta\n",
    "\n",
    "    if gene_cols:\n",
    "        g = gene_cols[0]\n",
    "        sub = df.copy().drop_duplicates(subset=[g]).set_index(g)\n",
    "        num = sub.select_dtypes(include=[np.number])\n",
    "        if num.shape[1] == 0:\n",
    "            num = sub.apply(pd.to_numeric, errors=\"coerce\")\n",
    "        num = num.dropna(how=\"all\", axis=1)\n",
    "        E = num.to_numpy(dtype=float)\n",
    "        genes = [str(i) for i in num.index.to_list()]\n",
    "        samples = [str(c) for c in num.columns.to_list()]\n",
    "        meta.update({\"format\":\"wide\",\"gene_col\":g})\n",
    "        return E, genes, samples, meta\n",
    "\n",
    "    # Fallback: use first column as gene identifier\n",
    "    first = df.columns[0]\n",
    "    sub = df.copy().drop_duplicates(subset=[first]).set_index(first)\n",
    "    num = sub.select_dtypes(include=[np.number])\n",
    "    if num.shape[1] == 0:\n",
    "        num = sub.apply(pd.to_numeric, errors=\"coerce\")\n",
    "    num = num.dropna(how=\"all\", axis=1)\n",
    "    E = num.to_numpy(dtype=float)\n",
    "    genes = [str(i) for i in num.index.to_list()]\n",
    "    samples = [str(c) for c in num.columns.to_list()]\n",
    "    meta.update({\"format\":\"wide/fallback\",\"gene_col\":str(first)})\n",
    "    return E, genes, samples, meta\n",
    "\n",
    "def summarize_matrix(E, gene_names, sample_names, k_top=25):\n",
    "    X = E.copy()\n",
    "    if np.nanmin(X) < 0:\n",
    "        X = X - np.nanmin(X)\n",
    "    X = np.nan_to_num(X, nan=0.0)\n",
    "\n",
    "    var  = np.nanvar(X, axis=1)\n",
    "    mean = np.nanmean(X, axis=1) + 1e-12\n",
    "    cv   = np.sqrt(var) / mean\n",
    "\n",
    "    def gini(row, eps=1e-12):\n",
    "        r = np.asarray(row, dtype=float)\n",
    "        mn = np.nanmin(r)\n",
    "        if mn < 0: r = r - mn\n",
    "        r = np.nan_to_num(r, nan=0.0)\n",
    "        mu = r.mean() + eps\n",
    "        diff_sum = np.abs(r[:, None] - r[None, :]).mean()\n",
    "        return 0.5 * diff_sum / mu\n",
    "\n",
    "    def Hn_row(p, eps=1e-12):\n",
    "        p = np.clip(p, eps, None)\n",
    "        p = p / p.sum()\n",
    "        H = float(-(p * np.log(p)).sum())\n",
    "        return H / (np.log(X.shape[1]) if X.shape[1] > 1 else 1.0)\n",
    "\n",
    "    gini_v = np.array([gini(row) for row in X])\n",
    "    Hn     = np.array([Hn_row(row) for row in X])\n",
    "\n",
    "    idx_g  = np.argsort(-gini_v)[:k_top]\n",
    "    idx_lo = np.argsort(Hn)[:k_top]\n",
    "    idx_hi = np.argsort(-Hn)[:k_top]\n",
    "\n",
    "    def take(idx): return [(gene_names[i], float(gini_v[i]), float(Hn[i]), float(cv[i]), float(mean[i])) for i in idx]\n",
    "\n",
    "    tops = {\n",
    "        \"top_gini\": take(idx_g),\n",
    "        \"top_specialized_low_entropy\": take(idx_lo),\n",
    "        \"top_housekeeping_high_entropy\": take(idx_hi),\n",
    "    }\n",
    "    summary = {\n",
    "        \"n_genes\": int(X.shape[0]),\n",
    "        \"n_samples\": int(X.shape[1]),\n",
    "        \"gini_mean\": float(np.nanmean(gini_v)),\n",
    "        \"gini_median\": float(np.nanmedian(gini_v)),\n",
    "        \"entropy_mean\": float(np.nanmean(Hn)),\n",
    "        \"entropy_median\": float(np.nanmedian(Hn)),\n",
    "        \"cv_mean\": float(np.nanmean(cv)),\n",
    "    }\n",
    "    per_gene = {\"var\": var.tolist(), \"mean\": mean.tolist(), \"cv\": cv.tolist(), \"gini\": gini_v.tolist(), \"H_norm\": Hn.tolist()}\n",
    "    return summary, per_gene, tops\n",
    "\n",
    "def to_csv(path, rows, header):\n",
    "    ensure_dir(Path(path).parent)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\",\".join(header) + \"\\n\")\n",
    "        for r in rows:\n",
    "            f.write(\",\".join(map(lambda x: str(x).replace(\",\",\";\"), r)) + \"\\n\")\n",
    "\n",
    "# ---------------- Plots -------------------------------------------------------\n",
    "def plot_hist(arr, path, title, xlabel):\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    ensure_dir(Path(path).parent)\n",
    "    plt.figure()\n",
    "    plt.hist([a for a in arr if not np.isnan(a)], bins=50)\n",
    "    plt.title(title); plt.xlabel(xlabel); plt.ylabel(\"Count\")\n",
    "    plt.tight_layout(); plt.savefig(path, dpi=150); plt.close()\n",
    "\n",
    "def plot_bar(items, path, title, ylabel):\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as _np\n",
    "    ensure_dir(Path(path).parent)\n",
    "    labels = [i[0] for i in items]; vals = [i[1] for i in items]\n",
    "    plt.figure(figsize=(10, max(3, 0.3*len(items))))\n",
    "    y = _np.arange(len(items))\n",
    "    plt.barh(y, vals); plt.yticks(y, labels)\n",
    "    plt.title(title); plt.xlabel(ylabel); plt.ylabel(\"Gene\")\n",
    "    plt.tight_layout(); plt.savefig(path, dpi=150, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "def plot_scatter(Y, path, title, xlabel=\"Dim 1\", ylabel=\"Dim 2\"):\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    ensure_dir(Path(path).parent)\n",
    "    plt.figure()\n",
    "    plt.scatter(Y[:,0], Y[:,1], s=12, alpha=0.8)\n",
    "    plt.title(title); plt.xlabel(xlabel); plt.ylabel(ylabel)\n",
    "    plt.tight_layout(); plt.savefig(path, dpi=150); plt.close()\n",
    "\n",
    "# ---------------- PDF (Unicode-safe) -----------------------------------------\n",
    "def write_pdf(report_md_path: Path, images, out_pdf: Path, title=\"3I Atlas Check-In\"):\n",
    "    if FPDF is None:\n",
    "        return False\n",
    "    try:\n",
    "        from fpdf.enums import XPos, YPos\n",
    "        HAVE_ENUMS = True\n",
    "    except Exception:\n",
    "        HAVE_ENUMS = False\n",
    "    REPL = {\"\\u2011\":\"-\",\"\\u2013\":\"-\",\"\\u2014\":\"-\",\"\\u2018\":\"'\",\"\\u2019\":\"'\",\"\\u201c\":'\"',\"\\u201d\":'\"',\"\\u2026\":\"...\"}\n",
    "    def ascii_fallback(s: str):\n",
    "        for k,v in REPL.items(): s = s.replace(k, v)\n",
    "        return s\n",
    "    ttf_candidates = [\n",
    "        r\"C:\\Windows\\Fonts\\arial.ttf\",\n",
    "        r\"C:\\Windows\\Fonts\\DejaVuSans.ttf\",\n",
    "        r\"C:\\Windows\\Fonts\\Calibri.ttf\",\n",
    "        r\"C:\\Windows\\Fonts\\segoeui.ttf\",\n",
    "    ]\n",
    "    pdf = FPDF(orientation=\"P\", unit=\"mm\", format=\"A4\")\n",
    "    pdf.set_auto_page_break(auto=True, margin=12); pdf.add_page()\n",
    "    used_unicode = False\n",
    "    for ttf in ttf_candidates:\n",
    "        if Path(ttf).exists():\n",
    "            try:\n",
    "                try: pdf.add_font(\"U\", \"\", ttf, uni=True)\n",
    "                except TypeError: pdf.add_font(\"U\", \"\", ttf)\n",
    "                pdf.set_font(\"U\", \"\", 16); used_unicode = True; break\n",
    "            except Exception:\n",
    "                pass\n",
    "    if not used_unicode: pdf.set_font(\"helvetica\", \"\", 16)\n",
    "    safe_title = title if used_unicode else ascii_fallback(title)\n",
    "    if HAVE_ENUMS: pdf.cell(0, 10, safe_title, new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
    "    else:          pdf.cell(0, 10, safe_title, ln=1)\n",
    "    pdf.set_font(\"U\" if used_unicode else \"helvetica\", \"\", 10)\n",
    "    with open(report_md_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip().startswith(\"!\"): continue\n",
    "            pdf.multi_cell(0, 5, line if used_unicode else ascii_fallback(line))\n",
    "    for img in images:\n",
    "        if img and Path(img).exists():\n",
    "            pdf.add_page(); pdf.image(str(img), x=10, y=20, w=180)\n",
    "            if HAVE_ENUMS: pdf.cell(0, 6, Path(img).name, new_x=XPos.LMARGIN, new_y=YPos.NEXT)\n",
    "            else:          pdf.ln(6)\n",
    "    ensure_dir(Path(out_pdf).parent); pdf.output(str(out_pdf)); return True\n",
    "\n",
    "# ---------------- Main --------------------------------------------------------\n",
    "RUN_BASE = pick_run_base()\n",
    "STAMP = ts_utc(); RUN_DIR = ensure_dir(Path(RUN_BASE) / STAMP)\n",
    "\n",
    "print(f\"[{ts_local()}] 3I Atlas Check-In (ATLAS SEEKER v5) starting…\")\n",
    "print(f\"  Run dir: {RUN_DIR}\")\n",
    "\n",
    "# Gather candidate files (global across roots, plus PACK_DIR if set)\n",
    "cands = []\n",
    "roots = all_existing(ROOT_HINTS)\n",
    "if PACK_DIR:\n",
    "    roots = [normalize_pack_dir(Path(PACK_DIR))] + roots\n",
    "for r in roots:\n",
    "    print(f\"  Scanning: {r}\")\n",
    "    cands.extend(list_datafiles(r))\n",
    "\n",
    "# Dedup\n",
    "uniq, seen = [], set()\n",
    "for p in cands:\n",
    "    k = str(p.resolve()).lower()\n",
    "    if k not in seen:\n",
    "        seen.add(k)\n",
    "        uniq.append(p)\n",
    "cands = uniq\n",
    "\n",
    "if not cands:\n",
    "    raise SystemExit(\"No candidate files found. Add/point PACK_DIR to your atlas pack.\")\n",
    "\n",
    "# Rank in two passes: strict → relax\n",
    "def pick_best(min_samples, min_genes):\n",
    "    scored = []\n",
    "    for f in cands:\n",
    "        score, note = rank_file_for_atlas(f, min_samples=min_samples, min_genes=min_genes)\n",
    "        scored.append((score, note, f))\n",
    "    scored.sort(key=lambda x: x[0], reverse=True)\n",
    "    return scored\n",
    "\n",
    "scored = pick_best(MIN_SAMPLES_STRICT, MIN_GENES_STRICT)\n",
    "best_score, best_note, best_path = scored[0]\n",
    "if best_score < 0:\n",
    "    print(\"  Strict pass found no qualifying atlas. Relaxing thresholds…\")\n",
    "    scored = pick_best(MIN_SAMPLES_RELAX, MIN_GENES_RELAX)\n",
    "    best_score, best_note, best_path = scored[0]\n",
    "    if best_score < 0:\n",
    "        diag = \"\\n\".join([f\"  {sc:>6}  {nt:30}  {fp}\" for sc, nt, fp in scored[:40]])\n",
    "        raise SystemExit(\"Atlas selection failed (even relaxed). Diagnostics:\\n\" + diag)\n",
    "\n",
    "print(f\"  Using data file: {best_path}  [{best_note}, score={best_score}]\")\n",
    "\n",
    "# Load full table and infer matrix\n",
    "df_any = read_table_any(best_path, nrows=None)\n",
    "df = to_pandas(df_any)\n",
    "E, gene_names, sample_names, meta = infer_matrix(df)\n",
    "print(f\"  Inferred matrix: genes={len(gene_names)}, samples={len(sample_names)}  format={meta['format']}\")\n",
    "\n",
    "# Summaries & tops\n",
    "summary, per_gene, tops = summarize_matrix(E, gene_names, sample_names, k_top=25)\n",
    "\n",
    "# Outputs: CSVs\n",
    "to_csv(Path(RUN_DIR/\"top_gini_genes.csv\"), tops[\"top_gini\"], [\"gene\",\"gini\",\"H_norm\",\"cv\",\"mean\"])\n",
    "to_csv(Path(RUN_DIR/\"top_specialized_low_entropy.csv\"), tops[\"top_specialized_low_entropy\"], [\"gene\",\"gini\",\"H_norm\",\"cv\",\"mean\"])\n",
    "to_csv(Path(RUN_DIR/\"top_housekeeping_high_entropy.csv\"), tops[\"top_housekeeping_high_entropy\"], [\"gene\",\"gini\",\"H_norm\",\"cv\",\"mean\"])\n",
    "to_csv(Path(RUN_DIR/\"summary_stats.csv\"), [[k, v] for k, v in summary.items()], [\"metric\",\"value\"])\n",
    "\n",
    "# Plots\n",
    "plots = {}\n",
    "plot_hist(per_gene[\"gini\"], Path(RUN_DIR/\"plots/gini_hist.png\"), \"Gini distribution (gene specialization)\", \"Gini\")\n",
    "plots[\"gini_hist\"] = str(Path(RUN_DIR/\"plots/gini_hist.png\"))\n",
    "plot_hist(per_gene[\"H_norm\"], Path(RUN_DIR/\"plots/entropy_hist.png\"), \"Normalized entropy across samples\", \"H_norm\")\n",
    "plots[\"entropy_hist\"] = str(Path(RUN_DIR/\"plots/entropy_hist.png\"))\n",
    "plot_bar(tops[\"top_gini\"], Path(RUN_DIR/\"plots/top_gini_bar.png\"), \"Top specialized genes (by Gini)\", \"Gini\")\n",
    "plots[\"top_gini_bar\"] = str(Path(RUN_DIR/\"plots/top_gini_bar.png\"))\n",
    "\n",
    "# Embeddings (optional)\n",
    "def try_pca(E, n=2, random_state=42):\n",
    "    if PCA is None: return None, None\n",
    "    X = np.nan_to_num(E, nan=0.0)\n",
    "    X = X - X.mean(axis=1, keepdims=True)\n",
    "    pca = PCA(n_components=min(n, min(X.shape)-1), random_state=random_state)\n",
    "    try:\n",
    "        Y = pca.fit_transform(X.T)\n",
    "        return Y, pca.explained_variance_ratio_.tolist()\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "def try_umap(E, n=2, random_state=42):\n",
    "    if umap is None: return None\n",
    "    X = np.nan_to_num(E, nan=0.0); X = X - X.mean(axis=1, keepdims=True)\n",
    "    try: return umap.UMAP(n_components=n, random_state=random_state).fit_transform(X.T)\n",
    "    except Exception: return None\n",
    "\n",
    "pca_pts, pca_var = try_pca(E, n=2, random_state=42)\n",
    "if pca_pts is not None:\n",
    "    plot_scatter(pca_pts, Path(RUN_DIR/\"plots/pca_scatter.png\"),\n",
    "                 f\"PCA on samples (var={sum(pca_var):.2%})\", \"PC1\", \"PC2\")\n",
    "    plots[\"pca_scatter\"] = str(Path(RUN_DIR/\"plots/pca_scatter.png\"))\n",
    "else:\n",
    "    print(\"  PCA not available or failed; skipping PCA plot.\")\n",
    "\n",
    "umap_pts = try_umap(E, n=2, random_state=42)\n",
    "if umap_pts is not None:\n",
    "    plot_scatter(umap_pts, Path(RUN_DIR/\"plots/umap_scatter.png\"),\n",
    "                 \"UMAP on samples\", \"UMAP-1\", \"UMAP-2\")\n",
    "    plots[\"umap_scatter\"] = str(Path(RUN_DIR/\"plots/umap_scatter.png\"))\n",
    "\n",
    "# Snapshot & deltas\n",
    "def read_json(path: Path):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f: return json.load(f)\n",
    "    except Exception: return None\n",
    "\n",
    "def write_json(path: Path, obj):\n",
    "    ensure_dir(Path(path).parent)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f: json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def last_snapshot(dir_base: Path):\n",
    "    files = glob.glob(str(dir_base / \"*\" / \"snapshot.json\"))\n",
    "    if not files: return None, None\n",
    "    files.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "    path = Path(files[0]); return path, read_json(path)\n",
    "\n",
    "SNAPSHOT_PATH = Path(RUN_DIR/\"snapshot.json\")\n",
    "prev_path, prev = last_snapshot(Path(RUN_BASE))\n",
    "deltas = None\n",
    "if prev:\n",
    "    deltas = {\n",
    "        \"n_genes_delta\": summary[\"n_genes\"] - int(prev.get(\"summary\",{}).get(\"n_genes\", 0)),\n",
    "        \"n_samples_delta\": summary[\"n_samples\"] - int(prev.get(\"summary\",{}).get(\"n_samples\", 0)),\n",
    "        \"gini_mean_delta\": summary[\"gini_mean\"] - float(prev.get(\"summary\",{}).get(\"gini_mean\", 0.0)),\n",
    "        \"entropy_mean_delta\": summary[\"entropy_mean\"] - float(prev.get(\"summary\",{}).get(\"entropy_mean\", 0.0)),\n",
    "        \"cv_mean_delta\": summary[\"cv_mean\"] - float(prev.get(\"summary\",{}).get(\"cv_mean\", 0.0)),\n",
    "        \"changed_samples\": False, \"added_samples\": [], \"removed_samples\": [],\n",
    "    }\n",
    "    try:\n",
    "        prev_samples = set(prev.get(\"sample_names\", [])); cur_samples = set(sample_names)\n",
    "        add = sorted(cur_samples - prev_samples); rem = sorted(prev_samples - cur_samples)\n",
    "        if add or rem:\n",
    "            deltas[\"changed_samples\"] = True\n",
    "            deltas[\"added_samples\"] = add; deltas[\"removed_samples\"] = rem\n",
    "    except Exception: pass\n",
    "    write_json(Path(RUN_DIR/\"delta_summary.json\"), deltas)\n",
    "    print(f\"  Δ written: {Path(RUN_DIR/'delta_summary.json')}\")\n",
    "else:\n",
    "    print(\"  No prior snapshot found; this will serve as the baseline.\")\n",
    "\n",
    "snapshot = {\n",
    "    \"meta\": {\n",
    "        \"stamp_utc\": ts_utc(), \"stamp_local\": ts_local(),\n",
    "        \"host\": platform.node(), \"python\": sys.version.split()[0],\n",
    "        \"pack_dir\": str(roots[0]) if roots else \"n/a\", \"data_file\": str(best_path),\n",
    "    },\n",
    "    \"summary\": summary, \"sample_names\": sample_names[:5000],\n",
    "    \"top_gini\": tops[\"top_gini\"],\n",
    "    \"top_housekeeping_high_entropy\": tops[\"top_housekeeping_high_entropy\"],\n",
    "}\n",
    "write_json(SNAPSHOT_PATH, snapshot)\n",
    "\n",
    "# Markdown report\n",
    "def write_report_md(path: Path, info):\n",
    "    ensure_dir(Path(path).parent)\n",
    "    L = []\n",
    "    L.append(f\"# 3I Atlas Check-In — {info['meta']['stamp_local']}\"); L.append(\"\")\n",
    "    L.append(f\"- **Data file**: `{info['meta']['data_file']}`\")\n",
    "    L.append(f\"- **Run dir**: `{info['meta']['run_dir']}`\")\n",
    "    L.append(f\"- **Rows (genes)**: **{info['summary']['n_genes']}**, **Samples**: **{info['summary']['n_samples']}**\")\n",
    "    L.append(f\"- Gini (mean/median): **{info['summary']['gini_mean']:.4f} / {info['summary']['gini_median']:.4f}**\")\n",
    "    L.append(f\"- Entropy_n (mean/median): **{info['summary']['entropy_mean']:.4f} / {info['summary']['entropy_median']:.4f}**\")\n",
    "    L.append(f\"- CV (mean): **{info['summary']['cv_mean']:.4f}**\")\n",
    "    L.append(\"\")\n",
    "    for key in (\"gini_hist\",\"entropy_hist\",\"top_gini_bar\",\"pca_scatter\",\"umap_scatter\"):\n",
    "        p = info[\"plots\"].get(key)\n",
    "        if p:\n",
    "            L.append(f\"![{key}]({Path(p).name})\")\n",
    "    L.append(\"\")\n",
    "    L.append(\"## Top specialized (by Gini) — preview\")\n",
    "    for (name,g,h,cv,mu) in info[\"tops\"][\"top_gini\"][:10]:\n",
    "        L.append(f\"- {name}: Gini={g:.4f}, H_n={h:.4f}, CV={cv:.3f}, mean={mu:.3g}\")\n",
    "    L.append(\"\")\n",
    "    L.append(\"## Top housekeeping (high normalized entropy) — preview\")\n",
    "    for (name,g,h,cv,mu) in info[\"tops\"][\"top_housekeeping_high_entropy\"][:10]:\n",
    "        L.append(f\"- {name}: H_n={h:.4f}, Gini={g:.4f}, CV={cv:.3f}, mean={mu:.3g}\")\n",
    "    L.append(\"\")\n",
    "    if info.get(\"deltas\"):\n",
    "        d = info[\"deltas\"]\n",
    "        L.append(\"## Delta vs last snapshot\")\n",
    "        L.append(f\"- Genes: **{d.get('n_genes_delta',0):+d}**, Samples: **{d.get('n_samples_delta',0):+d}**\")\n",
    "        if \"gini_mean_delta\" in d:\n",
    "            L.append(f\"- Δ Gini mean: **{d['gini_mean_delta']:+.4f}**, Δ Entropy_n mean: **{d.get('entropy_mean_delta',0):+.4f}**\")\n",
    "        if d.get(\"changed_samples\"):\n",
    "            L.append(f\"- Changed sample set: +{len(d['added_samples'])} / -{len(d['removed_samples'])}\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(L))\n",
    "\n",
    "info = {\n",
    "    \"meta\": {\"stamp_local\": ts_local(), \"run_dir\": str(RUN_DIR), \"data_file\": str(best_path)},\n",
    "    \"summary\": summary, \"tops\": tops, \"deltas\": deltas, \"plots\": plots\n",
    "}\n",
    "REPORT_MD = Path(RUN_DIR/\"report.md\"); write_report_md(REPORT_MD, info); print(f\"  Wrote: {REPORT_MD}\")\n",
    "\n",
    "# Atlas facts (concise NEW information)\n",
    "def write_atlas_facts(path: Path, summary, tops, sample_names, data_file):\n",
    "    ensure_dir(Path(path).parent)\n",
    "    lines = []\n",
    "    lines.append(\"# Atlas Facts (new)\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"- Data source: `{data_file}`\")\n",
    "    lines.append(f\"- Samples detected: **{len(sample_names)}** → {', '.join(map(str, sample_names[:12]))}{' …' if len(sample_names)>12 else ''}\")\n",
    "    lines.append(f\"- Genes: **{summary['n_genes']}**\")\n",
    "    lines.append(f\"- Specialization mean Gini: **{summary['gini_mean']:.4f}** (median **{summary['gini_median']:.4f}**)\")\n",
    "    lines.append(f\"- Ubiquity mean normalized entropy: **{summary['entropy_mean']:.4f}** (median **{summary['entropy_median']:.4f}**)\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Top 10 specialized genes (highest Gini)\")\n",
    "    for (name,g,h,cv,mu) in tops[\"top_gini\"][:10]:\n",
    "        lines.append(f\"- {name}: Gini={g:.4f}, H_n={h:.4f}, CV={cv:.3f}, mean={mu:.3g}\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(\"## Top 10 housekeeping genes (highest H_n)\")\n",
    "    for (name,g,h,cv,mu) in tops[\"top_housekeeping_high_entropy\"][:10]:\n",
    "        lines.append(f\"- {name}: H_n={h:.4f}, Gini={g:.4f}, CV={cv:.3f}, mean={mu:.3g}\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "write_atlas_facts(Path(RUN_DIR/\"atlas_facts.md\"), summary, tops, sample_names, best_path)\n",
    "print(f\"  Wrote: {RUN_DIR/'atlas_facts.md'}\")\n",
    "\n",
    "# PDF\n",
    "REPORT_PDF = Path(RUN_DIR/\"report.pdf\")\n",
    "ok_pdf = write_pdf(REPORT_MD,\n",
    "                   images=[plots.get(\"gini_hist\"), plots.get(\"entropy_hist\"),\n",
    "                           plots.get(\"top_gini_bar\"), plots.get(\"pca_scatter\"), plots.get(\"umap_scatter\")],\n",
    "                   out_pdf=REPORT_PDF, title=\"3I Atlas Check-In\")\n",
    "print(f\"  PDF:   {REPORT_PDF if ok_pdf else '(skipped; fpdf missing)'}\")\n",
    "\n",
    "print(f\"[{ts_local()}] Done. — ATLAS facts produced (v5).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16ccd02d-bcb9-4bb2-a64a-b4318a189792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-29 02:56:15] 3I Atlas — Comet Watch starting…\n",
      "  Run dir: C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_runs\\comet_watch_checkin\\20251029-065615Z\n",
      "  Scanning: C:\\Users\\caleb\\CNT_Lab\n",
      "  Scanning: E:\\CNT\n",
      "  Scanning: E:\\CNT\\notebooks\\archive\n",
      "  Scanning: E:\\CNT\\notebooks\\archive\n",
      "  Selected sources: {'plasma': 'C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\artifacts\\\\tables\\\\migrated__gwas-catalog-all-associations__21f38b1a.tsv', 'spectrum': 'C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\artifacts\\\\tables\\\\migrated__sim-theta__fc536f2f.csv', 'mag': 'C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\notebooks\\\\archive\\\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\\\data\\\\noaa_mag_3d.csv', 'lightcurve': 'C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\notebooks\\\\archive\\\\cnt_3i_atlas_all8_20251024-034610Z_0f216bd2\\\\out\\\\tables\\\\lightcurve_theta.csv'}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DatetimeIndex' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 459\u001b[39m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mplasma\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m bucket:\n\u001b[32m    458\u001b[39m     df = read_pd(bucket[\u001b[33m\"\u001b[39m\u001b[33mplasma\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     pla_res, ev = \u001b[43manalyze_plasma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m     events.extend(ev)\n\u001b[32m    461\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pla_res:\n\u001b[32m    462\u001b[39m         \u001b[38;5;66;03m# plot first field\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 297\u001b[39m, in \u001b[36manalyze_plasma\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m    295\u001b[39m         seg = \u001b[38;5;28mslice\u001b[39m(a,b+\u001b[32m1\u001b[39m)\n\u001b[32m    296\u001b[39m         peak_i = a + \u001b[38;5;28mint\u001b[39m(np.nanargmax(z[seg]))\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m         events.append(\u001b[38;5;28mdict\u001b[39m(source=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mplasma:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, start=\u001b[38;5;28mstr\u001b[39m(\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m[a]), end=\u001b[38;5;28mstr\u001b[39m(t.iloc[b]),\n\u001b[32m    298\u001b[39m                            peak_time=\u001b[38;5;28mstr\u001b[39m(t.iloc[peak_i]), peak_z=\u001b[38;5;28mfloat\u001b[39m(z[peak_i]),\n\u001b[32m    299\u001b[39m                            peak_val=\u001b[38;5;28mfloat\u001b[39m(x[peak_i]), count=\u001b[38;5;28mint\u001b[39m(b-a+\u001b[32m1\u001b[39m)))\n\u001b[32m    300\u001b[39m     series[\u001b[38;5;28mstr\u001b[39m(col)] = \u001b[38;5;28mdict\u001b[39m(time=t, value=x, z=z, dt=dt)\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m series, events\n",
      "\u001b[31mAttributeError\u001b[39m: 'DatetimeIndex' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "# === 3I Atlas — COMET / Atmosphere Watch Check-In (single cell) ==============\n",
    "# Purpose: Scan your CNT roots for space-weather series (NOAA MAG, lightcurves,\n",
    "#          plasma, spectra), analyze anomalies (robust z), and emit a tidy bundle:\n",
    "#          - comet_watch_facts.md (human)\n",
    "#          - report.md (+ report.pdf, Unicode-safe)\n",
    "#          - events.csv (all detections with start/end/peak z)\n",
    "#          - summary_stats.csv (per-stream stats)\n",
    "#          - plots/: time series, spectra/spectrograms\n",
    "# Notes:\n",
    "#   - Offline only; uses numpy/pandas/matplotlib (+fpdf if present).\n",
    "#   - Zero gene/DNA selection; hard-excludes those.\n",
    "#   - Robust Z via MAD; rolls adapt to sampling cadence.\n",
    "# ============================================================================\n",
    "\n",
    "import os, re, sys, json, glob, math, platform\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# ---------- Config ------------------------------------------------------------\n",
    "PACK_DIR = None  # set to your pack to skip discovery (e.g., the ..._vector_embedding dir)\n",
    "ROOT_HINTS = [\n",
    "    r\"C:\\Users\\caleb\\CNT_Lab\",\n",
    "    r\"E:\\CNT\",\n",
    "    r\"E:\\CNT\\notebooks\\archive\",\n",
    "    r\"D:\\CNT\",\n",
    "    r\"C:\\CNT\",\n",
    "    str(Path.cwd()),\n",
    "]\n",
    "\n",
    "RUN_BASE_HINTS = [\n",
    "    r\"C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_runs\\comet_watch_checkin\",\n",
    "    r\"E:\\CNT\\notebooks\\archive\\cnt_runs\\comet_watch_checkin\",\n",
    "    str(Path.cwd() / \"cnt_runs\" / \"comet_watch_checkin\"),\n",
    "]\n",
    "\n",
    "INCLUDE_PATTERNS = [\n",
    "    \"noaa\", \"mag\", \"magnet\", \"bz\", \"bt\",\n",
    "    \"lightcurve\", \"flux\", \"brightness\",\n",
    "    \"plasma\", \"ion\", \"density\", \"velocity\", \"temp\",\n",
    "    \"spectrum\", \"spectra\", \"theta\", \"freq\", \"frequency\", \"fft\",\n",
    "    \"aurora\", \"iono\", \"solar_wind\"\n",
    "]\n",
    "EXCLUDE_PATTERNS = [\n",
    "    \"gene\", \"genome\", \"rna\", \"tpm\", \"fpkm\", \"counts\", \"expr\", \"thread_edges\", \"gtex\"\n",
    "]\n",
    "ALLOWED_EXT = [\".csv\", \".tsv\", \".parquet\", \".feather\"]  # (time series; skip npz/npy here)\n",
    "\n",
    "Z_THR_MAG = 3.5\n",
    "Z_THR_LC  = 4.0\n",
    "Z_THR_PLA = 3.5\n",
    "\n",
    "# ---------- Optional deps -----------------------------------------------------\n",
    "USE_POLARS = False\n",
    "try:\n",
    "    import pandas as pd\n",
    "except Exception:\n",
    "    pd = None\n",
    "\n",
    "if USE_POLARS:\n",
    "    try:\n",
    "        import polars as pl\n",
    "    except Exception:\n",
    "        USE_POLARS = False\n",
    "\n",
    "try:\n",
    "    from fpdf import FPDF\n",
    "except Exception:\n",
    "    FPDF = None\n",
    "\n",
    "# ---------- Utils -------------------------------------------------------------\n",
    "def ts_utc():   return datetime.now(timezone.utc).strftime(\"%Y%m%d-%H%M%SZ\")\n",
    "def ts_local(): return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "def ensure_dir(p: Path): p.mkdir(parents=True, exist_ok=True); return p\n",
    "\n",
    "def pick_run_base():\n",
    "    for p in RUN_BASE_HINTS:\n",
    "        path = Path(p)\n",
    "        try:\n",
    "            path.mkdir(parents=True, exist_ok=True)\n",
    "            return str(path)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return str(Path.cwd() / \"cnt_runs\" / \"comet_watch_checkin\")\n",
    "\n",
    "def normalize_pack_dir(p: Path) -> Path:\n",
    "    parts = list(p.parts)\n",
    "    if len(parts) >= 2 and parts[-1].lower() == \"vector_embedding\" and parts[-2].lower() == \"vector_embedding\":\n",
    "        return Path(*parts[:-1])\n",
    "    name = p.name.lower()\n",
    "    if name.endswith(\"_vector_embedding_vector_embedding\"):\n",
    "        return p.with_name(p.name[: -len(\"_vector_embedding\")])\n",
    "    return p\n",
    "\n",
    "def all_existing(paths): return [Path(p) for p in paths if Path(p).exists()]\n",
    "\n",
    "def list_spaceweather_files(root: Path):\n",
    "    hits = []\n",
    "    for ext in ALLOWED_EXT:\n",
    "        for key in INCLUDE_PATTERNS:\n",
    "            pat = str(root / \"**\" / f\"*{key}*{ext}\")\n",
    "            hits.extend([Path(p) for p in glob.glob(pat, recursive=True)])\n",
    "    # dedup & filter excludes\n",
    "    z, seen = [], set()\n",
    "    for h in hits:\n",
    "        if not h.is_file(): continue\n",
    "        low = str(h).lower()\n",
    "        if any(x in low for x in EXCLUDE_PATTERNS): continue\n",
    "        k = str(h.resolve()).lower()\n",
    "        if k not in seen:\n",
    "            seen.add(k); z.append(h)\n",
    "    z.sort(key=lambda p: p.stat().st_size if p.exists() else 0, reverse=True)\n",
    "    return z\n",
    "\n",
    "def detect_format_read(path: Path, nrows=None):\n",
    "    suff = path.suffix.lower()\n",
    "    if USE_POLARS:\n",
    "        if 'pl' not in globals():\n",
    "            raise RuntimeError(\"polars not available\")\n",
    "        if suff in (\".csv\", \".tsv\"):\n",
    "            sep = \",\" if suff==\".csv\" else \"\\t\"\n",
    "            df = pl.read_csv(str(path), separator=sep)\n",
    "            return df if nrows is None else df.head(nrows)\n",
    "        elif suff == \".parquet\":\n",
    "            df = pl.read_parquet(str(path)); return df if nrows is None else df.head(nrows)\n",
    "        elif suff == \".feather\":\n",
    "            df = pl.read_ipc(str(path));    return df if nrows is None else df.head(nrows)\n",
    "        else:\n",
    "            raise RuntimeError(f\"unsupported: {suff}\")\n",
    "    else:\n",
    "        if pd is None: raise RuntimeError(\"pandas not available\")\n",
    "        if suff in (\".csv\", \".tsv\"):\n",
    "            sep = \",\" if suff==\".csv\" else \"\\t\"\n",
    "            return pd.read_csv(path, nrows=nrows, sep=sep)\n",
    "        elif suff == \".parquet\":\n",
    "            return pd.read_parquet(path)\n",
    "        elif suff == \".feather\":\n",
    "            return pd.read_feather(path)\n",
    "        else:\n",
    "            raise RuntimeError(f\"unsupported: {suff}\")\n",
    "\n",
    "def to_pd(df):\n",
    "    if pd is None: raise RuntimeError(\"pandas not available\")\n",
    "    if USE_POLARS: return df.to_pandas()\n",
    "    return df\n",
    "\n",
    "TIME_COLS = {\"time\",\"timestamp\",\"datetime\",\"date\",\"utc\",\"t\"}\n",
    "\n",
    "def parse_time(df):\n",
    "    cols = [c for c in df.columns]\n",
    "    # choose a time column if present\n",
    "    tcol = None\n",
    "    for c in cols:\n",
    "        if str(c).lower() in TIME_COLS:\n",
    "            tcol = c; break\n",
    "    if tcol is not None:\n",
    "        t = pd.to_datetime(df[tcol], errors=\"coerce\", utc=False)\n",
    "        # if most NaT, maybe it is numeric seconds\n",
    "        if t.isna().mean() > 0.8:\n",
    "            try:\n",
    "                base = pd.to_datetime(\"1970-01-01\")\n",
    "                t = base + pd.to_timedelta(pd.to_numeric(df[tcol], errors=\"coerce\"), unit=\"s\")\n",
    "            except Exception:\n",
    "                t = pd.to_datetime(df.index, errors=\"coerce\")\n",
    "    else:\n",
    "        # fallback: index or a monotonic column\n",
    "        try:\n",
    "            t = pd.to_datetime(df.index, errors=\"coerce\")\n",
    "        except Exception:\n",
    "            t = pd.Series(pd.NaT, index=df.index)\n",
    "    return t\n",
    "\n",
    "def sampling_seconds(t: \"pd.Series\"):\n",
    "    try:\n",
    "        dt = (t.dropna().diff().dt.total_seconds()).median()\n",
    "        if np.isnan(dt) or dt <= 0: return 60.0\n",
    "        return float(dt)\n",
    "    except Exception:\n",
    "        return 60.0\n",
    "\n",
    "def robust_stats(x: np.ndarray):\n",
    "    x = x[np.isfinite(x)]\n",
    "    if x.size == 0: return np.nan, np.nan\n",
    "    med = np.median(x)\n",
    "    mad = np.median(np.abs(x - med)) * 1.4826\n",
    "    if mad < 1e-9: mad = 1e-9\n",
    "    return med, mad\n",
    "\n",
    "def rolling_robust_z(x: np.ndarray, win: int):\n",
    "    # compute rolling median & MAD with simple edges\n",
    "    n = len(x); z = np.full(n, np.nan)\n",
    "    half = max(1, win//2)\n",
    "    for i in range(n):\n",
    "        a = max(0, i-half); b = min(n, i+half+1)\n",
    "        med, mad = robust_stats(x[a:b])\n",
    "        z[i] = (x[i] - med)/mad\n",
    "    return z\n",
    "\n",
    "def cluster_bool_runs(t, mask):\n",
    "    # return list of (start_idx, end_idx) contiguous True runs\n",
    "    runs = []\n",
    "    in_run = False; s = 0\n",
    "    for i, m in enumerate(mask):\n",
    "        if m and not in_run:\n",
    "            in_run = True; s = i\n",
    "        elif not m and in_run:\n",
    "            runs.append((s, i-1)); in_run = False\n",
    "    if in_run: runs.append((s, len(mask)-1))\n",
    "    return runs\n",
    "\n",
    "def analyze_magnetometer(df):\n",
    "    # find Bx,By,Bz or x,y,z columns\n",
    "    cols = [c for c in df.columns if str(c).lower() not in TIME_COLS]\n",
    "    low = [str(c).lower() for c in cols]\n",
    "    def pick(name): \n",
    "        for i,c in enumerate(cols):\n",
    "            if name in low[i]: return c\n",
    "        return None\n",
    "    bx = pick(\"bx\") or pick(\"x\")\n",
    "    by = pick(\"by\") or pick(\"y\")\n",
    "    bz = pick(\"bz\") or pick(\"z\")\n",
    "    M = None\n",
    "    if bx and by and bz:\n",
    "        vec = df[[bx,by,bz]].astype(float).values\n",
    "        M = np.sqrt((vec**2).sum(axis=1))\n",
    "    else:\n",
    "        # fall back to any single numeric column\n",
    "        num = df.select_dtypes(include=[np.number])\n",
    "        if num.shape[1] >= 1:\n",
    "            M = num.iloc[:,0].astype(float).values\n",
    "    if M is None: return None, []\n",
    "    t = parse_time(df)\n",
    "    dt = sampling_seconds(t)\n",
    "    win = int(max(5, min(601, round(300.0/dt))))  # ~5min window, min 5, max 601\n",
    "    z  = rolling_robust_z(M, win)\n",
    "    hits = np.array(z) > Z_THR_MAG\n",
    "    runs = cluster_bool_runs(t, hits)\n",
    "    ev = []\n",
    "    for a,b in runs:\n",
    "        seg = slice(a,b+1)\n",
    "        peak_i = a + int(np.nanargmax(z[seg]))\n",
    "        ev.append(dict(source=\"mag\", start=str(t.iloc[a]), end=str(t.iloc[b]),\n",
    "                       peak_time=str(t.iloc[peak_i]), peak_z=float(z[peak_i]),\n",
    "                       peak_val=float(M[peak_i]), count=int(b-a+1)))\n",
    "    return dict(time=t, value=M, z=z, dt=dt), ev\n",
    "\n",
    "def analyze_lightcurve(df):\n",
    "    # look for flux/brightness columns\n",
    "    lc_cols = [c for c in df.columns if any(k in str(c).lower() for k in [\"flux\",\"bright\",\"intensity\",\"counts\",\"mag_\",\"light\"])]\n",
    "    if not lc_cols:\n",
    "        num = df.select_dtypes(include=[np.number])\n",
    "        if num.shape[1] == 0: return None, []\n",
    "        c = num.columns[0]\n",
    "    else:\n",
    "        c = lc_cols[0]\n",
    "    x = df[c].astype(float).values\n",
    "    t = parse_time(df)\n",
    "    dt = sampling_seconds(t)\n",
    "    win = int(max(5, min(1201, round(600.0/dt))))  # ~10min window\n",
    "    z  = rolling_robust_z(x, win)\n",
    "    hits = np.array(z) > Z_THR_LC\n",
    "    runs = cluster_bool_runs(t, hits)\n",
    "    ev = []\n",
    "    for a,b in runs:\n",
    "        seg = slice(a,b+1)\n",
    "        peak_i = a + int(np.nanargmax(z[seg]))\n",
    "        ev.append(dict(source=\"lightcurve\", start=str(t.iloc[a]), end=str(t.iloc[b]),\n",
    "                       peak_time=str(t.iloc[peak_i]), peak_z=float(z[peak_i]),\n",
    "                       peak_val=float(x[peak_i]), count=int(b-a+1)))\n",
    "    return dict(time=t, value=x, z=z, dt=dt, col=str(c)), ev\n",
    "\n",
    "def analyze_plasma(df):\n",
    "    # look for density, speed/velocity, temperature\n",
    "    cols = {str(c).lower(): c for c in df.columns}\n",
    "    fields = []\n",
    "    for key in [\"density\",\"dens\",\"n\",\"speed\",\"velocity\",\"vel\",\"temp\",\"temperature\"]:\n",
    "        for k,v in cols.items():\n",
    "            if key == k or key in k:\n",
    "                fields.append(v)\n",
    "    if not fields:\n",
    "        num = df.select_dtypes(include=[np.number])\n",
    "        if num.shape[1]==0: return None,[]\n",
    "        fields = [num.columns[0]]\n",
    "    t = parse_time(df)\n",
    "    dt = sampling_seconds(t)\n",
    "    events = []\n",
    "    series = {}\n",
    "    for col in fields[:3]:  # limit to a few\n",
    "        x = df[col].astype(float).values\n",
    "        win = int(max(5, min(1201, round(600.0/dt))))\n",
    "        z  = rolling_robust_z(x, win)\n",
    "        hits = np.array(z) > Z_THR_PLA\n",
    "        runs = cluster_bool_runs(t, hits)\n",
    "        for a,b in runs:\n",
    "            seg = slice(a,b+1)\n",
    "            peak_i = a + int(np.nanargmax(z[seg]))\n",
    "            events.append(dict(source=f\"plasma:{col}\", start=str(t.iloc[a]), end=str(t.iloc[b]),\n",
    "                               peak_time=str(t.iloc[peak_i]), peak_z=float(z[peak_i]),\n",
    "                               peak_val=float(x[peak_i]), count=int(b-a+1)))\n",
    "        series[str(col)] = dict(time=t, value=x, z=z, dt=dt)\n",
    "    return series, events\n",
    "\n",
    "def classify_kind(path: Path):\n",
    "    s = path.name.lower()\n",
    "    if any(k in s for k in [\"mag\",\"bz\",\"bt\",\"magnet\"]): return \"mag\"\n",
    "    if \"lightcurve\" in s or (\"light\" in s and \"curve\" in s) or \"flux\" in s or \"brightness\" in s: return \"lightcurve\"\n",
    "    if \"plasma\" in s or \"ion\" in s or \"density\" in s or \"velocity\" in s or \"temp\" in s: return \"plasma\"\n",
    "    if \"spectrum\" in s or \"theta\" in s or \"freq\" in s or \"frequency\" in s: return \"spectrum\"\n",
    "    return \"other\"\n",
    "\n",
    "def read_pd(path: Path, nrows=None):\n",
    "    return to_pd(detect_format_read(path, nrows=nrows))\n",
    "\n",
    "# ---------- PDF (Unicode-safe) -----------------------------------------------\n",
    "def write_pdf(report_md_path: Path, images, out_pdf: Path, title=\"3I Atlas — Comet Watch\"):\n",
    "    if FPDF is None: return False\n",
    "    try:\n",
    "        from fpdf.enums import XPos, YPos\n",
    "        HAVE_ENUMS = True\n",
    "    except Exception:\n",
    "        HAVE_ENUMS = False\n",
    "    REPL = {\"\\u2011\":\"-\",\"\\u2013\":\"-\",\"\\u2014\":\"-\",\"\\u2018\":\"'\",\"\\u2019\":\"'\",\"\\u201c\":'\"',\"\\u201d\":'\"',\"\\u2026\":\"...\"}\n",
    "    def ascii_fallback(s: str):\n",
    "        for k,v in REPL.items(): s = s.replace(k, v)\n",
    "        return s\n",
    "    ttf_candidates = [r\"C:\\Windows\\Fonts\\arial.ttf\", r\"C:\\Windows\\Fonts\\DejaVuSans.ttf\",\n",
    "                      r\"C:\\Windows\\Fonts\\Calibri.ttf\", r\"C:\\Windows\\Fonts\\segoeui.ttf\"]\n",
    "    pdf = FPDF(orientation=\"P\", unit=\"mm\", format=\"A4\")\n",
    "    pdf.set_auto_page_break(auto=True, margin=12); pdf.add_page()\n",
    "    used_unicode = False\n",
    "    for ttf in ttf_candidates:\n",
    "        if Path(ttf).exists():\n",
    "            try:\n",
    "                try: pdf.add_font(\"U\", \"\", ttf, uni=True)\n",
    "                except TypeError: pdf.add_font(\"U\", \"\", ttf)\n",
    "                pdf.set_font(\"U\", \"\", 16); used_unicode = True; break\n",
    "            except Exception: pass\n",
    "    if not used_unicode: pdf.set_font(\"helvetica\", \"\", 16)\n",
    "    safe_title = title if used_unicode else ascii_fallback(title)\n",
    "    if HAVE_ENUMS: pdf.cell(0,10,safe_title,new_x=XPos.LMARGIN,new_y=YPos.NEXT)\n",
    "    else:          pdf.cell(0,10,safe_title,ln=1)\n",
    "    pdf.set_font(\"U\" if used_unicode else \"helvetica\", \"\", 10)\n",
    "    with open(report_md_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip().startswith(\"!\"): continue\n",
    "            pdf.multi_cell(0,5,line if used_unicode else ascii_fallback(line))\n",
    "    for img in images:\n",
    "        if img and Path(img).exists():\n",
    "            pdf.add_page(); pdf.image(str(img), x=10, y=20, w=180)\n",
    "            if HAVE_ENUMS: pdf.cell(0,6,Path(img).name,new_x=XPos.LMARGIN,new_y=YPos.NEXT)\n",
    "            else:          pdf.ln(6)\n",
    "    ensure_dir(Path(out_pdf).parent); pdf.output(str(out_pdf)); return True\n",
    "\n",
    "# ---------- Main --------------------------------------------------------------\n",
    "RUN_BASE = pick_run_base()\n",
    "STAMP    = ts_utc()\n",
    "RUN_DIR  = ensure_dir(Path(RUN_BASE) / STAMP)\n",
    "print(f\"[{ts_local()}] 3I Atlas — Comet Watch starting…\")\n",
    "print(f\"  Run dir: {RUN_DIR}\")\n",
    "\n",
    "# Discover candidates\n",
    "cand_files = []\n",
    "roots = all_existing(ROOT_HINTS)\n",
    "if PACK_DIR:\n",
    "    roots = [normalize_pack_dir(Path(PACK_DIR))] + roots\n",
    "for r in roots:\n",
    "    print(f\"  Scanning: {r}\")\n",
    "    cand_files.extend(list_spaceweather_files(r))\n",
    "\n",
    "# Dedup by realpath\n",
    "uniq, seen = [], set()\n",
    "for f in cand_files:\n",
    "    k = str(f.resolve()).lower()\n",
    "    if k not in seen:\n",
    "        seen.add(k); uniq.append(f)\n",
    "cand_files = uniq\n",
    "if not cand_files:\n",
    "    raise SystemExit(\"No space-weather files found. Set PACK_DIR to your 3I pack root or add NOAA/lightcurve/plasma tables.\")\n",
    "\n",
    "# Pick at most one per kind (largest)\n",
    "bucket = {}\n",
    "for f in cand_files:\n",
    "    kind = classify_kind(f)\n",
    "    if kind == \"other\": continue\n",
    "    if kind not in bucket: bucket[kind] = f\n",
    "    else:\n",
    "        if f.stat().st_size > bucket[kind].stat().st_size:\n",
    "            bucket[kind] = f\n",
    "\n",
    "print(\"  Selected sources:\", {k:str(v) for k,v in bucket.items()})\n",
    "\n",
    "# Read & analyze\n",
    "events = []\n",
    "summaries = []\n",
    "plots = {}\n",
    "\n",
    "def plot_series(t, y, out_path, title, xlabel=\"time\", ylabel=\"value\"):\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    ensure_dir(Path(out_path).parent)\n",
    "    plt.figure()\n",
    "    try:\n",
    "        plt.plot(t, y)\n",
    "    except Exception:\n",
    "        # fallback if t not datetime\n",
    "        plt.plot(np.arange(len(y)), y)\n",
    "        xlabel = \"index\"\n",
    "    plt.title(title); plt.xlabel(xlabel); plt.ylabel(ylabel)\n",
    "    plt.tight_layout(); plt.savefig(out_path, dpi=150); plt.close()\n",
    "\n",
    "def plot_psd(y, dt, out_path, title):\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    ensure_dir(Path(out_path).parent)\n",
    "    n = len(y); \n",
    "    if n < 16:\n",
    "        return\n",
    "    # simple periodogram\n",
    "    Y = np.fft.rfft(y - np.nanmean(y))\n",
    "    f = np.fft.rfftfreq(n, d=max(dt,1e-6))\n",
    "    P = (np.abs(Y)**2)/n\n",
    "    plt.figure()\n",
    "    plt.semilogy(f, P + 1e-12)\n",
    "    plt.title(title); plt.xlabel(\"Hz\"); plt.ylabel(\"Power\")\n",
    "    plt.tight_layout(); plt.savefig(out_path, dpi=150); plt.close()\n",
    "\n",
    "# Magnetometer\n",
    "mag_res = None\n",
    "if \"mag\" in bucket:\n",
    "    df = read_pd(bucket[\"mag\"])\n",
    "    mag_res, ev = analyze_magnetometer(df)\n",
    "    events.extend(ev)\n",
    "    if mag_res:\n",
    "        plot_series(mag_res[\"time\"], mag_res[\"value\"], RUN_DIR/\"plots/mag_timeseries.png\", \"Magnetometer | vector magnitude\")\n",
    "        plots[\"mag_timeseries\"] = str(RUN_DIR/\"plots/mag_timeseries.png\")\n",
    "        plot_psd(np.nan_to_num(mag_res[\"value\"]), mag_res[\"dt\"], RUN_DIR/\"plots/mag_psd.png\", \"Magnetometer | spectrum\")\n",
    "        plots[\"mag_psd\"] = str(RUN_DIR/\"plots/mag_psd.png\")\n",
    "        summaries.append((\"mag\", bucket[\"mag\"].name, len(ev)))\n",
    "\n",
    "# Lightcurve\n",
    "lc_res = None\n",
    "if \"lightcurve\" in bucket:\n",
    "    df = read_pd(bucket[\"lightcurve\"])\n",
    "    lc_res, ev = analyze_lightcurve(df)\n",
    "    events.extend(ev)\n",
    "    if lc_res:\n",
    "        plot_series(lc_res[\"time\"], lc_res[\"value\"], RUN_DIR/\"plots/lightcurve_timeseries.png\", f\"Lightcurve | {lc_res.get('col','flux')}\")\n",
    "        plots[\"lightcurve_timeseries\"] = str(RUN_DIR/\"plots/lightcurve_timeseries.png\")\n",
    "        plot_psd(np.nan_to_num(lc_res[\"value\"]), lc_res[\"dt\"], RUN_DIR/\"plots/lightcurve_psd.png\", \"Lightcurve | spectrum\")\n",
    "        plots[\"lightcurve_psd\"] = str(RUN_DIR/\"plots/lightcurve_psd.png\")\n",
    "        summaries.append((\"lightcurve\", bucket[\"lightcurve\"].name, len(ev)))\n",
    "\n",
    "# Plasma\n",
    "pla_res = None\n",
    "if \"plasma\" in bucket:\n",
    "    df = read_pd(bucket[\"plasma\"])\n",
    "    pla_res, ev = analyze_plasma(df)\n",
    "    events.extend(ev)\n",
    "    if pla_res:\n",
    "        # plot first field\n",
    "        k0 = next(iter(pla_res.keys()))\n",
    "        plot_series(pla_res[k0][\"time\"], pla_res[k0][\"value\"], RUN_DIR/\"plots/plasma_timeseries.png\", f\"Plasma | {k0}\")\n",
    "        plots[\"plasma_timeseries\"] = str(RUN_DIR/\"plots/plasma_timeseries.png\")\n",
    "        plot_psd(np.nan_to_num(pla_res[k0][\"value\"]), pla_res[k0][\"dt\"], RUN_DIR/\"plots/plasma_psd.png\", f\"Plasma | {k0} spectrum\")\n",
    "        plots[\"plasma_psd\"] = str(RUN_DIR/\"plots/plasma_psd.png\")\n",
    "        summaries.append((\"plasma\", bucket[\"plasma\"].name, len([e for e in events if e['source'].startswith('plasma:')])))\n",
    "\n",
    "# Build events table & stats\n",
    "import csv\n",
    "ensure_dir(Path(RUN_DIR/\"events.csv\").parent)\n",
    "with open(RUN_DIR/\"events.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"source\",\"start\",\"end\",\"peak_time\",\"peak_z\",\"peak_val\",\"count\"])\n",
    "    for e in events:\n",
    "        w.writerow([e[\"source\"], e[\"start\"], e[\"end\"], e[\"peak_time\"], f\"{e['peak_z']:.3f}\", f\"{e['peak_val']:.6g}\", e[\"count\"]])\n",
    "\n",
    "# Simple cross-stream correlation (if mag + lightcurve available)\n",
    "corr_note = \"n/a\"\n",
    "try:\n",
    "    if mag_res and lc_res:\n",
    "        # align by min length after dropping NaNs\n",
    "        v1 = np.nan_to_num(mag_res[\"value\"]).astype(float)\n",
    "        v2 = np.nan_to_num(lc_res[\"value\"]).astype(float)\n",
    "        n = min(len(v1), len(v2))\n",
    "        if n >= 32:\n",
    "            c = np.corrcoef(v1[-n:], v2[-n:])[0,1]\n",
    "            corr_note = f\"{c:.3f}\"\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Summary stats CSV\n",
    "summary_rows = [\n",
    "    [\"mag_file\", bucket.get(\"mag\").name if \"mag\" in bucket else \"\"],\n",
    "    [\"lightcurve_file\", bucket.get(\"lightcurve\").name if \"lightcurve\" in bucket else \"\"],\n",
    "    [\"plasma_file\", bucket.get(\"plasma\").name if \"plasma\" in bucket else \"\"],\n",
    "    [\"events_total\", len(events)],\n",
    "    [\"mag_lightcurve_corr\", corr_note],\n",
    "]\n",
    "with open(RUN_DIR/\"summary_stats.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    w = csv.writer(f); w.writerow([\"metric\",\"value\"]); w.writerows(summary_rows)\n",
    "\n",
    "# Snapshot & deltas\n",
    "def read_json(path: Path):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f: return json.load(f)\n",
    "    except Exception: return None\n",
    "def write_json(path: Path, obj):\n",
    "    ensure_dir(Path(path).parent)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f: json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "def last_snapshot(dir_base: Path):\n",
    "    files = glob.glob(str(dir_base / \"*\" / \"snapshot.json\"))\n",
    "    if not files: return None, None\n",
    "    files.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "    p = Path(files[0]); return p, read_json(p)\n",
    "\n",
    "SNAPSHOT_PATH = Path(RUN_DIR/\"snapshot.json\")\n",
    "RUN_BASE = Path(RUN_BASE_HINTS[0]) if Path(RUN_BASE_HINTS[0]).exists() else Path(pick_run_base())\n",
    "prev_path, prev = last_snapshot(RUN_BASE)\n",
    "deltas = None\n",
    "if prev:\n",
    "    prev_events = int(prev.get(\"summary\",{}).get(\"events_total\", 0))\n",
    "    deltas = dict(events_delta=len(events)-prev_events)\n",
    "    write_json(RUN_DIR/\"delta_summary.json\", deltas)\n",
    "else:\n",
    "    print(\"  No prior comet snapshot; this is the baseline.\")\n",
    "\n",
    "snapshot = {\n",
    "    \"meta\": {\n",
    "        \"stamp_utc\": ts_utc(), \"stamp_local\": ts_local(),\n",
    "        \"host\": platform.node(), \"python\": sys.version.split()[0],\n",
    "        \"sources\": {k: str(v) for k,v in bucket.items()},\n",
    "    },\n",
    "    \"summary\": {\"events_total\": len(events), \"mag_lightcurve_corr\": corr_note},\n",
    "}\n",
    "write_json(SNAPSHOT_PATH, snapshot)\n",
    "\n",
    "# Human facts\n",
    "facts = []\n",
    "facts.append(\"# Comet Watch — Facts (new)\")\n",
    "facts.append(\"\")\n",
    "facts.append(f\"- Sources: { {k: v.name for k,v in bucket.items()} }\")\n",
    "facts.append(f\"- Total events flagged (robust z): **{len(events)}**\")\n",
    "facts.append(f\"- Mag↔Lightcurve correlation (rough, last overlap): **{corr_note}**\")\n",
    "if events:\n",
    "    facts.append(\"\")\n",
    "    facts.append(\"## Top event peaks\")\n",
    "    # top by peak_z\n",
    "    top = sorted(events, key=lambda e: e[\"peak_z\"], reverse=True)[:10]\n",
    "    for e in top:\n",
    "        facts.append(f\"- [{e['source']}] {e['peak_time']}  z={e['peak_z']:.2f}  val={e['peak_val']:.6g}  window={e['start']}→{e['end']}\")\n",
    "else:\n",
    "    facts.append(\"\")\n",
    "    facts.append(\"_No events crossed the thresholds; consider lowering Z_THR_* or checking data windows._\")\n",
    "\n",
    "with open(RUN_DIR/\"comet_watch_facts.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(facts))\n",
    "print(f\"  Wrote: {RUN_DIR/'comet_watch_facts.md'}\")\n",
    "\n",
    "# Report.md (with images)\n",
    "def write_report_md(path: Path, plots):\n",
    "    lines = []\n",
    "    lines.append(f\"# 3I Atlas — Comet Watch Report ({ts_local()})\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"- Run dir: `{RUN_DIR}`\")\n",
    "    lines.append(f\"- Events: **{len(events)}**\")\n",
    "    lines.append(f\"- Mag↔Lightcurve correlation: **{corr_note}**\")\n",
    "    lines.append(\"\")\n",
    "    for key in (\"mag_timeseries\",\"mag_psd\",\"lightcurve_timeseries\",\"lightcurve_psd\",\"plasma_timeseries\",\"plasma_psd\"):\n",
    "        p = plots.get(key)\n",
    "        if p:\n",
    "            lines.append(f\"![{key}]({Path(p).name})\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "write_report_md(RUN_DIR/\"report.md\", plots)\n",
    "print(f\"  Wrote: {RUN_DIR/'report.md'}\")\n",
    "\n",
    "# PDF\n",
    "ok_pdf = write_pdf(RUN_DIR/\"report.md\",\n",
    "                   images=[plots.get(\"mag_timeseries\"), plots.get(\"mag_psd\"),\n",
    "                           plots.get(\"lightcurve_timeseries\"), plots.get(\"lightcurve_psd\"),\n",
    "                           plots.get(\"plasma_timeseries\"), plots.get(\"plasma_psd\")],\n",
    "                   out_pdf=RUN_DIR/\"report.pdf\",\n",
    "                   title=\"3I Atlas — Comet Watch\")\n",
    "print(f\"  PDF:   {RUN_DIR/'report.pdf' if ok_pdf else '(skipped; fpdf missing)'}\")\n",
    "\n",
    "print(f\"[{ts_local()}] Done. — Comet Watch bundle ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "357cac23-c81e-4717-aaa5-507c818c2ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- HOTFIX PATCH (paste into your Comet Watch cell after imports) -----------\n",
    "\n",
    "# 1) Harden the exclude list so DNA/association tables never get picked as space-weather.\n",
    "try:\n",
    "    EXCLUDE_PATTERNS\n",
    "except NameError:\n",
    "    EXCLUDE_PATTERNS = []\n",
    "EXCLUDE_PATTERNS = list(set(EXCLUDE_PATTERNS + [\n",
    "    \"gwas\", \"association\", \"associations\", \"assoc\", \"catalog\", \"genetic\", \"phe\"\n",
    "]))\n",
    "\n",
    "# 2) Token-aware kind classifier so \"association\" no longer matches \"ion\".\n",
    "import re\n",
    "def _has_token(name: str, token: str) -> bool:\n",
    "    # word/segment boundaries: start or [_ - . space], then token, then boundary\n",
    "    return re.search(rf'(^|[_\\-\\s\\.]){re.escape(token)}([_\\-\\s\\.]|$)', name) is not None\n",
    "\n",
    "def classify_kind(path: Path) -> str:\n",
    "    s = path.name.lower()\n",
    "    if \"mag\" in s or _has_token(s, \"bz\") or _has_token(s, \"bt\") or \"magnet\" in s:\n",
    "        return \"mag\"\n",
    "    if \"lightcurve\" in s or (\"light\" in s and \"curve\" in s) or \"flux\" in s or \"brightness\" in s:\n",
    "        return \"lightcurve\"\n",
    "    if \"plasma\" in s or _has_token(s,\"density\") or _has_token(s,\"velocity\") or _has_token(s,\"temp\"):\n",
    "        return \"plasma\"\n",
    "    if _has_token(s,\"spectrum\") or _has_token(s,\"spectra\") or _has_token(s,\"theta\") or _has_token(s,\"freq\") or \"frequency\" in s:\n",
    "        return \"spectrum\"\n",
    "    return \"other\"\n",
    "\n",
    "# 3) Always return a pandas Series from parse_time (prevents DatetimeIndex .iloc errors).\n",
    "def parse_time(df):\n",
    "    cols = list(df.columns)\n",
    "    tcol = next((c for c in cols if str(c).lower() in TIME_COLS), None)\n",
    "    if tcol is not None:\n",
    "        t = pd.to_datetime(df[tcol], errors=\"coerce\", utc=False)\n",
    "        # If parsing fails (lots of NaT), try treating values as epoch seconds; then fall back to index.\n",
    "        if hasattr(t, \"isna\") and t.isna().mean() > 0.8:\n",
    "            try:\n",
    "                base = pd.to_datetime(\"1970-01-01\")\n",
    "                t = base + pd.to_timedelta(pd.to_numeric(df[tcol], errors=\"coerce\"), unit=\"s\")\n",
    "            except Exception:\n",
    "                t = pd.to_datetime(df.index, errors=\"coerce\")\n",
    "    else:\n",
    "        t = pd.to_datetime(df.index, errors=\"coerce\")\n",
    "\n",
    "    # Ensure a Series, not an Index (so .iloc works reliably downstream)\n",
    "    if isinstance(t, (pd.DatetimeIndex, pd.Index)):\n",
    "        t = pd.Series(t, index=df.index, name=\"time\")\n",
    "    else:\n",
    "        t = t.rename(\"time\")\n",
    "    return t\n",
    "# --- end HOTFIX PATCH --------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7625ffc-ccab-45d9-b29b-9fbfa2bc5862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-10-29 03:06:12] 3I Atlas — Comet Watch starting…\n",
      "  Run dir: C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_runs\\comet_watch_checkin\\20251029-070612Z\n",
      "  Scanning: C:\\Users\\caleb\\CNT_Lab\n",
      "  Scanning: E:\\CNT\n",
      "  Scanning: E:\\CNT\\notebooks\\archive\n",
      "  Scanning: E:\\CNT\\notebooks\\archive\n",
      "  Selected sources: {'plasma': 'C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\artifacts\\\\tables\\\\migrated__gwas-catalog-all-associations__21f38b1a.tsv', 'spectrum': 'C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\artifacts\\\\tables\\\\migrated__sim-theta__fc536f2f.csv', 'mag': 'C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\notebooks\\\\archive\\\\cnt_3i_atlas_all8_20251024-054159Z_3de16d1a\\\\data\\\\noaa_mag_3d.csv', 'lightcurve': 'C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\notebooks\\\\archive\\\\cnt_3i_atlas_all8_20251024-034610Z_0f216bd2\\\\out\\\\tables\\\\lightcurve_theta.csv'}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DatetimeIndex' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 459\u001b[39m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mplasma\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m bucket:\n\u001b[32m    458\u001b[39m     df = read_pd(bucket[\u001b[33m\"\u001b[39m\u001b[33mplasma\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     pla_res, ev = \u001b[43manalyze_plasma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m     events.extend(ev)\n\u001b[32m    461\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pla_res:\n\u001b[32m    462\u001b[39m         \u001b[38;5;66;03m# plot first field\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 297\u001b[39m, in \u001b[36manalyze_plasma\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m    295\u001b[39m         seg = \u001b[38;5;28mslice\u001b[39m(a,b+\u001b[32m1\u001b[39m)\n\u001b[32m    296\u001b[39m         peak_i = a + \u001b[38;5;28mint\u001b[39m(np.nanargmax(z[seg]))\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m         events.append(\u001b[38;5;28mdict\u001b[39m(source=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mplasma:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, start=\u001b[38;5;28mstr\u001b[39m(\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m[a]), end=\u001b[38;5;28mstr\u001b[39m(t.iloc[b]),\n\u001b[32m    298\u001b[39m                            peak_time=\u001b[38;5;28mstr\u001b[39m(t.iloc[peak_i]), peak_z=\u001b[38;5;28mfloat\u001b[39m(z[peak_i]),\n\u001b[32m    299\u001b[39m                            peak_val=\u001b[38;5;28mfloat\u001b[39m(x[peak_i]), count=\u001b[38;5;28mint\u001b[39m(b-a+\u001b[32m1\u001b[39m)))\n\u001b[32m    300\u001b[39m     series[\u001b[38;5;28mstr\u001b[39m(col)] = \u001b[38;5;28mdict\u001b[39m(time=t, value=x, z=z, dt=dt)\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m series, events\n",
      "\u001b[31mAttributeError\u001b[39m: 'DatetimeIndex' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "# === 3I Atlas — COMET / Atmosphere Watch Check-In (single cell) ==============\n",
    "# Purpose: Scan your CNT roots for space-weather series (NOAA MAG, lightcurves,\n",
    "#          plasma, spectra), analyze anomalies (robust z), and emit a tidy bundle:\n",
    "#          - comet_watch_facts.md (human)\n",
    "#          - report.md (+ report.pdf, Unicode-safe)\n",
    "#          - events.csv (all detections with start/end/peak z)\n",
    "#          - summary_stats.csv (per-stream stats)\n",
    "#          - plots/: time series, spectra/spectrograms\n",
    "# Notes:\n",
    "#   - Offline only; uses numpy/pandas/matplotlib (+fpdf if present).\n",
    "#   - Zero gene/DNA selection; hard-excludes those.\n",
    "#   - Robust Z via MAD; rolls adapt to sampling cadence.\n",
    "# ============================================================================\n",
    "\n",
    "import os, re, sys, json, glob, math, platform\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# ---------- Config ------------------------------------------------------------\n",
    "PACK_DIR = None  # set to your pack to skip discovery (e.g., the ..._vector_embedding dir)\n",
    "ROOT_HINTS = [\n",
    "    r\"C:\\Users\\caleb\\CNT_Lab\",\n",
    "    r\"E:\\CNT\",\n",
    "    r\"E:\\CNT\\notebooks\\archive\",\n",
    "    r\"D:\\CNT\",\n",
    "    r\"C:\\CNT\",\n",
    "    str(Path.cwd()),\n",
    "]\n",
    "\n",
    "RUN_BASE_HINTS = [\n",
    "    r\"C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\cnt_runs\\comet_watch_checkin\",\n",
    "    r\"E:\\CNT\\notebooks\\archive\\cnt_runs\\comet_watch_checkin\",\n",
    "    str(Path.cwd() / \"cnt_runs\" / \"comet_watch_checkin\"),\n",
    "]\n",
    "\n",
    "INCLUDE_PATTERNS = [\n",
    "    \"noaa\", \"mag\", \"magnet\", \"bz\", \"bt\",\n",
    "    \"lightcurve\", \"flux\", \"brightness\",\n",
    "    \"plasma\", \"ion\", \"density\", \"velocity\", \"temp\",\n",
    "    \"spectrum\", \"spectra\", \"theta\", \"freq\", \"frequency\", \"fft\",\n",
    "    \"aurora\", \"iono\", \"solar_wind\"\n",
    "]\n",
    "EXCLUDE_PATTERNS = [\n",
    "    \"gene\", \"genome\", \"rna\", \"tpm\", \"fpkm\", \"counts\", \"expr\", \"thread_edges\", \"gtex\"\n",
    "]\n",
    "ALLOWED_EXT = [\".csv\", \".tsv\", \".parquet\", \".feather\"]  # (time series; skip npz/npy here)\n",
    "\n",
    "Z_THR_MAG = 3.5\n",
    "Z_THR_LC  = 4.0\n",
    "Z_THR_PLA = 3.5\n",
    "\n",
    "# ---------- Optional deps -----------------------------------------------------\n",
    "USE_POLARS = False\n",
    "try:\n",
    "    import pandas as pd\n",
    "except Exception:\n",
    "    pd = None\n",
    "\n",
    "if USE_POLARS:\n",
    "    try:\n",
    "        import polars as pl\n",
    "    except Exception:\n",
    "        USE_POLARS = False\n",
    "\n",
    "try:\n",
    "    from fpdf import FPDF\n",
    "except Exception:\n",
    "    FPDF = None\n",
    "\n",
    "# ---------- Utils -------------------------------------------------------------\n",
    "def ts_utc():   return datetime.now(timezone.utc).strftime(\"%Y%m%d-%H%M%SZ\")\n",
    "def ts_local(): return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "def ensure_dir(p: Path): p.mkdir(parents=True, exist_ok=True); return p\n",
    "\n",
    "def pick_run_base():\n",
    "    for p in RUN_BASE_HINTS:\n",
    "        path = Path(p)\n",
    "        try:\n",
    "            path.mkdir(parents=True, exist_ok=True)\n",
    "            return str(path)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return str(Path.cwd() / \"cnt_runs\" / \"comet_watch_checkin\")\n",
    "\n",
    "def normalize_pack_dir(p: Path) -> Path:\n",
    "    parts = list(p.parts)\n",
    "    if len(parts) >= 2 and parts[-1].lower() == \"vector_embedding\" and parts[-2].lower() == \"vector_embedding\":\n",
    "        return Path(*parts[:-1])\n",
    "    name = p.name.lower()\n",
    "    if name.endswith(\"_vector_embedding_vector_embedding\"):\n",
    "        return p.with_name(p.name[: -len(\"_vector_embedding\")])\n",
    "    return p\n",
    "\n",
    "def all_existing(paths): return [Path(p) for p in paths if Path(p).exists()]\n",
    "\n",
    "def list_spaceweather_files(root: Path):\n",
    "    hits = []\n",
    "    for ext in ALLOWED_EXT:\n",
    "        for key in INCLUDE_PATTERNS:\n",
    "            pat = str(root / \"**\" / f\"*{key}*{ext}\")\n",
    "            hits.extend([Path(p) for p in glob.glob(pat, recursive=True)])\n",
    "    # dedup & filter excludes\n",
    "    z, seen = [], set()\n",
    "    for h in hits:\n",
    "        if not h.is_file(): continue\n",
    "        low = str(h).lower()\n",
    "        if any(x in low for x in EXCLUDE_PATTERNS): continue\n",
    "        k = str(h.resolve()).lower()\n",
    "        if k not in seen:\n",
    "            seen.add(k); z.append(h)\n",
    "    z.sort(key=lambda p: p.stat().st_size if p.exists() else 0, reverse=True)\n",
    "    return z\n",
    "\n",
    "def detect_format_read(path: Path, nrows=None):\n",
    "    suff = path.suffix.lower()\n",
    "    if USE_POLARS:\n",
    "        if 'pl' not in globals():\n",
    "            raise RuntimeError(\"polars not available\")\n",
    "        if suff in (\".csv\", \".tsv\"):\n",
    "            sep = \",\" if suff==\".csv\" else \"\\t\"\n",
    "            df = pl.read_csv(str(path), separator=sep)\n",
    "            return df if nrows is None else df.head(nrows)\n",
    "        elif suff == \".parquet\":\n",
    "            df = pl.read_parquet(str(path)); return df if nrows is None else df.head(nrows)\n",
    "        elif suff == \".feather\":\n",
    "            df = pl.read_ipc(str(path));    return df if nrows is None else df.head(nrows)\n",
    "        else:\n",
    "            raise RuntimeError(f\"unsupported: {suff}\")\n",
    "    else:\n",
    "        if pd is None: raise RuntimeError(\"pandas not available\")\n",
    "        if suff in (\".csv\", \".tsv\"):\n",
    "            sep = \",\" if suff==\".csv\" else \"\\t\"\n",
    "            return pd.read_csv(path, nrows=nrows, sep=sep)\n",
    "        elif suff == \".parquet\":\n",
    "            return pd.read_parquet(path)\n",
    "        elif suff == \".feather\":\n",
    "            return pd.read_feather(path)\n",
    "        else:\n",
    "            raise RuntimeError(f\"unsupported: {suff}\")\n",
    "\n",
    "def to_pd(df):\n",
    "    if pd is None: raise RuntimeError(\"pandas not available\")\n",
    "    if USE_POLARS: return df.to_pandas()\n",
    "    return df\n",
    "\n",
    "TIME_COLS = {\"time\",\"timestamp\",\"datetime\",\"date\",\"utc\",\"t\"}\n",
    "\n",
    "def parse_time(df):\n",
    "    cols = [c for c in df.columns]\n",
    "    # choose a time column if present\n",
    "    tcol = None\n",
    "    for c in cols:\n",
    "        if str(c).lower() in TIME_COLS:\n",
    "            tcol = c; break\n",
    "    if tcol is not None:\n",
    "        t = pd.to_datetime(df[tcol], errors=\"coerce\", utc=False)\n",
    "        # if most NaT, maybe it is numeric seconds\n",
    "        if t.isna().mean() > 0.8:\n",
    "            try:\n",
    "                base = pd.to_datetime(\"1970-01-01\")\n",
    "                t = base + pd.to_timedelta(pd.to_numeric(df[tcol], errors=\"coerce\"), unit=\"s\")\n",
    "            except Exception:\n",
    "                t = pd.to_datetime(df.index, errors=\"coerce\")\n",
    "    else:\n",
    "        # fallback: index or a monotonic column\n",
    "        try:\n",
    "            t = pd.to_datetime(df.index, errors=\"coerce\")\n",
    "        except Exception:\n",
    "            t = pd.Series(pd.NaT, index=df.index)\n",
    "    return t\n",
    "\n",
    "def sampling_seconds(t: \"pd.Series\"):\n",
    "    try:\n",
    "        dt = (t.dropna().diff().dt.total_seconds()).median()\n",
    "        if np.isnan(dt) or dt <= 0: return 60.0\n",
    "        return float(dt)\n",
    "    except Exception:\n",
    "        return 60.0\n",
    "\n",
    "def robust_stats(x: np.ndarray):\n",
    "    x = x[np.isfinite(x)]\n",
    "    if x.size == 0: return np.nan, np.nan\n",
    "    med = np.median(x)\n",
    "    mad = np.median(np.abs(x - med)) * 1.4826\n",
    "    if mad < 1e-9: mad = 1e-9\n",
    "    return med, mad\n",
    "\n",
    "def rolling_robust_z(x: np.ndarray, win: int):\n",
    "    # compute rolling median & MAD with simple edges\n",
    "    n = len(x); z = np.full(n, np.nan)\n",
    "    half = max(1, win//2)\n",
    "    for i in range(n):\n",
    "        a = max(0, i-half); b = min(n, i+half+1)\n",
    "        med, mad = robust_stats(x[a:b])\n",
    "        z[i] = (x[i] - med)/mad\n",
    "    return z\n",
    "\n",
    "def cluster_bool_runs(t, mask):\n",
    "    # return list of (start_idx, end_idx) contiguous True runs\n",
    "    runs = []\n",
    "    in_run = False; s = 0\n",
    "    for i, m in enumerate(mask):\n",
    "        if m and not in_run:\n",
    "            in_run = True; s = i\n",
    "        elif not m and in_run:\n",
    "            runs.append((s, i-1)); in_run = False\n",
    "    if in_run: runs.append((s, len(mask)-1))\n",
    "    return runs\n",
    "\n",
    "def analyze_magnetometer(df):\n",
    "    # find Bx,By,Bz or x,y,z columns\n",
    "    cols = [c for c in df.columns if str(c).lower() not in TIME_COLS]\n",
    "    low = [str(c).lower() for c in cols]\n",
    "    def pick(name): \n",
    "        for i,c in enumerate(cols):\n",
    "            if name in low[i]: return c\n",
    "        return None\n",
    "    bx = pick(\"bx\") or pick(\"x\")\n",
    "    by = pick(\"by\") or pick(\"y\")\n",
    "    bz = pick(\"bz\") or pick(\"z\")\n",
    "    M = None\n",
    "    if bx and by and bz:\n",
    "        vec = df[[bx,by,bz]].astype(float).values\n",
    "        M = np.sqrt((vec**2).sum(axis=1))\n",
    "    else:\n",
    "        # fall back to any single numeric column\n",
    "        num = df.select_dtypes(include=[np.number])\n",
    "        if num.shape[1] >= 1:\n",
    "            M = num.iloc[:,0].astype(float).values\n",
    "    if M is None: return None, []\n",
    "    t = parse_time(df)\n",
    "    dt = sampling_seconds(t)\n",
    "    win = int(max(5, min(601, round(300.0/dt))))  # ~5min window, min 5, max 601\n",
    "    z  = rolling_robust_z(M, win)\n",
    "    hits = np.array(z) > Z_THR_MAG\n",
    "    runs = cluster_bool_runs(t, hits)\n",
    "    ev = []\n",
    "    for a,b in runs:\n",
    "        seg = slice(a,b+1)\n",
    "        peak_i = a + int(np.nanargmax(z[seg]))\n",
    "        ev.append(dict(source=\"mag\", start=str(t.iloc[a]), end=str(t.iloc[b]),\n",
    "                       peak_time=str(t.iloc[peak_i]), peak_z=float(z[peak_i]),\n",
    "                       peak_val=float(M[peak_i]), count=int(b-a+1)))\n",
    "    return dict(time=t, value=M, z=z, dt=dt), ev\n",
    "\n",
    "def analyze_lightcurve(df):\n",
    "    # look for flux/brightness columns\n",
    "    lc_cols = [c for c in df.columns if any(k in str(c).lower() for k in [\"flux\",\"bright\",\"intensity\",\"counts\",\"mag_\",\"light\"])]\n",
    "    if not lc_cols:\n",
    "        num = df.select_dtypes(include=[np.number])\n",
    "        if num.shape[1] == 0: return None, []\n",
    "        c = num.columns[0]\n",
    "    else:\n",
    "        c = lc_cols[0]\n",
    "    x = df[c].astype(float).values\n",
    "    t = parse_time(df)\n",
    "    dt = sampling_seconds(t)\n",
    "    win = int(max(5, min(1201, round(600.0/dt))))  # ~10min window\n",
    "    z  = rolling_robust_z(x, win)\n",
    "    hits = np.array(z) > Z_THR_LC\n",
    "    runs = cluster_bool_runs(t, hits)\n",
    "    ev = []\n",
    "    for a,b in runs:\n",
    "        seg = slice(a,b+1)\n",
    "        peak_i = a + int(np.nanargmax(z[seg]))\n",
    "        ev.append(dict(source=\"lightcurve\", start=str(t.iloc[a]), end=str(t.iloc[b]),\n",
    "                       peak_time=str(t.iloc[peak_i]), peak_z=float(z[peak_i]),\n",
    "                       peak_val=float(x[peak_i]), count=int(b-a+1)))\n",
    "    return dict(time=t, value=x, z=z, dt=dt, col=str(c)), ev\n",
    "\n",
    "def analyze_plasma(df):\n",
    "    # look for density, speed/velocity, temperature\n",
    "    cols = {str(c).lower(): c for c in df.columns}\n",
    "    fields = []\n",
    "    for key in [\"density\",\"dens\",\"n\",\"speed\",\"velocity\",\"vel\",\"temp\",\"temperature\"]:\n",
    "        for k,v in cols.items():\n",
    "            if key == k or key in k:\n",
    "                fields.append(v)\n",
    "    if not fields:\n",
    "        num = df.select_dtypes(include=[np.number])\n",
    "        if num.shape[1]==0: return None,[]\n",
    "        fields = [num.columns[0]]\n",
    "    t = parse_time(df)\n",
    "    dt = sampling_seconds(t)\n",
    "    events = []\n",
    "    series = {}\n",
    "    for col in fields[:3]:  # limit to a few\n",
    "        x = df[col].astype(float).values\n",
    "        win = int(max(5, min(1201, round(600.0/dt))))\n",
    "        z  = rolling_robust_z(x, win)\n",
    "        hits = np.array(z) > Z_THR_PLA\n",
    "        runs = cluster_bool_runs(t, hits)\n",
    "        for a,b in runs:\n",
    "            seg = slice(a,b+1)\n",
    "            peak_i = a + int(np.nanargmax(z[seg]))\n",
    "            events.append(dict(source=f\"plasma:{col}\", start=str(t.iloc[a]), end=str(t.iloc[b]),\n",
    "                               peak_time=str(t.iloc[peak_i]), peak_z=float(z[peak_i]),\n",
    "                               peak_val=float(x[peak_i]), count=int(b-a+1)))\n",
    "        series[str(col)] = dict(time=t, value=x, z=z, dt=dt)\n",
    "    return series, events\n",
    "\n",
    "def classify_kind(path: Path):\n",
    "    s = path.name.lower()\n",
    "    if any(k in s for k in [\"mag\",\"bz\",\"bt\",\"magnet\"]): return \"mag\"\n",
    "    if \"lightcurve\" in s or (\"light\" in s and \"curve\" in s) or \"flux\" in s or \"brightness\" in s: return \"lightcurve\"\n",
    "    if \"plasma\" in s or \"ion\" in s or \"density\" in s or \"velocity\" in s or \"temp\" in s: return \"plasma\"\n",
    "    if \"spectrum\" in s or \"theta\" in s or \"freq\" in s or \"frequency\" in s: return \"spectrum\"\n",
    "    return \"other\"\n",
    "\n",
    "def read_pd(path: Path, nrows=None):\n",
    "    return to_pd(detect_format_read(path, nrows=nrows))\n",
    "\n",
    "# ---------- PDF (Unicode-safe) -----------------------------------------------\n",
    "def write_pdf(report_md_path: Path, images, out_pdf: Path, title=\"3I Atlas — Comet Watch\"):\n",
    "    if FPDF is None: return False\n",
    "    try:\n",
    "        from fpdf.enums import XPos, YPos\n",
    "        HAVE_ENUMS = True\n",
    "    except Exception:\n",
    "        HAVE_ENUMS = False\n",
    "    REPL = {\"\\u2011\":\"-\",\"\\u2013\":\"-\",\"\\u2014\":\"-\",\"\\u2018\":\"'\",\"\\u2019\":\"'\",\"\\u201c\":'\"',\"\\u201d\":'\"',\"\\u2026\":\"...\"}\n",
    "    def ascii_fallback(s: str):\n",
    "        for k,v in REPL.items(): s = s.replace(k, v)\n",
    "        return s\n",
    "    ttf_candidates = [r\"C:\\Windows\\Fonts\\arial.ttf\", r\"C:\\Windows\\Fonts\\DejaVuSans.ttf\",\n",
    "                      r\"C:\\Windows\\Fonts\\Calibri.ttf\", r\"C:\\Windows\\Fonts\\segoeui.ttf\"]\n",
    "    pdf = FPDF(orientation=\"P\", unit=\"mm\", format=\"A4\")\n",
    "    pdf.set_auto_page_break(auto=True, margin=12); pdf.add_page()\n",
    "    used_unicode = False\n",
    "    for ttf in ttf_candidates:\n",
    "        if Path(ttf).exists():\n",
    "            try:\n",
    "                try: pdf.add_font(\"U\", \"\", ttf, uni=True)\n",
    "                except TypeError: pdf.add_font(\"U\", \"\", ttf)\n",
    "                pdf.set_font(\"U\", \"\", 16); used_unicode = True; break\n",
    "            except Exception: pass\n",
    "    if not used_unicode: pdf.set_font(\"helvetica\", \"\", 16)\n",
    "    safe_title = title if used_unicode else ascii_fallback(title)\n",
    "    if HAVE_ENUMS: pdf.cell(0,10,safe_title,new_x=XPos.LMARGIN,new_y=YPos.NEXT)\n",
    "    else:          pdf.cell(0,10,safe_title,ln=1)\n",
    "    pdf.set_font(\"U\" if used_unicode else \"helvetica\", \"\", 10)\n",
    "    with open(report_md_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip().startswith(\"!\"): continue\n",
    "            pdf.multi_cell(0,5,line if used_unicode else ascii_fallback(line))\n",
    "    for img in images:\n",
    "        if img and Path(img).exists():\n",
    "            pdf.add_page(); pdf.image(str(img), x=10, y=20, w=180)\n",
    "            if HAVE_ENUMS: pdf.cell(0,6,Path(img).name,new_x=XPos.LMARGIN,new_y=YPos.NEXT)\n",
    "            else:          pdf.ln(6)\n",
    "    ensure_dir(Path(out_pdf).parent); pdf.output(str(out_pdf)); return True\n",
    "\n",
    "# ---------- Main --------------------------------------------------------------\n",
    "RUN_BASE = pick_run_base()\n",
    "STAMP    = ts_utc()\n",
    "RUN_DIR  = ensure_dir(Path(RUN_BASE) / STAMP)\n",
    "print(f\"[{ts_local()}] 3I Atlas — Comet Watch starting…\")\n",
    "print(f\"  Run dir: {RUN_DIR}\")\n",
    "\n",
    "# Discover candidates\n",
    "cand_files = []\n",
    "roots = all_existing(ROOT_HINTS)\n",
    "if PACK_DIR:\n",
    "    roots = [normalize_pack_dir(Path(PACK_DIR))] + roots\n",
    "for r in roots:\n",
    "    print(f\"  Scanning: {r}\")\n",
    "    cand_files.extend(list_spaceweather_files(r))\n",
    "\n",
    "# Dedup by realpath\n",
    "uniq, seen = [], set()\n",
    "for f in cand_files:\n",
    "    k = str(f.resolve()).lower()\n",
    "    if k not in seen:\n",
    "        seen.add(k); uniq.append(f)\n",
    "cand_files = uniq\n",
    "if not cand_files:\n",
    "    raise SystemExit(\"No space-weather files found. Set PACK_DIR to your 3I pack root or add NOAA/lightcurve/plasma tables.\")\n",
    "\n",
    "# Pick at most one per kind (largest)\n",
    "bucket = {}\n",
    "for f in cand_files:\n",
    "    kind = classify_kind(f)\n",
    "    if kind == \"other\": continue\n",
    "    if kind not in bucket: bucket[kind] = f\n",
    "    else:\n",
    "        if f.stat().st_size > bucket[kind].stat().st_size:\n",
    "            bucket[kind] = f\n",
    "\n",
    "print(\"  Selected sources:\", {k:str(v) for k,v in bucket.items()})\n",
    "\n",
    "# Read & analyze\n",
    "events = []\n",
    "summaries = []\n",
    "plots = {}\n",
    "\n",
    "def plot_series(t, y, out_path, title, xlabel=\"time\", ylabel=\"value\"):\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    ensure_dir(Path(out_path).parent)\n",
    "    plt.figure()\n",
    "    try:\n",
    "        plt.plot(t, y)\n",
    "    except Exception:\n",
    "        # fallback if t not datetime\n",
    "        plt.plot(np.arange(len(y)), y)\n",
    "        xlabel = \"index\"\n",
    "    plt.title(title); plt.xlabel(xlabel); plt.ylabel(ylabel)\n",
    "    plt.tight_layout(); plt.savefig(out_path, dpi=150); plt.close()\n",
    "\n",
    "def plot_psd(y, dt, out_path, title):\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    ensure_dir(Path(out_path).parent)\n",
    "    n = len(y); \n",
    "    if n < 16:\n",
    "        return\n",
    "    # simple periodogram\n",
    "    Y = np.fft.rfft(y - np.nanmean(y))\n",
    "    f = np.fft.rfftfreq(n, d=max(dt,1e-6))\n",
    "    P = (np.abs(Y)**2)/n\n",
    "    plt.figure()\n",
    "    plt.semilogy(f, P + 1e-12)\n",
    "    plt.title(title); plt.xlabel(\"Hz\"); plt.ylabel(\"Power\")\n",
    "    plt.tight_layout(); plt.savefig(out_path, dpi=150); plt.close()\n",
    "\n",
    "# Magnetometer\n",
    "mag_res = None\n",
    "if \"mag\" in bucket:\n",
    "    df = read_pd(bucket[\"mag\"])\n",
    "    mag_res, ev = analyze_magnetometer(df)\n",
    "    events.extend(ev)\n",
    "    if mag_res:\n",
    "        plot_series(mag_res[\"time\"], mag_res[\"value\"], RUN_DIR/\"plots/mag_timeseries.png\", \"Magnetometer | vector magnitude\")\n",
    "        plots[\"mag_timeseries\"] = str(RUN_DIR/\"plots/mag_timeseries.png\")\n",
    "        plot_psd(np.nan_to_num(mag_res[\"value\"]), mag_res[\"dt\"], RUN_DIR/\"plots/mag_psd.png\", \"Magnetometer | spectrum\")\n",
    "        plots[\"mag_psd\"] = str(RUN_DIR/\"plots/mag_psd.png\")\n",
    "        summaries.append((\"mag\", bucket[\"mag\"].name, len(ev)))\n",
    "\n",
    "# Lightcurve\n",
    "lc_res = None\n",
    "if \"lightcurve\" in bucket:\n",
    "    df = read_pd(bucket[\"lightcurve\"])\n",
    "    lc_res, ev = analyze_lightcurve(df)\n",
    "    events.extend(ev)\n",
    "    if lc_res:\n",
    "        plot_series(lc_res[\"time\"], lc_res[\"value\"], RUN_DIR/\"plots/lightcurve_timeseries.png\", f\"Lightcurve | {lc_res.get('col','flux')}\")\n",
    "        plots[\"lightcurve_timeseries\"] = str(RUN_DIR/\"plots/lightcurve_timeseries.png\")\n",
    "        plot_psd(np.nan_to_num(lc_res[\"value\"]), lc_res[\"dt\"], RUN_DIR/\"plots/lightcurve_psd.png\", \"Lightcurve | spectrum\")\n",
    "        plots[\"lightcurve_psd\"] = str(RUN_DIR/\"plots/lightcurve_psd.png\")\n",
    "        summaries.append((\"lightcurve\", bucket[\"lightcurve\"].name, len(ev)))\n",
    "\n",
    "# Plasma\n",
    "pla_res = None\n",
    "if \"plasma\" in bucket:\n",
    "    df = read_pd(bucket[\"plasma\"])\n",
    "    pla_res, ev = analyze_plasma(df)\n",
    "    events.extend(ev)\n",
    "    if pla_res:\n",
    "        # plot first field\n",
    "        k0 = next(iter(pla_res.keys()))\n",
    "        plot_series(pla_res[k0][\"time\"], pla_res[k0][\"value\"], RUN_DIR/\"plots/plasma_timeseries.png\", f\"Plasma | {k0}\")\n",
    "        plots[\"plasma_timeseries\"] = str(RUN_DIR/\"plots/plasma_timeseries.png\")\n",
    "        plot_psd(np.nan_to_num(pla_res[k0][\"value\"]), pla_res[k0][\"dt\"], RUN_DIR/\"plots/plasma_psd.png\", f\"Plasma | {k0} spectrum\")\n",
    "        plots[\"plasma_psd\"] = str(RUN_DIR/\"plots/plasma_psd.png\")\n",
    "        summaries.append((\"plasma\", bucket[\"plasma\"].name, len([e for e in events if e['source'].startswith('plasma:')])))\n",
    "\n",
    "# Build events table & stats\n",
    "import csv\n",
    "ensure_dir(Path(RUN_DIR/\"events.csv\").parent)\n",
    "with open(RUN_DIR/\"events.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow([\"source\",\"start\",\"end\",\"peak_time\",\"peak_z\",\"peak_val\",\"count\"])\n",
    "    for e in events:\n",
    "        w.writerow([e[\"source\"], e[\"start\"], e[\"end\"], e[\"peak_time\"], f\"{e['peak_z']:.3f}\", f\"{e['peak_val']:.6g}\", e[\"count\"]])\n",
    "\n",
    "# Simple cross-stream correlation (if mag + lightcurve available)\n",
    "corr_note = \"n/a\"\n",
    "try:\n",
    "    if mag_res and lc_res:\n",
    "        # align by min length after dropping NaNs\n",
    "        v1 = np.nan_to_num(mag_res[\"value\"]).astype(float)\n",
    "        v2 = np.nan_to_num(lc_res[\"value\"]).astype(float)\n",
    "        n = min(len(v1), len(v2))\n",
    "        if n >= 32:\n",
    "            c = np.corrcoef(v1[-n:], v2[-n:])[0,1]\n",
    "            corr_note = f\"{c:.3f}\"\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Summary stats CSV\n",
    "summary_rows = [\n",
    "    [\"mag_file\", bucket.get(\"mag\").name if \"mag\" in bucket else \"\"],\n",
    "    [\"lightcurve_file\", bucket.get(\"lightcurve\").name if \"lightcurve\" in bucket else \"\"],\n",
    "    [\"plasma_file\", bucket.get(\"plasma\").name if \"plasma\" in bucket else \"\"],\n",
    "    [\"events_total\", len(events)],\n",
    "    [\"mag_lightcurve_corr\", corr_note],\n",
    "]\n",
    "with open(RUN_DIR/\"summary_stats.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    w = csv.writer(f); w.writerow([\"metric\",\"value\"]); w.writerows(summary_rows)\n",
    "\n",
    "# Snapshot & deltas\n",
    "def read_json(path: Path):\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f: return json.load(f)\n",
    "    except Exception: return None\n",
    "def write_json(path: Path, obj):\n",
    "    ensure_dir(Path(path).parent)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f: json.dump(obj, f, ensure_ascii=False, indent=2)\n",
    "def last_snapshot(dir_base: Path):\n",
    "    files = glob.glob(str(dir_base / \"*\" / \"snapshot.json\"))\n",
    "    if not files: return None, None\n",
    "    files.sort(key=lambda p: os.path.getmtime(p), reverse=True)\n",
    "    p = Path(files[0]); return p, read_json(p)\n",
    "\n",
    "SNAPSHOT_PATH = Path(RUN_DIR/\"snapshot.json\")\n",
    "RUN_BASE = Path(RUN_BASE_HINTS[0]) if Path(RUN_BASE_HINTS[0]).exists() else Path(pick_run_base())\n",
    "prev_path, prev = last_snapshot(RUN_BASE)\n",
    "deltas = None\n",
    "if prev:\n",
    "    prev_events = int(prev.get(\"summary\",{}).get(\"events_total\", 0))\n",
    "    deltas = dict(events_delta=len(events)-prev_events)\n",
    "    write_json(RUN_DIR/\"delta_summary.json\", deltas)\n",
    "else:\n",
    "    print(\"  No prior comet snapshot; this is the baseline.\")\n",
    "\n",
    "snapshot = {\n",
    "    \"meta\": {\n",
    "        \"stamp_utc\": ts_utc(), \"stamp_local\": ts_local(),\n",
    "        \"host\": platform.node(), \"python\": sys.version.split()[0],\n",
    "        \"sources\": {k: str(v) for k,v in bucket.items()},\n",
    "    },\n",
    "    \"summary\": {\"events_total\": len(events), \"mag_lightcurve_corr\": corr_note},\n",
    "}\n",
    "write_json(SNAPSHOT_PATH, snapshot)\n",
    "\n",
    "# Human facts\n",
    "facts = []\n",
    "facts.append(\"# Comet Watch — Facts (new)\")\n",
    "facts.append(\"\")\n",
    "facts.append(f\"- Sources: { {k: v.name for k,v in bucket.items()} }\")\n",
    "facts.append(f\"- Total events flagged (robust z): **{len(events)}**\")\n",
    "facts.append(f\"- Mag↔Lightcurve correlation (rough, last overlap): **{corr_note}**\")\n",
    "if events:\n",
    "    facts.append(\"\")\n",
    "    facts.append(\"## Top event peaks\")\n",
    "    # top by peak_z\n",
    "    top = sorted(events, key=lambda e: e[\"peak_z\"], reverse=True)[:10]\n",
    "    for e in top:\n",
    "        facts.append(f\"- [{e['source']}] {e['peak_time']}  z={e['peak_z']:.2f}  val={e['peak_val']:.6g}  window={e['start']}→{e['end']}\")\n",
    "else:\n",
    "    facts.append(\"\")\n",
    "    facts.append(\"_No events crossed the thresholds; consider lowering Z_THR_* or checking data windows._\")\n",
    "\n",
    "with open(RUN_DIR/\"comet_watch_facts.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(facts))\n",
    "print(f\"  Wrote: {RUN_DIR/'comet_watch_facts.md'}\")\n",
    "\n",
    "# Report.md (with images)\n",
    "def write_report_md(path: Path, plots):\n",
    "    lines = []\n",
    "    lines.append(f\"# 3I Atlas — Comet Watch Report ({ts_local()})\")\n",
    "    lines.append(\"\")\n",
    "    lines.append(f\"- Run dir: `{RUN_DIR}`\")\n",
    "    lines.append(f\"- Events: **{len(events)}**\")\n",
    "    lines.append(f\"- Mag↔Lightcurve correlation: **{corr_note}**\")\n",
    "    lines.append(\"\")\n",
    "    for key in (\"mag_timeseries\",\"mag_psd\",\"lightcurve_timeseries\",\"lightcurve_psd\",\"plasma_timeseries\",\"plasma_psd\"):\n",
    "        p = plots.get(key)\n",
    "        if p:\n",
    "            lines.append(f\"![{key}]({Path(p).name})\")\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "write_report_md(RUN_DIR/\"report.md\", plots)\n",
    "print(f\"  Wrote: {RUN_DIR/'report.md'}\")\n",
    "\n",
    "# PDF\n",
    "ok_pdf = write_pdf(RUN_DIR/\"report.md\",\n",
    "                   images=[plots.get(\"mag_timeseries\"), plots.get(\"mag_psd\"),\n",
    "                           plots.get(\"lightcurve_timeseries\"), plots.get(\"lightcurve_psd\"),\n",
    "                           plots.get(\"plasma_timeseries\"), plots.get(\"plasma_psd\")],\n",
    "                   out_pdf=RUN_DIR/\"report.pdf\",\n",
    "                   title=\"3I Atlas — Comet Watch\")\n",
    "print(f\"  PDF:   {RUN_DIR/'report.pdf' if ok_pdf else '(skipped; fpdf missing)'}\")\n",
    "\n",
    "print(f\"[{ts_local()}] Done. — Comet Watch bundle ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473e1613-25cc-4d52-9094-ba705b773f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CNT Lab (Py3.13)",
   "language": "python",
   "name": "cnt_lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
