{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d08ddaed",
   "metadata": {},
   "source": [
    "\n",
    "# CNT Topology Validation & Stability Map — Phase IV\n",
    "\n",
    "**Purpose**  \n",
    "Turn CNT's *visible topology* into *verified science* by:\n",
    "1. **Quantitative validation** — compare CNT field dynamics against real networks (EEG, gene co‑expression, etc.).  \n",
    "2. **Stability mapping** — find collapse margins (\\(K_c\\), knee, resilience) via Kuramoto/Ising on your graphs.  \n",
    "3. **Auto‑report** — export a concise markdown/HTML report + CSV artifacts.\n",
    "\n",
    "**Outputs** (saved under `artifacts_cnt_validation_<timestamp>/`):  \n",
    "- `metrics_summary.csv` — graph + dynamics metrics per dataset  \n",
    "- `validation_results.csv` — permutation tests, effect sizes, p‑values  \n",
    "- `stability_map.csv` — K vs R_mean per dataset  \n",
    "- `report.md` and `report.html` — one‑pager: methods, tables, highlights\n",
    "\n",
    "> Tip: You can run the **One‑Click Pipeline** near the bottom once you set input paths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d210116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNT_LAB_DIR: E:\\CNT\n",
      "Artifacts  : E:\\CNT\\notebooks\\archive\\artifacts_cnt_validation_20251104-031414Z\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% [setup] Environment & Paths\n",
    "import os, sys, math, json, glob, time, warnings, pathlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import networkx as nx\n",
    "except Exception:\n",
    "    print(\"Installing networkx…\")\n",
    "    %pip -q install networkx\n",
    "    import networkx as nx\n",
    "\n",
    "try:\n",
    "    import scipy\n",
    "    from scipy import stats, linalg, signal\n",
    "except Exception:\n",
    "    print(\"Installing scipy…\")\n",
    "    %pip -q install scipy\n",
    "    import scipy\n",
    "    from scipy import stats, linalg, signal\n",
    "\n",
    "# Optional: plotly for 3D previews (not required for headless runs)\n",
    "try:\n",
    "    import plotly.graph_objects as go\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Resolve CNT base dirs (customize if needed)\n",
    "CNT_LAB_DIR = os.environ.get(\"CNT_LAB_DIR\")\n",
    "WINDOWS_GUESSES = [r\"E:\\CNT\", r\"C:\\Users\\caleb\\CNT_Lab\", r\"C:\\Users\\caleb\\CNT\"]\n",
    "if CNT_LAB_DIR is None:\n",
    "    for guess in WINDOWS_GUESSES:\n",
    "        if Path(guess).exists():\n",
    "            CNT_LAB_DIR = guess\n",
    "            break\n",
    "if CNT_LAB_DIR is None:\n",
    "    CNT_LAB_DIR = str(Path.cwd())  # fallback\n",
    "\n",
    "# Timestamped artifact dir (created inside current working dir)\n",
    "_ts = datetime.utcnow().strftime(\"%Y%m%d-%H%M%SZ\")\n",
    "ARTIFACTS_DIR = Path(f\"artifacts_cnt_validation_{_ts}\")\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"CNT_LAB_DIR:\", CNT_LAB_DIR)\n",
    "print(\"Artifacts  :\", ARTIFACTS_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbf67ef",
   "metadata": {},
   "source": [
    "### Data — discover & load\n",
    "Provide paths to **real** networks (adjacency or correlation matrices) and optional **3D coordinates**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9cf60b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered 74 candidate CSV(s).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('cnt_correlates_report_20251015-163558',\n",
       "  'E:\\\\CNT\\\\notebooks\\\\archive\\\\cnt_correlates_report_20251015-163558.csv'),\n",
       " ('cnt_correlates_report_20251015-164130',\n",
       "  'E:\\\\CNT\\\\notebooks\\\\archive\\\\cnt_correlates_report_20251015-164130.csv'),\n",
       " ('top_gini_genes',\n",
       "  'E:\\\\CNT\\\\notebooks\\\\archive\\\\cnt_runs\\\\3i_atlas_checkin\\\\20251029-054356Z\\\\top_gini_genes.csv'),\n",
       " ('top_gini_genes__88859',\n",
       "  'E:\\\\CNT\\\\notebooks\\\\archive\\\\cnt_runs\\\\3i_atlas_checkin\\\\20251029-060737Z\\\\top_gini_genes.csv'),\n",
       " ('cfe_EEGBCI_subj1_motor_20251015-143523',\n",
       "  'E:\\\\CNT\\\\notebooks\\\\archive\\\\cnt_mega_out\\\\tables\\\\cfe_EEGBCI_subj1_motor_20251015-143523.csv')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# %% [data] Discovery & loaders\n",
    "import itertools\n",
    "from typing import Optional, Tuple, Dict\n",
    "from pathlib import Path\n",
    "\n",
    "def discover_csvs(base_dirs, patterns=(\"eeg\",\"coexp\",\"gene\",\"corr\",\"adj\",\"connect\",\"matrix\")):\n",
    "    \"\"\"\n",
    "    Recursively search for CSV files whose names hint at adjacency/correlation matrices.\n",
    "    Returns a dict {name: path}.\n",
    "    \"\"\"\n",
    "    hits = {}\n",
    "    for base in base_dirs:\n",
    "        base = Path(base)\n",
    "        if not base.exists(): \n",
    "            continue\n",
    "        for p in base.rglob(\"*.csv\"):\n",
    "            name = p.name.lower()\n",
    "            if any(k in name for k in patterns):\n",
    "                key = p.stem\n",
    "                if key in hits:\n",
    "                    key = f\"{key}__{abs(hash(str(p)))%99999}\"\n",
    "                hits[key] = str(p)\n",
    "    return hits\n",
    "\n",
    "def load_matrix(path: str, header=True) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load an NxN numeric matrix from CSV. Accepts header/no-header.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path, header=0 if header else None)\n",
    "    # If first column looks like an index, drop it\n",
    "    if df.shape[1] == df.shape[0] + 1 and not pd.to_numeric(df.columns[0], errors='coerce').notna().all():\n",
    "        df = df.iloc[:,1:]\n",
    "    mat = df.values.astype(float)\n",
    "    if mat.shape[0] == mat.shape[1]:\n",
    "        mat = (mat + mat.T) / 2.0\n",
    "        np.fill_diagonal(mat, 0.0)\n",
    "    return mat\n",
    "\n",
    "def corr_to_graph(corr: np.ndarray, threshold: float=0.35, keep_sign=False) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Build an undirected graph from a correlation matrix using a threshold.\n",
    "    If keep_sign=True, store edge attribute \"sign\" as np.sign(corr_ij).\n",
    "    \"\"\"\n",
    "    n = corr.shape[0]\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            c = corr[i,j]\n",
    "            if abs(c) >= threshold:\n",
    "                sgn = 1 if c >= 0 else -1\n",
    "                G.add_edge(i,j, weight=float(abs(c)))\n",
    "                if keep_sign:\n",
    "                    G[i][j]['sign'] = sgn\n",
    "    return G\n",
    "\n",
    "def adj_to_graph(adj: np.ndarray, weighted=False) -> nx.Graph:\n",
    "    \"\"\"\n",
    "    Build a graph from adjacency matrix; if weighted=False, any nonzero is an edge with weight=1.\n",
    "    \"\"\"\n",
    "    n = adj.shape[0]\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            v = adj[i,j]\n",
    "            if v != 0:\n",
    "                w = float(v if weighted else 1.0)\n",
    "                G.add_edge(i,j, weight=w)\n",
    "    return G\n",
    "\n",
    "# Example: Auto-discover candidates (preview first 5)\n",
    "CANDIDATES = discover_csvs([CNT_LAB_DIR, Path(CNT_LAB_DIR)/'artifacts', Path.cwd()])\n",
    "print(f\"Discovered {len(CANDIDATES)} candidate CSV(s).\")\n",
    "list(itertools.islice(CANDIDATES.items(), 5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b85751",
   "metadata": {},
   "source": [
    "### Metrics & Nulls — compute graph properties and degree‑preserving randomizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8fa7789",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [metrics] Graph metrics & nulls\n",
    "\n",
    "import itertools\n",
    "from math import log2\n",
    "\n",
    "def shannon_entropy(seq):\n",
    "    if len(seq)==0: return 0.0\n",
    "    vals, counts = np.unique(seq, return_counts=True)\n",
    "    p = counts / counts.sum()\n",
    "    return float(-(p * np.log2(p)).sum())\n",
    "\n",
    "def graph_metrics(G: nx.Graph) -> dict:\n",
    "    n = G.number_of_nodes()\n",
    "    m = G.number_of_edges()\n",
    "    deg = np.array([d for _,d in G.degree()])\n",
    "    cc = nx.average_clustering(G, weight='weight' if nx.is_weighted(G) else None)\n",
    "    # Greedy modularity communities (may be slow for very large graphs)\n",
    "    try:\n",
    "        from networkx.algorithms.community import greedy_modularity_communities, modularity\n",
    "        comms = list(greedy_modularity_communities(G, weight='weight' if nx.is_weighted(G) else None))\n",
    "        Q = modularity(G, comms, weight='weight' if nx.is_weighted(G) else None)\n",
    "        n_comms = len(comms)\n",
    "    except Exception:\n",
    "        Q, n_comms = np.nan, np.nan\n",
    "    return {\n",
    "        \"n\": n, \"m\": m,\n",
    "        \"degree_mean\": float(deg.mean()) if n>0 else np.nan,\n",
    "        \"degree_std\": float(deg.std()) if n>0 else np.nan,\n",
    "        \"degree_entropy\": shannon_entropy(deg),\n",
    "        \"clustering_avg\": float(cc),\n",
    "        \"modularity_Q\": float(Q),\n",
    "        \"communities\": float(n_comms) if isinstance(n_comms,(int,float)) else np.nan,\n",
    "        \"density\": nx.density(G),\n",
    "    }\n",
    "\n",
    "def degree_preserving_null(G: nx.Graph, swaps_per_edge=3, seed=1) -> nx.Graph:\n",
    "    H = G.copy()\n",
    "    try:\n",
    "        nx.double_edge_swap(H, nswap=max(1, swaps_per_edge*H.number_of_edges()), max_tries=100*H.number_of_edges(), seed=seed)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return H\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5b1a4a",
   "metadata": {},
   "source": [
    "### Dynamics — Kuramoto sweep to estimate $K_c$, knee, and resilience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f0b974",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [dynamics] Kuramoto + Kc detection\n",
    "\n",
    "def simulate_kuramoto(G: nx.Graph, K: float, T: float=50.0, dt: float=0.05, seed: int=0, omega_std: float=1.0):\n",
    "    \"\"\"\n",
    "    Simple Kuramoto on an undirected (possibly weighted) graph.\n",
    "    Returns order parameter R_mean and the time-series R_t.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = G.number_of_nodes()\n",
    "    if n == 0:\n",
    "        return 0.0, np.array([])\n",
    "    theta = rng.uniform(0, 2*np.pi, size=n)\n",
    "    omega = rng.normal(0.0, omega_std, size=n)\n",
    "    A = nx.to_numpy_array(G, weight='weight' if nx.is_weighted(G) else None)\n",
    "    deg = A.sum(axis=1)\n",
    "    t_steps = int(T/dt)\n",
    "    R_t = np.zeros(t_steps, dtype=float)\n",
    "    for t in range(t_steps):\n",
    "        coupling = (A * np.sin(theta[:,None] - theta[None,:])).sum(axis=1)\n",
    "        theta = theta + dt * (omega + (K * coupling / np.maximum(deg, 1e-9)))\n",
    "        R = np.abs(np.mean(np.exp(1j*theta)))\n",
    "        R_t[t] = R\n",
    "    burn = int(0.3*t_steps)\n",
    "    return float(R_t[burn:].mean()), R_t\n",
    "\n",
    "def find_knee(x, y, direction='increasing'):\n",
    "    \"\"\"\n",
    "    Lightweight knee detector using smoothed curve and curvature proxy.\n",
    "    \"\"\"\n",
    "    x = np.asarray(x); y = np.asarray(y)\n",
    "    from scipy.ndimage import gaussian_filter1d\n",
    "    ys = gaussian_filter1d(y, sigma=1.0)\n",
    "    dy = np.gradient(ys, x)\n",
    "    d2 = np.gradient(dy, x)\n",
    "    if direction == 'increasing':\n",
    "        score = (dy - dy.min())/(dy.max()-dy.min()+1e-9) + (d2 - d2.min())/(d2.max()-d2.min()+1e-9)\n",
    "    else:\n",
    "        score = (dy.max()-dy)/(dy.max()-dy.min()+1e-9) + (d2.max()-d2)/(d2.max()-d2.min()+1e-9)\n",
    "    idx = int(np.argmax(score))\n",
    "    return idx\n",
    "\n",
    "def sweep_k(G, k_grid=None, **sim_kw):\n",
    "    if k_grid is None:\n",
    "        k_grid = np.linspace(0.0, 3.0, 25)\n",
    "    records = []\n",
    "    R_means = []\n",
    "    for K in k_grid:\n",
    "        R_mean, R_t = simulate_kuramoto(G, K=K, **sim_kw)\n",
    "        R_means.append(R_mean)\n",
    "        records.append({\"K\": float(K), \"R_mean\": float(R_mean)})\n",
    "    R = np.array(R_means)\n",
    "    idx_knee = find_knee(k_grid, R, direction='increasing')\n",
    "    K_knee = float(k_grid[idx_knee])\n",
    "    idx_Kc = int(np.argmin(np.abs(R - 0.5)))\n",
    "    K_c = float(k_grid[idx_Kc])\n",
    "    return pd.DataFrame(records), K_c, K_knee\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b0de25",
   "metadata": {},
   "source": [
    "### Validation — permutation tests vs degree‑preserving nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab1e3d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [validation] Permutation tests\n",
    "\n",
    "def validate_against_nulls(G: nx.Graph, k_grid=None, n_nulls: int=64, seed: int=1):\n",
    "    \"\"\"\n",
    "    Compare real graph's K_c and R(K) curve vs degree-preserving nulls.\n",
    "    Returns: summary dict and a DataFrame of null results.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    sweep_df, K_c_real, K_knee_real = sweep_k(G, k_grid=k_grid, seed=seed)\n",
    "    near = sweep_df.iloc[(sweep_df['K']-K_c_real).abs().argsort()[:3]]['R_mean'].mean()\n",
    "\n",
    "    null_rows = []\n",
    "    for i in range(n_nulls):\n",
    "        H = degree_preserving_null(G, swaps_per_edge=3, seed=int(seed+i))\n",
    "        dfH, KcH, KkH = sweep_k(H, k_grid=k_grid, seed=int(seed+i))\n",
    "        nearH = dfH.iloc[(dfH['K']-K_c_real).abs().argsort()[:3]]['R_mean'].mean()\n",
    "        null_rows.append({\"run\": i, \"K_c\": KcH, \"K_knee\": KkH, \"R_near_Kc(realKc)\": float(nearH)})\n",
    "    null_df = pd.DataFrame(null_rows)\n",
    "    p_Kc = ( (null_df[\"K_c\"] <= K_c_real).sum() + 1 ) / (len(null_df)+1) if not math.isnan(K_c_real) else np.nan\n",
    "    p_knee = ( (null_df[\"K_knee\"] <= K_knee_real).sum() + 1 ) / (len(null_df)+1) if not math.isnan(K_knee_real) else np.nan\n",
    "    p_near = ( (null_df[\"R_near_Kc(realKc)\"] >= near).sum() + 1 ) / (len(null_df)+1) if not math.isnan(near) else np.nan\n",
    "\n",
    "    summary = {\n",
    "        \"K_c_real\": float(K_c_real),\n",
    "        \"K_knee_real\": float(K_knee_real),\n",
    "        \"R_near_realKc\": float(near),\n",
    "        \"perm_p_Kc_le\": float(p_Kc),\n",
    "        \"perm_p_Kknee_le\": float(p_knee),\n",
    "        \"perm_p_Rnear_ge\": float(p_near),\n",
    "    }\n",
    "    return summary, sweep_df, null_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb37b025",
   "metadata": {},
   "source": [
    "### 3D Field — edge‑density & memory nodes (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb13775f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [3D] Field stability scoring (optional)\n",
    "\n",
    "def load_coords_csv(path: str, cols=('x','y','z')) -> np.ndarray:\n",
    "    df = pd.read_csv(path)\n",
    "    for c in cols:\n",
    "        assert c in df.columns, f\"Missing column '{c}' in {path}\"\n",
    "    return df[list(cols)].values.astype(float)\n",
    "\n",
    "def node_edge_density_score(G: nx.Graph, coords: np.ndarray, radius: float=0.2) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each node, score local edge density within a ball of \"radius\" in 3D.\n",
    "    Returns DataFrame with \"node\", \"density3D\", \"deg\", \"betweenness\".\n",
    "    \"\"\"\n",
    "    from sklearn.neighbors import KDTree\n",
    "    N = G.number_of_nodes()\n",
    "    assert coords.shape[0] == N, \"coords must align with G nodes (0..N-1)\"\n",
    "    tree = KDTree(coords)\n",
    "    deg = dict(G.degree())\n",
    "    btw = nx.betweenness_centrality(G, normalized=True, weight='weight' if nx.is_weighted(G) else None)\n",
    "    rows = []\n",
    "    for i in range(N):\n",
    "        idx = tree.query_radius(coords[[i]], r=radius)[0]\n",
    "        S = G.subgraph(idx)\n",
    "        density = nx.density(S) if S.number_of_nodes()>1 else 0.0\n",
    "        rows.append({\"node\": i, \"density3D\": float(density), \"deg\": float(deg.get(i,0)), \"betweenness\": float(btw.get(i,0.0))})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def memory_node_candidates(score_df: pd.DataFrame, top_pct: float=0.05) -> pd.DataFrame:\n",
    "    # Z-scores\n",
    "    for col in [\"density3D\",\"betweenness\",\"deg\"]:\n",
    "        score_df[f\"z_{col}\"] = (score_df[col] - score_df[col].mean()) / (score_df[col].std()+1e-9)\n",
    "    score_df[\"memory_score\"] = score_df[\"z_density3D\"] + 0.7*score_df[\"z_betweenness\"] - 0.3*score_df[\"z_deg\"]\n",
    "    k = max(1, int(len(score_df)*top_pct))\n",
    "    return score_df.sort_values(\"memory_score\", ascending=False).head(k).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bdd5fd",
   "metadata": {},
   "source": [
    "### Reporting — CSVs + Markdown/HTML summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97b0a4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [report] Writers\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def write_csv(df: pd.DataFrame, name: str) -> Path:\n",
    "    p = ARTIFACTS_DIR / name\n",
    "    df.to_csv(p, index=False)\n",
    "    print(\"Saved:\", p)\n",
    "    return p\n",
    "\n",
    "def render_markdown_report(summaries: list, metrics_rows: list, stability_rows: list, outfile_md=\"report.md\"):\n",
    "    dt = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M UTC\")\n",
    "    md = []\n",
    "    md.append(\"# CNT Topology Validation — Auto Report\\n\")\n",
    "    md.append(f\"_Generated: {dt}_\\n\")\n",
    "    md.append(\"\\n## Highlights\\n\")\n",
    "    for sm in summaries:\n",
    "        s = sm[\"summary\"]\n",
    "        line = \"- **{label}** — K_c={Kc:.3f}, knee={Kk:.3f}, p(K_c)={pKc:.3f}, p(knee)={pKk:.3f}, p(R_near)={pRn:.3f}\".format(\n",
    "            label=sm[\"label\"], Kc=s[\"K_c_real\"], Kk=s[\"K_knee_real\"], pKc=s[\"perm_p_Kc_le\"], pKk=s[\"perm_p_Kknee_le\"], pRn=s[\"perm_p_Rnear_ge\"]\n",
    "        )\n",
    "        md.append(line)\n",
    "    md.append(\"\\n## Methods\\n- Kuramoto sweep on input graphs; degree-preserving nulls for permutation tests.\\n\"\n",
    "              \"- Graph metrics: degree entropy, clustering, modularity, density.\\n\"\n",
    "              \"- Optional 3D memory-node scoring via local edge density.\\n\")\n",
    "    md.append(\"\\n## Files\\n\")\n",
    "    md.append(\"- `metrics_summary.csv` — graph properties per dataset\")\n",
    "    md.append(\"- `validation_results.csv` — permutation results per dataset\")\n",
    "    md.append(\"- `stability_map.csv` — K vs R_mean per dataset\\n\")\n",
    "    text = \"\\n\".join(md)\n",
    "    p = ARTIFACTS_DIR / outfile_md\n",
    "    p.write_text(text, encoding=\"utf-8\")\n",
    "    print(\"Saved:\", p)\n",
    "    html = \"<html><head><meta charset='utf-8'><title>CNT Validation Report</title></head><body><pre style='font-family:ui-monospace,Consolas,monospace'>{}</pre></body></html>\".format(text)\n",
    "    ph = ARTIFACTS_DIR / \"report.html\"\n",
    "    ph.write_text(html, encoding=\"utf-8\")\n",
    "    print(\"Saved:\", ph)\n",
    "    return p, ph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f21f9c9",
   "metadata": {},
   "source": [
    "### One‑Click Pipeline — set your inputs and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "258731f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %% [pipeline] Configure your inputs here\n",
    "\n",
    "# Example inputs (edit these). Provide either correlation matrices (will threshold) or adjacency matrices.\n",
    "INPUTS = [\n",
    "    # {\"label\": \"EEG_example\", \"path\": r\"C:\\Users\\caleb\\CNT_Lab\\artifacts\\tables\\eeg_adj.csv\", \"kind\": \"adj\", \"weighted\": False},\n",
    "    # {\"label\": \"GENE_example\", \"path\": r\"C:\\Users\\caleb\\CNT_Lab\\artifacts\\tables\\gene_corr.csv\", \"kind\": \"corr\", \"threshold\": 0.35, \"keep_sign\": False},\n",
    "]\n",
    "\n",
    "# Optional 3D coordinates (x,y,z) per dataset label (for the same node order)\n",
    "COORDS = {\n",
    "    # \"GENE_example\": r\"C:\\Users\\caleb\\CNT_Lab\\artifacts\\tables\\gene_coords.csv\"\n",
    "}\n",
    "\n",
    "# Kuramoto sweep parameters\n",
    "K_GRID = np.linspace(0.0, 3.0, 25)\n",
    "SIM_KW = dict(T=40.0, dt=0.05, seed=42, omega_std=1.0)\n",
    "N_NULLS = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "af292328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No INPUTS configured yet. Edit the INPUTS list above and re-run this cell.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# %% [pipeline] Run\n",
    "metrics_rows = []\n",
    "validation_rows = []\n",
    "stability_rows = []\n",
    "summaries = []\n",
    "\n",
    "if not INPUTS:\n",
    "    print(\"⚠️ No INPUTS configured yet. Edit the INPUTS list above and re-run this cell.\")\n",
    "else:\n",
    "    for cfg in INPUTS:\n",
    "        label = cfg[\"label\"]\n",
    "        path = cfg[\"path\"]\n",
    "        kind = cfg.get(\"kind\",\"corr\")\n",
    "        print(f\"\\n=== {label} ===\")\n",
    "        print(\"Loading:\", path)\n",
    "        mat = load_matrix(path, header=True)\n",
    "\n",
    "        if kind == \"corr\":\n",
    "            thr = cfg.get(\"threshold\", 0.35)\n",
    "            keep_sign = cfg.get(\"keep_sign\", False)\n",
    "            G = corr_to_graph(mat, threshold=thr, keep_sign=keep_sign)\n",
    "        else:\n",
    "            G = adj_to_graph(mat, weighted=cfg.get(\"weighted\", False))\n",
    "\n",
    "        # Metrics\n",
    "        gm = graph_metrics(G); gm[\"label\"] = label\n",
    "        metrics_rows.append(gm)\n",
    "\n",
    "        # Validation vs nulls\n",
    "        summary, sweep_df, null_df = validate_against_nulls(G, k_grid=K_GRID, n_nulls=N_NULLS, seed=123)\n",
    "        summaries.append({\"label\": label, \"summary\": summary})\n",
    "\n",
    "        # Stability map rows\n",
    "        s = sweep_df.copy(); s[\"label\"] = label\n",
    "        stability_rows.append(s)\n",
    "\n",
    "        # Validation rows\n",
    "        nd = null_df.copy(); nd[\"label\"] = label\n",
    "        validation_rows.append(nd)\n",
    "\n",
    "        # Optional 3D stability scoring\n",
    "        if label in COORDS:\n",
    "            try:\n",
    "                coords = load_coords_csv(COORDS[label])\n",
    "                score_df = node_edge_density_score(G, coords, radius=0.2)\n",
    "                mem_df = memory_node_candidates(score_df, top_pct=0.05)\n",
    "                write_csv(score_df, f\"{label}_3d_density_scores.csv\")\n",
    "                write_csv(mem_df, f\"{label}_memory_nodes_top5pct.csv\")\n",
    "            except Exception as e:\n",
    "                print(\"3D scoring skipped:\", e)\n",
    "\n",
    "    # Write artifacts\n",
    "    metrics_df = pd.DataFrame(metrics_rows)\n",
    "    validation_df = pd.concat(validation_rows, ignore_index=True) if validation_rows else pd.DataFrame()\n",
    "    stability_df = pd.concat(stability_rows, ignore_index=True) if stability_rows else pd.DataFrame()\n",
    "    write_csv(metrics_df, \"metrics_summary.csv\")\n",
    "    if not validation_df.empty:\n",
    "        write_csv(validation_df, \"validation_results.csv\")\n",
    "    if not stability_df.empty:\n",
    "        write_csv(stability_df, \"stability_map.csv\")\n",
    "\n",
    "    # Report\n",
    "    render_markdown_report(summaries, metrics_rows, stability_rows, outfile_md=\"report.md\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730e3efb",
   "metadata": {},
   "source": [
    "### (Optional) Smoke Test — run on a synthetic small‑world graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67275258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph metrics: {'n': 200, 'm': 800, 'degree_mean': 8.0, 'degree_std': 1.0816653826391966, 'degree_entropy': 2.121956571486788, 'clustering_avg': 0.4112683982683983, 'modularity_Q': 0.62972109375, 'communities': 5.0, 'density': 0.04020100502512563}\n",
      "Estimated K_c ~ 0.000, knee ~ 0.789\n",
      "Saved: artifacts_cnt_validation_20251104-031414Z\\SMOKETEST_stability_map.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('artifacts_cnt_validation_20251104-031414Z/SMOKETEST_stability_map.csv')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# %% [test] Quick smoke test (no real data required)\n",
    "\n",
    "G_test = nx.watts_strogatz_graph(200, k=8, p=0.15, seed=1)\n",
    "gm_test = graph_metrics(G_test)\n",
    "print(\"Graph metrics:\", gm_test)\n",
    "\n",
    "sweep_df, Kc, Kk = sweep_k(G_test, k_grid=nDiscovered 74 candidate CSV(s).\n",
    "[('cnt_correlates_report_20251015-163558',\n",
    "  'E:\\\\CNT\\\\notebooks\\\\archive\\\\cnt_correlates_report_20251015-163558.csv'),\n",
    " ('cnt_correlates_report_20251015-164130',\n",
    "  'E:\\\\CNT\\\\notebooks\\\\archive\\\\cnt_correlates_report_20251015-164130.csv'),\n",
    " ('top_gini_genes',\n",
    "  'E:\\\\CNT\\\\notebooks\\\\archive\\\\cnt_runs\\\\3i_atlas_checkin\\\\20251029-054356Z\\\\top_gini_genes.csv'),\n",
    " ('top_gini_genes__88859',\n",
    "  'E:\\\\CNT\\\\notebooks\\\\archive\\\\cnt_runs\\\\3i_atlas_checkin\\\\20251029-060737Z\\\\top_gini_genes.csv'),\n",
    " ('cfe_EEGBCI_subj1_motor_20251015-143523',\n",
    "  'E:\\\\CNT\\\\notebooks\\\\archive\\\\cnt_mega_out\\\\tables\\\\cfe_EEGBCI_subj1_motor_20251015-143523.csv')]p.linspace(0,3,20), seed=1, T=20.0, dt=0.05)\n",
    "print(f\"Estimated K_c ~ {Kc:.3f}, knee ~ {Kk:.3f}\")\n",
    "write_csv(sweep_df, \"SMOKETEST_stability_map.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bef74d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "**Notes**  \n",
    "- `degree_preserving_null` uses `double_edge_swap` which may not fully randomize for small graphs; increase `swaps_per_edge` if needed.  \n",
    "- Kuramoto settings (`T`, `dt`, `omega_std`) can be tuned per dataset; report captures your exact config.  \n",
    "- For Ising dynamics, add a parallel sweep block using Glauber/Metropolis; Kuramoto alone suffices for synchronization thresholds.\n",
    "\n",
    "**License**  \n",
    "MIT — Use freely in CNT Lab. Please cite Cognitive Nexus Theory (CNT) work by Telos.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CNT Lab (Py3.13)",
   "language": "python",
   "name": "cnt_lab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
