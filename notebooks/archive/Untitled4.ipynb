{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79c51feb-9938-4c06-8b18-57dc6feb8e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytrends\n",
      "  Downloading pytrends-4.9.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: requests>=2.0 in c:\\users\\caleb\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pytrends) (2.32.5)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\caleb\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pytrends) (2.3.2)\n",
      "Collecting lxml (from pytrends)\n",
      "  Downloading lxml-6.0.2-cp313-cp313-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\caleb\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=0.25->pytrends) (2.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\caleb\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=0.25->pytrends) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\caleb\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=0.25->pytrends) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\caleb\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas>=0.25->pytrends) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\caleb\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=0.25->pytrends) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\caleb\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.0->pytrends) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\caleb\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.0->pytrends) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\caleb\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.0->pytrends) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\caleb\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.0->pytrends) (2025.8.3)\n",
      "Downloading pytrends-4.9.2-py3-none-any.whl (15 kB)\n",
      "Downloading lxml-6.0.2-cp313-cp313-win_amd64.whl (4.0 MB)\n",
      "   ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 2.4/4.0 MB 20.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.0/4.0 MB 17.4 MB/s  0:00:00\n",
      "Installing collected packages: lxml, pytrends\n",
      "\n",
      "   ---------------------------------------- 0/2 [lxml]\n",
      "   ---------------------------------------- 0/2 [lxml]\n",
      "   -------------------- ------------------- 1/2 [pytrends]\n",
      "   ---------------------------------------- 2/2 [pytrends]\n",
      "\n",
      "Successfully installed lxml-6.0.2 pytrends-4.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pytrends\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc54aafb-5c61-49b6-acaa-781bb38827de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trends fetch error for government surveillance -> The request failed: Google returned a response with code 429\n",
      "Trends fetch error for censorship -> The request failed: Google returned a response with code 429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pytrends\\request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.fillna(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trends fetch error for free speech -> The request failed: Google returned a response with code 429\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_12652\\3021297629.py:167: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid level/trend specification: 'local'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 209\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# Align and fit Unobserved Components (local level + weekly seasonality optional)\u001b[39;00m\n\u001b[32m    208\u001b[39m y = pd.Series(OAI.values, index=OAI.index, name=\u001b[33m\"\u001b[39m\u001b[33mOAI\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m model = \u001b[43mUnobservedComponents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlocal\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m=\u001b[49m\u001b[43mXreg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m res = model.fit(disp=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# Produce a HORIZON forecast (with exog rolled forward by last observed values)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\structural.py:490\u001b[39m, in \u001b[36mUnobservedComponents.__init__\u001b[39m\u001b[34m(self, endog, level, trend, seasonal, freq_seasonal, cycle, autoregressive, exog, irregular, stochastic_level, stochastic_trend, stochastic_seasonal, stochastic_freq_seasonal, stochastic_cycle, damped_cycle, cycle_period_bounds, mle_regression, use_exact_diffuse, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28mself\u001b[39m.trend_specification = \u001b[33m'\u001b[39m\u001b[33mrandom trend\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInvalid level/trend specification: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m                          % spec)\n\u001b[32m    493\u001b[39m \u001b[38;5;66;03m# Check for a model that makes sense\u001b[39;00m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trend \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m level:\n",
      "\u001b[31mValueError\u001b[39m: Invalid level/trend specification: 'local'"
     ]
    }
   ],
   "source": [
    "# === CNT \"Wake-Up\" Single-Cell Pipeline ======================================\n",
    "# Dependencies: pandas, numpy, requests, statsmodels, scikit-learn, pytrends, matplotlib\n",
    "# If needed: pip installs (uncomment)\n",
    "# import sys, subprocess; [subprocess.check_call([sys.executable,\"-m\",\"pip\",\"install\",p]) for p in\n",
    "#  [\"pandas\",\"numpy\",\"requests\",\"statsmodels\",\"scikit-learn\",\"pytrends\",\"matplotlib\"]]\n",
    "\n",
    "import os, io, json, math, time, datetime as dt\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, requests\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.statespace.structural import UnobservedComponents\n",
    "from pytrends.request import TrendReq\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------- CONFIG -----------------------------------------\n",
    "CFG = dict(\n",
    "    ROOT                = r\"C:\\Users\\caleb\\CNT_Lab\",   # change if needed\n",
    "    REGION              = \"US\",\n",
    "    HORIZON_WEEKS       = 52,       # forecast horizon\n",
    "    THRESHOLD_TAU       = 0.65,     # OAI threshold in [0,1]\n",
    "    RUN_LENGTH_K        = 6,        # sustained weeks above threshold\n",
    "    BACKTEST_START      = \"2012-01-01\",\n",
    "    OAI_TOPICS          = [\"government surveillance\",\"censorship\",\"civil liberties\",\"free speech\"],\n",
    "    GDELT_QUERY         = '(censorship OR surveillance OR \"civil liberties\")',\n",
    "    PEW_CSV             = r\"C:\\Users\\caleb\\CNT_Lab\\data\\pew_trust_government.csv\",   # optional\n",
    "    GALLUP_CSV          = r\"C:\\Users\\caleb\\CNT_Lab\\data\\gallup_trust_confidence.csv\",# optional\n",
    "    SAVE_DIR            = \"artifacts\",\n",
    "    FIG_DIR             = \"artifacts\\\\figures\",\n",
    "    RNG_SEED            = 1337\n",
    ")\n",
    "\n",
    "np.random.seed(CFG[\"RNG_SEED\"])\n",
    "ROOT = Path(CFG[\"ROOT\"]); (ROOT/CFG[\"SAVE_DIR\"]).mkdir(parents=True, exist_ok=True); (ROOT/CFG[\"FIG_DIR\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------ UTILITIES ------------------------------------------\n",
    "def as_week_index(dts):\n",
    "    \"\"\"Snap to Monday weeks for consistent alignment.\"\"\"\n",
    "    return pd.to_datetime(dts).to_period('W-MON').to_timestamp()\n",
    "\n",
    "def spectral_entropy(x, nseg=8, eps=1e-12):\n",
    "    \"\"\"Simple forecastability proxy: 1 - normalized spectral entropy.\"\"\"\n",
    "    x = pd.Series(x).dropna().values\n",
    "    if len(x) < 16: return np.nan\n",
    "    x = (x - x.mean()) / (x.std()+1e-9)\n",
    "    # Welch-ish periodogram\n",
    "    seg = max(16, len(x)//nseg)\n",
    "    ps = []\n",
    "    for i in range(0, len(x)-seg+1, seg):\n",
    "        X = np.fft.rfft(x[i:i+seg])\n",
    "        P = (X*np.conj(X)).real\n",
    "        P = P / (P.sum()+eps)\n",
    "        ps.append(P)\n",
    "    Pm = np.mean(ps, axis=0)\n",
    "    H = -(Pm*np.log(Pm+eps)).sum()\n",
    "    Hmax = math.log(len(Pm))\n",
    "    return float(1.0 - H/Hmax)  # higher => more forecastable\n",
    "\n",
    "def zscore(s):\n",
    "    m, sd = np.nanmean(s), np.nanstd(s)\n",
    "    if sd==0 or np.isnan(sd): return pd.Series([0]*len(s), index=s.index)\n",
    "    return (s-m)/sd\n",
    "\n",
    "def logistic_scale(s):\n",
    "    \"\"\"Scale to ~[0,1] via robust logistic mapping.\"\"\"\n",
    "    s = pd.Series(s)\n",
    "    q1, q2, q3 = s.quantile([0.1,0.5,0.9])\n",
    "    scale = (q3 - q1)/2.0 if q3>q1 else (s.std() or 1.0)\n",
    "    return 1/(1+np.exp(-(s-q2)/(scale+1e-9)))\n",
    "\n",
    "def weekly_resample_sum(df, col, datecol):\n",
    "    x = df.copy()\n",
    "    x[datecol] = as_week_index(x[datecol])\n",
    "    return x.groupby(datecol, as_index=True)[col].sum().sort_index()\n",
    "\n",
    "def weekly_resample_mean(df, col, datecol):\n",
    "    x = df.copy()\n",
    "    x[datecol] = as_week_index(x[datecol])\n",
    "    return x.groupby(datecol, as_index=True)[col].mean().sort_index()\n",
    "\n",
    "# ------------------------ DATA SOURCES ---------------------------------------\n",
    "# 1) Google Trends (topics as keywords; you can map to topic IDs later)\n",
    "def fetch_trends(topics, geo=\"US\", since=\"2012-01-01\"):\n",
    "    pytrends = TrendReq(hl=\"en-US\", tz=360)\n",
    "    frames = []\n",
    "    for kw in topics:\n",
    "        try:\n",
    "            pytrends.build_payload([kw], cat=0, timeframe=f\"{since} {dt.date.today():%Y-%m-%d}\", geo=geo, gprop=\"\")\n",
    "            df = pytrends.interest_over_time()\n",
    "            if df.empty: continue\n",
    "            s = df[kw].rename(kw)\n",
    "            s.index = as_week_index(s.index)\n",
    "            frames.append(s)\n",
    "            time.sleep(1.2)  # be gentle\n",
    "        except Exception as e:\n",
    "            print(\"Trends fetch error for\", kw, \"->\", e)\n",
    "    if not frames: \n",
    "        return pd.DataFrame()\n",
    "    out = pd.concat(frames, axis=1).sort_index()\n",
    "    return out\n",
    "\n",
    "# 2) GDELT counts (simple doc volume via GDELT 2.1 doc API)\n",
    "def fetch_gdelt_counts(query, since=\"2012-01-01\", until=None):\n",
    "    # GDELT Doc API time series; we’ll chunk by month to keep it simple.\n",
    "    # Reference: https://blog.gdeltproject.org/gdelt-doc-2-0-api-debuts/\n",
    "    if until is None: until = dt.date.today().isoformat()\n",
    "    start = pd.Timestamp(since).to_period(\"M\").to_timestamp()\n",
    "    end   = pd.Timestamp(until).to_period(\"M\").to_timestamp()\n",
    "    months = pd.period_range(start, end, freq=\"M\").to_timestamp()\n",
    "    rows = []\n",
    "    for m in months:\n",
    "        try:\n",
    "            url = (\"https://api.gdeltproject.org/api/v2/doc/doc?\"\n",
    "                   f\"query={requests.utils.quote(query)}&mode=TimelineVol\"\n",
    "                   f\"&format=json&startdatetime={m:%Y%m%d000000}&enddatetime={(m+pd.offsets.MonthEnd(0)):%Y%m%d235959}\")\n",
    "            r = requests.get(url, timeout=20)\n",
    "            js = r.json()\n",
    "            if \"timelines\" not in js or not js[\"timelines\"]:\n",
    "                continue\n",
    "            for pt in js[\"timelines\"][0].get(\"data\", []):\n",
    "                rows.append((pd.to_datetime(pt[\"date\"]), int(pt[\"value\"])))\n",
    "            time.sleep(0.7)\n",
    "        except Exception as e:\n",
    "            print(\"GDELT fetch error:\", e)\n",
    "    if not rows: return pd.Series(dtype=float)\n",
    "    s = pd.Series({d:v for d,v in rows}).sort_index()\n",
    "    s = s.resample(\"W-MON\").sum()\n",
    "    s.name = \"gdelt_volume\"\n",
    "    return s\n",
    "\n",
    "# 3) Polling spine (optional local CSVs; expected columns)\n",
    "#    PEW CSV columns: date,value  (value in [0,100] trust/approval or inverse—your choice)\n",
    "#    GALLUP CSV columns: date,value\n",
    "def load_optional_poll(csv_path, colname):\n",
    "    p = Path(csv_path)\n",
    "    if not p.exists(): return None\n",
    "    df = pd.read_csv(p)\n",
    "    # Heuristic parse\n",
    "    datecol = [c for c in df.columns if \"date\" in c.lower()][0]\n",
    "    valcol  = [c for c in df.columns if c!=datecol][0]\n",
    "    s = weekly_resample_mean(df, valcol, datecol)\n",
    "    s.name = colname\n",
    "    return s\n",
    "\n",
    "# -------------------- INGEST & ALIGN -----------------------------------------\n",
    "since = CFG[\"BACKTEST_START\"]\n",
    "trends = fetch_trends(CFG[\"OAI_TOPICS\"], geo=CFG[\"REGION\"], since=since)\n",
    "gdelt  = fetch_gdelt_counts(CFG[\"GDELT_QUERY\"], since=since)\n",
    "\n",
    "pew    = load_optional_poll(CFG[\"PEW_CSV\"], \"pew_trust\")\n",
    "gallup = load_optional_poll(CFG[\"GALLUP_CSV\"], \"gallup_conf\")\n",
    "\n",
    "frames = []\n",
    "if trends.shape[0]: frames.append(trends)\n",
    "if isinstance(gdelt, pd.Series) and gdelt.shape[0]: frames.append(gdelt.to_frame())\n",
    "if pew is not None: frames.append(pew.to_frame())\n",
    "if gallup is not None: frames.append(gallup.to_frame())\n",
    "\n",
    "if not frames:\n",
    "    raise RuntimeError(\"No data sources available. Provide internet or local CSVs.\")\n",
    "\n",
    "df = pd.concat(frames, axis=1).sort_index()\n",
    "df = df[~df.index.duplicated(keep=\"first\")]\n",
    "\n",
    "# Fill small gaps with Kalman-ish fallback (here: forward/back fill + small EMA)\n",
    "df = df.astype(float)\n",
    "df = df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "df = df.ewm(span=3, adjust=False).mean()\n",
    "\n",
    "# -------------------- FEATURES & FORECASTABILITY ------------------------------\n",
    "# Derive basic transforms\n",
    "feat = pd.DataFrame(index=df.index)\n",
    "for c in df.columns:\n",
    "    feat[c] = df[c]\n",
    "    feat[c+\"_z\"] = zscore(df[c])\n",
    "    # volatility\n",
    "    feat[c+\"_vol4\"] = df[c].pct_change().rolling(4).std()\n",
    "\n",
    "# Forecastability weights (ForeCA proxy)\n",
    "weights = {}\n",
    "for c in df.columns:\n",
    "    w = spectral_entropy(df[c].values)\n",
    "    weights[c] = w\n",
    "w_series = pd.Series(weights).replace({np.nan: w_series.mean() if 'w_series' in locals() else 0.5})\n",
    "w_series = (w_series - w_series.min())/(w_series.max()-w_series.min()+1e-12)\n",
    "w_series = w_series.clip(0.05, 1.0)\n",
    "\n",
    "# -------------------- BUILD OAI (weighted PCA -> logistic scaling) ------------\n",
    "# Stack standardized base channels only (original columns)\n",
    "base = df.copy()\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(base.values)\n",
    "# weight columns by sqrt(weight) before PCA\n",
    "W = np.diag(np.sqrt(w_series.reindex(base.columns).fillna(0.5).values))\n",
    "Xw = X.dot(W)\n",
    "\n",
    "pca = PCA(n_components=1, random_state=CFG[\"RNG_SEED\"])\n",
    "oai_raw = pca.fit_transform(Xw).ravel()\n",
    "OAI = pd.Series(oai_raw, index=base.index, name=\"OAI_raw\")\n",
    "OAI = logistic_scale(zscore(OAI))  # [0,1] scaled\n",
    "\n",
    "# -------------------- NOWCAST via LocalLevel state-space ----------------------\n",
    "# Use OAI as target; regressors = z-scored exogenous (Trends+GDELT+Polls derivatives)\n",
    "exo_cols = [c for c in feat.columns if c.endswith(\"_z\") or c.endswith(\"_vol4\")]\n",
    "Xreg = feat[exo_cols].fillna(0.0)\n",
    "\n",
    "# Align and fit Unobserved Components (local level + weekly seasonality optional)\n",
    "y = pd.Series(OAI.values, index=OAI.index, name=\"OAI\")\n",
    "model = UnobservedComponents(endog=y, level='local', exog=Xreg)\n",
    "res = model.fit(disp=False)\n",
    "\n",
    "# Produce a HORIZON forecast (with exog rolled forward by last observed values)\n",
    "h = CFG[\"HORIZON_WEEKS\"]\n",
    "lastX = Xreg.iloc[-1:].values\n",
    "X_future = np.repeat(lastX, h, axis=0)\n",
    "fc = res.get_forecast(steps=h, exog=X_future)\n",
    "mean_fc = fc.predicted_mean\n",
    "ci_fc   = fc.conf_int(alpha=0.20)  # 80% band\n",
    "\n",
    "idx_future = pd.date_range(y.index[-1] + pd.offsets.Week(1), periods=h, freq=\"W-MON\")\n",
    "OAI_f = pd.Series(mean_fc, index=idx_future, name=\"OAI_fc\")\n",
    "OAI_f = OAI_f.clip(0,1)\n",
    "\n",
    "# -------------------- SIMPLE CHANGE-POINT (CUSUM) ----------------------------\n",
    "# CUSUM on residuals as a pragmatic online shift detector\n",
    "resid = y - res.fittedvalues.reindex_like(y).fillna(method=\"bfill\")\n",
    "k_cusum = resid.std() * 0.25  # reference\n",
    "h_cusum = resid.std() * 3.0   # threshold\n",
    "pos, neg = 0.0, 0.0\n",
    "alarms = []\n",
    "for t, e in resid.items():\n",
    "    pos = max(0.0, pos + e - k_cusum)\n",
    "    neg = min(0.0, neg + e + k_cusum)\n",
    "    if pos > h_cusum or abs(neg) > h_cusum:\n",
    "        alarms.append(t); pos = 0.0; neg = 0.0\n",
    "cp_dates = alarms[-5:]  # keep last few for plotting\n",
    "\n",
    "# -------------------- TIME-TO-EVENT (threshold + sustain) --------------------\n",
    "tau = CFG[\"THRESHOLD_TAU\"]; k = CFG[\"RUN_LENGTH_K\"]\n",
    "# Monte Carlo from Gaussian forecast errors using in-sample resid volatility\n",
    "sig = float(resid.std() or 0.05)\n",
    "n_sims = 2000\n",
    "paths = []\n",
    "for _ in range(n_sims):\n",
    "    noise = np.random.normal(0, sig, size=h)\n",
    "    sim = (OAI_f.values + noise)\n",
    "    sim = np.clip(sim, 0, 1)\n",
    "    paths.append(sim)\n",
    "paths = np.vstack(paths)\n",
    "\n",
    "def first_sustained(sim, tau, k):\n",
    "    above = sim >= tau\n",
    "    run = 0\n",
    "    for i, a in enumerate(above):\n",
    "        run = run + 1 if a else 0\n",
    "        if run >= k: return i  # index within horizon\n",
    "    return None\n",
    "\n",
    "hits = [first_sustained(p, tau, k) for p in paths]\n",
    "hit_idxs = [x for x in hits if x is not None]\n",
    "if hit_idxs:\n",
    "    dates = [idx_future[i] for i in hit_idxs]\n",
    "    median_date = pd.to_datetime(pd.Series(dates)).median()\n",
    "    p_next_H = len(hit_idxs)/n_sims\n",
    "    # credible window\n",
    "    d80 = (pd.Series(dates).sort_values().iloc[int(0.10*len(dates))],\n",
    "           pd.Series(dates).sort_values().iloc[int(0.90*len(dates))])\n",
    "else:\n",
    "    median_date, p_next_H, d80 = None, 0.0, (None, None)\n",
    "\n",
    "# -------------------- PLOTS ---------------------------------------------------\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(y.index, y.values, label=\"OAI (nowcasted fit)\")\n",
    "plt.plot(res.fittedvalues.index, res.fittedvalues.values.clip(0,1), linestyle=\"--\", label=\"State-space fit\")\n",
    "plt.axhline(tau, linestyle=\":\", label=f\"Threshold τ={tau}\")\n",
    "for d in cp_dates:\n",
    "    plt.axvline(d, linestyle=\":\", alpha=0.6)\n",
    "plt.title(\"Overreach Awareness Index (OAI) + fit + change-point hints\")\n",
    "plt.legend()\n",
    "fig1 = ROOT/CFG[\"FIG_DIR\"]/f\"oai_fit_{int(time.time())}.png\"\n",
    "plt.tight_layout(); plt.savefig(fig1, dpi=160); plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(OAI_f.index, OAI_f.values, label=\"Forecast mean\")\n",
    "plt.fill_between(OAI_f.index, ci_fc.iloc[:,0].clip(0,1), ci_fc.iloc[:,1].clip(0,1), alpha=0.2, label=\"80% band\")\n",
    "plt.axhline(tau, linestyle=\":\", label=f\"τ={tau}\")\n",
    "plt.title(\"OAI Forecast (next {} weeks)\".format(h))\n",
    "plt.legend()\n",
    "fig2 = ROOT/CFG[\"FIG_DIR\"]/f\"oai_fc_{int(time.time())}.png\"\n",
    "plt.tight_layout(); plt.savefig(fig2, dpi=160); plt.close()\n",
    "\n",
    "# Probability curve: chance of sustained crossing within t weeks (empirical CDF)\n",
    "prob_curve = []\n",
    "for t in range(h):\n",
    "    prob_curve.append(np.mean([ (x is not None and x<=t) for x in hits ]))\n",
    "prob_s = pd.Series(prob_curve, index=idx_future, name=\"P(T* ≤ t)\")\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(prob_s.index, prob_s.values, label=\"Pr(sustained crossing by week t)\")\n",
    "plt.ylim(0,1.0)\n",
    "plt.title(\"Time-to-Event Probability (sustained τ for k={} weeks)\".format(k))\n",
    "plt.legend()\n",
    "fig3 = ROOT/CFG[\"FIG_DIR\"]/f\"oai_event_prob_{int(time.time())}.png\"\n",
    "plt.tight_layout(); plt.savefig(fig3, dpi=160); plt.close()\n",
    "\n",
    "# -------------------- SAVE SUMMARY -------------------------------------------\n",
    "summary = dict(\n",
    "    generated_at = dt.datetime.utcnow().isoformat()+\"Z\",\n",
    "    horizon_weeks = h,\n",
    "    threshold_tau = tau,\n",
    "    run_length_k = k,\n",
    "    last_obs_week = y.index[-1].strftime(\"%Y-%m-%d\"),\n",
    "    cp_dates = [pd.Timestamp(d).strftime(\"%Y-%m-%d\") for d in cp_dates],\n",
    "    prob_within_horizon = round(float(p_next_H),4),\n",
    "    median_event_date = (pd.Timestamp(median_date).strftime(\"%Y-%m-%d\") if median_date is not None else None),\n",
    "    event_window_80 = tuple(d.strftime(\"%Y-%m-%d\") if d is not None else None for d in d80),\n",
    "    figures = [str(fig1), str(fig2), str(fig3)],\n",
    "    weights = {k: float(v) for k,v in w_series.to_dict().items()},\n",
    "    pca_explained_var = float(pca.explained_variance_ratio_[0])\n",
    ")\n",
    "out_json = ROOT/CFG[\"SAVE_DIR\"]/f\"oai_event_forecast_{int(time.time())}.json\"\n",
    "with open(out_json, \"w\") as f: json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"== CNT Wake-Up Single-Cell ==\")\n",
    "print(json.dumps(summary, indent=2))\n",
    "print(\"\\nSaved figures:\")\n",
    "print(\" -\", fig1)\n",
    "print(\" -\", fig2)\n",
    "print(\" -\", fig3)\n",
    "print(\"\\n-> Summary JSON:\", out_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "316f26ae-a725-476b-917c-dcf7f7f50b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trends fetch error for government surveillance -> The request failed: Google returned a response with code 429\n",
      "Trends fetch error for censorship -> The request failed: Google returned a response with code 429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pytrends\\request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.fillna(False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pytrends\\request.py:260: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df = df.fillna(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n",
      "GDELT fetch error: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_12652\\3021297629.py:167: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df = df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid level/trend specification: 'local'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 209\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# Align and fit Unobserved Components (local level + weekly seasonality optional)\u001b[39;00m\n\u001b[32m    208\u001b[39m y = pd.Series(OAI.values, index=OAI.index, name=\u001b[33m\"\u001b[39m\u001b[33mOAI\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m model = \u001b[43mUnobservedComponents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mendog\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlocal\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexog\u001b[49m\u001b[43m=\u001b[49m\u001b[43mXreg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m res = model.fit(disp=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# Produce a HORIZON forecast (with exog rolled forward by last observed values)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\statsmodels\\tsa\\statespace\\structural.py:490\u001b[39m, in \u001b[36mUnobservedComponents.__init__\u001b[39m\u001b[34m(self, endog, level, trend, seasonal, freq_seasonal, cycle, autoregressive, exog, irregular, stochastic_level, stochastic_trend, stochastic_seasonal, stochastic_freq_seasonal, stochastic_cycle, damped_cycle, cycle_period_bounds, mle_regression, use_exact_diffuse, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m         \u001b[38;5;28mself\u001b[39m.trend_specification = \u001b[33m'\u001b[39m\u001b[33mrandom trend\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    489\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m490\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInvalid level/trend specification: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m                          % spec)\n\u001b[32m    493\u001b[39m \u001b[38;5;66;03m# Check for a model that makes sense\u001b[39;00m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trend \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m level:\n",
      "\u001b[31mValueError\u001b[39m: Invalid level/trend specification: 'local'"
     ]
    }
   ],
   "source": [
    "# === CNT \"Wake-Up\" Single-Cell Pipeline ======================================\n",
    "# Dependencies: pandas, numpy, requests, statsmodels, scikit-learn, pytrends, matplotlib\n",
    "# If needed: pip installs (uncomment)\n",
    "# import sys, subprocess; [subprocess.check_call([sys.executable,\"-m\",\"pip\",\"install\",p]) for p in\n",
    "#  [\"pandas\",\"numpy\",\"requests\",\"statsmodels\",\"scikit-learn\",\"pytrends\",\"matplotlib\"]]\n",
    "\n",
    "import os, io, json, math, time, datetime as dt\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd, requests\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from statsmodels.tsa.statespace.structural import UnobservedComponents\n",
    "from pytrends.request import TrendReq\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------- CONFIG -----------------------------------------\n",
    "CFG = dict(\n",
    "    ROOT                = r\"C:\\Users\\caleb\\CNT_Lab\",   # change if needed\n",
    "    REGION              = \"US\",\n",
    "    HORIZON_WEEKS       = 52,       # forecast horizon\n",
    "    THRESHOLD_TAU       = 0.65,     # OAI threshold in [0,1]\n",
    "    RUN_LENGTH_K        = 6,        # sustained weeks above threshold\n",
    "    BACKTEST_START      = \"2012-01-01\",\n",
    "    OAI_TOPICS          = [\"government surveillance\",\"censorship\",\"civil liberties\",\"free speech\"],\n",
    "    GDELT_QUERY         = '(censorship OR surveillance OR \"civil liberties\")',\n",
    "    PEW_CSV             = r\"C:\\Users\\caleb\\CNT_Lab\\data\\pew_trust_government.csv\",   # optional\n",
    "    GALLUP_CSV          = r\"C:\\Users\\caleb\\CNT_Lab\\data\\gallup_trust_confidence.csv\",# optional\n",
    "    SAVE_DIR            = \"artifacts\",\n",
    "    FIG_DIR             = \"artifacts\\\\figures\",\n",
    "    RNG_SEED            = 1337\n",
    ")\n",
    "\n",
    "np.random.seed(CFG[\"RNG_SEED\"])\n",
    "ROOT = Path(CFG[\"ROOT\"]); (ROOT/CFG[\"SAVE_DIR\"]).mkdir(parents=True, exist_ok=True); (ROOT/CFG[\"FIG_DIR\"]).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ------------------------ UTILITIES ------------------------------------------\n",
    "def as_week_index(dts):\n",
    "    \"\"\"Snap to Monday weeks for consistent alignment.\"\"\"\n",
    "    return pd.to_datetime(dts).to_period('W-MON').to_timestamp()\n",
    "\n",
    "def spectral_entropy(x, nseg=8, eps=1e-12):\n",
    "    \"\"\"Simple forecastability proxy: 1 - normalized spectral entropy.\"\"\"\n",
    "    x = pd.Series(x).dropna().values\n",
    "    if len(x) < 16: return np.nan\n",
    "    x = (x - x.mean()) / (x.std()+1e-9)\n",
    "    # Welch-ish periodogram\n",
    "    seg = max(16, len(x)//nseg)\n",
    "    ps = []\n",
    "    for i in range(0, len(x)-seg+1, seg):\n",
    "        X = np.fft.rfft(x[i:i+seg])\n",
    "        P = (X*np.conj(X)).real\n",
    "        P = P / (P.sum()+eps)\n",
    "        ps.append(P)\n",
    "    Pm = np.mean(ps, axis=0)\n",
    "    H = -(Pm*np.log(Pm+eps)).sum()\n",
    "    Hmax = math.log(len(Pm))\n",
    "    return float(1.0 - H/Hmax)  # higher => more forecastable\n",
    "\n",
    "def zscore(s):\n",
    "    m, sd = np.nanmean(s), np.nanstd(s)\n",
    "    if sd==0 or np.isnan(sd): return pd.Series([0]*len(s), index=s.index)\n",
    "    return (s-m)/sd\n",
    "\n",
    "def logistic_scale(s):\n",
    "    \"\"\"Scale to ~[0,1] via robust logistic mapping.\"\"\"\n",
    "    s = pd.Series(s)\n",
    "    q1, q2, q3 = s.quantile([0.1,0.5,0.9])\n",
    "    scale = (q3 - q1)/2.0 if q3>q1 else (s.std() or 1.0)\n",
    "    return 1/(1+np.exp(-(s-q2)/(scale+1e-9)))\n",
    "\n",
    "def weekly_resample_sum(df, col, datecol):\n",
    "    x = df.copy()\n",
    "    x[datecol] = as_week_index(x[datecol])\n",
    "    return x.groupby(datecol, as_index=True)[col].sum().sort_index()\n",
    "\n",
    "def weekly_resample_mean(df, col, datecol):\n",
    "    x = df.copy()\n",
    "    x[datecol] = as_week_index(x[datecol])\n",
    "    return x.groupby(datecol, as_index=True)[col].mean().sort_index()\n",
    "\n",
    "# ------------------------ DATA SOURCES ---------------------------------------\n",
    "# 1) Google Trends (topics as keywords; you can map to topic IDs later)\n",
    "def fetch_trends(topics, geo=\"US\", since=\"2012-01-01\"):\n",
    "    pytrends = TrendReq(hl=\"en-US\", tz=360)\n",
    "    frames = []\n",
    "    for kw in topics:\n",
    "        try:\n",
    "            pytrends.build_payload([kw], cat=0, timeframe=f\"{since} {dt.date.today():%Y-%m-%d}\", geo=geo, gprop=\"\")\n",
    "            df = pytrends.interest_over_time()\n",
    "            if df.empty: continue\n",
    "            s = df[kw].rename(kw)\n",
    "            s.index = as_week_index(s.index)\n",
    "            frames.append(s)\n",
    "            time.sleep(1.2)  # be gentle\n",
    "        except Exception as e:\n",
    "            print(\"Trends fetch error for\", kw, \"->\", e)\n",
    "    if not frames: \n",
    "        return pd.DataFrame()\n",
    "    out = pd.concat(frames, axis=1).sort_index()\n",
    "    return out\n",
    "\n",
    "# 2) GDELT counts (simple doc volume via GDELT 2.1 doc API)\n",
    "def fetch_gdelt_counts(query, since=\"2012-01-01\", until=None):\n",
    "    # GDELT Doc API time series; we’ll chunk by month to keep it simple.\n",
    "    # Reference: https://blog.gdeltproject.org/gdelt-doc-2-0-api-debuts/\n",
    "    if until is None: until = dt.date.today().isoformat()\n",
    "    start = pd.Timestamp(since).to_period(\"M\").to_timestamp()\n",
    "    end   = pd.Timestamp(until).to_period(\"M\").to_timestamp()\n",
    "    months = pd.period_range(start, end, freq=\"M\").to_timestamp()\n",
    "    rows = []\n",
    "    for m in months:\n",
    "        try:\n",
    "            url = (\"https://api.gdeltproject.org/api/v2/doc/doc?\"\n",
    "                   f\"query={requests.utils.quote(query)}&mode=TimelineVol\"\n",
    "                   f\"&format=json&startdatetime={m:%Y%m%d000000}&enddatetime={(m+pd.offsets.MonthEnd(0)):%Y%m%d235959}\")\n",
    "            r = requests.get(url, timeout=20)\n",
    "            js = r.json()\n",
    "            if \"timelines\" not in js or not js[\"timelines\"]:\n",
    "                continue\n",
    "            for pt in js[\"timelines\"][0].get(\"data\", []):\n",
    "                rows.append((pd.to_datetime(pt[\"date\"]), int(pt[\"value\"])))\n",
    "            time.sleep(0.7)\n",
    "        except Exception as e:\n",
    "            print(\"GDELT fetch error:\", e)\n",
    "    if not rows: return pd.Series(dtype=float)\n",
    "    s = pd.Series({d:v for d,v in rows}).sort_index()\n",
    "    s = s.resample(\"W-MON\").sum()\n",
    "    s.name = \"gdelt_volume\"\n",
    "    return s\n",
    "\n",
    "# 3) Polling spine (optional local CSVs; expected columns)\n",
    "#    PEW CSV columns: date,value  (value in [0,100] trust/approval or inverse—your choice)\n",
    "#    GALLUP CSV columns: date,value\n",
    "def load_optional_poll(csv_path, colname):\n",
    "    p = Path(csv_path)\n",
    "    if not p.exists(): return None\n",
    "    df = pd.read_csv(p)\n",
    "    # Heuristic parse\n",
    "    datecol = [c for c in df.columns if \"date\" in c.lower()][0]\n",
    "    valcol  = [c for c in df.columns if c!=datecol][0]\n",
    "    s = weekly_resample_mean(df, valcol, datecol)\n",
    "    s.name = colname\n",
    "    return s\n",
    "\n",
    "# -------------------- INGEST & ALIGN -----------------------------------------\n",
    "since = CFG[\"BACKTEST_START\"]\n",
    "trends = fetch_trends(CFG[\"OAI_TOPICS\"], geo=CFG[\"REGION\"], since=since)\n",
    "gdelt  = fetch_gdelt_counts(CFG[\"GDELT_QUERY\"], since=since)\n",
    "\n",
    "pew    = load_optional_poll(CFG[\"PEW_CSV\"], \"pew_trust\")\n",
    "gallup = load_optional_poll(CFG[\"GALLUP_CSV\"], \"gallup_conf\")\n",
    "\n",
    "frames = []\n",
    "if trends.shape[0]: frames.append(trends)\n",
    "if isinstance(gdelt, pd.Series) and gdelt.shape[0]: frames.append(gdelt.to_frame())\n",
    "if pew is not None: frames.append(pew.to_frame())\n",
    "if gallup is not None: frames.append(gallup.to_frame())\n",
    "\n",
    "if not frames:\n",
    "    raise RuntimeError(\"No data sources available. Provide internet or local CSVs.\")\n",
    "\n",
    "df = pd.concat(frames, axis=1).sort_index()\n",
    "df = df[~df.index.duplicated(keep=\"first\")]\n",
    "\n",
    "# Fill small gaps with Kalman-ish fallback (here: forward/back fill + small EMA)\n",
    "df = df.astype(float)\n",
    "df = df.fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "df = df.ewm(span=3, adjust=False).mean()\n",
    "\n",
    "# -------------------- FEATURES & FORECASTABILITY ------------------------------\n",
    "# Derive basic transforms\n",
    "feat = pd.DataFrame(index=df.index)\n",
    "for c in df.columns:\n",
    "    feat[c] = df[c]\n",
    "    feat[c+\"_z\"] = zscore(df[c])\n",
    "    # volatility\n",
    "    feat[c+\"_vol4\"] = df[c].pct_change().rolling(4).std()\n",
    "\n",
    "# Forecastability weights (ForeCA proxy)\n",
    "weights = {}\n",
    "for c in df.columns:\n",
    "    w = spectral_entropy(df[c].values)\n",
    "    weights[c] = w\n",
    "w_series = pd.Series(weights).replace({np.nan: w_series.mean() if 'w_series' in locals() else 0.5})\n",
    "w_series = (w_series - w_series.min())/(w_series.max()-w_series.min()+1e-12)\n",
    "w_series = w_series.clip(0.05, 1.0)\n",
    "\n",
    "# -------------------- BUILD OAI (weighted PCA -> logistic scaling) ------------\n",
    "# Stack standardized base channels only (original columns)\n",
    "base = df.copy()\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(base.values)\n",
    "# weight columns by sqrt(weight) before PCA\n",
    "W = np.diag(np.sqrt(w_series.reindex(base.columns).fillna(0.5).values))\n",
    "Xw = X.dot(W)\n",
    "\n",
    "pca = PCA(n_components=1, random_state=CFG[\"RNG_SEED\"])\n",
    "oai_raw = pca.fit_transform(Xw).ravel()\n",
    "OAI = pd.Series(oai_raw, index=base.index, name=\"OAI_raw\")\n",
    "OAI = logistic_scale(zscore(OAI))  # [0,1] scaled\n",
    "\n",
    "# -------------------- NOWCAST via LocalLevel state-space ----------------------\n",
    "# Use OAI as target; regressors = z-scored exogenous (Trends+GDELT+Polls derivatives)\n",
    "exo_cols = [c for c in feat.columns if c.endswith(\"_z\") or c.endswith(\"_vol4\")]\n",
    "Xreg = feat[exo_cols].fillna(0.0)\n",
    "\n",
    "# Align and fit Unobserved Components (local level + weekly seasonality optional)\n",
    "y = pd.Series(OAI.values, index=OAI.index, name=\"OAI\")\n",
    "model = UnobservedComponents(endog=y, level='local', exog=Xreg)\n",
    "res = model.fit(disp=False)\n",
    "\n",
    "# Produce a HORIZON forecast (with exog rolled forward by last observed values)\n",
    "h = CFG[\"HORIZON_WEEKS\"]\n",
    "lastX = Xreg.iloc[-1:].values\n",
    "X_future = np.repeat(lastX, h, axis=0)\n",
    "fc = res.get_forecast(steps=h, exog=X_future)\n",
    "mean_fc = fc.predicted_mean\n",
    "ci_fc   = fc.conf_int(alpha=0.20)  # 80% band\n",
    "\n",
    "idx_future = pd.date_range(y.index[-1] + pd.offsets.Week(1), periods=h, freq=\"W-MON\")\n",
    "OAI_f = pd.Series(mean_fc, index=idx_future, name=\"OAI_fc\")\n",
    "OAI_f = OAI_f.clip(0,1)\n",
    "\n",
    "# -------------------- SIMPLE CHANGE-POINT (CUSUM) ----------------------------\n",
    "# CUSUM on residuals as a pragmatic online shift detector\n",
    "resid = y - res.fittedvalues.reindex_like(y).fillna(method=\"bfill\")\n",
    "k_cusum = resid.std() * 0.25  # reference\n",
    "h_cusum = resid.std() * 3.0   # threshold\n",
    "pos, neg = 0.0, 0.0\n",
    "alarms = []\n",
    "for t, e in resid.items():\n",
    "    pos = max(0.0, pos + e - k_cusum)\n",
    "    neg = min(0.0, neg + e + k_cusum)\n",
    "    if pos > h_cusum or abs(neg) > h_cusum:\n",
    "        alarms.append(t); pos = 0.0; neg = 0.0\n",
    "cp_dates = alarms[-5:]  # keep last few for plotting\n",
    "\n",
    "# -------------------- TIME-TO-EVENT (threshold + sustain) --------------------\n",
    "tau = CFG[\"THRESHOLD_TAU\"]; k = CFG[\"RUN_LENGTH_K\"]\n",
    "# Monte Carlo from Gaussian forecast errors using in-sample resid volatility\n",
    "sig = float(resid.std() or 0.05)\n",
    "n_sims = 2000\n",
    "paths = []\n",
    "for _ in range(n_sims):\n",
    "    noise = np.random.normal(0, sig, size=h)\n",
    "    sim = (OAI_f.values + noise)\n",
    "    sim = np.clip(sim, 0, 1)\n",
    "    paths.append(sim)\n",
    "paths = np.vstack(paths)\n",
    "\n",
    "def first_sustained(sim, tau, k):\n",
    "    above = sim >= tau\n",
    "    run = 0\n",
    "    for i, a in enumerate(above):\n",
    "        run = run + 1 if a else 0\n",
    "        if run >= k: return i  # index within horizon\n",
    "    return None\n",
    "\n",
    "hits = [first_sustained(p, tau, k) for p in paths]\n",
    "hit_idxs = [x for x in hits if x is not None]\n",
    "if hit_idxs:\n",
    "    dates = [idx_future[i] for i in hit_idxs]\n",
    "    median_date = pd.to_datetime(pd.Series(dates)).median()\n",
    "    p_next_H = len(hit_idxs)/n_sims\n",
    "    # credible window\n",
    "    d80 = (pd.Series(dates).sort_values().iloc[int(0.10*len(dates))],\n",
    "           pd.Series(dates).sort_values().iloc[int(0.90*len(dates))])\n",
    "else:\n",
    "    median_date, p_next_H, d80 = None, 0.0, (None, None)\n",
    "\n",
    "# -------------------- PLOTS ---------------------------------------------------\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(y.index, y.values, label=\"OAI (nowcasted fit)\")\n",
    "plt.plot(res.fittedvalues.index, res.fittedvalues.values.clip(0,1), linestyle=\"--\", label=\"State-space fit\")\n",
    "plt.axhline(tau, linestyle=\":\", label=f\"Threshold τ={tau}\")\n",
    "for d in cp_dates:\n",
    "    plt.axvline(d, linestyle=\":\", alpha=0.6)\n",
    "plt.title(\"Overreach Awareness Index (OAI) + fit + change-point hints\")\n",
    "plt.legend()\n",
    "fig1 = ROOT/CFG[\"FIG_DIR\"]/f\"oai_fit_{int(time.time())}.png\"\n",
    "plt.tight_layout(); plt.savefig(fig1, dpi=160); plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(OAI_f.index, OAI_f.values, label=\"Forecast mean\")\n",
    "plt.fill_between(OAI_f.index, ci_fc.iloc[:,0].clip(0,1), ci_fc.iloc[:,1].clip(0,1), alpha=0.2, label=\"80% band\")\n",
    "plt.axhline(tau, linestyle=\":\", label=f\"τ={tau}\")\n",
    "plt.title(\"OAI Forecast (next {} weeks)\".format(h))\n",
    "plt.legend()\n",
    "fig2 = ROOT/CFG[\"FIG_DIR\"]/f\"oai_fc_{int(time.time())}.png\"\n",
    "plt.tight_layout(); plt.savefig(fig2, dpi=160); plt.close()\n",
    "\n",
    "# Probability curve: chance of sustained crossing within t weeks (empirical CDF)\n",
    "prob_curve = []\n",
    "for t in range(h):\n",
    "    prob_curve.append(np.mean([ (x is not None and x<=t) for x in hits ]))\n",
    "prob_s = pd.Series(prob_curve, index=idx_future, name=\"P(T* ≤ t)\")\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(prob_s.index, prob_s.values, label=\"Pr(sustained crossing by week t)\")\n",
    "plt.ylim(0,1.0)\n",
    "plt.title(\"Time-to-Event Probability (sustained τ for k={} weeks)\".format(k))\n",
    "plt.legend()\n",
    "fig3 = ROOT/CFG[\"FIG_DIR\"]/f\"oai_event_prob_{int(time.time())}.png\"\n",
    "plt.tight_layout(); plt.savefig(fig3, dpi=160); plt.close()\n",
    "\n",
    "# -------------------- SAVE SUMMARY -------------------------------------------\n",
    "summary = dict(\n",
    "    generated_at = dt.datetime.utcnow().isoformat()+\"Z\",\n",
    "    horizon_weeks = h,\n",
    "    threshold_tau = tau,\n",
    "    run_length_k = k,\n",
    "    last_obs_week = y.index[-1].strftime(\"%Y-%m-%d\"),\n",
    "    cp_dates = [pd.Timestamp(d).strftime(\"%Y-%m-%d\") for d in cp_dates],\n",
    "    prob_within_horizon = round(float(p_next_H),4),\n",
    "    median_event_date = (pd.Timestamp(median_date).strftime(\"%Y-%m-%d\") if median_date is not None else None),\n",
    "    event_window_80 = tuple(d.strftime(\"%Y-%m-%d\") if d is not None else None for d in d80),\n",
    "    figures = [str(fig1), str(fig2), str(fig3)],\n",
    "    weights = {k: float(v) for k,v in w_series.to_dict().items()},\n",
    "    pca_explained_var = float(pca.explained_variance_ratio_[0])\n",
    ")\n",
    "out_json = ROOT/CFG[\"SAVE_DIR\"]/f\"oai_event_forecast_{int(time.time())}.json\"\n",
    "with open(out_json, \"w\") as f: json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"== CNT Wake-Up Single-Cell ==\")\n",
    "print(json.dumps(summary, indent=2))\n",
    "print(\"\\nSaved figures:\")\n",
    "print(\" -\", fig1)\n",
    "print(\" -\", fig2)\n",
    "print(\" -\", fig3)\n",
    "print(\"\\n-> Summary JSON:\", out_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb3f32a-4b06-4981-9868-6ded1f79b23b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
