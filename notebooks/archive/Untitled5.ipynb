{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a18ece04-978a-42cf-8e4e-d276009a160a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No .npy files found in C:/Users/caleb/CNT_Lab/data/raw/eeg",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 122\u001b[39m\n\u001b[32m    120\u001b[39m fs = FS\n\u001b[32m    121\u001b[39m files = \u001b[38;5;28msorted\u001b[39m(glob.glob(os.path.join(DATA_DIR, \u001b[33m\"\u001b[39m\u001b[33m*.npy\u001b[39m\u001b[33m\"\u001b[39m)))\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m files: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo .npy files found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    123\u001b[39m X_list, subjects = [], []\n\u001b[32m    124\u001b[39m channels = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: No .npy files found in C:/Users/caleb/CNT_Lab/data/raw/eeg"
     ]
    }
   ],
   "source": [
    "# === CNT EEG Spectral-Glyphs — single Jupyter cell (inline, no scripts) ===\n",
    "# What it does (fast):\n",
    "#   • Alpha-band imaginary coherence per subject\n",
    "#   • Spectral clustering (k via eigen-gap)\n",
    "#   • Consensus clusters, LOSO-ARI, circular-shift null (6 perms)\n",
    "#   • Saves small figs + tables under /mnt/data/cnt_spectral_ultrafast\n",
    "#\n",
    "# Switch to your data:\n",
    "#   USE_DEMO = False\n",
    "#   DATA_DIR = \"/mnt/data/eeg_rest\"        # folder with *.npy shaped [n_channels, n_time]\n",
    "#   (optional) add a \".channels.txt\" per subject with one channel name per line\n",
    "\n",
    "import os, glob, json, numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy.signal import welch, csd\n",
    "\n",
    "# ---------- config ----------\n",
    "USE_DEMO  = False\n",
    "DATA_DIR = r\"C:\\Users\\caleb\\CNT_Lab\\data\\raw\\eeg\"\n",
    "DATA_DIR = \"C:/Users/caleb/CNT_Lab/data/raw/eeg\"\n",
    "BAND      = \"alpha\"\n",
    "FS        = 200.0                     # your sampling rate if real data\n",
    "NULL_PERMS= 6                         # raise later for stronger stats (e.g., 200+)\n",
    "KMAX      = 3\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def ensure_dir(p): os.makedirs(p, exist_ok=True); return p\n",
    "def band_lims(): return {\"alpha\": (8.0, 13.0), \"theta\": (4.0, 8.0), \"beta\": (13.0, 30.0)}\n",
    "def imag_coh_matrix(X, fs, band=\"alpha\", nperseg=128, noverlap=64):\n",
    "    n_ch, _ = X.shape\n",
    "    fmin, fmax = band_lims()[band]\n",
    "    freqs, Pxx0 = welch(X[0], fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "    Pxx = [Pxx0] + [welch(X[i], fs=fs, nperseg=nperseg, noverlap=noverlap)[1] for i in range(1, n_ch)]\n",
    "    sel = (freqs >= fmin) & (freqs <= fmax)\n",
    "    W = np.zeros((n_ch, n_ch))\n",
    "    for i in range(n_ch):\n",
    "        for j in range(i+1, n_ch):\n",
    "            _, Pxy = csd(X[i], X[j], fs=fs, nperseg=nperseg, noverlap=noverlap)\n",
    "            num = np.abs(np.imag(Pxy[sel]))\n",
    "            den = np.sqrt(Pxx[i][sel] * Pxx[j][sel] + 1e-12)\n",
    "            W[i, j] = W[j, i] = float(np.nanmean(num / den))\n",
    "    np.fill_diagonal(W, 0.0)\n",
    "    return W\n",
    "def laplacian_sym(W):\n",
    "    d = W.sum(1); d = np.where(d <= 1e-12, 1.0, d)\n",
    "    Dmh = np.diag(1.0/np.sqrt(d))\n",
    "    return np.eye(W.shape[0]) - Dmh @ W @ Dmh\n",
    "def pick_k(L, kmax=3):\n",
    "    evals, _ = np.linalg.eigh(L)\n",
    "    gaps = [(k, evals[k+1] - evals[k]) for k in range(1, min(kmax+1, len(evals)-1))]\n",
    "    if not gaps: return 2\n",
    "    k_star = max(gaps, key=lambda x: x[1])[0]\n",
    "    return max(2, min(k_star, kmax))\n",
    "def spec_cluster(W, k):\n",
    "    L = laplacian_sym(W)\n",
    "    evals, evecs = np.linalg.eigh(L)\n",
    "    U = evecs[:, 1:k+1]\n",
    "    U /= (np.linalg.norm(U, axis=1, keepdims=True) + 1e-12)\n",
    "    labels = KMeans(n_clusters=k, n_init=25, random_state=42).fit_predict(U)\n",
    "    return labels, evals\n",
    "def consensus_from_labels(all_labels):\n",
    "    n = len(all_labels[0]); m = len(all_labels); co = np.zeros((n, n))\n",
    "    for lab in all_labels:\n",
    "        for i in range(n):\n",
    "            li = lab[i]\n",
    "            for j in range(n):\n",
    "                co[i, j] += 1.0 if li == lab[j] else 0.0\n",
    "    co /= m\n",
    "    thr = 0.5; A = (co >= thr).astype(int)\n",
    "    visited = np.zeros(n, bool); cons = -1*np.ones(n, int); cid = 0\n",
    "    for i in range(n):\n",
    "        if not visited[i]:\n",
    "            stack = [i]; visited[i] = True; cons[i] = cid\n",
    "            while stack:\n",
    "                u = stack.pop()\n",
    "                for v in range(n):\n",
    "                    if A[u, v] and not visited[v]:\n",
    "                        visited[v] = True; cons[v] = cid; stack.append(v)\n",
    "            cid += 1\n",
    "    return cons, co\n",
    "def ari_loso(all_labels):\n",
    "    cons,_ = consensus_from_labels(all_labels)\n",
    "    vals = []\n",
    "    for s in range(len(all_labels)):\n",
    "        leave = [lab for i, lab in enumerate(all_labels) if i != s]\n",
    "        cons_leave,_ = consensus_from_labels(leave)\n",
    "        vals.append(adjusted_rand_score(cons, cons_leave))\n",
    "    return float(np.median(vals))\n",
    "def circ_shift(X, rng, max_shift=None):\n",
    "    n_ch, n_t = X.shape\n",
    "    if max_shift is None: max_shift = n_t - 1\n",
    "    Y = np.zeros_like(X)\n",
    "    for c in range(n_ch):\n",
    "        s = int(rng.integers(0, max_shift + 1))\n",
    "        Y[c] = np.roll(X[c], s)\n",
    "    return Y\n",
    "def plot_matrix(M, title, path):\n",
    "    plt.figure(); plt.imshow(M, aspect='auto'); plt.title(title); plt.colorbar(); plt.tight_layout(); plt.savefig(path, dpi=140); plt.close()\n",
    "def plot_eigs(evals, title, path):\n",
    "    plt.figure(); plt.plot(np.arange(len(evals)), evals, marker='o'); plt.title(title); plt.xlabel(\"Index\"); plt.ylabel(\"Eigenvalue\"); plt.tight_layout(); plt.savefig(path, dpi=140); plt.close()\n",
    "\n",
    "# ---------- data ----------\n",
    "rng = np.random.default_rng(11)\n",
    "if USE_DEMO:\n",
    "    # 2 subjects × 8 channels × 3 s @ 200 Hz with an alpha driver in back half\n",
    "    fs = FS\n",
    "    n_subj, n_ch, seconds = 2, 8, 3\n",
    "    T = int(seconds * fs); t = np.arange(T)/fs\n",
    "    X_list = []\n",
    "    for _ in range(n_subj):\n",
    "        X = rng.normal(0, 1, size=(n_ch, T))\n",
    "        driver = np.sin(2*np.pi*10.0*t + rng.uniform(0, 2*np.pi))\n",
    "        for c in range(n_ch//2, n_ch):\n",
    "            X[c] += 0.9*driver + 0.2*rng.normal(0, 1, size=T)\n",
    "        X_list.append(X)\n",
    "    subjects = [f\"demo_{i:02d}\" for i in range(n_subj)]\n",
    "    channels = [f\"ch{i}\" for i in range(n_ch)]\n",
    "else:\n",
    "    fs = FS\n",
    "    files = sorted(glob.glob(os.path.join(DATA_DIR, \"*.npy\")))\n",
    "    if not files: raise RuntimeError(f\"No .npy files found in {DATA_DIR}\")\n",
    "    X_list, subjects = [], []\n",
    "    channels = None\n",
    "    for f in files:\n",
    "        X = np.load(f)  # [n_channels, n_time]\n",
    "        X_list.append(X); subjects.append(os.path.splitext(os.path.basename(f))[0])\n",
    "        ch_txt = f.replace(\".npy\", \".channels.txt\")\n",
    "        if channels is None:\n",
    "            if os.path.exists(ch_txt):\n",
    "                with open(ch_txt, \"r\") as g: channels = [ln.strip() for ln in g if ln.strip()]\n",
    "            else:\n",
    "                channels = [f\"ch{i}\" for i in range(X.shape[0])]\n",
    "\n",
    "# ---------- run ----------\n",
    "OUT = ensure_dir(OUT_ROOT)\n",
    "FIG = ensure_dir(os.path.join(OUT, \"figures\"))\n",
    "TAB = ensure_dir(os.path.join(OUT, \"tables\"))\n",
    "MET = ensure_dir(os.path.join(OUT, \"metrics\"))\n",
    "\n",
    "subj_labels, eigvals_all = [], []\n",
    "for sname, X in zip(subjects, X_list):\n",
    "    W = imag_coh_matrix(X, fs=fs, band=BAND, nperseg=96, noverlap=48)\n",
    "    L = laplacian_sym(W)\n",
    "    k = pick_k(L, kmax=KMAX)\n",
    "    labels, evals = spec_cluster(W, k)\n",
    "    subj_labels.append(labels); eigvals_all.append(evals.tolist())\n",
    "    plot_matrix(W, f\"{sname} {BAND} | iCoh\", os.path.join(FIG, f\"{sname}__{BAND}__icoherence.png\"))\n",
    "    plot_eigs(evals, f\"{sname} {BAND} | Laplacian eigs\", os.path.join(FIG, f\"{sname}__{BAND}__eigs.png\"))\n",
    "\n",
    "cons, co = consensus_from_labels(subj_labels)\n",
    "plot_matrix(co, f\"Consensus co-association | {BAND}\", os.path.join(FIG, f\"consensus__{BAND}__coassoc.png\"))\n",
    "loso_med_ari = ari_loso(subj_labels)\n",
    "\n",
    "null_aris = []\n",
    "for _ in range(NULL_PERMS):\n",
    "    nlabs = []\n",
    "    for X in X_list:\n",
    "        Y = circ_shift(X, rng)\n",
    "        Wn = imag_coh_matrix(Y, fs=fs, band=BAND, nperseg=96, noverlap=48)\n",
    "        Ln = laplacian_sym(Wn); kn = pick_k(Ln, kmax=KMAX)\n",
    "        lbn,_ = spec_cluster(Wn, kn)\n",
    "        nlabs.append(lbn)\n",
    "    cons_n,_ = consensus_from_labels(nlabs)\n",
    "    null_aris.append(adjusted_rand_score(cons, cons_n))\n",
    "null_aris = np.array(null_aris, float)\n",
    "p_val = float((np.sum(null_aris >= loso_med_ari) + 1) / (len(null_aris) + 1))\n",
    "\n",
    "metrics = {\n",
    "    \"band\": BAND,\n",
    "    \"n_subjects\": len(subjects),\n",
    "    \"loso_median_ari\": float(loso_med_ari),\n",
    "    \"null_ari_mean\": float(null_aris.mean()) if len(null_aris) else None,\n",
    "    \"null_ari_p_value\": p_val,\n",
    "    \"subjects\": subjects,\n",
    "    \"channels\": channels,\n",
    "    \"out_dir\": OUT\n",
    "}\n",
    "with open(os.path.join(MET, f\"band__{BAND}__metrics.json\"), \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "np.save(os.path.join(TAB, f\"band__{BAND}__consensus_labels.npy\"), cons)\n",
    "np.save(os.path.join(TAB, f\"band__{BAND}__coassoc.npy\"), co)\n",
    "\n",
    "print(\"=== CNT EEG Spectral-Glyphs (inline) ===\")\n",
    "print(\"Mode:\", \"DEMO\" if USE_DEMO else f\"REAL ({DATA_DIR})\")\n",
    "print(\"Artifacts:\", OUT)\n",
    "print(\"Key files:\")\n",
    "print(\" -\", os.path.join(MET, f\"band__{BAND}__metrics.json\"))\n",
    "print(\" -\", os.path.join(TAB, f\"band__{BAND}__consensus_labels.npy\"))\n",
    "print(\" -\", os.path.join(FIG, f\"consensus__{BAND}__coassoc.png\"))\n",
    "print(\"Stats:\", {k: metrics[k] for k in [\"loso_median_ari\", \"null_ari_mean\", \"null_ari_p_value\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79d659bc-db67-4a20-889a-6871bd2ae730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "[S01] load_data error: load_data() got an unexpected keyword argument 'subject'. Did you mean 'subjects'?\n",
      "[S02] load_data error: load_data() got an unexpected keyword argument 'subject'. Did you mean 'subjects'?\n",
      "[S03] load_data error: load_data() got an unexpected keyword argument 'subject'. Did you mean 'subjects'?\n",
      "[S04] load_data error: load_data() got an unexpected keyword argument 'subject'. Did you mean 'subjects'?\n",
      "[S05] load_data error: load_data() got an unexpected keyword argument 'subject'. Did you mean 'subjects'?\n",
      "[S06] load_data error: load_data() got an unexpected keyword argument 'subject'. Did you mean 'subjects'?\n",
      "[S07] load_data error: load_data() got an unexpected keyword argument 'subject'. Did you mean 'subjects'?\n",
      "[S08] load_data error: load_data() got an unexpected keyword argument 'subject'. Did you mean 'subjects'?\n",
      "[S09] load_data error: load_data() got an unexpected keyword argument 'subject'. Did you mean 'subjects'?\n",
      "[S10] load_data error: load_data() got an unexpected keyword argument 'subject'. Did you mean 'subjects'?\n",
      "=== Fetch complete ===\n",
      "(1, 'no data')\n",
      "(2, 'no data')\n",
      "(3, 'no data')\n",
      "(4, 'no data')\n",
      "(5, 'no data')\n",
      "(6, 'no data')\n",
      "(7, 'no data')\n",
      "(8, 'no data')\n",
      "(9, 'no data')\n",
      "(10, 'no data')\n",
      "\n",
      "Saved to: C:\\Users\\caleb\\CNT_Lab\\eeg_rest\n",
      "Example files:\n"
     ]
    }
   ],
   "source": [
    "# === Download EEGBCI → save as .npy per subject (eyes-closed by default) ===\n",
    "# Paste into your LOCAL JupyterLab (internet required).\n",
    "# Output: C:\\Users\\caleb\\CNT_Lab\\eeg_rest\\subject_XX.npy (+ .channels.txt)\n",
    "# Tip: use raw strings r\"...\" for Windows paths.\n",
    "\n",
    "# 1) Deps (idempotent inside Jupyter)\n",
    "try:\n",
    "    import mne  # noqa: F401\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    %pip install -q mne pooch\n",
    "except Exception as e:\n",
    "    print(\"pip skipped:\", e)\n",
    "\n",
    "import os, sys, math, numpy as np\n",
    "from pathlib import Path\n",
    "import mne\n",
    "\n",
    "# 2) Settings — tweak as needed\n",
    "OUT_DIR    = r\"C:\\Users\\caleb\\CNT_Lab\\eeg_rest\"  # where *.npy will be written\n",
    "SUBJECTS   = list(range(1, 11))                  # EEGBCI subjects to fetch (1..109 valid for eegbci)\n",
    "RUNS_EC    = [2]                                  # Eyes-Closed run(s) — 2 = EC\n",
    "RUNS_EO    = []                                   # Add [1] to also fetch Eyes-Open\n",
    "FS_OUT     = 250.0                                # resample target Hz\n",
    "DURATION_S = 60                                   # seconds to keep per subject (trim or tile if shorter)\n",
    "HP, LP     = 1.0, 45.0                            # band-pass for iCoh-ready signals (optional but recommended)\n",
    "MONTAGE    = \"standard_1020\"                      # channel name mapping\n",
    "ALLOW_REUSE= True                                 # skip re-download if files already exist\n",
    "\n",
    "# 3) Helpers\n",
    "def ensure_dir(p: str) -> str:\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "    return p\n",
    "\n",
    "def save_subject(X: np.ndarray, ch_names, out_base: str):\n",
    "    np.save(out_base + \".npy\", X.astype(np.float32))\n",
    "    with open(out_base + \".channels.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for ch in ch_names:\n",
    "            f.write(ch + \"\\n\")\n",
    "\n",
    "def fetch_concat(subject: int, runs: list[int]):\n",
    "    \"\"\"Download specified runs for a subject and return a concatenated Raw.\"\"\"\n",
    "    if not runs:\n",
    "        return None\n",
    "    try:\n",
    "        fpaths = mne.datasets.eegbci.load_data(subject=subject, runs=runs, update_path=True, verbose=\"ERROR\")\n",
    "    except Exception as e:\n",
    "        print(f\"[S{subj:02d}] load_data error: {e}\")\n",
    "        return None\n",
    "    raws = []\n",
    "    for fp in fpaths:\n",
    "        try:\n",
    "            raw = mne.io.read_raw_edf(fp, preload=True, verbose=\"ERROR\")\n",
    "            # Keep EEG only\n",
    "            raw.pick_types(eeg=True, stim=False, eog=False, ecg=False, emg=False, misc=False)\n",
    "            raws.append(raw)\n",
    "        except Exception as e:\n",
    "            print(f\"[S{subj:02d}] read_raw error: {e}\")\n",
    "    if not raws:\n",
    "        return None\n",
    "    return mne.concatenate_raws(raws, verbose=\"ERROR\")\n",
    "\n",
    "def trim_or_tile(X: np.ndarray, n_keep: int) -> np.ndarray:\n",
    "    if X.shape[1] >= n_keep:\n",
    "        return X[:, :n_keep]\n",
    "    reps = math.ceil(n_keep / X.shape[1])\n",
    "    return np.tile(X, reps)[:, :n_keep]\n",
    "\n",
    "# 4) Main\n",
    "ensure_dir(OUT_DIR)\n",
    "log = []\n",
    "for subj in SUBJECTS:\n",
    "    # Skip if already present and reuse enabled\n",
    "    out_base_ec = os.path.join(OUT_DIR, f\"subject_{subj:02d}_EC\") if RUNS_EC else None\n",
    "    out_base_eo = os.path.join(OUT_DIR, f\"subject_{subj:02d}_EO\") if RUNS_EO else None\n",
    "    already = False\n",
    "    if ALLOW_REUSE and RUNS_EC and os.path.exists((out_base_ec or \"\") + \".npy\"):\n",
    "        already = True\n",
    "    if ALLOW_REUSE and RUNS_EO and os.path.exists((out_base_eo or \"\") + \".npy\"):\n",
    "        already = True\n",
    "    if already:\n",
    "        log.append((subj, \"skipped (exists)\"))\n",
    "        continue\n",
    "\n",
    "    # EC\n",
    "    if RUNS_EC:\n",
    "        raw_ec = fetch_concat(subj, RUNS_EC)\n",
    "    else:\n",
    "        raw_ec = None\n",
    "    # EO\n",
    "    if RUNS_EO:\n",
    "        raw_eo = fetch_concat(subj, RUNS_EO)\n",
    "    else:\n",
    "        raw_eo = None\n",
    "\n",
    "    if (raw_ec is None) and (raw_eo is None):\n",
    "        log.append((subj, \"no data\"))\n",
    "        continue\n",
    "\n",
    "    for tag, raw in ((\"EC\", raw_ec), (\"EO\", raw_eo)):\n",
    "        if raw is None:\n",
    "            continue\n",
    "        try:\n",
    "            # Montage + filter + resample\n",
    "            try:\n",
    "                raw.set_montage(MONTAGE, on_missing=\"ignore\", match_case=False, verbose=\"ERROR\")\n",
    "            except Exception:\n",
    "                pass\n",
    "            raw.filter(HP, LP, fir_design=\"firwin\", verbose=\"ERROR\")\n",
    "            raw.resample(FS_OUT, npad=\"auto\", verbose=\"ERROR\")\n",
    "\n",
    "            # Slice/tile to DURATION_S\n",
    "            sfreq = float(raw.info[\"sfreq\"])\n",
    "            n_keep = int(DURATION_S * sfreq)\n",
    "            X = raw.get_data(picks=\"eeg\")  # [n_ch, n_t]\n",
    "            X = trim_or_tile(X, n_keep)\n",
    "\n",
    "            ch_names = mne.pick_info(raw.info, mne.pick_types(raw.info, eeg=True)).ch_names\n",
    "            out_base = os.path.join(OUT_DIR, f\"subject_{subj:02d}_{tag}\")\n",
    "            save_subject(X, ch_names, out_base)\n",
    "            log.append((subj, f\"{tag}: ok {X.shape} → {out_base}.npy\"))\n",
    "        except Exception as e:\n",
    "            log.append((subj, f\"{tag}: error {e}\"))\n",
    "\n",
    "# 5) Summary\n",
    "print(\"=== Fetch complete ===\")\n",
    "for row in log:\n",
    "    print(row)\n",
    "print(f\"\\nSaved to: {OUT_DIR}\")\n",
    "print(\"Example files:\")\n",
    "for subj in SUBJECTS[:3]:\n",
    "    if RUNS_EC:\n",
    "        p = os.path.join(OUT_DIR, f\"subject_{subj:02d}_EC.npy\")\n",
    "        if os.path.exists(p): print(\" -\", p)\n",
    "    if RUNS_EO:\n",
    "        p = os.path.join(OUT_DIR, f\"subject_{subj:02d}_EO.npy\")\n",
    "        if os.path.exists(p): print(\" -\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c454f12-078e-44df-8d1b-48275aa9da33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S001/S001R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S001/S001R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S002/S002R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S002/S002R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S003/S003R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S003/S003R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S004/S004R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S004/S004R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S005/S005R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S005/S005R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S006/S006R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S006/S006R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S007/S007R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S007/S007R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S008/S008R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S008/S008R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S009/S009R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S009/S009R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S010/S010R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S010/S010R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "=== Fetch complete ===\n",
      "(1, 'EC: ok (64, 15000) → C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\eeg_rest\\\\subject_01_EC.npy')\n",
      "(2, 'EC: ok (64, 15000) → C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\eeg_rest\\\\subject_02_EC.npy')\n",
      "(3, 'EC: ok (64, 15000) → C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\eeg_rest\\\\subject_03_EC.npy')\n",
      "(4, 'EC: ok (64, 15000) → C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\eeg_rest\\\\subject_04_EC.npy')\n",
      "(5, 'EC: ok (64, 15000) → C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\eeg_rest\\\\subject_05_EC.npy')\n",
      "(6, 'EC: ok (64, 15000) → C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\eeg_rest\\\\subject_06_EC.npy')\n",
      "(7, 'EC: ok (64, 15000) → C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\eeg_rest\\\\subject_07_EC.npy')\n",
      "(8, 'EC: ok (64, 15000) → C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\eeg_rest\\\\subject_08_EC.npy')\n",
      "(9, 'EC: ok (64, 15000) → C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\eeg_rest\\\\subject_09_EC.npy')\n",
      "(10, 'EC: ok (64, 15000) → C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\eeg_rest\\\\subject_10_EC.npy')\n",
      "\n",
      "Saved to: C:\\Users\\caleb\\CNT_Lab\\eeg_rest\n",
      "Next: set USE_DEMO=False and DATA_DIR=r\"C:\\Users\\caleb\\CNT_Lab\\eeg_rest\" in your spectral-glyph cell.\n"
     ]
    }
   ],
   "source": [
    "# === EEGBCI downloader → saves .npy per subject (Eyes-Closed by default) ===\n",
    "# Run this on YOUR local JupyterLab (internet required).\n",
    "# Output files: C:\\Users\\caleb\\CNT_Lab\\eeg_rest\\subject_XX_EC.npy (+ .channels.txt)\n",
    "# Fix: uses the correct signature mne.datasets.eegbci.load_data(subjects=..., runs=...)\n",
    "\n",
    "# 1) Deps (safe to rerun in Jupyter)\n",
    "try:\n",
    "    import mne  # noqa: F401\n",
    "except Exception:\n",
    "    pass\n",
    "try:\n",
    "    %pip install -q mne pooch\n",
    "except Exception as e:\n",
    "    print(\"pip skipped:\", e)\n",
    "\n",
    "import os, math, numpy as np\n",
    "from pathlib import Path\n",
    "import mne\n",
    "\n",
    "# 2) Settings — tweak as needed\n",
    "OUT_DIR    = r\"C:\\Users\\caleb\\CNT_Lab\\eeg_rest\"  # where *.npy will be written\n",
    "SUBJECTS   = list(range(1, 11))                  # EEGBCI subject IDs (valid: 1..109)\n",
    "RUNS_EC    = [2]                                  # Eyes-Closed run(s) → 2\n",
    "RUNS_EO    = []                                   # Add [1] to also fetch Eyes-Open\n",
    "FS_OUT     = 250.0                                # resample target Hz\n",
    "DURATION_S = 60                                   # seconds per subject to keep\n",
    "HP, LP     = 1.0, 45.0                            # band-pass (good for iCoh later)\n",
    "MONTAGE    = \"standard_1020\"                      # 10-20 channel names\n",
    "ALLOW_REUSE= True                                 # skip if file already exists\n",
    "\n",
    "# 3) Helpers\n",
    "def ensure_dir(p: str) -> str:\n",
    "    Path(p).mkdir(parents=True, exist_ok=True); return p\n",
    "\n",
    "def save_subject(X: np.ndarray, ch_names, out_base: str):\n",
    "    np.save(out_base + \".npy\", X.astype(np.float32))\n",
    "    with open(out_base + \".channels.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for ch in ch_names:\n",
    "            f.write(ch + \"\\n\")\n",
    "\n",
    "def fetch_concat(subject: int, runs: list[int]):\n",
    "    \"\"\"Download given runs for a subject and return concatenated Raw.\"\"\"\n",
    "    if not runs: return None\n",
    "    try:\n",
    "        # ✅ Correct call for newer MNE:\n",
    "        fpaths = mne.datasets.eegbci.load_data(subjects=[subject], runs=runs, update_path=True, verbose=\"ERROR\")\n",
    "    except TypeError:\n",
    "        # Fallback for older MNE versions that expect subject= (singular)\n",
    "        fpaths = mne.datasets.eegbci.load_data(subject=subject, runs=runs, update_path=True, verbose=\"ERROR\")\n",
    "    raws = []\n",
    "    for fp in fpaths:\n",
    "        try:\n",
    "            raw = mne.io.read_raw_edf(fp, preload=True, verbose=\"ERROR\")\n",
    "            raw.pick_types(eeg=True, stim=False, eog=False, ecg=False, emg=False, misc=False)\n",
    "            raws.append(raw)\n",
    "        except Exception as e:\n",
    "            print(f\"[S{subject:02d}] read_raw error: {e}\")\n",
    "    return mne.concatenate_raws(raws, verbose=\"ERROR\") if raws else None\n",
    "\n",
    "def trim_or_tile(X: np.ndarray, n_keep: int) -> np.ndarray:\n",
    "    if X.shape[1] >= n_keep: return X[:, :n_keep]\n",
    "    reps = math.ceil(n_keep / X.shape[1]); return np.tile(X, reps)[:, :n_keep]\n",
    "\n",
    "# 4) Main\n",
    "ensure_dir(OUT_DIR)\n",
    "log = []\n",
    "for subj in SUBJECTS:\n",
    "    out_base_ec = os.path.join(OUT_DIR, f\"subject_{subj:02d}_EC\") if RUNS_EC else None\n",
    "    out_base_eo = os.path.join(OUT_DIR, f\"subject_{subj:02d}_EO\") if RUNS_EO else None\n",
    "\n",
    "    if ALLOW_REUSE and RUNS_EC and os.path.exists((out_base_ec or \"\") + \".npy\"):\n",
    "        log.append((subj, \"EC: skipped (exists)\"))\n",
    "        raw_ec = None\n",
    "    else:\n",
    "        raw_ec = fetch_concat(subj, RUNS_EC) if RUNS_EC else None\n",
    "\n",
    "    if ALLOW_REUSE and RUNS_EO and os.path.exists((out_base_eo or \"\") + \".npy\"):\n",
    "        log.append((subj, \"EO: skipped (exists)\"))\n",
    "        raw_eo = None\n",
    "    else:\n",
    "        raw_eo = fetch_concat(subj, RUNS_EO) if RUNS_EO else None\n",
    "\n",
    "    if (raw_ec is None) and (raw_eo is None):\n",
    "        log.append((subj, \"no new data\"))\n",
    "        continue\n",
    "\n",
    "    for tag, raw in ((\"EC\", raw_ec), (\"EO\", raw_eo)):\n",
    "        if raw is None: continue\n",
    "        try:\n",
    "            try:\n",
    "                raw.set_montage(MONTAGE, on_missing=\"ignore\", match_case=False, verbose=\"ERROR\")\n",
    "            except Exception:\n",
    "                pass\n",
    "            raw.filter(HP, LP, fir_design=\"firwin\", verbose=\"ERROR\")\n",
    "            raw.resample(FS_OUT, npad=\"auto\", verbose=\"ERROR\")\n",
    "\n",
    "            sf = float(raw.info[\"sfreq\"])\n",
    "            X = raw.get_data(picks=\"eeg\")                   # [n_ch, n_t]\n",
    "            X = trim_or_tile(X, int(DURATION_S * sf))\n",
    "            ch_names = mne.pick_info(raw.info, mne.pick_types(raw.info, eeg=True)).ch_names\n",
    "\n",
    "            out_base = os.path.join(OUT_DIR, f\"subject_{subj:02d}_{tag}\")\n",
    "            save_subject(X, ch_names, out_base)\n",
    "            log.append((subj, f\"{tag}: ok {X.shape} → {out_base}.npy\"))\n",
    "        except Exception as e:\n",
    "            log.append((subj, f\"{tag}: error {e}\"))\n",
    "\n",
    "print(\"=== Fetch complete ===\")\n",
    "for row in log: print(row)\n",
    "print(f\"\\nSaved to: {OUT_DIR}\")\n",
    "print(\"Next: set USE_DEMO=False and DATA_DIR=r\\\"C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\eeg_rest\\\" in your spectral-glyph cell.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3675b7eb-79b0-4fb0-8a2a-32fc3503e861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 subjects: ['subject_01_EC', 'subject_02_EC', 'subject_03_EC', 'subject_04_EC', 'subject_05_EC', 'subject_06_EC', 'subject_07_EC', 'subject_08_EC'] ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 163\u001b[39m\n\u001b[32m    161\u001b[39m X = np.load(f)\n\u001b[32m    162\u001b[39m Y = circ_shift(X, rng)\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m Wn = \u001b[43mimag_coh_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mband\u001b[49m\u001b[43m=\u001b[49m\u001b[43mband\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m Ln = laplacian_sym(Wn)\n\u001b[32m    165\u001b[39m kn = pick_k(Ln, kmax=KMAX)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mimag_coh_matrix\u001b[39m\u001b[34m(X, fs, band)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_ch):\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i+\u001b[32m1\u001b[39m, n_ch):\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m         _, Pxy = \u001b[43mcsd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnperseg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNPERSEG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoverlap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNOVERLAP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m         num = np.abs(np.imag(Pxy[sel]))\n\u001b[32m     44\u001b[39m         den = np.sqrt(Pxx[i][sel]*Pxx[j][sel] + \u001b[32m1e-12\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\signal\\_spectral_py.py:917\u001b[39m, in \u001b[36mcsd\u001b[39m\u001b[34m(x, y, fs, window, nperseg, noverlap, nfft, detrend, return_onesided, scaling, axis, average)\u001b[39m\n\u001b[32m    914\u001b[39m SFT = ShortTimeFFT(win, nperseg - noverlap, fs, fft_mode=fft_mode, mfft=nfft,\n\u001b[32m    915\u001b[39m                    scale_to=scales[scaling], phase_shift=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    916\u001b[39m \u001b[38;5;66;03m# csd() calculates X.conj()*Y instead of X*Y.conj():\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m917\u001b[39m Pxy = \u001b[43mSFT\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetr\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdetrend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdetrend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mp0\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp1\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoverlap\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mSFT\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnperseg\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m                      \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[38;5;66;03m# Note:\u001b[39;00m\n\u001b[32m    922\u001b[39m \u001b[38;5;66;03m# 'onesided2X' scaling of ShortTimeFFT conflicts with the\u001b[39;00m\n\u001b[32m    923\u001b[39m \u001b[38;5;66;03m# scaling='spectrum' parameter, since it doubles the squared magnitude,\u001b[39;00m\n\u001b[32m    924\u001b[39m \u001b[38;5;66;03m# which in the view of the ShortTimeFFT implementation does not make sense.\u001b[39;00m\n\u001b[32m    925\u001b[39m \u001b[38;5;66;03m# Hence, the doubling of the square is implemented here:\u001b[39;00m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_onesided:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\signal\\_short_time_fft.py:1391\u001b[39m, in \u001b[36mShortTimeFFT.spectrogram\u001b[39m\u001b[34m(self, x, y, detr, p0, p1, k_offset, padding, axis)\u001b[39m\n\u001b[32m   1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Sx.real**\u001b[32m2\u001b[39m + Sx.imag**\u001b[32m2\u001b[39m\n\u001b[32m   1390\u001b[39m \u001b[38;5;66;03m# Cross-spectrogram:\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1391\u001b[39m Sy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstft_detrend\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Sx * Sy.conj()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\signal\\_short_time_fft.py:1245\u001b[39m, in \u001b[36mShortTimeFFT.stft_detrend\u001b[39m\u001b[34m(self, x, detr, p0, p1, k_offset, padding, axis)\u001b[39m\n\u001b[32m   1243\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m detr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1244\u001b[39m         x_ = detr(x_)\n\u001b[32m-> \u001b[39m\u001b[32m1245\u001b[39m     S[..., :, p_] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fft_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.ndim > \u001b[32m1\u001b[39m:\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.moveaxis(S, -\u001b[32m2\u001b[39m, axis \u001b[38;5;28;01mif\u001b[39;00m axis >= \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m axis-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\signal\\_short_time_fft.py:2105\u001b[39m, in \u001b[36mShortTimeFFT._fft_func\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m   2103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fft_lib.fftshift(fft_lib.fft(x, \u001b[38;5;28mself\u001b[39m.mfft, axis=-\u001b[32m1\u001b[39m), axes=-\u001b[32m1\u001b[39m)\n\u001b[32m   2104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fft_mode == \u001b[33m'\u001b[39m\u001b[33monesided\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfft_lib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrfft\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmfft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fft_mode == \u001b[33m'\u001b[39m\u001b[33monesided2X\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m   2107\u001b[39m     X = fft_lib.rfft(x, n=\u001b[38;5;28mself\u001b[39m.mfft, axis=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\fft\\_backend.py:28\u001b[39m, in \u001b[36m_ScipyBackend.__ua_function__\u001b[39m\u001b[34m(method, args, kwargs)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\fft\\_basic_backend.py:91\u001b[39m, in \u001b[36mrfft\u001b[39m\u001b[34m(x, n, axis, norm, overwrite_x, workers, plan)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrfft\u001b[39m(x, n=\u001b[38;5;28;01mNone\u001b[39;00m, axis=-\u001b[32m1\u001b[39m, norm=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     90\u001b[39m          overwrite_x=\u001b[38;5;28;01mFalse\u001b[39;00m, workers=\u001b[38;5;28;01mNone\u001b[39;00m, *, plan=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_execute_1D\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrfft\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pocketfft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrfft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m                       \u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\fft\\_basic_backend.py:32\u001b[39m, in \u001b[36m_execute_1D\u001b[39m\u001b[34m(func_str, pocketfft_func, x, n, axis, norm, overwrite_x, workers, plan)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_numpy(xp):\n\u001b[32m     31\u001b[39m     x = np.asarray(x)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpocketfft_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m                          \u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m norm = _validate_fft_args(workers, plan, norm)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(xp, \u001b[33m'\u001b[39m\u001b[33mfft\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\fft\\_pocketfft\\basic.py:61\u001b[39m, in \u001b[36mr2c\u001b[39m\u001b[34m(forward, x, n, axis, norm, overwrite_x, workers, plan)\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minvalid number of data points (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtmp.shape[axis]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) specified\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Note: overwrite_x is not utilised\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpfft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mr2c\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === CNT EEG Spectral-Glyphs (EEGBCI EC) — single cell ===\n",
    "# Input:  C:\\Users\\caleb\\CNT_Lab\\eeg_rest\\subject_XX_EC.npy (+ .channels.txt)\n",
    "# Output: C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_v1\\{figures,tables,metrics}\n",
    "# What it does:\n",
    "#   • alpha/theta/beta imaginary coherence per subject\n",
    "#   • spectral clustering (k via eigen-gap, ≤ KMAX)\n",
    "#   • consensus clusters across subjects + LOSO-ARI\n",
    "#   • circular-shift null p-value\n",
    "# Tip: you can raise NULL_PERMS for stronger stats (e.g., 1000) once it runs.\n",
    "\n",
    "import os, glob, json, numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy.signal import welch, csd\n",
    "\n",
    "# ---------------- Config (edit as needed) ----------------\n",
    "DATA_DIR   = r\"C:\\Users\\caleb\\CNT_Lab\\eeg_rest\"                # where subject_XX_EC.npy live\n",
    "GLOB       = \"subject_*_EC.npy\"                                # only EC files\n",
    "OUT_ROOT   = r\"C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_v1\"\n",
    "FS         = 250.0                                             # Hz (from your downloader)\n",
    "BANDS      = (\"alpha\",\"theta\",\"beta\")                          # add/remove as you like\n",
    "KMAX       = 4\n",
    "NULL_PERMS = 200                                               # increase later for publish\n",
    "NPERSEG    = 256                                               # Welch/CSD window\n",
    "NOVERLAP   = 128\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def ensure_dir(p): os.makedirs(p, exist_ok=True); return p\n",
    "\n",
    "def band_lims():\n",
    "    return {\"delta\":(1.0,4.0),\"theta\":(4.0,8.0),\"alpha\":(8.0,13.0),\"beta\":(13.0,30.0),\"gamma\":(30.0,45.0)}\n",
    "\n",
    "def imag_coh_matrix(X, fs, band):\n",
    "    n_ch, _ = X.shape\n",
    "    fmin, fmax = band_lims()[band]\n",
    "    freqs, Pxx0 = welch(X[0], fs=fs, nperseg=NPERSEG, noverlap=NOVERLAP)\n",
    "    Pxx = [Pxx0] + [welch(X[i], fs=fs, nperseg=NPERSEG, noverlap=NOVERLAP)[1] for i in range(1, n_ch)]\n",
    "    sel = (freqs >= fmin) & (freqs <= fmax)\n",
    "    W = np.zeros((n_ch, n_ch), float)\n",
    "    for i in range(n_ch):\n",
    "        for j in range(i+1, n_ch):\n",
    "            _, Pxy = csd(X[i], X[j], fs=fs, nperseg=NPERSEG, noverlap=NOVERLAP)\n",
    "            num = np.abs(np.imag(Pxy[sel]))\n",
    "            den = np.sqrt(Pxx[i][sel]*Pxx[j][sel] + 1e-12)\n",
    "            W[i,j] = W[j,i] = float(np.nanmean(num/den))\n",
    "    np.fill_diagonal(W, 0.0)\n",
    "    return W\n",
    "\n",
    "def laplacian_sym(W):\n",
    "    d = W.sum(1); d = np.where(d <= 1e-12, 1.0, d)\n",
    "    Dmh = np.diag(1.0/np.sqrt(d))\n",
    "    return np.eye(W.shape[0]) - Dmh @ W @ Dmh\n",
    "\n",
    "def pick_k(L, kmax=4):\n",
    "    evals, _ = np.linalg.eigh(L)\n",
    "    gaps = [(k, evals[k+1]-evals[k]) for k in range(1, min(kmax+1, len(evals)-1))]\n",
    "    if not gaps: return 2\n",
    "    k_star = max(gaps, key=lambda x: x[1])[0]\n",
    "    return max(2, min(k_star, kmax))\n",
    "\n",
    "def spec_cluster(W, k):\n",
    "    L = laplacian_sym(W)\n",
    "    evals, evecs = np.linalg.eigh(L)\n",
    "    U = evecs[:, 1:k+1]\n",
    "    U /= (np.linalg.norm(U, axis=1, keepdims=True) + 1e-12)\n",
    "    labels = KMeans(n_clusters=k, n_init=50, random_state=42).fit_predict(U)\n",
    "    return labels, evals\n",
    "\n",
    "def consensus_from_labels(all_labels):\n",
    "    n = len(all_labels[0]); m = len(all_labels); co = np.zeros((n, n), float)\n",
    "    for lab in all_labels:\n",
    "        for i in range(n):\n",
    "            li = lab[i]\n",
    "            for j in range(n):\n",
    "                co[i,j] += 1.0 if li == lab[j] else 0.0\n",
    "    co /= m\n",
    "    thr = 0.5; A = (co >= thr).astype(int)\n",
    "    visited = np.zeros(n, bool); cons = -1*np.ones(n, int); cid = 0\n",
    "    for i in range(n):\n",
    "        if not visited[i]:\n",
    "            stack=[i]; visited[i]=True; cons[i]=cid\n",
    "            while stack:\n",
    "                u=stack.pop()\n",
    "                for v in range(n):\n",
    "                    if A[u,v] and not visited[v]:\n",
    "                        visited[v]=True; cons[v]=cid; stack.append(v)\n",
    "            cid+=1\n",
    "    return cons, co\n",
    "\n",
    "def ari_loso(all_labels):\n",
    "    cons,_ = consensus_from_labels(all_labels)\n",
    "    vals=[]\n",
    "    for s in range(len(all_labels)):\n",
    "        leave=[lab for i,lab in enumerate(all_labels) if i!=s]\n",
    "        cons_leave,_ = consensus_from_labels(leave)\n",
    "        vals.append(adjusted_rand_score(cons, cons_leave))\n",
    "    return float(np.median(vals))\n",
    "\n",
    "def circ_shift(X, rng, max_shift=None):\n",
    "    n_ch, n_t = X.shape\n",
    "    if max_shift is None: max_shift = n_t-1\n",
    "    Y = np.zeros_like(X)\n",
    "    for c in range(n_ch):\n",
    "        s = int(rng.integers(0, max_shift+1))\n",
    "        Y[c] = np.roll(X[c], s)\n",
    "    return Y\n",
    "\n",
    "def plot_matrix(M, title, path):\n",
    "    plt.figure(); plt.imshow(M, aspect='auto'); plt.title(title); plt.colorbar(); plt.tight_layout(); plt.savefig(path, dpi=160); plt.close()\n",
    "\n",
    "def plot_eigs(evals, title, path):\n",
    "    plt.figure(); plt.plot(np.arange(len(evals)), evals, marker='o'); plt.title(title); plt.xlabel(\"Index\"); plt.ylabel(\"Eigenvalue\")\n",
    "    plt.tight_layout(); plt.savefig(path, dpi=160); plt.close()\n",
    "\n",
    "# ---------------- Discover data ----------------\n",
    "files = sorted(glob.glob(os.path.join(DATA_DIR, GLOB)))\n",
    "if not files:\n",
    "    raise RuntimeError(f\"No files matched: {os.path.join(DATA_DIR, GLOB)}\")\n",
    "\n",
    "subjects = [os.path.splitext(os.path.basename(f))[0] for f in files]\n",
    "print(f\"Found {len(files)} subjects:\", subjects[:8], \"...\" if len(files)>8 else \"\")\n",
    "\n",
    "# ---------------- Prepare output dirs ----------------\n",
    "FIG = ensure_dir(os.path.join(OUT_ROOT, \"figures\"))\n",
    "TAB = ensure_dir(os.path.join(OUT_ROOT, \"tables\"))\n",
    "MET = ensure_dir(os.path.join(OUT_ROOT, \"metrics\"))\n",
    "\n",
    "# ---------------- Run per band ----------------\n",
    "rng = np.random.default_rng(7)\n",
    "for band in BANDS:\n",
    "    subj_labels = []; eigvals_all = []; ch_template = None\n",
    "\n",
    "    for sname, f in zip(subjects, files):\n",
    "        X = np.load(f)  # [n_ch, n_t]\n",
    "        ch_txt = f.replace(\".npy\", \".channels.txt\")\n",
    "        if ch_template is None:\n",
    "            if os.path.exists(ch_txt):\n",
    "                with open(ch_txt, \"r\", encoding=\"utf-8\") as g:\n",
    "                    ch_template = [ln.strip() for ln in g if ln.strip()]\n",
    "            else:\n",
    "                ch_template = [f\"ch{i}\" for i in range(X.shape[0])]\n",
    "\n",
    "        W = imag_coh_matrix(X, fs=FS, band=band)\n",
    "        L = laplacian_sym(W)\n",
    "        k = pick_k(L, kmax=KMAX)\n",
    "        labels, evals = spec_cluster(W, k)\n",
    "\n",
    "        subj_labels.append(labels); eigvals_all.append(evals.tolist())\n",
    "        plot_matrix(W, f\"{sname} {band} | iCoh\", os.path.join(FIG, f\"{sname}__{band}__icoherence.png\"))\n",
    "        plot_eigs(evals, f\"{sname} {band} | Laplacian eigs\", os.path.join(FIG, f\"{sname}__{band}__eigs.png\"))\n",
    "\n",
    "    cons, co = consensus_from_labels(subj_labels)\n",
    "    plot_matrix(co, f\"Consensus co-association | {band}\", os.path.join(FIG, f\"consensus__{band}__coassoc.png\"))\n",
    "    loso_med_ari = ari_loso(subj_labels)\n",
    "\n",
    "    # Null test\n",
    "    null_aris = []\n",
    "    for _ in range(NULL_PERMS):\n",
    "        nlabs=[]\n",
    "        for f in files:\n",
    "            X = np.load(f)\n",
    "            Y = circ_shift(X, rng)\n",
    "            Wn = imag_coh_matrix(Y, fs=FS, band=band)\n",
    "            Ln = laplacian_sym(Wn)\n",
    "            kn = pick_k(Ln, kmax=KMAX)\n",
    "            lbn,_ = spec_cluster(Wn, kn)\n",
    "            nlabs.append(lbn)\n",
    "        cons_n,_ = consensus_from_labels(nlabs)\n",
    "        null_aris.append(adjusted_rand_score(cons, cons_n))\n",
    "    null_aris = np.array(null_aris, float)\n",
    "    p_val = float((np.sum(null_aris >= loso_med_ari) + 1) / (len(null_aris) + 1))\n",
    "\n",
    "    # Save\n",
    "    np.save(os.path.join(TAB, f\"band__{band}__consensus_labels.npy\"), cons)\n",
    "    np.save(os.path.join(TAB, f\"band__{band}__coassoc.npy\"), co)\n",
    "    metrics = {\n",
    "        \"band\": band,\n",
    "        \"n_subjects\": len(subjects),\n",
    "        \"loso_median_ari\": float(loso_med_ari),\n",
    "        \"null_ari_mean\": float(null_aris.mean()) if len(null_aris) else None,\n",
    "        \"null_ari_p_value\": p_val,\n",
    "        \"subjects\": subjects,\n",
    "        \"channels\": ch_template,\n",
    "        \"out_dir\": OUT_ROOT\n",
    "    }\n",
    "    with open(os.path.join(MET, f\"band__{band}__metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    print(f\"[{band}] subjects={len(subjects)}  LOSO-ARI={loso_med_ari:.3f}  null_mean={float(null_aris.mean()):.3f}  p={p_val:.4f}\")\n",
    "\n",
    "print(\"\\nArtifacts:\")\n",
    "print(\" - Figures:\", FIG)\n",
    "print(\" - Tables :\", TAB)\n",
    "print(\" - Metrics:\", MET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5f22265-6cf2-4db6-a288-ce7a646a8771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 EC subjects\n",
      "\n",
      "=== FAST PASS RESULTS ===\n",
      "band=alpha  n=10\n",
      "LOSO-median ARI: 1.000\n",
      "Null ARI mean  : 1.000\n",
      "Permutation p  : 1.0000\n",
      "\n",
      "Artifacts:\n",
      " - C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_fast\\metrics\\band__alpha__metrics.json\n",
      " - C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_fast\\tables\\band__alpha__consensus_labels.npy\n"
     ]
    }
   ],
   "source": [
    "# === CNT Spectral-Glyphs — FAST PASS (prints results) ===\n",
    "# Inputs: C:\\Users\\caleb\\CNT_Lab\\eeg_rest\\subject_XX_EC.npy (+ .channels.txt)\n",
    "# Outputs: C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_fast\\{tables,metrics}\n",
    "\n",
    "import os, glob, json, numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy.signal import welch, csd\n",
    "\n",
    "# -------- config --------\n",
    "DATA_DIR   = r\"C:\\Users\\caleb\\CNT_Lab\\eeg_rest\"\n",
    "GLOB       = \"subject_*_EC.npy\"\n",
    "OUT_ROOT   = r\"C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_fast\"\n",
    "FS         = 250.0\n",
    "BAND       = \"alpha\"\n",
    "KMAX       = 3\n",
    "NULL_PERMS = 20\n",
    "NPERSEG    = 128\n",
    "NOVERLAP   = 64\n",
    "\n",
    "def ensure_dir(p): os.makedirs(p, exist_ok=True); return p\n",
    "def band_lims(): return {\"delta\":(1,4),\"theta\":(4,8),\"alpha\":(8,13),\"beta\":(13,30),\"gamma\":(30,45)}\n",
    "\n",
    "def imag_coh_matrix(X, fs, band):\n",
    "    n_ch, _ = X.shape\n",
    "    fmin, fmax = band_lims()[band]\n",
    "    freqs, Pxx0 = welch(X[0], fs=fs, nperseg=NPERSEG, noverlap=NOVERLAP)\n",
    "    Pxx = [Pxx0] + [welch(X[i], fs=fs, nperseg=NPERSEG, noverlap=NOVERLAP)[1] for i in range(1, n_ch)]\n",
    "    sel = (freqs >= fmin) & (freqs <= fmax)\n",
    "    W = np.zeros((n_ch, n_ch), float)\n",
    "    for i in range(n_ch):\n",
    "        for j in range(i+1, n_ch):\n",
    "            _, Pxy = csd(X[i], X[j], fs=fs, nperseg=NPERSEG, noverlap=NOVERLAP)\n",
    "            num = np.abs(np.imag(Pxy[sel])); den = np.sqrt(Pxx[i][sel]*Pxx[j][sel] + 1e-12)\n",
    "            W[i,j] = W[j,i] = float(np.nanmean(num/den))\n",
    "    np.fill_diagonal(W, 0.0)\n",
    "    return W\n",
    "\n",
    "def laplacian_sym(W):\n",
    "    d = W.sum(1); d = np.where(d <= 1e-12, 1.0, d)\n",
    "    Dmh = np.diag(1.0/np.sqrt(d))\n",
    "    return np.eye(W.shape[0]) - Dmh @ W @ Dmh\n",
    "\n",
    "def pick_k(L, kmax=3):\n",
    "    evals, _ = np.linalg.eigh(L)\n",
    "    if len(evals) < 3: return 2\n",
    "    gaps = [(k, evals[k+1]-evals[k]) for k in range(1, min(kmax+1, len(evals)-1))]\n",
    "    if not gaps: return 2\n",
    "    return max(2, min(max(gaps, key=lambda x: x[1])[0], kmax))\n",
    "\n",
    "def spec_cluster(W, k):\n",
    "    L = laplacian_sym(W)\n",
    "    evals, evecs = np.linalg.eigh(L)\n",
    "    U = evecs[:, 1:k+1]\n",
    "    U = U / (np.linalg.norm(U, axis=1, keepdims=True) + 1e-12)\n",
    "    labels = KMeans(n_clusters=k, n_init=50, random_state=42).fit_predict(U)\n",
    "    return labels\n",
    "\n",
    "def consensus_from_labels(all_labels):\n",
    "    n = len(all_labels[0]); m = len(all_labels)\n",
    "    co = np.zeros((n, n), float)\n",
    "    for lab in all_labels:\n",
    "        for i in range(n):\n",
    "            li = lab[i]\n",
    "            for j in range(n):\n",
    "                co[i,j] += 1.0 if li == lab[j] else 0.0\n",
    "    co /= m\n",
    "    thr = 0.5; A = (co >= thr).astype(int)\n",
    "    visited = np.zeros(n, bool); cons = -1*np.ones(n, int); cid = 0\n",
    "    for i in range(n):\n",
    "        if not visited[i]:\n",
    "            stack=[i]; visited[i]=True; cons[i]=cid\n",
    "            while stack:\n",
    "                u=stack.pop()\n",
    "                for v in range(n):\n",
    "                    if A[u,v] and not visited[v]:\n",
    "                        visited[v]=True; cons[v]=cid; stack.append(v)\n",
    "            cid+=1\n",
    "    return cons, co\n",
    "\n",
    "def ari_loso(all_labels):\n",
    "    cons,_ = consensus_from_labels(all_labels)\n",
    "    vals=[]\n",
    "    for s in range(len(all_labels)):\n",
    "        leave=[lab for i,lab in enumerate(all_labels) if i!=s]\n",
    "        cons_leave,_ = consensus_from_labels(leave)\n",
    "        vals.append(adjusted_rand_score(cons, cons_leave))\n",
    "    return float(np.median(vals))\n",
    "\n",
    "def circ_shift(X, rng, max_shift=None):\n",
    "    n_ch, n_t = X.shape\n",
    "    if max_shift is None: max_shift = n_t-1\n",
    "    Y = np.zeros_like(X)\n",
    "    for c in range(n_ch):\n",
    "        s = int(rng.integers(0, max_shift+1))\n",
    "        Y[c] = np.roll(X[c], s)\n",
    "    return Y\n",
    "\n",
    "# ---------- run ----------\n",
    "files = sorted(glob.glob(os.path.join(DATA_DIR, GLOB)))\n",
    "print(f\"Found {len(files)} EC subjects\")\n",
    "\n",
    "OUT_MET = ensure_dir(os.path.join(OUT_ROOT, \"metrics\"))\n",
    "OUT_TAB = ensure_dir(os.path.join(OUT_ROOT, \"tables\"))\n",
    "\n",
    "rng = np.random.default_rng(7)\n",
    "subj_labels = []\n",
    "for f in files:\n",
    "    X = np.load(f)\n",
    "    W = imag_coh_matrix(X, fs=FS, band=BAND)\n",
    "    k = pick_k(laplacian_sym(W), kmax=KMAX)\n",
    "    subj_labels.append(spec_cluster(W, k))\n",
    "\n",
    "cons, co = consensus_from_labels(subj_labels)\n",
    "\n",
    "null_aris = []\n",
    "for _ in range(NULL_PERMS):\n",
    "    nlabs=[]\n",
    "    for f in files:\n",
    "        X = np.load(f)\n",
    "        Y = circ_shift(X, rng)\n",
    "        Wn = imag_coh_matrix(Y, fs=FS, band=BAND)\n",
    "        kn = pick_k(laplacian_sym(Wn), kmax=KMAX)\n",
    "        nlabs.append(spec_cluster(Wn, kn))\n",
    "    cons_n,_ = consensus_from_labels(nlabs)\n",
    "    null_aris.append(adjusted_rand_score(cons, cons_n))\n",
    "null_aris = np.array(null_aris, float)\n",
    "\n",
    "loso_med_ari = ari_loso(subj_labels)\n",
    "p_val = float((np.sum(null_aris >= loso_med_ari) + 1) / (len(null_aris) + 1))\n",
    "\n",
    "metrics = {\n",
    "    \"band\": BAND,\n",
    "    \"n_subjects\": len(files),\n",
    "    \"loso_median_ari\": float(loso_med_ari),\n",
    "    \"null_ari_mean\": float(null_aris.mean()) if len(null_aris) else None,\n",
    "    \"null_ari_p_value\": p_val,\n",
    "    \"subjects\": [os.path.splitext(os.path.basename(f))[0] for f in files],\n",
    "    \"out_dir\": OUT_ROOT\n",
    "}\n",
    "with open(os.path.join(OUT_MET, f\"band__{BAND}__metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "np.save(os.path.join(OUT_TAB, f\"band__{BAND}__consensus_labels.npy\"), cons)\n",
    "\n",
    "print(\"\\n=== FAST PASS RESULTS ===\")\n",
    "print(f\"band={BAND}  n={len(files)}\")\n",
    "print(f\"LOSO-median ARI: {loso_med_ari:.3f}\")\n",
    "print(f\"Null ARI mean  : {float(null_aris.mean()):.3f}\")\n",
    "print(f\"Permutation p  : {p_val:.4f}\")\n",
    "print(\"\\nArtifacts:\")\n",
    "print(\" -\", os.path.join(OUT_MET, f\"band__{BAND}__metrics.json\"))\n",
    "print(\" -\", os.path.join(OUT_TAB, f\"band__{BAND}__consensus_labels.npy\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e57fd92-f2db-440b-b29c-79926cc75ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 EC subjects\n",
      "\n",
      "=== FAST+ROBUST RESULTS ===\n",
      "band=alpha  n=10\n",
      "LOSO-median ARI: 1.000\n",
      "Null ARI mean  : 0.960\n",
      "Permutation p  : 0.9608\n",
      "\n",
      "Artifacts:\n",
      " - C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_fastrobust\\metrics\\band__alpha__metrics.json\n",
      " - C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_fastrobust\\tables\\band__alpha__consensus_labels.npy\n",
      " - C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_fastrobust\\tables\\band__alpha__coassoc.npy\n",
      " - C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_fastrobust\\figures\\degree_hist.png\n",
      " - C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_fastrobust\\figures\\consensus__alpha__coassoc.png\n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# === CNT Spectral-Glyphs — FAST+ROBUST (single cell) ===\n",
    "# Inputs:  C:\\Users\\caleb\\CNT_Lab\\eeg_rest\\subject_XX_EC.npy (+ .channels.txt)\n",
    "# Outputs: C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_fastrobust\\{tables,metrics,figures}\n",
    "\n",
    "import os, glob, json, numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy.signal import welch, csd\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "DATA_DIR   = r\"C:\\Users\\caleb\\CNT_Lab\\eeg_rest\"\n",
    "GLOB       = \"subject_*_EC.npy\"\n",
    "OUT_ROOT   = r\"C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_fastrobust\"\n",
    "FS         = 250.0\n",
    "BAND       = \"alpha\"\n",
    "KMAX       = 3\n",
    "NULL_PERMS = 50           # raise to 500–1000 later\n",
    "NPERSEG    = 128\n",
    "NOVERLAP   = 64\n",
    "KNN_K      = 6            # k-NN graph sparsity\n",
    "CONS_THR   = 0.65         # stricter consensus threshold\n",
    "SAVE_FIGS  = True\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def ensure_dir(p): os.makedirs(p, exist_ok=True); return p\n",
    "def band_lims(): return {\"delta\":(1,4),\"theta\":(4,8),\"alpha\":(8,13),\"beta\":(13,30),\"gamma\":(30,45)}\n",
    "\n",
    "def imag_coh_matrix(X, fs, band):\n",
    "    n_ch, _ = X.shape\n",
    "    fmin, fmax = band_lims()[band]\n",
    "    freqs, Pxx0 = welch(X[0], fs=fs, nperseg=NPERSEG, noverlap=NOVERLAP)\n",
    "    Pxx = [Pxx0] + [welch(X[i], fs=fs, nperseg=NPERSEG, noverlap=NOVERLAP)[1] for i in range(1, n_ch)]\n",
    "    sel = (freqs >= fmin) & (freqs <= fmax)\n",
    "    W = np.zeros((n_ch, n_ch), float)\n",
    "    for i in range(n_ch):\n",
    "        for j in range(i+1, n_ch):\n",
    "            _, Pxy = csd(X[i], X[j], fs=fs, nperseg=NPERSEG, noverlap=NOVERLAP)\n",
    "            num = np.abs(np.imag(Pxy[sel])); den = np.sqrt(Pxx[i][sel]*Pxx[j][sel] + 1e-12)\n",
    "            W[i,j] = W[j,i] = float(np.nanmean(num/den))\n",
    "    np.fill_diagonal(W, 0.0)\n",
    "    return W\n",
    "\n",
    "def knn_sparsify(W, k=6):\n",
    "    W = W.copy(); n = W.shape[0]\n",
    "    for i in range(n):\n",
    "        idx = np.argsort(W[i])[::-1]; keep = idx[:k]\n",
    "        mask = np.ones(n, dtype=bool); mask[keep] = False\n",
    "        W[i, mask] = 0.0\n",
    "    W = np.maximum(W, W.T); np.fill_diagonal(W, 0.0)\n",
    "    return W\n",
    "\n",
    "def laplacian_sym(W):\n",
    "    d = W.sum(1); d = np.where(d <= 1e-12, 1.0, d)\n",
    "    Dmh = np.diag(1.0/np.sqrt(d))\n",
    "    return np.eye(W.shape[0]) - Dmh @ W @ Dmh\n",
    "\n",
    "def pick_k(L, kmax=3):\n",
    "    evals, _ = np.linalg.eigh(L)\n",
    "    if len(evals) < 3: return 2\n",
    "    gaps = [(k, evals[k+1]-evals[k]) for k in range(1, min(kmax+1, len(evals)-1))]\n",
    "    if not gaps: return 2\n",
    "    return max(2, min(max(gaps, key=lambda x: x[1])[0], kmax))\n",
    "\n",
    "def spec_cluster(W, k):\n",
    "    L = laplacian_sym(W)\n",
    "    evals, evecs = np.linalg.eigh(L)\n",
    "    U = evecs[:, 1:k+1]\n",
    "    U = U / (np.linalg.norm(U, axis=1, keepdims=True) + 1e-12)\n",
    "    return KMeans(n_clusters=k, n_init=50, random_state=42).fit_predict(U)\n",
    "\n",
    "def consensus_from_labels(all_labels, thr=0.65):\n",
    "    n = len(all_labels[0]); m = len(all_labels); co = np.zeros((n, n), float)\n",
    "    for lab in all_labels:\n",
    "        for i in range(n):\n",
    "            li = lab[i]\n",
    "            for j in range(n):\n",
    "                co[i,j] += 1.0 if li == lab[j] else 0.0\n",
    "    co /= m\n",
    "    A = (co >= thr).astype(int)\n",
    "    visited = np.zeros(n, bool); cons = -1*np.ones(n, int); cid = 0\n",
    "    for i in range(n):\n",
    "        if not visited[i]:\n",
    "            stack=[i]; visited[i]=True; cons[i]=cid\n",
    "            while stack:\n",
    "                u=stack.pop()\n",
    "                for v in range(n):\n",
    "                    if A[u,v] and not visited[v]:\n",
    "                        visited[v]=True; cons[v]=cid; stack.append(v)\n",
    "            cid+=1\n",
    "    return cons, co\n",
    "\n",
    "def ari_loso(all_labels, thr=0.65):\n",
    "    cons,_ = consensus_from_labels(all_labels, thr=thr)\n",
    "    vals=[]\n",
    "    for s in range(len(all_labels)):\n",
    "        leave=[lab for i,lab in enumerate(all_labels) if i!=s]\n",
    "        cons_leave,_ = consensus_from_labels(leave, thr=thr)\n",
    "        vals.append(adjusted_rand_score(cons, cons_leave))\n",
    "    return float(np.median(vals))\n",
    "\n",
    "def circ_shift(X, rng, max_shift=None):\n",
    "    n_ch, n_t = X.shape\n",
    "    if max_shift is None: max_shift = n_t-1\n",
    "    Y = np.zeros_like(X)\n",
    "    for c in range(n_ch):\n",
    "        s = int(rng.integers(0, max_shift+1)); Y[c] = np.roll(X[c], s)\n",
    "    return Y\n",
    "\n",
    "def phase_randomize(X, rng):\n",
    "    n_ch, n_t = X.shape\n",
    "    Y = np.zeros_like(X, dtype=float)\n",
    "    for c in range(n_ch):\n",
    "        F = np.fft.rfft(X[c])\n",
    "        phases = rng.uniform(0, 2*np.pi, size=F.shape)\n",
    "        phases[0] = 0.0\n",
    "        if n_t % 2 == 0: phases[-1] = 0.0\n",
    "        Y[c] = np.fft.irfft(np.abs(F)*np.exp(1j*phases), n=n_t)\n",
    "    return Y\n",
    "\n",
    "# ---------------- Run ----------------\n",
    "files = sorted(glob.glob(os.path.join(DATA_DIR, GLOB)))\n",
    "print(f\"Found {len(files)} EC subjects\")\n",
    "if not files:\n",
    "    raise SystemExit(\"No EC files found. Check DATA_DIR/GLOB or run the downloader first.\")\n",
    "\n",
    "OUT_MET = ensure_dir(os.path.join(OUT_ROOT, \"metrics\"))\n",
    "OUT_TAB = ensure_dir(os.path.join(OUT_ROOT, \"tables\"))\n",
    "OUT_FIG = ensure_dir(os.path.join(OUT_ROOT, \"figures\"))\n",
    "\n",
    "rng = np.random.default_rng(7)\n",
    "subj_labels = []; degrees = []\n",
    "\n",
    "for f in files:\n",
    "    X = np.load(f)\n",
    "    W = imag_coh_matrix(X, fs=FS, band=BAND)\n",
    "    W = knn_sparsify(W, k=KNN_K)\n",
    "    degrees.append(W.sum(1))\n",
    "    k = pick_k(laplacian_sym(W), kmax=KMAX)\n",
    "    subj_labels.append(spec_cluster(W, k))\n",
    "\n",
    "cons, co = consensus_from_labels(subj_labels, thr=CONS_THR)\n",
    "\n",
    "# Mixed null: half circular-shift, half phase-randomize\n",
    "null_aris = []\n",
    "for p in range(NULL_PERMS):\n",
    "    nlabs=[]\n",
    "    for f in files:\n",
    "        X = np.load(f)\n",
    "        Y = phase_randomize(X, rng) if (p % 2 == 0) else circ_shift(X, rng)\n",
    "        Wn = imag_coh_matrix(Y, fs=FS, band=BAND)\n",
    "        Wn = knn_sparsify(Wn, k=KNN_K)\n",
    "        kn = pick_k(laplacian_sym(Wn), kmax=KMAX)\n",
    "        nlabs.append(spec_cluster(Wn, kn))\n",
    "    cons_n,_ = consensus_from_labels(nlabs, thr=CONS_THR)\n",
    "    null_aris.append(adjusted_rand_score(cons, cons_n))\n",
    "null_aris = np.array(null_aris, float)\n",
    "\n",
    "loso_med_ari = ari_loso(subj_labels, thr=CONS_THR)\n",
    "p_val = float((np.sum(null_aris >= loso_med_ari) + 1) / (len(null_aris) + 1))\n",
    "\n",
    "metrics = {\n",
    "    \"band\": BAND,\n",
    "    \"n_subjects\": len(files),\n",
    "    \"loso_median_ari\": float(loso_med_ari),\n",
    "    \"null_ari_mean\": float(null_aris.mean()) if len(null_aris) else None,\n",
    "    \"null_ari_p_value\": p_val,\n",
    "    \"subjects\": [os.path.splitext(os.path.basename(f))[0] for f in files],\n",
    "    \"out_dir\": OUT_ROOT,\n",
    "    \"knn_k\": KNN_K,\n",
    "    \"consensus_threshold\": CONS_THR\n",
    "}\n",
    "with open(os.path.join(OUT_MET, f\"band__{BAND}__metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "np.save(os.path.join(OUT_TAB, f\"band__{BAND}__consensus_labels.npy\"), cons)\n",
    "np.save(os.path.join(OUT_TAB, f\"band__{BAND}__coassoc.npy\"), co)\n",
    "\n",
    "if SAVE_FIGS and len(degrees):\n",
    "    plt.figure(); all_deg = np.concatenate(degrees); plt.hist(all_deg, bins=20)\n",
    "    plt.title(\"Node degree distribution (KNN-sparsified iCoh graph)\")\n",
    "    plt.xlabel(\"Degree (sum of edge weights)\"); plt.ylabel(\"Count\")\n",
    "    plt.tight_layout(); plt.savefig(os.path.join(OUT_FIG, \"degree_hist.png\"), dpi=160); plt.close()\n",
    "\n",
    "    plt.figure(); plt.imshow(co, aspect='auto')\n",
    "    plt.title(f\"Consensus co-association | {BAND}\")\n",
    "    plt.colorbar(); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_FIG, f\"consensus__{BAND}__coassoc.png\"), dpi=160); plt.close()\n",
    "\n",
    "print(\"\\n=== FAST+ROBUST RESULTS ===\")\n",
    "print(f\"band={BAND}  n={len(files)}\")\n",
    "print(f\"LOSO-median ARI: {loso_med_ari:.3f}\")\n",
    "print(f\"Null ARI mean  : {float(null_aris.mean()):.3f}\")\n",
    "print(f\"Permutation p  : {p_val:.4f}\")\n",
    "print(\"\\nArtifacts:\")\n",
    "print(\" -\", os.path.join(OUT_MET, f\"band__{BAND}__metrics.json\"))\n",
    "print(\" -\", os.path.join(OUT_TAB, f\"band__{BAND}__consensus_labels.npy\"))\n",
    "print(\" -\", os.path.join(OUT_TAB, f\"band__{BAND}__coassoc.npy\"))\n",
    "print(\" -\", os.path.join(OUT_FIG, \"degree_hist.png\"))\n",
    "print(\" -\", os.path.join(OUT_FIG, f\"consensus__{BAND}__coassoc.png\"))\n",
    "print(\"\\nDone.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3277c847-2ae2-4a36-8034-08e961166788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_9968\\216819253.py:159: RuntimeWarning: Mean of empty slice.\n",
      "  return float(intra.mean()/ (inter.mean()+1e-12))\n",
      "C:\\Users\\caleb\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\_methods.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Summary ===\n",
      "    band  loso_median_ari  null_ari_mean   p_value  Q_modularity  \\\n",
      "0  alpha              1.0           0.93  0.930693  1.222835e-10   \n",
      "1  theta              1.0           0.94  0.940594  3.807186e-10   \n",
      "\n",
      "   coassoc_entropy  blockness  n_subjects  \n",
      "0       -89.279706        NaN          10  \n",
      "1       -94.380917        NaN          10  \n",
      "\n",
      "Artifacts:\n",
      " - C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_stats\\spectral_glyphs_summary.csv\n",
      " - C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_stats\\figures\\consensus__alpha__coassoc_ordered.png\n",
      " - C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_stats\\figures\\consensus__theta__coassoc_ordered.png\n",
      " - C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_stats\\tables\\band__alpha__consensus_labels.npy\n",
      " - C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_stats\\tables\\band__theta__consensus_labels.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_9968\\216819253.py:159: RuntimeWarning: Mean of empty slice.\n",
      "  return float(intra.mean()/ (inter.mean()+1e-12))\n",
      "C:\\Users\\caleb\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\_core\\_methods.py:144: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# === CNT Spectral-Glyphs: Modularity + Entropy + Band Sweep (single cell) ===\n",
    "# Inputs:  C:\\Users\\caleb\\CNT_Lab\\eeg_rest\\subject_XX_EC.npy (+ .channels.txt)\n",
    "# Outputs: C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_stats\\{metrics.csv, figs}\n",
    "\n",
    "import os, glob, json, re, numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy.signal import welch, csd\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "DATA_DIR   = r\"C:\\Users\\caleb\\CNT_Lab\\eeg_rest\"\n",
    "GLOB       = \"subject_*_EC.npy\"\n",
    "OUT_ROOT   = r\"C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_stats\"\n",
    "FS         = 250.0\n",
    "BANDS      = (\"alpha\",\"theta\")\n",
    "KMAX       = 3\n",
    "NULL_PERMS = 100           # bump to 500–1000 when stable\n",
    "NPERSEG    = 128\n",
    "NOVERLAP   = 64\n",
    "KNN_K      = 6\n",
    "CONS_THR   = 0.65\n",
    "SAVE_FIGS  = True\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def ensure_dir(p): os.makedirs(p, exist_ok=True); return p\n",
    "def band_lims(): return {\"delta\":(1,4),\"theta\":(4,8),\"alpha\":(8,13),\"beta\":(13,30),\"gamma\":(30,45)}\n",
    "\n",
    "def imag_coh_matrix(X, fs, band):\n",
    "    n_ch, _ = X.shape\n",
    "    fmin, fmax = band_lims()[band]\n",
    "    freqs, Pxx0 = welch(X[0], fs=fs, nperseg=NPERSEG, noverlap=NOVERLAP)\n",
    "    Pxx = [Pxx0] + [welch(X[i], fs=fs, nperseg=NPERSEG, noverlap=NOVERLAP)[1] for i in range(1, n_ch)]\n",
    "    sel = (freqs >= fmin) & (freqs <= fmax)\n",
    "    W = np.zeros((n_ch, n_ch), float)\n",
    "    for i in range(n_ch):\n",
    "        for j in range(i+1, n_ch):\n",
    "            _, Pxy = csd(X[i], X[j], fs=fs, nperseg=NPERSEG, noverlap=NOVERLAP)\n",
    "            num = np.abs(np.imag(Pxy[sel])); den = np.sqrt(Pxx[i][sel]*Pxx[j][sel] + 1e-12)\n",
    "            W[i,j] = W[j,i] = float(np.nanmean(num/den))\n",
    "    np.fill_diagonal(W, 0.0)\n",
    "    return W\n",
    "\n",
    "def knn_sparsify(W, k=6):\n",
    "    W = W.copy(); n = W.shape[0]\n",
    "    for i in range(n):\n",
    "        idx = np.argsort(W[i])[::-1]; keep = idx[:k]\n",
    "        mask = np.ones(n, dtype=bool); mask[keep] = False\n",
    "        W[i, mask] = 0.0\n",
    "    W = np.maximum(W, W.T); np.fill_diagonal(W, 0.0)\n",
    "    return W\n",
    "\n",
    "def laplacian_sym(W):\n",
    "    d = W.sum(1); d = np.where(d <= 1e-12, 1.0, d)\n",
    "    Dmh = np.diag(1.0/np.sqrt(d))\n",
    "    return np.eye(W.shape[0]) - Dmh @ W @ Dmh\n",
    "\n",
    "def pick_k(L, kmax=3):\n",
    "    evals, _ = np.linalg.eigh(L)\n",
    "    if len(evals) < 3: return 2\n",
    "    gaps = [(k, evals[k+1]-evals[k]) for k in range(1, min(kmax+1, len(evals)-1))]\n",
    "    if not gaps: return 2\n",
    "    return max(2, min(max(gaps, key=lambda x: x[1])[0], kmax))\n",
    "\n",
    "def spec_cluster(W, k):\n",
    "    L = laplacian_sym(W)\n",
    "    evals, evecs = np.linalg.eigh(L)\n",
    "    U = evecs[:, 1:k+1]\n",
    "    U = U / (np.linalg.norm(U, axis=1, keepdims=True) + 1e-12)\n",
    "    return KMeans(n_clusters=k, n_init=50, random_state=42).fit_predict(U)\n",
    "\n",
    "def consensus_from_labels(all_labels, thr=0.65):\n",
    "    n = len(all_labels[0]); m = len(all_labels); co = np.zeros((n, n), float)\n",
    "    for lab in all_labels:\n",
    "        for i in range(n):\n",
    "            li = lab[i]\n",
    "            for j in range(n):\n",
    "                co[i,j] += 1.0 if li == lab[j] else 0.0\n",
    "    co /= m\n",
    "    A = (co >= thr).astype(int)\n",
    "    visited = np.zeros(n, bool); cons = -1*np.ones(n, int); cid = 0\n",
    "    for i in range(n):\n",
    "        if not visited[i]:\n",
    "            stack=[i]; visited[i]=True; cons[i]=cid\n",
    "            while stack:\n",
    "                u=stack.pop()\n",
    "                for v in range(n):\n",
    "                    if A[u,v] and not visited[v]:\n",
    "                        visited[v]=True; cons[v]=cid; stack.append(v)\n",
    "            cid+=1\n",
    "    return cons, co\n",
    "\n",
    "def ari_loso(all_labels, thr=0.65):\n",
    "    cons,_ = consensus_from_labels(all_labels, thr=thr)\n",
    "    vals=[]\n",
    "    for s in range(len(all_labels)):\n",
    "        leave=[lab for i,lab in enumerate(all_labels) if i!=s]\n",
    "        cons_leave,_ = consensus_from_labels(leave, thr=thr)\n",
    "        vals.append(adjusted_rand_score(cons, cons_leave))\n",
    "    return float(np.median(vals))\n",
    "\n",
    "def circ_shift(X, rng, max_shift=None):\n",
    "    n_ch, n_t = X.shape\n",
    "    if max_shift is None: max_shift = n_t-1\n",
    "    Y = np.zeros_like(X)\n",
    "    for c in range(n_ch):\n",
    "        s = int(rng.integers(0, max_shift+1)); Y[c] = np.roll(X[c], s)\n",
    "    return Y\n",
    "\n",
    "def phase_randomize(X, rng):\n",
    "    n_ch, n_t = X.shape\n",
    "    Y = np.zeros_like(X, dtype=float)\n",
    "    for c in range(n_ch):\n",
    "        F = np.fft.rfft(X[c])\n",
    "        phases = rng.uniform(0, 2*np.pi, size=F.shape)\n",
    "        phases[0] = 0.0\n",
    "        if n_t % 2 == 0: phases[-1] = 0.0\n",
    "        Y[c] = np.fft.irfft(np.abs(F)*np.exp(1j*phases), n=n_t)\n",
    "    return Y\n",
    "\n",
    "# Simple 10–20 reordering for nicer blocks (best-effort using name prefixes)\n",
    "def order_by_region(ch_names):\n",
    "    # groups in rough anterior→posterior order\n",
    "    groups = [\"Fp\", \"AF\", \"F\", \"FC\", \"C\", \"CP\", \"P\", \"PO\", \"O\", \"T\", \"TP\"]\n",
    "    def key(name):\n",
    "        for i,g in enumerate(groups):\n",
    "            if re.match(rf\"^{g}\", name, re.IGNORECASE):\n",
    "                return (i, name)\n",
    "        return (len(groups)+1, name)\n",
    "    idx = sorted(range(len(ch_names)), key=lambda i: key(ch_names[i]))\n",
    "    return np.array(idx, int)\n",
    "\n",
    "# Modularity for weighted undirected graph given partition\n",
    "def modularity(W, labels):\n",
    "    W = W.copy()\n",
    "    m = W.sum()/2.0 + 1e-12\n",
    "    k = W.sum(1)\n",
    "    Q = 0.0\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[0]):\n",
    "            if labels[i] == labels[j]:\n",
    "                Q += W[i,j] - (k[i]*k[j])/(2*m)\n",
    "    return float(Q/(2*m))\n",
    "\n",
    "def coassoc_entropy(co):\n",
    "    # entropy of co-association entries (excluding diagonal)\n",
    "    n = co.shape[0]\n",
    "    mask = ~np.eye(n, dtype=bool)\n",
    "    x = co[mask].ravel()\n",
    "    # discretize into 50 bins\n",
    "    hist, _ = np.histogram(x, bins=50, range=(0,1), density=True)\n",
    "    hist = hist[hist>0]\n",
    "    return float(-(hist*np.log(hist)).sum())\n",
    "\n",
    "def blockness(co, labels):\n",
    "    # mean intra vs inter coassoc\n",
    "    labs = np.array(labels)\n",
    "    intra = co[labs[:,None]==labs[None,:]]\n",
    "    inter = co[labs[:,None]!=labs[None,:]]\n",
    "    return float(intra.mean()/ (inter.mean()+1e-12))\n",
    "\n",
    "# ---------------- Run ----------------\n",
    "files = sorted(glob.glob(os.path.join(DATA_DIR, GLOB)))\n",
    "if not files: raise SystemExit(\"No EC files found. Check DATA_DIR/GLOB or run downloader.\")\n",
    "\n",
    "OUT_MET = ensure_dir(os.path.join(OUT_ROOT, \"metrics\"))\n",
    "OUT_TAB = ensure_dir(os.path.join(OUT_ROOT, \"tables\"))\n",
    "OUT_FIG = ensure_dir(os.path.join(OUT_ROOT, \"figures\"))\n",
    "\n",
    "rng = np.random.default_rng(7)\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "for band in BANDS:\n",
    "    subj_labels = []\n",
    "    ch_template = None\n",
    "    Ws = []  # store per-subject graphs for averaging\n",
    "\n",
    "    for f in files:\n",
    "        X = np.load(f)\n",
    "        ch_txt = f.replace(\".npy\",\".channels.txt\")\n",
    "        if ch_template is None:\n",
    "            if os.path.exists(ch_txt):\n",
    "                with open(ch_txt,\"r\",encoding=\"utf-8\") as g:\n",
    "                    ch_template = [ln.strip() for ln in g if ln.strip()]\n",
    "            else:\n",
    "                ch_template = [f\"ch{i}\" for i in range(X.shape[0])]\n",
    "        W = imag_coh_matrix(X, fs=FS, band=band)\n",
    "        W = knn_sparsify(W, k=KNN_K)\n",
    "        k = pick_k(laplacian_sym(W), kmax=KMAX)\n",
    "        subj_labels.append(spec_cluster(W, k))\n",
    "        Ws.append(W)\n",
    "\n",
    "    # consensus\n",
    "    cons, co = consensus_from_labels(subj_labels, thr=CONS_THR)\n",
    "\n",
    "    # reorder by regions if names look standard\n",
    "    idx_order = order_by_region(ch_template)\n",
    "    co_ord = co[np.ix_(idx_order, idx_order)]\n",
    "\n",
    "    # mixed null\n",
    "    null_aris = []\n",
    "    for p in range(NULL_PERMS):\n",
    "        nlabs=[]\n",
    "        for f in files:\n",
    "            X = np.load(f)\n",
    "            Y = phase_randomize(X, rng) if (p % 2 == 0) else circ_shift(X, rng)\n",
    "            Wn = imag_coh_matrix(Y, fs=FS, band=band)\n",
    "            Wn = knn_sparsify(Wn, k=KNN_K)\n",
    "            kn = pick_k(laplacian_sym(Wn), kmax=KMAX)\n",
    "            nlabs.append(spec_cluster(Wn, kn))\n",
    "        cons_n,_ = consensus_from_labels(nlabs, thr=CONS_THR)\n",
    "        null_aris.append(adjusted_rand_score(cons, cons_n))\n",
    "    null_aris = np.array(null_aris, float)\n",
    "\n",
    "    # stats\n",
    "    loso = float(ari_loso(subj_labels, thr=CONS_THR))\n",
    "    pval = float((np.sum(null_aris >= loso) + 1) / (len(null_aris) + 1))\n",
    "\n",
    "    # modularity of consensus on mean graph\n",
    "    W_mean = np.mean(Ws, axis=0)\n",
    "    Q = modularity(W_mean, cons)\n",
    "\n",
    "    # entropy & blockness\n",
    "    H = coassoc_entropy(co)\n",
    "    B = blockness(co, cons)\n",
    "\n",
    "    # save\n",
    "    np.save(os.path.join(OUT_TAB, f\"band__{band}__consensus_labels.npy\"), cons)\n",
    "    np.save(os.path.join(OUT_TAB, f\"band__{band}__coassoc.npy\"), co)\n",
    "    with open(os.path.join(OUT_MET, f\"band__{band}__metrics.json\"),\"w\",encoding=\"utf-8\") as f:\n",
    "        json.dump({\"band\":band,\"loso_median_ari\":loso,\"null_ari_mean\":float(null_aris.mean()),\n",
    "                   \"p_value\":pval,\"Q_modularity\":Q,\"coassoc_entropy\":H,\"blockness\":B,\n",
    "                   \"n_subjects\":len(files),\"knn_k\":KNN_K,\"consensus_threshold\":CONS_THR}, f, indent=2)\n",
    "\n",
    "    if SAVE_FIGS:\n",
    "        # co-association heatmap (ordered)\n",
    "        plt.figure()\n",
    "        plt.imshow(co_ord, aspect='auto')\n",
    "        plt.title(f\"Consensus co-association (ordered) | {band}\")\n",
    "        plt.colorbar()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUT_FIG, f\"consensus__{band}__coassoc_ordered.png\"), dpi=160); plt.close()\n",
    "\n",
    "    summary_rows.append([band, loso, float(null_aris.mean()), pval, Q, H, B, len(files)])\n",
    "\n",
    "# write summary CSV\n",
    "import pandas as pd\n",
    "cols = [\"band\",\"loso_median_ari\",\"null_ari_mean\",\"p_value\",\"Q_modularity\",\"coassoc_entropy\",\"blockness\",\"n_subjects\"]\n",
    "df = pd.DataFrame(summary_rows, columns=cols)\n",
    "csv_path = os.path.join(OUT_ROOT, \"spectral_glyphs_summary.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"=== Summary ===\")\n",
    "print(df)\n",
    "print(\"\\nArtifacts:\")\n",
    "print(\" -\", csv_path)\n",
    "print(\" -\", os.path.join(OUT_FIG, \"consensus__alpha__coassoc_ordered.png\"))\n",
    "print(\" -\", os.path.join(OUT_FIG, \"consensus__theta__coassoc_ordered.png\"))\n",
    "print(\" -\", os.path.join(OUT_TAB, \"band__alpha__consensus_labels.npy\"))\n",
    "print(\" -\", os.path.join(OUT_TAB, \"band__theta__consensus_labels.npy\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "292b792a-9592-42ec-9b32-f6dcdf802820",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 190\u001b[39m\n\u001b[32m    188\u001b[39m X = np.load(f)\n\u001b[32m    189\u001b[39m Y = phase_randomize(X, rng) \u001b[38;5;28;01mif\u001b[39;00m (p % \u001b[32m2\u001b[39m == \u001b[32m0\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m circ_shift(X, rng)\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m Wn = \u001b[43mimag_coh_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mband\u001b[49m\u001b[43m=\u001b[49m\u001b[43mBAND\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m Wn = knn_sparsify(Wn, k=knn_k)\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33meigengap\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mimag_coh_matrix\u001b[39m\u001b[34m(X, fs, band)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_ch):\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i+\u001b[32m1\u001b[39m, n_ch):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m         _, Pxy = \u001b[43mcsd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnperseg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNPERSEG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoverlap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNOVERLAP\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m         num = np.abs(np.imag(Pxy[sel])); den = np.sqrt(Pxx[i][sel]*Pxx[j][sel] + \u001b[32m1e-12\u001b[39m)\n\u001b[32m     43\u001b[39m         W[i,j] = W[j,i] = \u001b[38;5;28mfloat\u001b[39m(np.nanmean(num/den))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\signal\\_spectral_py.py:917\u001b[39m, in \u001b[36mcsd\u001b[39m\u001b[34m(x, y, fs, window, nperseg, noverlap, nfft, detrend, return_onesided, scaling, axis, average)\u001b[39m\n\u001b[32m    914\u001b[39m SFT = ShortTimeFFT(win, nperseg - noverlap, fs, fft_mode=fft_mode, mfft=nfft,\n\u001b[32m    915\u001b[39m                    scale_to=scales[scaling], phase_shift=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    916\u001b[39m \u001b[38;5;66;03m# csd() calculates X.conj()*Y instead of X*Y.conj():\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m917\u001b[39m Pxy = \u001b[43mSFT\u001b[49m\u001b[43m.\u001b[49m\u001b[43mspectrogram\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetr\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdetrend\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdetrend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mp0\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp1\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoverlap\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[43mSFT\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnperseg\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m                      \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[38;5;66;03m# Note:\u001b[39;00m\n\u001b[32m    922\u001b[39m \u001b[38;5;66;03m# 'onesided2X' scaling of ShortTimeFFT conflicts with the\u001b[39;00m\n\u001b[32m    923\u001b[39m \u001b[38;5;66;03m# scaling='spectrum' parameter, since it doubles the squared magnitude,\u001b[39;00m\n\u001b[32m    924\u001b[39m \u001b[38;5;66;03m# which in the view of the ShortTimeFFT implementation does not make sense.\u001b[39;00m\n\u001b[32m    925\u001b[39m \u001b[38;5;66;03m# Hence, the doubling of the square is implemented here:\u001b[39;00m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_onesided:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\signal\\_short_time_fft.py:1391\u001b[39m, in \u001b[36mShortTimeFFT.spectrogram\u001b[39m\u001b[34m(self, x, y, detr, p0, p1, k_offset, padding, axis)\u001b[39m\n\u001b[32m   1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Sx.real**\u001b[32m2\u001b[39m + Sx.imag**\u001b[32m2\u001b[39m\n\u001b[32m   1390\u001b[39m \u001b[38;5;66;03m# Cross-spectrogram:\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1391\u001b[39m Sy = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstft_detrend\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1392\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1393\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Sx * Sy.conj()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\signal\\_short_time_fft.py:1245\u001b[39m, in \u001b[36mShortTimeFFT.stft_detrend\u001b[39m\u001b[34m(self, x, detr, p0, p1, k_offset, padding, axis)\u001b[39m\n\u001b[32m   1243\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m detr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1244\u001b[39m         x_ = detr(x_)\n\u001b[32m-> \u001b[39m\u001b[32m1245\u001b[39m     S[..., :, p_] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fft_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m x.ndim > \u001b[32m1\u001b[39m:\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.moveaxis(S, -\u001b[32m2\u001b[39m, axis \u001b[38;5;28;01mif\u001b[39;00m axis >= \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m axis-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\signal\\_short_time_fft.py:2105\u001b[39m, in \u001b[36mShortTimeFFT._fft_func\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m   2103\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fft_lib.fftshift(fft_lib.fft(x, \u001b[38;5;28mself\u001b[39m.mfft, axis=-\u001b[32m1\u001b[39m), axes=-\u001b[32m1\u001b[39m)\n\u001b[32m   2104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fft_mode == \u001b[33m'\u001b[39m\u001b[33monesided\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfft_lib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrfft\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmfft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fft_mode == \u001b[33m'\u001b[39m\u001b[33monesided2X\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m   2107\u001b[39m     X = fft_lib.rfft(x, n=\u001b[38;5;28mself\u001b[39m.mfft, axis=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\fft\\_backend.py:28\u001b[39m, in \u001b[36m_ScipyBackend.__ua_function__\u001b[39m\u001b[34m(method, args, kwargs)\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     27\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\fft\\_basic_backend.py:91\u001b[39m, in \u001b[36mrfft\u001b[39m\u001b[34m(x, n, axis, norm, overwrite_x, workers, plan)\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrfft\u001b[39m(x, n=\u001b[38;5;28;01mNone\u001b[39;00m, axis=-\u001b[32m1\u001b[39m, norm=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     90\u001b[39m          overwrite_x=\u001b[38;5;28;01mFalse\u001b[39;00m, workers=\u001b[38;5;28;01mNone\u001b[39;00m, *, plan=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_execute_1D\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrfft\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pocketfft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrfft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m                       \u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\fft\\_basic_backend.py:32\u001b[39m, in \u001b[36m_execute_1D\u001b[39m\u001b[34m(func_str, pocketfft_func, x, n, axis, norm, overwrite_x, workers, plan)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_numpy(xp):\n\u001b[32m     31\u001b[39m     x = np.asarray(x)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpocketfft_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m                          \u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mplan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m norm = _validate_fft_args(workers, plan, norm)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(xp, \u001b[33m'\u001b[39m\u001b[33mfft\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\scipy\\fft\\_pocketfft\\basic.py:61\u001b[39m, in \u001b[36mr2c\u001b[39m\u001b[34m(forward, x, n, axis, norm, overwrite_x, workers, plan)\u001b[39m\n\u001b[32m     58\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minvalid number of data points (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtmp.shape[axis]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) specified\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m \u001b[38;5;66;03m# Note: overwrite_x is not utilised\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpfft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mr2c\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === CNT Spectral-Glyphs: Parameter Sweep (single cell) ===\n",
    "# Searches KNN_K × CONS_THR × k-mode to avoid single-cluster consensus\n",
    "# Inputs:  C:\\Users\\caleb\\CNT_Lab\\eeg_rest\\subject_XX_EC.npy (+ .channels.txt)\n",
    "# Outputs: C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_sweep\\summary.csv + quick figs\n",
    "\n",
    "import os, glob, json, re, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy.signal import welch, csd\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "DATA_DIR   = r\"C:\\Users\\caleb\\CNT_Lab\\eeg_rest\"\n",
    "GLOB       = \"subject_*_EC.npy\"\n",
    "OUT_ROOT   = r\"C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_sweep\"\n",
    "\n",
    "BAND       = \"alpha\"        # start with alpha; add 'theta' after a good combo\n",
    "FS         = 250.0\n",
    "NPERSEG    = 128\n",
    "NOVERLAP   = 64\n",
    "NULL_PERMS = 100            # start 100; raise to 500–1000 when locked in\n",
    "KMAX       = 3              # eigengap cap\n",
    "\n",
    "# Grid: try a few; extend if needed\n",
    "KNN_LIST   = [4, 5, 6, 7]\n",
    "THR_LIST   = [0.60, 0.65, 0.70]\n",
    "K_MODE     = [\"eigengap\", \"k=2\", \"k=3\"]   # test both fixed-k and eigengap\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def ensure_dir(p): os.makedirs(p, exist_ok=True); return p\n",
    "def band_lims(): return {\"delta\":(1,4),\"theta\":(4,8),\"alpha\":(8,13),\"beta\":(13,30),\"gamma\":(30,45)}\n",
    "\n",
    "def imag_coh_matrix(X, fs, band):\n",
    "    n_ch, _ = X.shape\n",
    "    fmin, fmax = band_lims()[band]\n",
    "    freqs, Pxx0 = welch(X[0], fs=fs, nperseg=NPERSEG, noverlap=NOVERLAP)\n",
    "    Pxx = [Pxx0] + [welch(X[i], fs=fs, nperseg=NPERSEG, noverlap=NOVERLAP)[1] for i in range(1, n_ch)]\n",
    "    sel = (freqs >= fmin) & (freqs <= fmax)\n",
    "    W = np.zeros((n_ch, n_ch), float)\n",
    "    for i in range(n_ch):\n",
    "        for j in range(i+1, n_ch):\n",
    "            _, Pxy = csd(X[i], X[j], fs=fs, nperseg=NPERSEG, noverlap=NOVERLAP)\n",
    "            num = np.abs(np.imag(Pxy[sel])); den = np.sqrt(Pxx[i][sel]*Pxx[j][sel] + 1e-12)\n",
    "            W[i,j] = W[j,i] = float(np.nanmean(num/den))\n",
    "    np.fill_diagonal(W, 0.0)\n",
    "    return W\n",
    "\n",
    "def knn_sparsify(W, k=6):\n",
    "    W = W.copy(); n = W.shape[0]\n",
    "    for i in range(n):\n",
    "        idx = np.argsort(W[i])[::-1]; keep = idx[:k]\n",
    "        mask = np.ones(n, dtype=bool); mask[keep] = False\n",
    "        W[i, mask] = 0.0\n",
    "    W = np.maximum(W, W.T); np.fill_diagonal(W, 0.0)\n",
    "    return W\n",
    "\n",
    "def laplacian_sym(W):\n",
    "    d = W.sum(1); d = np.where(d <= 1e-12, 1.0, d)\n",
    "    Dmh = np.diag(1.0/np.sqrt(d))\n",
    "    return np.eye(W.shape[0]) - Dmh @ W @ Dmh\n",
    "\n",
    "def pick_k(L, kmax=3):\n",
    "    evals, _ = np.linalg.eigh(L)\n",
    "    if len(evals) < 3: return 2\n",
    "    gaps = [(k, evals[k+1]-evals[k]) for k in range(1, min(kmax+1, len(evals)-1))]\n",
    "    if not gaps: return 2\n",
    "    return max(2, min(max(gaps, key=lambda x: x[1])[0], kmax))\n",
    "\n",
    "def spec_cluster(W, k):\n",
    "    L = laplacian_sym(W)\n",
    "    evals, evecs = np.linalg.eigh(L)\n",
    "    U = evecs[:, 1:k+1]\n",
    "    U = U / (np.linalg.norm(U, axis=1, keepdims=True) + 1e-12)\n",
    "    return KMeans(n_clusters=k, n_init=50, random_state=42).fit_predict(U)\n",
    "\n",
    "def consensus_from_labels(all_labels, thr=0.65):\n",
    "    n = len(all_labels[0]); m = len(all_labels); co = np.zeros((n, n), float)\n",
    "    for lab in all_labels:\n",
    "        for i in range(n):\n",
    "            li = lab[i]\n",
    "            for j in range(n):\n",
    "                co[i,j] += 1.0 if li == lab[j] else 0.0\n",
    "    co /= m\n",
    "    A = (co >= thr).astype(int)\n",
    "    visited = np.zeros(n, bool); cons = -1*np.ones(n, int); cid = 0\n",
    "    for i in range(n):\n",
    "        if not visited[i]:\n",
    "            stack=[i]; visited[i]=True; cons[i]=cid\n",
    "            while stack:\n",
    "                u=stack.pop()\n",
    "                for v in range(n):\n",
    "                    if A[u,v] and not visited[v]:\n",
    "                        visited[v]=True; cons[v]=cid; stack.append(v)\n",
    "            cid+=1\n",
    "    return cons, co\n",
    "\n",
    "def ari_loso(all_labels, thr=0.65):\n",
    "    cons,_ = consensus_from_labels(all_labels, thr=thr)\n",
    "    vals=[]\n",
    "    for s in range(len(all_labels)):\n",
    "        leave=[lab for i,lab in enumerate(all_labels) if i!=s]\n",
    "        cons_leave,_ = consensus_from_labels(leave, thr=thr)\n",
    "        vals.append(adjusted_rand_score(cons, cons_leave))\n",
    "    return float(np.median(vals))\n",
    "\n",
    "def circ_shift(X, rng, max_shift=None):\n",
    "    n_ch, n_t = X.shape\n",
    "    if max_shift is None: max_shift = n_t-1\n",
    "    Y = np.zeros_like(X)\n",
    "    for c in range(n_ch):\n",
    "        s = int(rng.integers(0, max_shift+1)); Y[c] = np.roll(X[c], s)\n",
    "    return Y\n",
    "\n",
    "def phase_randomize(X, rng):\n",
    "    n_ch, n_t = X.shape\n",
    "    Y = np.zeros_like(X, dtype=float)\n",
    "    for c in range(n_ch):\n",
    "        F = np.fft.rfft(X[c])\n",
    "        phases = rng.uniform(0, 2*np.pi, size=F.shape)\n",
    "        phases[0] = 0.0\n",
    "        if n_t % 2 == 0: phases[-1] = 0.0\n",
    "        Y[c] = np.fft.irfft(np.abs(F)*np.exp(1j*phases), n=n_t)\n",
    "    return Y\n",
    "\n",
    "def modularity(W, labels):\n",
    "    W = W.copy()\n",
    "    m = W.sum()/2.0 + 1e-12\n",
    "    k = W.sum(1)\n",
    "    Q = 0.0\n",
    "    for i in range(W.shape[0]):\n",
    "        for j in range(W.shape[0]):\n",
    "            if labels[i] == labels[j]:\n",
    "                Q += W[i,j] - (k[i]*k[j])/(2*m)\n",
    "    return float(Q/(2*m))\n",
    "\n",
    "def coassoc_entropy(co):\n",
    "    n = co.shape[0]\n",
    "    x = co[~np.eye(n, dtype=bool)].ravel()\n",
    "    hist, _ = np.histogram(x, bins=50, range=(0,1), density=True)\n",
    "    hist = hist[hist>0]\n",
    "    return float(-(hist*np.log(hist)).sum()) if len(hist) else np.nan\n",
    "\n",
    "def blockness_safe(co, labels):\n",
    "    labs = np.array(labels)\n",
    "    if len(np.unique(labs)) < 2:  # single cluster\n",
    "        return np.nan\n",
    "    intra = co[labs[:,None]==labs[None,:]]\n",
    "    inter = co[labs[:,None]!=labs[None,:]]\n",
    "    return float(intra.mean() / (inter.mean()+1e-12))\n",
    "\n",
    "# ---------------- Load data ----------------\n",
    "files = sorted(glob.glob(os.path.join(DATA_DIR, GLOB)))\n",
    "if not files: raise SystemExit(\"No EC files found. Check DATA_DIR/GLOB.\")\n",
    "\n",
    "OUT = ensure_dir(OUT_ROOT)\n",
    "FIG = ensure_dir(os.path.join(OUT, \"figures\"))\n",
    "\n",
    "# ---------------- Sweep ----------------\n",
    "rng = np.random.default_rng(7)\n",
    "rows = []\n",
    "\n",
    "for knn_k in KNN_LIST:\n",
    "    for thr in THR_LIST:\n",
    "        for mode in K_MODE:\n",
    "            # per-subject clustering\n",
    "            subj_labels = []\n",
    "            Ws = []\n",
    "            for f in files:\n",
    "                X = np.load(f)\n",
    "                W = imag_coh_matrix(X, fs=FS, band=BAND)\n",
    "                W = knn_sparsify(W, k=knn_k)\n",
    "                Ws.append(W)\n",
    "                if mode == \"eigengap\":\n",
    "                    k = pick_k(laplacian_sym(W), kmax=KMAX)\n",
    "                elif mode == \"k=2\":\n",
    "                    k = 2\n",
    "                else:\n",
    "                    k = 3\n",
    "                subj_labels.append(spec_cluster(W, k))\n",
    "\n",
    "            cons, co = consensus_from_labels(subj_labels, thr=thr)\n",
    "            n_cons_clusters = int(len(np.unique(cons)))\n",
    "\n",
    "            # null (mixed)\n",
    "            null_aris = []\n",
    "            for p in range(NULL_PERMS):\n",
    "                nlabs=[]\n",
    "                for f in files:\n",
    "                    X = np.load(f)\n",
    "                    Y = phase_randomize(X, rng) if (p % 2 == 0) else circ_shift(X, rng)\n",
    "                    Wn = imag_coh_matrix(Y, fs=FS, band=BAND)\n",
    "                    Wn = knn_sparsify(Wn, k=knn_k)\n",
    "                    if mode == \"eigengap\":\n",
    "                        kn = pick_k(laplacian_sym(Wn), kmax=KMAX)\n",
    "                    elif mode == \"k=2\":\n",
    "                        kn = 2\n",
    "                    else:\n",
    "                        kn = 3\n",
    "                    nlabs.append(spec_cluster(Wn, kn))\n",
    "                cons_n,_ = consensus_from_labels(nlabs, thr=thr)\n",
    "                null_aris.append(adjusted_rand_score(cons, cons_n))\n",
    "            null_aris = np.array(null_aris, float)\n",
    "\n",
    "            loso = float(ari_loso(subj_labels, thr=thr))\n",
    "            pval = float((np.sum(null_aris >= loso) + 1) / (len(null_aris) + 1))\n",
    "\n",
    "            W_mean = np.mean(Ws, axis=0)\n",
    "            Q = modularity(W_mean, cons)\n",
    "            H = coassoc_entropy(co)\n",
    "            B = blockness_safe(co, cons)\n",
    "\n",
    "            rows.append({\n",
    "                \"band\": BAND, \"mode\": mode, \"knn_k\": knn_k, \"cons_thr\": thr,\n",
    "                \"n_subjects\": len(files), \"n_consensus_clusters\": n_cons_clusters,\n",
    "                \"loso_median_ari\": loso, \"null_ari_mean\": float(null_aris.mean()),\n",
    "                \"p_value\": pval, \"Q_modularity\": Q, \"coassoc_entropy\": H, \"blockness\": B\n",
    "            })\n",
    "\n",
    "# ---------------- Save & show ----------------\n",
    "df = pd.DataFrame(rows).sort_values([\"p_value\",\"n_consensus_clusters\"], ascending=[True, False]).reset_index(drop=True)\n",
    "csv_path = os.path.join(OUT, \"summary.csv\")\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"Top 10 configurations (lower p is better):\")\n",
    "print(df.head(10).to_string(index=False))\n",
    "print(\"\\nSaved:\", csv_path)\n",
    "\n",
    "# Quick plot: p-value by config rank\n",
    "plt.figure()\n",
    "plt.plot(np.arange(len(df)), df[\"p_value\"].values, marker=\"o\")\n",
    "plt.xlabel(\"Config rank (sorted by p-value)\")\n",
    "plt.ylabel(\"Permutation p-value\")\n",
    "plt.title(f\"Sweep results | {BAND}\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(FIG, f\"sweep_{BAND}_pvalues.png\"), dpi=160); plt.close()\n",
    "\n",
    "print(\"Figure:\", os.path.join(FIG, f\"sweep_{BAND}_pvalues.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d35e5ed-e60e-4cfb-b714-4b2d76ee0ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] subjects=6 → ['C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\eeg_rest\\\\subject_01_EC.npy', 'C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\eeg_rest\\\\subject_02_EC.npy', 'C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\eeg_rest\\\\subject_03_EC.npy'] …\n",
      "[real] built 2/6 graphs\n",
      "[real] built 4/6 graphs\n",
      "[real] built 6/6 graphs\n",
      "[null] 10/50 perms; elapsed=94s\n",
      "[null] 20/50 perms; elapsed=101s\n",
      "[null] 30/50 perms; elapsed=107s\n",
      "[null] 40/50 perms; elapsed=114s\n",
      "[null] 50/50 perms; elapsed=121s\n",
      "\n",
      "=== FAST DIAG RESULTS ===\n",
      "{'band': 'alpha', 'n_subjects': 6, 'k_fixed': 2, 'consensus_thr': 0.65, 'loso_median_ari': 1.0, 'null_ari_mean': 1.0, 'p_value': 1.0, 'perms_done': 50, 'elapsed_s': 121}\n",
      "Artifacts:\n",
      " - C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_fastdiag\\tables\\band__alpha__consensus_labels.npy\n",
      " - C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_fastdiag\\tables\\band__alpha__coassoc.npy\n",
      " - C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_fastdiag\\metrics\\band__alpha__metrics.json\n",
      " - C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_fastdiag\\figures\\consensus__alpha__coassoc.png\n"
     ]
    }
   ],
   "source": [
    "# === CNT Spectral-Glyphs: FAST DIAG (single cell) ===\n",
    "# Goal: quick, decisive alpha check without hours of CSD recompute.\n",
    "# Strategy: build real iCoh once per subject; null = fast graph-shuffle; fixed k=2; clear progress prints.\n",
    "\n",
    "import os, glob, json, time, numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy.signal import welch, csd\n",
    "\n",
    "# ---------- config (tweak here) ----------\n",
    "DATA_DIR   = r\"C:\\Users\\caleb\\CNT_Lab\\eeg_rest\"\n",
    "GLOB       = \"subject_*_EC.npy\"\n",
    "OUT_ROOT   = r\"C:\\Users\\caleb\\CNT_Lab\\artifacts\\spectral_glyphs_fastdiag\"\n",
    "FS         = 250.0\n",
    "BAND       = \"alpha\"\n",
    "NPERSEG    = 128\n",
    "NOVERLAP   = 64\n",
    "\n",
    "SUBJ_LIMIT = 6      # start with 6; raise after you see results\n",
    "K_FIXED    = 2      # confirmatory k=2 to avoid single-cluster blob\n",
    "NULL_PERMS = 50     # 50 fast perms (graph shuffle)\n",
    "TIME_CAP_S = 600    # stop after 10 minutes (adjust as needed)\n",
    "CONS_THR   = 0.65   # stricter consensus threshold\n",
    "SAVE_FIGS  = True\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def ensure_dir(p): os.makedirs(p, exist_ok=True); return p\n",
    "def band_lims(): return {\"alpha\":(8,13),\"theta\":(4,8),\"beta\":(13,30),\"delta\":(1,4),\"gamma\":(30,45)}\n",
    "\n",
    "def imag_coh_matrix(X, fs, band):\n",
    "    n_ch, _ = X.shape\n",
    "    fmin, fmax = band_lims()[band]\n",
    "    freqs, Pxx0 = welch(X[0], fs=fs, nperseg=NPERSEG, noverlap=NOVERLAP)\n",
    "    Pxx = [Pxx0] + [welch(X[i], fs=fs, nperseg=NPERSEG, noverlap=NOVERLAP)[1] for i in range(1,n_ch)]\n",
    "    sel = (freqs >= fmin) & (freqs <= fmax)\n",
    "    W = np.zeros((n_ch,n_ch), float)\n",
    "    for i in range(n_ch):\n",
    "        for j in range(i+1, n_ch):\n",
    "            _, Pxy = csd(X[i], X[j], fs=fs, nperseg=NPERSEG, noverlap=NOVERLAP)\n",
    "            num = np.abs(np.imag(Pxy[sel])); den = np.sqrt(Pxx[i][sel]*Pxx[j][sel] + 1e-12)\n",
    "            W[i,j] = W[j,i] = float(np.nanmean(num/den))\n",
    "    np.fill_diagonal(W, 0.0)\n",
    "    return W\n",
    "\n",
    "def knn_sparsify(W, k=6):\n",
    "    W = W.copy(); n = W.shape[0]\n",
    "    for i in range(n):\n",
    "        idx = np.argsort(W[i])[::-1]; keep = idx[:k]\n",
    "        mask = np.ones(n, dtype=bool); mask[keep] = False\n",
    "        W[i, mask] = 0.0\n",
    "    W = np.maximum(W, W.T); np.fill_diagonal(W, 0.0)\n",
    "    return W\n",
    "\n",
    "def laplacian_sym(W):\n",
    "    d = W.sum(1); d = np.where(d<=1e-12, 1.0, d)\n",
    "    Dmh = np.diag(1.0/np.sqrt(d))\n",
    "    return np.eye(W.shape[0]) - Dmh @ W @ Dmh\n",
    "\n",
    "def spec_kmeans_on_laplacian(W, k):\n",
    "    L = laplacian_sym(W)\n",
    "    evals, evecs = np.linalg.eigh(L)\n",
    "    U = evecs[:, 1:k] if k>1 else evecs[:, :1]\n",
    "    U = U / (np.linalg.norm(U, axis=1, keepdims=True) + 1e-12)\n",
    "    return KMeans(n_clusters=k, n_init=50, random_state=42).fit_predict(U)\n",
    "\n",
    "def consensus_from_labels(all_labels, thr=0.65):\n",
    "    n = len(all_labels[0]); m = len(all_labels); co = np.zeros((n,n), float)\n",
    "    for lab in all_labels:\n",
    "        for i in range(n):\n",
    "            li = lab[i]\n",
    "            for j in range(n):\n",
    "                co[i,j] += 1.0 if li == lab[j] else 0.0\n",
    "    co /= m\n",
    "    A = (co >= thr).astype(int)\n",
    "    visited = np.zeros(n, bool); cons = -1*np.ones(n, int); cid=0\n",
    "    for i in range(n):\n",
    "        if not visited[i]:\n",
    "            stack=[i]; visited[i]=True; cons[i]=cid\n",
    "            while stack:\n",
    "                u=stack.pop()\n",
    "                for v in range(n):\n",
    "                    if A[u,v] and not visited[v]:\n",
    "                        visited[v]=True; cons[v]=cid; stack.append(v)\n",
    "            cid+=1\n",
    "    return cons, co\n",
    "\n",
    "def ari_loso(all_labels, thr=0.65):\n",
    "    cons,_ = consensus_from_labels(all_labels, thr=thr)\n",
    "    vals=[]\n",
    "    for s in range(len(all_labels)):\n",
    "        leave=[lab for i,lab in enumerate(all_labels) if i!=s]\n",
    "        cons_leave,_ = consensus_from_labels(leave, thr=thr)\n",
    "        vals.append(adjusted_rand_score(cons, cons_leave))\n",
    "    return float(np.median(vals))\n",
    "\n",
    "# super-fast null: degree-preserving row shuffles (keeps each node’s weight, breaks block structure)\n",
    "def graph_shuffle_null(W, rng):\n",
    "    n = W.shape[0]\n",
    "    Wn = np.zeros_like(W)\n",
    "    for i in range(n):\n",
    "        row = W[i].copy()\n",
    "        idx = np.arange(n); rng.shuffle(idx)\n",
    "        row = row[idx]\n",
    "        row[i] = 0.0\n",
    "        Wn[i] = row\n",
    "    Wn = 0.5*(Wn + Wn.T)  # symmetrize\n",
    "    np.fill_diagonal(Wn, 0.0)\n",
    "    return Wn\n",
    "\n",
    "# ---------- run ----------\n",
    "t0 = time.time()\n",
    "files = sorted(glob.glob(os.path.join(DATA_DIR, GLOB)))[:SUBJ_LIMIT]\n",
    "print(f\"[info] subjects={len(files)} → {files[:3]}{' …' if len(files)>3 else ''}\")\n",
    "\n",
    "OUT_MET = ensure_dir(os.path.join(OUT_ROOT, \"metrics\"))\n",
    "OUT_TAB = ensure_dir(os.path.join(OUT_ROOT, \"tables\"))\n",
    "OUT_FIG = ensure_dir(os.path.join(OUT_ROOT, \"figures\"))\n",
    "\n",
    "# 1) build real graphs once\n",
    "Ws = []\n",
    "subj_labels = []\n",
    "for ix, f in enumerate(files, 1):\n",
    "    X = np.load(f)\n",
    "    W = imag_coh_matrix(X, fs=FS, band=BAND)\n",
    "    W = knn_sparsify(W, k=6)\n",
    "    Ws.append(W)\n",
    "    labs = spec_kmeans_on_laplacian(W, k=K_FIXED)\n",
    "    subj_labels.append(labs)\n",
    "    if ix % 2 == 0:\n",
    "        print(f\"[real] built {ix}/{len(files)} graphs\")\n",
    "\n",
    "# 2) consensus on real labels\n",
    "cons, co = consensus_from_labels(subj_labels, thr=CONS_THR)\n",
    "loso = ari_loso(subj_labels, thr=CONS_THR)\n",
    "np.save(os.path.join(OUT_TAB, f\"band__{BAND}__consensus_labels.npy\"), cons)\n",
    "np.save(os.path.join(OUT_TAB, f\"band__{BAND}__coassoc.npy\"), co)\n",
    "\n",
    "# 3) fast nulls (graph shuffles)\n",
    "rng = np.random.default_rng(7)\n",
    "null_aris = []\n",
    "for p in range(NULL_PERMS):\n",
    "    if (time.time() - t0) > TIME_CAP_S:\n",
    "        print(\"[warn] time cap hit — stopping nulls early\")\n",
    "        break\n",
    "    nlabs=[]\n",
    "    for W in Ws:\n",
    "        Wn = graph_shuffle_null(W, rng)\n",
    "        lbn = spec_kmeans_on_laplacian(Wn, k=K_FIXED)\n",
    "        nlabs.append(lbn)\n",
    "    cons_n,_ = consensus_from_labels(nlabs, thr=CONS_THR)\n",
    "    null_aris.append(adjusted_rand_score(cons, cons_n))\n",
    "    if (p+1) % 10 == 0:\n",
    "        print(f\"[null] {p+1}/{NULL_PERMS} perms; elapsed={int(time.time()-t0)}s\")\n",
    "\n",
    "null_aris = np.array(null_aris) if len(null_aris) else np.array([1.0])\n",
    "p_val = float((np.sum(null_aris >= loso) + 1) / (len(null_aris) + 1))\n",
    "\n",
    "# 4) quick figs + metrics\n",
    "if SAVE_FIGS:\n",
    "    plt.figure()\n",
    "    plt.imshow(co, aspect='auto'); plt.title(f\"Consensus co-association | {BAND}\")\n",
    "    plt.colorbar(); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_FIG, f\"consensus__{BAND}__coassoc.png\"), dpi=160); plt.close()\n",
    "\n",
    "metrics = {\n",
    "    \"band\": BAND, \"n_subjects\": len(files),\n",
    "    \"k_fixed\": K_FIXED, \"consensus_thr\": CONS_THR,\n",
    "    \"loso_median_ari\": float(loso),\n",
    "    \"null_ari_mean\": float(np.mean(null_aris)),\n",
    "    \"p_value\": p_val, \"perms_done\": int(len(null_aris)),\n",
    "    \"elapsed_s\": int(time.time()-t0)\n",
    "}\n",
    "with open(os.path.join(OUT_MET, f\"band__{BAND}__metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"\\n=== FAST DIAG RESULTS ===\")\n",
    "print(metrics)\n",
    "print(\"Artifacts:\")\n",
    "print(\" -\", os.path.join(OUT_TAB, f\"band__{BAND}__consensus_labels.npy\"))\n",
    "print(\" -\", os.path.join(OUT_TAB, f\"band__{BAND}__coassoc.npy\"))\n",
    "print(\" -\", os.path.join(OUT_MET, f\"band__{BAND}__metrics.json\"))\n",
    "print(\" -\", os.path.join(OUT_FIG, f\"consensus__{BAND}__coassoc.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d65c9d4-404e-401f-864b-f1c87439d0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] subjects=6 → ['C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\eeg_rest\\\\subject_01_EC.npy', 'C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\eeg_rest\\\\subject_02_EC.npy', 'C:\\\\Users\\\\caleb\\\\CNT_Lab\\\\eeg_rest\\\\subject_03_EC.npy'] …\n",
      "[real] built 2/6 graphs\n",
      "[real] built 4/6 graphs\n",
      "[real] built 6/6 graphs\n",
      "[null] 10/50 perms; elapsed=8s\n",
      "[null] 20/50 perms; elapsed=15s\n",
      "[null] 30/50 perms; elapsed=21s\n",
      "[null] 40/50 perms; elapsed=28s\n",
      "[null] 50/50 perms; elapsed=34s\n",
      "\n",
      "=== PLI FAST DIAG RESULTS ===\n",
      "{'band': 'alpha', 'connectivity': 'PLI', 'n_subjects': 6, 'k_fixed': 2, 'consensus_thr': 0.65, 'loso_median_ari': 1.0, 'null_ari_mean': 1.0, 'p_value': 1.0, 'perms_done': 50, 'elapsed_s': 35}\n",
      "Artifacts:\n",
      " - C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_fastdiag\\tables\\band__alpha__consensus_labels.npy\n",
      " - C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_fastdiag\\tables\\band__alpha__coassoc.npy\n",
      " - C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_fastdiag\\metrics\\band__alpha__metrics.json\n",
      " - C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_fastdiag\\figures\\consensus__alpha__coassoc_pli.png\n"
     ]
    }
   ],
   "source": [
    "# === CNT Spectral-Glyphs: PLI Fast-Diag (single cell) ===\n",
    "# Connectivity: Phase-Lag Index (PLI) from Hilbert phases in the target band\n",
    "# Null: fast graph-shuffle (no CSD recompute)\n",
    "# Clustering: spectral (k=2) to force a non-trivial split\n",
    "# Outputs -> C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_fastdiag\n",
    "\n",
    "import os, glob, json, time, numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy.signal import butter, filtfilt, hilbert\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "DATA_DIR   = r\"C:\\Users\\caleb\\CNT_Lab\\eeg_rest\"\n",
    "GLOB       = \"subject_*_EC.npy\"\n",
    "OUT_ROOT   = r\"C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_fastdiag\"\n",
    "\n",
    "BAND       = \"alpha\"        # change to \"theta\" to compare\n",
    "F_LOW, F_HIGH = (8.0, 13.0) # alpha Hz (theta: 4,8)\n",
    "FS         = 250.0          # Hz\n",
    "SUBJ_LIMIT = 6              # start small; raise after it works\n",
    "K_FIXED    = 2              # confirmatory non-trivial split\n",
    "CONS_THR   = 0.65           # consensus threshold\n",
    "KNN_K      = 6              # keep k strongest neighbors per node (symmetric)\n",
    "NULL_PERMS = 50             # fast nulls\n",
    "TIME_CAP_S = 600            # safety cap\n",
    "SAVE_FIGS  = True\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def ensure_dir(p): os.makedirs(p, exist_ok=True); return p\n",
    "\n",
    "def bandpass(x, fs, f_lo, f_hi, order=4):\n",
    "    b, a = butter(order, [f_lo/(fs/2), f_hi/(fs/2)], btype=\"band\")\n",
    "    return filtfilt(b, a, x)\n",
    "\n",
    "def pli_matrix(X, fs, f_lo, f_hi):\n",
    "    \"\"\"\n",
    "    X: [n_ch, n_t]\n",
    "    Steps: bandpass -> Hilbert -> phases -> PLI = | mean(sign(sin(Δphi))) |\n",
    "    \"\"\"\n",
    "    n_ch, n_t = X.shape\n",
    "    Y = np.zeros_like(X, dtype=float)\n",
    "    for c in range(n_ch):\n",
    "        Y[c] = bandpass(X[c], fs, f_lo, f_hi)\n",
    "    ph = np.angle(hilbert(Y, axis=1))\n",
    "    W = np.zeros((n_ch, n_ch), float)\n",
    "    for i in range(n_ch):\n",
    "        for j in range(i+1, n_ch):\n",
    "            dphi = ph[i] - ph[j]\n",
    "            pli = np.abs(np.mean(np.sign(np.sin(dphi))))\n",
    "            W[i, j] = W[j, i] = pli\n",
    "    np.fill_diagonal(W, 0.0)\n",
    "    return W\n",
    "\n",
    "def knn_sparsify(W, k=6):\n",
    "    W = W.copy(); n = W.shape[0]\n",
    "    for i in range(n):\n",
    "        idx = np.argsort(W[i])[::-1]; keep = idx[:k]\n",
    "        mask = np.ones(n, dtype=bool); mask[keep] = False\n",
    "        W[i, mask] = 0.0\n",
    "    W = np.maximum(W, W.T); np.fill_diagonal(W, 0.0)\n",
    "    return W\n",
    "\n",
    "def laplacian_sym(W):\n",
    "    d = W.sum(1); d = np.where(d<=1e-12, 1.0, d)\n",
    "    Dmh = np.diag(1.0/np.sqrt(d))\n",
    "    return np.eye(W.shape[0]) - Dmh @ W @ Dmh\n",
    "\n",
    "def spec_kmeans_on_laplacian(W, k):\n",
    "    L = laplacian_sym(W)\n",
    "    evals, evecs = np.linalg.eigh(L)\n",
    "    U = evecs[:, 1:k] if k>1 else evecs[:, :1]\n",
    "    U = U / (np.linalg.norm(U, axis=1, keepdims=True) + 1e-12)\n",
    "    return KMeans(n_clusters=k, n_init=50, random_state=42).fit_predict(U)\n",
    "\n",
    "def consensus_from_labels(all_labels, thr=0.65):\n",
    "    n = len(all_labels[0]); m = len(all_labels); co = np.zeros((n,n), float)\n",
    "    for lab in all_labels:\n",
    "        for i in range(n):\n",
    "            li = lab[i]\n",
    "            for j in range(n):\n",
    "                co[i,j] += 1.0 if li == lab[j] else 0.0\n",
    "    co /= m\n",
    "    A = (co >= thr).astype(int)\n",
    "    visited = np.zeros(n, bool); cons = -1*np.ones(n, int); cid=0\n",
    "    for i in range(n):\n",
    "        if not visited[i]:\n",
    "            stack=[i]; visited[i]=True; cons[i]=cid\n",
    "            while stack:\n",
    "                u=stack.pop()\n",
    "                for v in range(n):\n",
    "                    if A[u,v] and not visited[v]:\n",
    "                        visited[v]=True; cons[v]=cid; stack.append(v)\n",
    "            cid+=1\n",
    "    return cons, co\n",
    "\n",
    "def ari_loso(all_labels, thr=0.65):\n",
    "    cons,_ = consensus_from_labels(all_labels, thr=thr)\n",
    "    vals=[]\n",
    "    for s in range(len(all_labels)):\n",
    "        leave=[lab for i,lab in enumerate(all_labels) if i!=s]\n",
    "        cons_leave,_ = consensus_from_labels(leave, thr=thr)\n",
    "        vals.append(adjusted_rand_score(cons, cons_leave))\n",
    "    return float(np.median(vals))\n",
    "\n",
    "def graph_shuffle_null(W, rng):\n",
    "    n = W.shape[0]\n",
    "    Wn = np.zeros_like(W)\n",
    "    for i in range(n):\n",
    "        row = W[i].copy()\n",
    "        idx = np.arange(n); rng.shuffle(idx)\n",
    "        row = row[idx]; row[i] = 0.0\n",
    "        Wn[i] = row\n",
    "    Wn = 0.5*(Wn + Wn.T); np.fill_diagonal(Wn, 0.0)\n",
    "    return Wn\n",
    "\n",
    "# ---------------- Run ----------------\n",
    "t0 = time.time()\n",
    "files = sorted(glob.glob(os.path.join(DATA_DIR, GLOB)))[:SUBJ_LIMIT]\n",
    "print(f\"[info] subjects={len(files)} → {files[:3]}{' …' if len(files)>3 else ''}\")\n",
    "\n",
    "OUT_MET = ensure_dir(os.path.join(OUT_ROOT, \"metrics\"))\n",
    "OUT_TAB = ensure_dir(os.path.join(OUT_ROOT, \"tables\"))\n",
    "OUT_FIG = ensure_dir(os.path.join(OUT_ROOT, \"figures\"))\n",
    "\n",
    "# 1) build PLI graphs once\n",
    "Ws = []; subj_labels = []\n",
    "for ix, f in enumerate(files, 1):\n",
    "    X = np.load(f)\n",
    "    W = pli_matrix(X, FS, F_LOW, F_HIGH)\n",
    "    W = knn_sparsify(W, k=KNN_K)\n",
    "    Ws.append(W)\n",
    "    labs = spec_kmeans_on_laplacian(W, k=K_FIXED)\n",
    "    subj_labels.append(labs)\n",
    "    if ix % 2 == 0:\n",
    "        print(f\"[real] built {ix}/{len(files)} graphs\")\n",
    "\n",
    "# 2) consensus on real labels\n",
    "cons, co = consensus_from_labels(subj_labels, thr=CONS_THR)\n",
    "loso = ari_loso(subj_labels, thr=CONS_THR)\n",
    "np.save(os.path.join(OUT_TAB, f\"band__{BAND}__consensus_labels.npy\"), cons)\n",
    "np.save(os.path.join(OUT_TAB, f\"band__{BAND}__coassoc.npy\"), co)\n",
    "\n",
    "# 3) fast nulls (graph shuffles)\n",
    "rng = np.random.default_rng(7)\n",
    "null_aris = []\n",
    "for p in range(NULL_PERMS):\n",
    "    if (time.time() - t0) > TIME_CAP_S:\n",
    "        print(\"[warn] time cap hit — stopping nulls early\")\n",
    "        break\n",
    "    nlabs=[]\n",
    "    for W in Ws:\n",
    "        Wn = graph_shuffle_null(W, rng)\n",
    "        lbn = spec_kmeans_on_laplacian(Wn, k=K_FIXED)\n",
    "        nlabs.append(lbn)\n",
    "    cons_n,_ = consensus_from_labels(nlabs, thr=CONS_THR)\n",
    "    null_aris.append(adjusted_rand_score(cons, cons_n))\n",
    "    if (p+1) % 10 == 0:\n",
    "        print(f\"[null] {p+1}/{NULL_PERMS} perms; elapsed={int(time.time()-t0)}s\")\n",
    "\n",
    "null_aris = np.array(null_aris) if len(null_aris) else np.array([1.0])\n",
    "p_val = float((np.sum(null_aris >= loso) + 1) / (len(null_aris) + 1))\n",
    "\n",
    "# 4) quick figs + metrics\n",
    "if SAVE_FIGS:\n",
    "    plt.figure()\n",
    "    plt.imshow(co, aspect='auto'); plt.title(f\"Consensus co-association | {BAND} (PLI)\")\n",
    "    plt.colorbar(); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_FIG, f\"consensus__{BAND}__coassoc_pli.png\"), dpi=160); plt.close()\n",
    "\n",
    "metrics = {\n",
    "    \"band\": BAND, \"connectivity\": \"PLI\",\n",
    "    \"n_subjects\": len(files), \"k_fixed\": K_FIXED, \"consensus_thr\": CONS_THR,\n",
    "    \"loso_median_ari\": float(loso), \"null_ari_mean\": float(np.mean(null_aris)),\n",
    "    \"p_value\": p_val, \"perms_done\": int(len(null_aris)), \"elapsed_s\": int(time.time()-t0)\n",
    "}\n",
    "with open(os.path.join(OUT_MET, f\"band__{BAND}__metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "print(\"\\n=== PLI FAST DIAG RESULTS ===\")\n",
    "print(metrics)\n",
    "print(\"Artifacts:\")\n",
    "print(\" -\", os.path.join(OUT_TAB, f\"band__{BAND}__consensus_labels.npy\"))\n",
    "print(\" -\", os.path.join(OUT_TAB, f\"band__{BAND}__coassoc.npy\"))\n",
    "print(\" -\", os.path.join(OUT_MET, f\"band__{BAND}__metrics.json\"))\n",
    "print(\" -\", os.path.join(OUT_FIG, f\"consensus__{BAND}__coassoc_pli.png\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09d22591-a979-495f-9a9c-c5ff60a489e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>=' not supported between instances of 'list' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[32m     85\u001b[39m     cons_n,_=consensus(nlabs,CONS_THR)\n\u001b[32m     86\u001b[39m     null_aris.append(adjusted_rand_score(cons,cons_n))\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m p=\u001b[38;5;28mfloat\u001b[39m((np.sum(\u001b[43mnull_aris\u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloso\u001b[49m)+\u001b[32m1\u001b[39m)/(\u001b[38;5;28mlen\u001b[39m(null_aris)+\u001b[32m1\u001b[39m))\n\u001b[32m     88\u001b[39m unique,counts=np.unique(cons,return_counts=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     89\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== PLI Weight-Resample Diagnostic ===\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: '>=' not supported between instances of 'list' and 'float'"
     ]
    }
   ],
   "source": [
    "# === CNT Spectral-Glyphs: PLI Null-Resample Diagnostic ===\n",
    "# Fast: 6 subjects × 30 perms. Adds weight-swap nulls to test real modularity.\n",
    "\n",
    "import os, glob, json, numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy.signal import butter, filtfilt, hilbert\n",
    "\n",
    "DATA_DIR   = r\"C:\\Users\\caleb\\CNT_Lab\\eeg_rest\"\n",
    "OUT_ROOT   = r\"C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_resamplediag\"\n",
    "FS         = 250.0\n",
    "F_LOW,F_HIGH = (8.0,13.0)\n",
    "GLOB       = \"subject_*_EC.npy\"\n",
    "SUBJ_LIMIT = 6\n",
    "K_FIXED    = 2\n",
    "KNN_K      = 6\n",
    "CONS_THR   = 0.65\n",
    "NULL_PERMS = 30\n",
    "SAVE_FIGS  = True\n",
    "\n",
    "def ensure_dir(p): os.makedirs(p,exist_ok=True); return p\n",
    "def bandpass(x,fs,f_lo,f_hi,order=4):\n",
    "    from scipy.signal import butter,filtfilt\n",
    "    b,a=butter(order,[f_lo/(fs/2),f_hi/(fs/2)],btype=\"band\"); return filtfilt(b,a,x)\n",
    "def pli_matrix(X,fs,f_lo,f_hi):\n",
    "    n=X.shape[0]; Y=np.zeros_like(X)\n",
    "    for c in range(n): Y[c]=bandpass(X[c],fs,f_lo,f_hi)\n",
    "    ph=np.angle(hilbert(Y,axis=1))\n",
    "    W=np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            dphi=ph[i]-ph[j]; pli=np.abs(np.mean(np.sign(np.sin(dphi))))\n",
    "            W[i,j]=W[j,i]=pli\n",
    "    np.fill_diagonal(W,0); return W\n",
    "def knn(W,k=6):\n",
    "    W=W.copy(); n=W.shape[0]\n",
    "    for i in range(n):\n",
    "        idx=np.argsort(W[i])[::-1]; keep=idx[:k]; mask=np.ones(n,bool); mask[keep]=False; W[i,mask]=0\n",
    "    W=np.maximum(W,W.T); np.fill_diagonal(W,0); return W\n",
    "def laplacian(W): d=W.sum(1); d=np.where(d<=1e-12,1.0,d); Dmh=np.diag(1.0/np.sqrt(d)); return np.eye(W.shape[0])-Dmh@W@Dmh\n",
    "def spec(W,k):\n",
    "    L=laplacian(W); e,v=np.linalg.eigh(L); U=v[:,1:k]; U/=np.linalg.norm(U,axis=1,keepdims=True)+1e-12\n",
    "    return KMeans(n_clusters=k,n_init=50,random_state=42).fit_predict(U)\n",
    "def consensus(all_labels,thr):\n",
    "    n=len(all_labels[0]); m=len(all_labels); co=np.zeros((n,n))\n",
    "    for lab in all_labels:\n",
    "        for i in range(n):\n",
    "            li=lab[i]\n",
    "            for j in range(n):\n",
    "                co[i,j]+=1 if li==lab[j] else 0\n",
    "    co/=m; A=(co>=thr).astype(int)\n",
    "    visited=np.zeros(n,bool); cons=-1*np.ones(n,int); cid=0\n",
    "    for i in range(n):\n",
    "        if not visited[i]:\n",
    "            stack=[i]; visited[i]=True; cons[i]=cid\n",
    "            while stack:\n",
    "                u=stack.pop()\n",
    "                for v in range(n):\n",
    "                    if A[u,v] and not visited[v]:\n",
    "                        visited[v]=True; cons[v]=cid; stack.append(v)\n",
    "            cid+=1\n",
    "    return cons,co\n",
    "def ari_loso(all_labels,thr):\n",
    "    cons,_=consensus(all_labels,thr); vals=[]\n",
    "    for s in range(len(all_labels)):\n",
    "        leave=[lab for i,lab in enumerate(all_labels) if i!=s]\n",
    "        cons_l,_=consensus(leave,thr); vals.append(adjusted_rand_score(cons,cons_l))\n",
    "    return float(np.median(vals))\n",
    "def weight_swap_null(W,rng):\n",
    "    n=W.shape[0]; tri=W[np.triu_indices(n,1)]\n",
    "    rng.shuffle(tri); Wn=np.zeros_like(W); Wn[np.triu_indices(n,1)]=tri; Wn+=Wn.T; np.fill_diagonal(Wn,0); return Wn\n",
    "\n",
    "files=sorted(glob.glob(os.path.join(DATA_DIR,GLOB)))[:SUBJ_LIMIT]\n",
    "Ws=[]; labs=[]; rng=np.random.default_rng(7)\n",
    "for f in files:\n",
    "    X=np.load(f); W=pli_matrix(X,FS,F_LOW,F_HIGH); W=knn(W,KNN_K)\n",
    "    Ws.append(W); labs.append(spec(W,K_FIXED))\n",
    "cons,co=consensus(labs,CONS_THR); loso=ari_loso(labs,CONS_THR)\n",
    "null_aris=[]\n",
    "for p in range(NULL_PERMS):\n",
    "    nlabs=[]\n",
    "    for W in Ws:\n",
    "        Wn=weight_swap_null(W,rng)\n",
    "        nlabs.append(spec(Wn,K_FIXED))\n",
    "    cons_n,_=consensus(nlabs,CONS_THR)\n",
    "    null_aris.append(adjusted_rand_score(cons,cons_n))\n",
    "p=float((np.sum(null_aris>=loso)+1)/(len(null_aris)+1))\n",
    "unique,counts=np.unique(cons,return_counts=True)\n",
    "print(\"=== PLI Weight-Resample Diagnostic ===\")\n",
    "print(f\"subjects={len(files)} clusters={len(unique)} sizes={counts.tolist()}\")\n",
    "print(f\"LOSO={loso:.3f}  NullMean={np.mean(null_aris):.3f}  p={p:.4f}\")\n",
    "intra=co[cons[:,None]==cons[None,:]].mean(); inter=co[cons[:,None]!=cons[None,:]].mean()\n",
    "print(f\"Intra={intra:.3f}  Inter={inter:.3f}  Intra/Inter={intra/(inter+1e-12):.2f}\")\n",
    "plt.figure(); plt.imshow(co,aspect='auto'); plt.title(f\"Consensus co-association | {BAND} (PLI weight-null)\")\n",
    "plt.colorbar(); plt.tight_layout()\n",
    "out_fig=os.path.join(OUT_ROOT,\"consensus_alpha_weightnull.png\"); ensure_dir(OUT_ROOT); plt.savefig(out_fig,dpi=160)\n",
    "print(\"Saved:\",out_fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1072e2f4-2435-40d1-8e97-43dd39b6fe71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PLI Weight-Resample Diagnostic ===\n",
      "subjects=6  clusters=1  sizes=[64]\n",
      "LOSO=1.000  NullMean=1.000  p=1.0000\n",
      "Intra=nan  Inter=nan  Intra/Inter=nan\n",
      "Saved: C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_resamplediag\\consensus_alpha_weightnull.png\n"
     ]
    }
   ],
   "source": [
    "# === FIXED: PLI Weight-Resample Diagnostic (single cell) ===\n",
    "\n",
    "import os, glob, json, numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy.signal import butter, filtfilt, hilbert\n",
    "\n",
    "DATA_DIR   = r\"C:\\Users\\caleb\\CNT_Lab\\eeg_rest\"\n",
    "OUT_ROOT   = r\"C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_resamplediag\"\n",
    "FS         = 250.0\n",
    "F_LOW,F_HIGH = (8.0,13.0)\n",
    "GLOB       = \"subject_*_EC.npy\"\n",
    "SUBJ_LIMIT = 6\n",
    "K_FIXED    = 2\n",
    "KNN_K      = 6\n",
    "CONS_THR   = 0.65\n",
    "NULL_PERMS = 30\n",
    "SAVE_FIGS  = True\n",
    "\n",
    "def ensure_dir(p): os.makedirs(p,exist_ok=True); return p\n",
    "def bandpass(x,fs,f_lo,f_hi,order=4):\n",
    "    b,a=butter(order,[f_lo/(fs/2),f_hi/(fs/2)],btype=\"band\"); return filtfilt(b,a,x)\n",
    "def pli_matrix(X,fs,f_lo,f_hi):\n",
    "    n=X.shape[0]; Y=np.zeros_like(X)\n",
    "    for c in range(n): Y[c]=bandpass(X[c],fs,f_lo,f_hi)\n",
    "    ph=np.angle(hilbert(Y,axis=1))\n",
    "    W=np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            dphi=ph[i]-ph[j]; pli=np.abs(np.mean(np.sign(np.sin(dphi))))\n",
    "            W[i,j]=W[j,i]=pli\n",
    "    np.fill_diagonal(W,0); return W\n",
    "def knn(W,k=6):\n",
    "    W=W.copy(); n=W.shape[0]\n",
    "    for i in range(n):\n",
    "        idx=np.argsort(W[i])[::-1]; keep=idx[:k]\n",
    "        mask=np.ones(n,bool); mask[keep]=False; W[i,mask]=0\n",
    "    W=np.maximum(W,W.T); np.fill_diagonal(W,0); return W\n",
    "def laplacian(W): d=W.sum(1); d=np.where(d<=1e-12,1.0,d); Dmh=np.diag(1.0/np.sqrt(d)); return np.eye(W.shape[0])-Dmh@W@Dmh\n",
    "def spec(W,k):\n",
    "    L=laplacian(W); e,v=np.linalg.eigh(L); U=v[:,1:k]; U/=np.linalg.norm(U,axis=1,keepdims=True)+1e-12\n",
    "    return KMeans(n_clusters=k,n_init=50,random_state=42).fit_predict(U)\n",
    "def consensus(all_labels,thr):\n",
    "    n=len(all_labels[0]); m=len(all_labels); co=np.zeros((n,n))\n",
    "    for lab in all_labels:\n",
    "        for i in range(n):\n",
    "            li=lab[i]\n",
    "            for j in range(n):\n",
    "                co[i,j]+=1 if li==lab[j] else 0\n",
    "    co/=m; A=(co>=thr).astype(int)\n",
    "    visited=np.zeros(n,bool); cons=-1*np.ones(n,int); cid=0\n",
    "    for i in range(n):\n",
    "        if not visited[i]:\n",
    "            stack=[i]; visited[i]=True; cons[i]=cid\n",
    "            while stack:\n",
    "                u=stack.pop()\n",
    "                for v in range(n):\n",
    "                    if A[u,v] and not visited[v]:\n",
    "                        visited[v]=True; cons[v]=cid; stack.append(v)\n",
    "            cid+=1\n",
    "    return cons,co\n",
    "def ari_loso(all_labels,thr):\n",
    "    cons,_=consensus(all_labels,thr); vals=[]\n",
    "    for s in range(len(all_labels)):\n",
    "        leave=[lab for i,lab in enumerate(all_labels) if i!=s]\n",
    "        cons_l,_=consensus(leave,thr); vals.append(adjusted_rand_score(cons,cons_l))\n",
    "    return float(np.median(vals))\n",
    "def weight_swap_null(W,rng):\n",
    "    n=W.shape[0]; tri=W[np.triu_indices(n,1)]\n",
    "    rng.shuffle(tri)\n",
    "    Wn=np.zeros_like(W); Wn[np.triu_indices(n,1)]=tri; Wn+=Wn.T; np.fill_diagonal(Wn,0); return Wn\n",
    "\n",
    "# Load subset\n",
    "files=sorted(glob.glob(os.path.join(DATA_DIR,GLOB)))[:SUBJ_LIMIT]\n",
    "Ws=[]; labs=[]; rng=np.random.default_rng(7)\n",
    "for f in files:\n",
    "    X=np.load(f); W=pli_matrix(X,FS,F_LOW,F_HIGH); W=knn(W,KNN_K)\n",
    "    Ws.append(W); labs.append(spec(W,K_FIXED))\n",
    "\n",
    "# Real consensus\n",
    "cons,co=consensus(labs,CONS_THR)\n",
    "loso=ari_loso(labs,CONS_THR)\n",
    "\n",
    "# Null (weight-resample)\n",
    "null_aris=[]\n",
    "for p in range(NULL_PERMS):\n",
    "    nlabs=[]\n",
    "    for W in Ws:\n",
    "        Wn=weight_swap_null(W,rng)\n",
    "        nlabs.append(spec(Wn,K_FIXED))\n",
    "    cons_n,_=consensus(nlabs,CONS_THR)\n",
    "    null_aris.append(adjusted_rand_score(cons,cons_n))\n",
    "\n",
    "null_aris=np.array(null_aris, dtype=float)   # <-- FIX: make it an array\n",
    "p=float((np.sum(null_aris>=loso)+1)/(len(null_aris)+1))\n",
    "\n",
    "# Cluster stats\n",
    "unique,counts=np.unique(cons,return_counts=True)\n",
    "if len(unique)>=2:\n",
    "    intra=co[cons[:,None]==cons[None,:]].mean()\n",
    "    inter=co[cons[:,None]!=cons[None,:]].mean()\n",
    "    ratio=float(intra/(inter+1e-12))\n",
    "else:\n",
    "    intra=inter=ratio=np.nan\n",
    "\n",
    "print(\"=== PLI Weight-Resample Diagnostic ===\")\n",
    "print(f\"subjects={len(files)}  clusters={len(unique)}  sizes={counts.tolist()}\")\n",
    "print(f\"LOSO={loso:.3f}  NullMean={float(null_aris.mean()):.3f}  p={p:.4f}\")\n",
    "print(f\"Intra={intra:.3f}  Inter={inter:.3f}  Intra/Inter={ratio:.2f}\")\n",
    "\n",
    "# Figure\n",
    "if SAVE_FIGS:\n",
    "    ensure_dir(OUT_ROOT)\n",
    "    plt.figure()\n",
    "    plt.imshow(co,aspect='auto')\n",
    "    plt.title(\"Consensus co-association | alpha (PLI weight-null)\")\n",
    "    plt.colorbar(); plt.tight_layout()\n",
    "    out_fig=os.path.join(OUT_ROOT,\"consensus_alpha_weightnull.png\")\n",
    "    plt.savefig(out_fig,dpi=160); plt.close()\n",
    "    print(\"Saved:\", out_fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64eea7cf-4002-4ec8-8c06-385c67aa1e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[real] 2/6\n",
      "[real] 4/6\n",
      "[real] 6/6\n",
      "\n",
      "=== PLI Spectral-on-CoAssoc Results ===\n",
      "subjects=6  clusters=2  sizes=[35, 29]\n",
      "LOSO (coassoc spectral) = 0.818\n",
      "Null ARI mean = 0.001   p = 0.0050\n",
      "Intra=0.746  Inter=0.281  Intra/Inter=2.65\n",
      "Figure: C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_coassoc_spectral\\figures\\consensus__alpha__coassoc_spectral.png\n",
      "Saved metrics: C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_coassoc_spectral\\metrics\\band__alpha__metrics.json\n"
     ]
    }
   ],
   "source": [
    "# === CNT PLI Consensus via Spectral-on-CoAssoc (single cell) ===\n",
    "# - Per-subject: PLI (alpha) → KNN → spectral (k=2) labels\n",
    "# - Consensus matrix: fraction of subject label-agreement (channels×channels)\n",
    "# - Final consensus: spectral clustering (k=2) directly on the co-association matrix\n",
    "# - Null: keep each subject’s graph; randomize that subject’s labels (preserve sizes); rebuild coassoc; spectral k=2\n",
    "# - Prints: #clusters (always 2), cluster sizes, LOSO-ARI vs leave-one-out consensus, null p-value, intra/inter coassoc\n",
    "\n",
    "import os, glob, json, numpy as np, matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy.signal import butter, filtfilt, hilbert\n",
    "\n",
    "# -------- config --------\n",
    "DATA_DIR   = r\"C:\\Users\\caleb\\CNT_Lab\\eeg_rest\"\n",
    "GLOB       = \"subject_*_EC.npy\"\n",
    "OUT_ROOT   = r\"C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_coassoc_spectral\"\n",
    "FS         = 250.0\n",
    "F_LOW,F_HIGH = (8.0, 13.0)   # alpha\n",
    "SUBJ_LIMIT = 6               # raise after it works\n",
    "K_FIXED    = 2               # per-subject k\n",
    "KNN_K      = 6               # sparsify\n",
    "NULL_PERMS = 200             # fast; raise to 500–1000 for paper\n",
    "SAVE_FIGS  = True\n",
    "\n",
    "# -------- helpers --------\n",
    "def ensure_dir(p): os.makedirs(p, exist_ok=True); return p\n",
    "def bandpass(x,fs,lo,hi,order=4):\n",
    "    b,a=butter(order,[lo/(fs/2), hi/(fs/2)], btype=\"band\"); return filtfilt(b,a,x)\n",
    "def pli_matrix(X,fs,lo,hi):\n",
    "    n=X.shape[0]; Y=np.zeros_like(X)\n",
    "    for c in range(n): Y[c]=bandpass(X[c],fs,lo,hi)\n",
    "    ph=np.angle(hilbert(Y,axis=1))\n",
    "    W=np.zeros((n,n))\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            dphi=ph[i]-ph[j]; pli=np.abs(np.mean(np.sign(np.sin(dphi))))\n",
    "            W[i,j]=W[j,i]=pli\n",
    "    np.fill_diagonal(W,0); return W\n",
    "def knn(W,k=6):\n",
    "    W=W.copy(); n=W.shape[0]\n",
    "    for i in range(n):\n",
    "        idx=np.argsort(W[i])[::-1]; keep=idx[:k]\n",
    "        mask=np.ones(n,bool); mask[keep]=False; W[i,mask]=0\n",
    "    W=np.maximum(W,W.T); np.fill_diagonal(W,0); return W\n",
    "def laplacian(W):\n",
    "    d=W.sum(1); d=np.where(d<=1e-12,1.0,d); Dmh=np.diag(1.0/np.sqrt(d)); return np.eye(W.shape[0]) - Dmh@W@Dmh\n",
    "def spec_labels(W, k):\n",
    "    L=laplacian(W); evals, evecs = np.linalg.eigh(L)\n",
    "    U = evecs[:, 1:k] if k>1 else evecs[:, :1]\n",
    "    U = U / (np.linalg.norm(U, axis=1, keepdims=True) + 1e-12)\n",
    "    return KMeans(n_clusters=k, n_init=50, random_state=42).fit_predict(U)\n",
    "\n",
    "def build_coassoc(label_list):\n",
    "    n = len(label_list[0]); m = len(label_list)\n",
    "    co = np.zeros((n,n), float)\n",
    "    for lab in label_list:\n",
    "        for i in range(n):\n",
    "            li = lab[i]\n",
    "            for j in range(n):\n",
    "                co[i,j] += 1.0 if li == lab[j] else 0.0\n",
    "    return co / m\n",
    "\n",
    "def loso_ari_via_coassoc(label_list):\n",
    "    # Build full-consensus labels by spectral on full coassoc\n",
    "    co_full = build_coassoc(label_list)\n",
    "    cons_full = spec_labels(co_full, k=2)\n",
    "    # Leave-one-subject-out consensus labels the same way\n",
    "    aris=[]\n",
    "    for s in range(len(label_list)):\n",
    "        Ls = [lab for i,lab in enumerate(label_list) if i!=s]\n",
    "        co_l = build_coassoc(Ls)\n",
    "        cons_l = spec_labels(co_l, k=2)\n",
    "        aris.append(adjusted_rand_score(cons_full, cons_l))\n",
    "    return float(np.median(aris)), cons_full, co_full\n",
    "\n",
    "def randomize_labels_same_sizes(lab, rng):\n",
    "    n = len(lab); uniq, cnts = np.unique(lab, return_counts=True)\n",
    "    idx = np.arange(n); rng.shuffle(idx)\n",
    "    # assign first cnts[0] to label uniq[0], etc.\n",
    "    out = np.empty(n, dtype=int); start=0\n",
    "    for label, c in zip(uniq, cnts):\n",
    "        seg = idx[start:start+c]; out[seg]=label; start+=c\n",
    "    return out\n",
    "\n",
    "# -------- run --------\n",
    "files = sorted(glob.glob(os.path.join(DATA_DIR, GLOB)))[:SUBJ_LIMIT]\n",
    "if not files: raise SystemExit(\"No EC files found. Check DATA_DIR/GLOB.\")\n",
    "OUT_MET = ensure_dir(os.path.join(OUT_ROOT, \"metrics\"))\n",
    "OUT_TAB = ensure_dir(os.path.join(OUT_ROOT, \"tables\"))\n",
    "OUT_FIG = ensure_dir(os.path.join(OUT_ROOT, \"figures\"))\n",
    "\n",
    "# Per-subject: PLI → KNN → spectral k=2\n",
    "subj_labels = []\n",
    "Ws = []\n",
    "for ix, f in enumerate(files, 1):\n",
    "    X = np.load(f)\n",
    "    W = pli_matrix(X, FS, F_LOW, F_HIGH)\n",
    "    W = knn(W, KNN_K)\n",
    "    labs = spec_labels(W, K_FIXED)\n",
    "    subj_labels.append(labs)\n",
    "    Ws.append(W)\n",
    "    if ix % 2 == 0: print(f\"[real] {ix}/{len(files)}\")\n",
    "\n",
    "# Real consensus via spectral-on-coassoc\n",
    "loso, cons_real, co_real = loso_ari_via_coassoc(subj_labels)\n",
    "np.save(os.path.join(OUT_TAB, f\"band__alpha__coassoc.npy\"), co_real)\n",
    "np.save(os.path.join(OUT_TAB, f\"band__alpha__consensus_labels.npy\"), cons_real)\n",
    "\n",
    "# Null: randomize each subject’s labels (preserve sizes) → spectral-on-coassoc\n",
    "rng = np.random.default_rng(7)\n",
    "null_aris = []\n",
    "for p in range(NULL_PERMS):\n",
    "    nlabs=[]\n",
    "    for lab in subj_labels:\n",
    "        nlabs.append(randomize_labels_same_sizes(lab, rng))\n",
    "    loso_n, cons_n, _ = loso_ari_via_coassoc(nlabs)\n",
    "    null_aris.append(adjusted_rand_score(cons_real, cons_n))\n",
    "null_aris = np.array(null_aris, float)\n",
    "p_val = float((np.sum(null_aris >= loso) + 1) / (len(null_aris) + 1))\n",
    "\n",
    "# Cluster diagnostics on coassoc spectral labels\n",
    "unique, counts = np.unique(cons_real, return_counts=True)\n",
    "intra = co_real[cons_real[:,None]==cons_real[None,:]].mean()\n",
    "inter = co_real[cons_real[:,None]!=cons_real[None,:]].mean()\n",
    "ratio = float(intra/(inter+1e-12))\n",
    "\n",
    "print(\"\\n=== PLI Spectral-on-CoAssoc Results ===\")\n",
    "print(f\"subjects={len(files)}  clusters=2  sizes={counts.tolist()}\")\n",
    "print(f\"LOSO (coassoc spectral) = {loso:.3f}\")\n",
    "print(f\"Null ARI mean = {float(null_aris.mean()):.3f}   p = {p_val:.4f}\")\n",
    "print(f\"Intra={intra:.3f}  Inter={inter:.3f}  Intra/Inter={ratio:.2f}\")\n",
    "\n",
    "if SAVE_FIGS:\n",
    "    plt.figure()\n",
    "    plt.imshow(co_real, aspect='auto'); plt.title(\"Consensus co-association (spectral) | alpha (PLI)\")\n",
    "    plt.colorbar(); plt.tight_layout()\n",
    "    figp = os.path.join(OUT_FIG, \"consensus__alpha__coassoc_spectral.png\")\n",
    "    plt.savefig(figp, dpi=160); plt.close()\n",
    "    print(\"Figure:\", figp)\n",
    "\n",
    "# Save metrics json\n",
    "metrics = {\n",
    "    \"band\": \"alpha\",\n",
    "    \"connectivity\": \"PLI\",\n",
    "    \"consensus_mode\": \"spectral_on_coassoc_k2\",\n",
    "    \"n_subjects\": len(files),\n",
    "    \"loso_coassoc\": float(loso),\n",
    "    \"null_ari_mean\": float(null_aris.mean()),\n",
    "    \"p_value\": p_val,\n",
    "    \"cluster_sizes\": counts.tolist(),\n",
    "    \"intra_mean\": float(intra),\n",
    "    \"inter_mean\": float(inter),\n",
    "    \"intra_over_inter\": ratio,\n",
    "}\n",
    "with open(os.path.join(OUT_MET, \"band__alpha__metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(\"Saved metrics:\", os.path.join(OUT_MET, \"band__alpha__metrics.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e444cd80-19fd-466b-b19e-c38d771457ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PLI spectral-on-coassoc (k=2) — Band Sweep ===\n",
      " band  n_subjects cluster_sizes     LOSO  null_ari_mean  p_value  intra_over_inter\n",
      "alpha          10      [34, 30] 0.877963      -0.001952 0.003322          2.327533\n",
      "theta          10      [27, 37] 0.790194       0.002869 0.003322          1.934356\n",
      " beta          10      [30, 34] 1.000000      -0.000808 0.003322          6.005028\n",
      "\n",
      "Saved:\n",
      "  - Summary CSV: C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_band_sweep\\summary.csv\n",
      "  - Metrics JSON: C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_band_sweep\\metrics\n",
      "  - Tables NPY : C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_band_sweep\\tables\n",
      "  - Figures    : C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_band_sweep\\figures\n"
     ]
    }
   ],
   "source": [
    "# === CNT PLI Consensus — α/θ/β sweep (single cell) ===\n",
    "# Connectivity: Phase-Lag Index (PLI) via Hilbert phases in each band\n",
    "# Consensus: spectral clustering (k=2) on the co-association matrix\n",
    "# Null: per-subject labels randomized with same cluster sizes (fast, alignment-specific)\n",
    "# Outputs:\n",
    "#   C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_band_sweep\\{metrics/*.json, tables/*.npy, figures/*.png, summary.csv}\n",
    "\n",
    "import os, glob, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from scipy.signal import butter, filtfilt, hilbert\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "DATA_DIR   = r\"C:\\Users\\caleb\\CNT_Lab\\eeg_rest\"\n",
    "GLOB       = \"subject_*_EC.npy\"\n",
    "OUT_ROOT   = r\"C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_band_sweep\"\n",
    "\n",
    "FS         = 250.0\n",
    "BANDS_HZ   = { \"alpha\": (8.0, 13.0), \"theta\": (4.0, 8.0), \"beta\": (13.0, 30.0) }\n",
    "\n",
    "SUBJ_LIMIT = 10           # use 6 for a quick pass; 10 for full\n",
    "K_FIXED    = 2            # force non-trivial split\n",
    "KNN_K      = 6            # try 4–7 if needed\n",
    "NULL_PERMS = 300          # raise to 1000 for paper-grade\n",
    "SAVE_FIGS  = True\n",
    "\n",
    "# ---------------- Helpers ----------------\n",
    "def ensure_dir(p): os.makedirs(p, exist_ok=True); return p\n",
    "def bandpass(x,fs,lo,hi,order=4):\n",
    "    b,a = butter(order, [lo/(fs/2), hi/(fs/2)], btype=\"band\")\n",
    "    return filtfilt(b,a,x)\n",
    "\n",
    "def pli_matrix(X,fs,lo,hi):\n",
    "    n = X.shape[0]; Y = np.zeros_like(X)\n",
    "    for c in range(n): Y[c] = bandpass(X[c], fs, lo, hi)\n",
    "    ph = np.angle(hilbert(Y, axis=1))\n",
    "    W = np.zeros((n,n), float)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            dphi = ph[i]-ph[j]\n",
    "            pli  = np.abs(np.mean(np.sign(np.sin(dphi))))\n",
    "            W[i,j] = W[j,i] = pli\n",
    "    np.fill_diagonal(W, 0.0)\n",
    "    return W\n",
    "\n",
    "def knn(W,k=6):\n",
    "    W = W.copy(); n = W.shape[0]\n",
    "    for i in range(n):\n",
    "        idx = np.argsort(W[i])[::-1]\n",
    "        keep = idx[:k]\n",
    "        mask = np.ones(n, dtype=bool); mask[keep] = False\n",
    "        W[i, mask] = 0.0\n",
    "    W = np.maximum(W, W.T)\n",
    "    np.fill_diagonal(W, 0.0)\n",
    "    return W\n",
    "\n",
    "def laplacian(W):\n",
    "    d = W.sum(1)\n",
    "    d = np.where(d<=1e-12, 1.0, d)\n",
    "    Dmh = np.diag(1.0/np.sqrt(d))\n",
    "    return np.eye(W.shape[0]) - Dmh @ W @ Dmh\n",
    "\n",
    "def spec_labels(W, k):\n",
    "    L = laplacian(W)\n",
    "    evals, evecs = np.linalg.eigh(L)\n",
    "    U = evecs[:, 1:k] if k>1 else evecs[:, :1]\n",
    "    U = U / (np.linalg.norm(U, axis=1, keepdims=True) + 1e-12)\n",
    "    return KMeans(n_clusters=k, n_init=50, random_state=42).fit_predict(U)\n",
    "\n",
    "def build_coassoc(label_list):\n",
    "    n = len(label_list[0]); m = len(label_list)\n",
    "    co = np.zeros((n,n), float)\n",
    "    for lab in label_list:\n",
    "        for i in range(n):\n",
    "            li = lab[i]\n",
    "            for j in range(n):\n",
    "                co[i,j] += 1.0 if li == lab[j] else 0.0\n",
    "    return co / m\n",
    "\n",
    "def loso_ari_via_coassoc(label_list):\n",
    "    co_full = build_coassoc(label_list)\n",
    "    cons_full = spec_labels(co_full, k=2)\n",
    "    aris=[]\n",
    "    for s in range(len(label_list)):\n",
    "        leave = [lab for i,lab in enumerate(label_list) if i!=s]\n",
    "        co_l  = build_coassoc(leave)\n",
    "        cons_l= spec_labels(co_l, k=2)\n",
    "        aris.append(adjusted_rand_score(cons_full, cons_l))\n",
    "    return float(np.median(aris)), cons_full, co_full\n",
    "\n",
    "def randomize_labels_same_sizes(lab, rng):\n",
    "    n = len(lab)\n",
    "    uniq, cnts = np.unique(lab, return_counts=True)\n",
    "    idx = np.arange(n); rng.shuffle(idx)\n",
    "    out = np.empty(n, dtype=int); start=0\n",
    "    for label, c in zip(uniq, cnts):\n",
    "        seg = idx[start:start+c]; out[seg]=label; start+=c\n",
    "    return out\n",
    "\n",
    "# ---------------- Run ----------------\n",
    "files = sorted(glob.glob(os.path.join(DATA_DIR, GLOB)))[:SUBJ_LIMIT]\n",
    "if not files: raise SystemExit(\"No EC files found. Check DATA_DIR/GLOB.\")\n",
    "OUT_MET = ensure_dir(os.path.join(OUT_ROOT, \"metrics\"))\n",
    "OUT_TAB = ensure_dir(os.path.join(OUT_ROOT, \"tables\"))\n",
    "OUT_FIG = ensure_dir(os.path.join(OUT_ROOT, \"figures\"))\n",
    "\n",
    "rows = []\n",
    "rng = np.random.default_rng(7)\n",
    "\n",
    "for band, (lo, hi) in BANDS_HZ.items():\n",
    "    # per-subject labels (PLI → KNN → spectral k=2)\n",
    "    subj_labels = []\n",
    "    for f in files:\n",
    "        X = np.load(f)\n",
    "        W = pli_matrix(X, FS, lo, hi)\n",
    "        W = knn(W, KNN_K)\n",
    "        labs = spec_labels(W, K_FIXED)\n",
    "        subj_labels.append(labs)\n",
    "\n",
    "    # consensus via spectral-on-coassoc\n",
    "    loso, cons_real, co_real = loso_ari_via_coassoc(subj_labels)\n",
    "    np.save(os.path.join(OUT_TAB, f\"band__{band}__coassoc.npy\"), co_real)\n",
    "    np.save(os.path.join(OUT_TAB, f\"band__{band}__consensus_labels.npy\"), cons_real)\n",
    "\n",
    "    # alignment-preserving null: label randomization per subject\n",
    "    null_aris = []\n",
    "    for p in range(NULL_PERMS):\n",
    "        nlabs=[]\n",
    "        for lab in subj_labels:\n",
    "            nlabs.append(randomize_labels_same_sizes(lab, rng))\n",
    "        _, cons_n, _ = loso_ari_via_coassoc(nlabs)\n",
    "        null_aris.append(adjusted_rand_score(cons_real, cons_n))\n",
    "    null_aris = np.array(null_aris, float)\n",
    "    p_val = float((np.sum(null_aris >= loso) + 1) / (len(null_aris) + 1))\n",
    "\n",
    "    # diagnostics\n",
    "    uniq, cnts = np.unique(cons_real, return_counts=True)\n",
    "    intra = co_real[cons_real[:,None]==cons_real[None,:]].mean()\n",
    "    inter = co_real[cons_real[:,None]!=cons_real[None,:]].mean()\n",
    "    ratio = float(intra/(inter+1e-12))\n",
    "\n",
    "    # save metrics\n",
    "    metrics = {\n",
    "        \"band\": band, \"connectivity\": \"PLI\", \"consensus_mode\": \"spectral_on_coassoc_k2\",\n",
    "        \"n_subjects\": len(files), \"loso_coassoc\": float(loso),\n",
    "        \"null_ari_mean\": float(null_aris.mean()), \"p_value\": p_val,\n",
    "        \"cluster_sizes\": cnts.tolist(), \"intra_mean\": float(intra),\n",
    "        \"inter_mean\": float(inter), \"intra_over_inter\": ratio\n",
    "    }\n",
    "    with open(os.path.join(OUT_MET, f\"band__{band}__metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    # figure\n",
    "    if SAVE_FIGS:\n",
    "        plt.figure()\n",
    "        plt.imshow(co_real, aspect='auto')\n",
    "        plt.title(f\"Consensus co-association (spectral) | {band} (PLI)\")\n",
    "        plt.colorbar(); plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUT_FIG, f\"consensus__{band}__coassoc_spectral.png\"), dpi=160)\n",
    "        plt.close()\n",
    "\n",
    "    rows.append([band, len(files), cnts.tolist(), float(loso), float(null_aris.mean()), p_val, ratio])\n",
    "\n",
    "# summary CSV + print\n",
    "df = pd.DataFrame(rows, columns=[\"band\",\"n_subjects\",\"cluster_sizes\",\"LOSO\",\"null_ari_mean\",\"p_value\",\"intra_over_inter\"])\n",
    "csv_path = os.path.join(OUT_ROOT, \"summary.csv\"); df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(\"=== PLI spectral-on-coassoc (k=2) — Band Sweep ===\")\n",
    "print(df.to_string(index=False))\n",
    "print(\"\\nSaved:\")\n",
    "print(\"  - Summary CSV:\", csv_path)\n",
    "print(\"  - Metrics JSON:\", OUT_MET)\n",
    "print(\"  - Tables NPY :\", OUT_TAB)\n",
    "print(\"  - Figures    :\", OUT_FIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7319df56-8c68-440a-8022-1e1eefdbdea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Did not find any electrode locations (in the info object), will attempt to use digitization points instead. However, if digitization points do not correspond to the EEG electrodes, this will lead to bad results. Please verify that the sensor locations in the plot are accurate.\n",
      "WARNING:root:Did not find any electrode locations (in the info object), will attempt to use digitization points instead. However, if digitization points do not correspond to the EEG electrodes, this will lead to bad results. Please verify that the sensor locations in the plot are accurate.\n",
      "WARNING:root:Did not find any electrode locations (in the info object), will attempt to use digitization points instead. However, if digitization points do not correspond to the EEG electrodes, this will lead to bad results. Please verify that the sensor locations in the plot are accurate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] topomap failed for alpha: Did not find any digitization points of kind 3 (FIFFV_POINT_EEG) in the info.\n",
      "[warn] topomap failed for theta: Did not find any digitization points of kind 3 (FIFFV_POINT_EEG) in the info.\n",
      "[warn] topomap failed for beta: Did not find any digitization points of kind 3 (FIFFV_POINT_EEG) in the info.\n",
      "\n",
      "=== Sweep summary (from metrics JSON) ===\n",
      " band  n cluster_sizes     LOSO  null_mean        p  intra/inter\n",
      "alpha 10      [34, 30] 0.877963  -0.001952 0.003322     2.327533\n",
      "theta 10      [27, 37] 0.790194   0.002869 0.003322     1.934356\n",
      " beta 10      [30, 34] 1.000000  -0.000808 0.003322     6.005028\n",
      "\n",
      "Saved topomaps to: C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_band_sweep\\figures\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 154\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NULL_PERMS):\n\u001b[32m    153\u001b[39m     nlabs=[randomize_labels_same_sizes(lab, rng) \u001b[38;5;28;01mfor\u001b[39;00m lab \u001b[38;5;129;01min\u001b[39;00m subj_labels]\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m     _, cons_n, _ = \u001b[43mloso_ari_via_coassoc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnlabs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m     null_aris.append(adjusted_rand_score(cons, cons_n))\n\u001b[32m    156\u001b[39m null_aris=np.array(null_aris,\u001b[38;5;28mfloat\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 131\u001b[39m, in \u001b[36mloso_ari_via_coassoc\u001b[39m\u001b[34m(label_list)\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(label_list)):\n\u001b[32m    130\u001b[39m     leave=[lab \u001b[38;5;28;01mfor\u001b[39;00m i,lab \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(label_list) \u001b[38;5;28;01mif\u001b[39;00m i!=s]\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     co_l=build_coassoc(leave); cons_l=\u001b[43mspec_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mco_l\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    132\u001b[39m     aris.append(adjusted_rand_score(cons_full, cons_l))\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(np.median(aris)), cons_full, co_full\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 117\u001b[39m, in \u001b[36mspec_labels\u001b[39m\u001b[34m(W, k)\u001b[39m\n\u001b[32m    115\u001b[39m L=laplacian(W); e,v=np.linalg.eigh(L); U=v[:,\u001b[32m1\u001b[39m:k] \u001b[38;5;28;01mif\u001b[39;00m k>\u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m v[:,:\u001b[32m1\u001b[39m]\n\u001b[32m    116\u001b[39m U/=np.linalg.norm(U,axis=\u001b[32m1\u001b[39m,keepdims=\u001b[38;5;28;01mTrue\u001b[39;00m)+\u001b[32m1e-12\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mKMeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_clusters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_init\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mU\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1064\u001b[39m, in \u001b[36m_BaseKMeans.fit_predict\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1041\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1042\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute cluster centers and predict cluster index for each sample.\u001b[39;00m\n\u001b[32m   1043\u001b[39m \n\u001b[32m   1044\u001b[39m \u001b[33;03m    Convenience method; equivalent to calling fit(X) followed by\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1062\u001b[39m \u001b[33;03m        Index of the cluster each sample belongs to.\u001b[39;00m\n\u001b[32m   1063\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1064\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m.labels_\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1510\u001b[39m, in \u001b[36mKMeans.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1507\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInitialization complete\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1509\u001b[39m \u001b[38;5;66;03m# run a k-means once\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1510\u001b[39m labels, inertia, centers, n_iter_ = \u001b[43mkmeans_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1511\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1512\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1513\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcenters_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1514\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1515\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1516\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1517\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_n_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1518\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1520\u001b[39m \u001b[38;5;66;03m# determine if these results are the best so far\u001b[39;00m\n\u001b[32m   1521\u001b[39m \u001b[38;5;66;03m# we chose a new run if it has a better inertia and the clustering is\u001b[39;00m\n\u001b[32m   1522\u001b[39m \u001b[38;5;66;03m# different from the best so far (it's possible that the inertia is\u001b[39;00m\n\u001b[32m   1523\u001b[39m \u001b[38;5;66;03m# slightly better even if the clustering is the same with potentially\u001b[39;00m\n\u001b[32m   1524\u001b[39m \u001b[38;5;66;03m# permuted labels, due to rounding errors)\u001b[39;00m\n\u001b[32m   1525\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m best_inertia \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1526\u001b[39m     inertia < best_inertia\n\u001b[32m   1527\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_same_clustering(labels, best_labels, \u001b[38;5;28mself\u001b[39m.n_clusters)\n\u001b[32m   1528\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\utils\\parallel.py:173\u001b[39m, in \u001b[36m_threadpool_controller_decorator.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    171\u001b[39m controller = _get_threadpool_controller()\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m controller.limit(limits=limits, user_api=user_api):\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:750\u001b[39m, in \u001b[36m_kmeans_single_lloyd\u001b[39m\u001b[34m(X, sample_weight, centers_init, max_iter, verbose, tol, n_threads)\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m strict_convergence:\n\u001b[32m    737\u001b[39m     \u001b[38;5;66;03m# rerun E-step so that predicted labels match cluster centers\u001b[39;00m\n\u001b[32m    738\u001b[39m     lloyd_iter(\n\u001b[32m    739\u001b[39m         X,\n\u001b[32m    740\u001b[39m         sample_weight,\n\u001b[32m   (...)\u001b[39m\u001b[32m    747\u001b[39m         update_centers=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    748\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m inertia = \u001b[43m_inertia\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcenters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m labels, inertia, centers, i + \u001b[32m1\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === CNT PLI Consensus — Topomaps + β sanity check (single cell) ===\n",
    "# Inputs: artifacts from the band sweep you just ran\n",
    "#   C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_band_sweep\\metrics\\band__{band}__metrics.json\n",
    "#   C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_band_sweep\\tables\\band__{band}__consensus_labels.npy\n",
    "# Also needs channel names from one subject: C:\\Users\\caleb\\CNT_Lab\\eeg_rest\\subject_01_EC.channels.txt\n",
    "#\n",
    "# Outputs:\n",
    "#   figures/topomap__{band}__cluster{0,1}.png\n",
    "#   metrics/beta_sanity_metrics.json  (KNN_K=4, CONS_THR=0.70, NULL_PERMS=1000)\n",
    "\n",
    "import os, glob, json, numpy as np, matplotlib.pyplot as plt\n",
    "import mne\n",
    "\n",
    "ROOT = r\"C:\\Users\\caleb\\CNT_Lab\"\n",
    "SWEEP = os.path.join(ROOT, r\"artifacts\\pli_band_sweep\")\n",
    "OUTF  = os.path.join(ROOT, r\"artifacts\\pli_band_sweep\\figures\")\n",
    "OUTM  = os.path.join(ROOT, r\"artifacts\\pli_band_sweep\\metrics\")\n",
    "OUTT  = os.path.join(ROOT, r\"artifacts\\pli_band_sweep\\tables\")\n",
    "ensure = lambda p: (os.makedirs(p, exist_ok=True) or p)\n",
    "ensure(OUTF)\n",
    "\n",
    "BANDS = [\"alpha\",\"theta\",\"beta\"]\n",
    "CHAN_TXT = os.path.join(ROOT, r\"eeg_rest\\subject_01_EC.channels.txt\")\n",
    "\n",
    "# ---------- helper: load channels ----------\n",
    "if os.path.exists(CHAN_TXT):\n",
    "    with open(CHAN_TXT,\"r\",encoding=\"utf-8\") as f:\n",
    "        ch_names = [ln.strip() for ln in f if ln.strip()]\n",
    "else:\n",
    "    # fallback to 64 generic names\n",
    "    ch_names = [f\"ch{i}\" for i in range(64)]\n",
    "\n",
    "# ---------- helper: simple topomap per cluster ----------\n",
    "def topomap_two_clusters(cons_labels, band, ch_names):\n",
    "    # Build fake \"values\" just to color nodes by cluster (0/1)\n",
    "    # We'll plot two images: cluster A highlighted, cluster B highlighted\n",
    "    montage = mne.channels.make_standard_montage(\"standard_1020\")\n",
    "    info = mne.create_info(ch_names, sfreq=250.0, ch_types=\"eeg\")\n",
    "    info.set_montage(montage, on_missing=\"ignore\", match_case=False)\n",
    "\n",
    "    arr = np.zeros(len(ch_names))\n",
    "    for cluster_id in [0,1]:\n",
    "        arr[:] = (cons_labels == cluster_id).astype(float)\n",
    "        ev = mne.EvokedArray(arr[:,None], info, tmin=0.0)  # one \"time\"\n",
    "        fig = ev.plot_topomap(times=[0.0], scalings=1.0, time_format=\"\", show=False)\n",
    "        # Save the figure mne returns as mpl fig\n",
    "        fig[0].savefig(os.path.join(OUTF, f\"topomap__{band}__cluster{cluster_id}.png\"), dpi=160)\n",
    "\n",
    "# ---------- 1) Load sweep outputs & draw topomaps ----------\n",
    "rows = []\n",
    "for b in BANDS:\n",
    "    met_fp = os.path.join(OUTM, f\"band__{b}__metrics.json\")\n",
    "    lab_fp = os.path.join(OUTT, f\"band__{b}__consensus_labels.npy\")\n",
    "    if not (os.path.exists(met_fp) and os.path.exists(lab_fp)):\n",
    "        print(f\"[skip] missing {b}\")\n",
    "        continue\n",
    "\n",
    "    with open(met_fp,\"r\",encoding=\"utf-8\") as f:\n",
    "        met = json.load(f)\n",
    "    cons = np.load(lab_fp)\n",
    "\n",
    "    # record summary line\n",
    "    rows.append([b, met[\"n_subjects\"], met[\"cluster_sizes\"], met[\"loso_coassoc\"],\n",
    "                 met[\"null_ari_mean\"], met[\"p_value\"], met[\"intra_over_inter\"]])\n",
    "\n",
    "    # draw topomaps\n",
    "    try:\n",
    "        topomap_two_clusters(cons, b, ch_names)\n",
    "        print(f\"[topomap] wrote topomap__{b}__cluster*.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] topomap failed for {b}: {e}\")\n",
    "\n",
    "# print a compact table\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(rows, columns=[\"band\",\"n\",\"cluster_sizes\",\"LOSO\",\"null_mean\",\"p\",\"intra/inter\"])\n",
    "print(\"\\n=== Sweep summary (from metrics JSON) ===\")\n",
    "print(df.to_string(index=False))\n",
    "print(\"\\nSaved topomaps to:\", OUTF)\n",
    "\n",
    "# ---------- 2) β sanity check: re-score with KNN_K=4, CONS_THR=0.70, NULL_PERMS=1000 ----------\n",
    "DATA_DIR = os.path.join(ROOT, r\"eeg_rest\")\n",
    "files = sorted(glob.glob(os.path.join(DATA_DIR, \"subject_*_EC.npy\")))[:10]\n",
    "if not files:\n",
    "    raise SystemExit(\"No EC files found.\")\n",
    "\n",
    "FS = 250.0\n",
    "F_LOW,F_HIGH = (13.0, 30.0)   # beta\n",
    "K_FIXED = 2\n",
    "KNN_K   = 4\n",
    "CONS_THR= 0.70\n",
    "NULL_PERMS = 1000\n",
    "\n",
    "from scipy.signal import butter, filtfilt, hilbert\n",
    "def bandpass(x,fs,lo,hi,order=4):\n",
    "    b,a = butter(order, [lo/(fs/2), hi/(fs/2)], btype=\"band\"); return filtfilt(b,a,x)\n",
    "def pli_matrix(X,fs,lo,hi):\n",
    "    n=X.shape[0]; Y=np.zeros_like(X)\n",
    "    for c in range(n): Y[c]=bandpass(X[c],fs,lo,hi)\n",
    "    ph=np.angle(hilbert(Y,axis=1))\n",
    "    W=np.zeros((n,n)); \n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            dphi=ph[i]-ph[j]; pli=np.abs(np.mean(np.sign(np.sin(dphi))))\n",
    "            W[i,j]=W[j,i]=pli\n",
    "    np.fill_diagonal(W,0); return W\n",
    "def knn(W,k=6):\n",
    "    W=W.copy(); n=W.shape[0]\n",
    "    for i in range(n):\n",
    "        idx=np.argsort(W[i])[::-1]; keep=idx[:k]\n",
    "        mask=np.ones(n,bool); mask[keep]=False; W[i,mask]=0\n",
    "    W=np.maximum(W,W.T); np.fill_diagonal(W,0); return W\n",
    "def laplacian(W):\n",
    "    d=W.sum(1); d=np.where(d<=1e-12,1.0,d); Dmh=np.diag(1.0/np.sqrt(d)); return np.eye(W.shape[0]) - Dmh@W@Dmh\n",
    "def spec_labels(W,k):\n",
    "    L=laplacian(W); e,v=np.linalg.eigh(L); U=v[:,1:k] if k>1 else v[:,:1]\n",
    "    U/=np.linalg.norm(U,axis=1,keepdims=True)+1e-12\n",
    "    return KMeans(n_clusters=k, n_init=50, random_state=42).fit_predict(U)\n",
    "def build_coassoc(label_list):\n",
    "    n=len(label_list[0]); m=len(label_list); co=np.zeros((n,n))\n",
    "    for lab in label_list:\n",
    "        for i in range(n):\n",
    "            li=lab[i]\n",
    "            for j in range(n):\n",
    "                co[i,j]+=1.0 if li==lab[j] else 0.0\n",
    "    return co/m\n",
    "def loso_ari_via_coassoc(label_list):\n",
    "    co_full=build_coassoc(label_list); cons_full=spec_labels(co_full, k=2)\n",
    "    aris=[]\n",
    "    for s in range(len(label_list)):\n",
    "        leave=[lab for i,lab in enumerate(label_list) if i!=s]\n",
    "        co_l=build_coassoc(leave); cons_l=spec_labels(co_l, k=2)\n",
    "        aris.append(adjusted_rand_score(cons_full, cons_l))\n",
    "    return float(np.median(aris)), cons_full, co_full\n",
    "def randomize_labels_same_sizes(lab, rng):\n",
    "    n=len(lab); uniq,cnts=np.unique(lab, return_counts=True)\n",
    "    idx=np.arange(n); rng.shuffle(idx); out=np.empty(n,int); start=0\n",
    "    for label,c in zip(uniq,cnts):\n",
    "        seg=idx[start:start+c]; out[seg]=label; start+=c\n",
    "    return out\n",
    "\n",
    "# per-subject beta with tighter settings\n",
    "subj_labels=[]\n",
    "for f in files:\n",
    "    X=np.load(f)\n",
    "    W=pli_matrix(X,FS,F_LOW,F_HIGH)\n",
    "    W=knn(W,KNN_K)\n",
    "    subj_labels.append(spec_labels(W, K_FIXED))\n",
    "\n",
    "loso, cons, co = loso_ari_via_coassoc(subj_labels)\n",
    "rng = np.random.default_rng(11)\n",
    "null_aris=[]\n",
    "for p in range(NULL_PERMS):\n",
    "    nlabs=[randomize_labels_same_sizes(lab, rng) for lab in subj_labels]\n",
    "    _, cons_n, _ = loso_ari_via_coassoc(nlabs)\n",
    "    null_aris.append(adjusted_rand_score(cons, cons_n))\n",
    "null_aris=np.array(null_aris,float)\n",
    "p_val=float((np.sum(null_aris>=loso)+1)/(len(null_aris)+1))\n",
    "uniq,cnts=np.unique(cons, return_counts=True)\n",
    "intra=co[cons[:,None]==cons[None,:]].mean()\n",
    "inter=co[cons[:,None]!=cons[None,:]].mean()\n",
    "ratio=float(intra/(inter+1e-12))\n",
    "\n",
    "beta_sanity = {\n",
    "    \"band\":\"beta\",\"connectivity\":\"PLI\",\"consensus_mode\":\"spectral_on_coassoc_k2\",\n",
    "    \"n_subjects\":len(files),\"KNN_K\":KNN_K,\"CONS_THR\":CONS_THR,\n",
    "    \"LOSO\":float(loso),\"null_mean\":float(null_aris.mean()),\"p_value\":p_val,\n",
    "    \"cluster_sizes\":cnts.tolist(),\"intra\":float(intra),\"inter\":float(inter),\"intra_over_inter\":ratio\n",
    "}\n",
    "with open(os.path.join(OUTM,\"beta_sanity_metrics.json\"),\"w\",encoding=\"utf-8\") as f:\n",
    "    json.dump(beta_sanity,f,indent=2)\n",
    "\n",
    "print(\"\\n=== β sanity (KNN_K=4, CONS_THR=0.70, NULL_PERMS=1000) ===\")\n",
    "print(beta_sanity)\n",
    "print(\"\\nTopomaps saved to:\", OUTF)\n",
    "print(\"β sanity metrics:\", os.path.join(OUTM,\"beta_sanity_metrics.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c72fd8b-97fb-4088-b172-eaeb6edcef4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_band_sweep\\figures\\scalp__alpha__clusters.png\n",
      "Saved: C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_band_sweep\\figures\\scalp__theta__clusters.png\n",
      "Saved: C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_band_sweep\\figures\\scalp__beta__clusters.png\n",
      "\n",
      "Done. If some points are missing, add aliases in the `alias` dict above and re-run.\n"
     ]
    }
   ],
   "source": [
    "# === FAST SCALP SCATTERS (no MNE) — α/θ/β consensus clusters ===\n",
    "# Inputs (from your sweep):\n",
    "#   C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_band_sweep\\tables\\band__{band}__consensus_labels.npy\n",
    "# Also needs channel names from any subject:\n",
    "#   C:\\Users\\caleb\\CNT_Lab\\eeg_rest\\subject_01_EC.channels.txt\n",
    "#\n",
    "# Outputs:\n",
    "#   C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_band_sweep\\figures\\scalp__{band}__clusters.png\n",
    "\n",
    "import os, json, re, numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "ROOT   = r\"C:\\Users\\caleb\\CNT_Lab\"\n",
    "SWEEP  = os.path.join(ROOT, r\"artifacts\\pli_band_sweep\")\n",
    "TABDIR = os.path.join(SWEEP, \"tables\")\n",
    "FIGDIR = os.path.join(SWEEP, \"figures\")\n",
    "os.makedirs(FIGDIR, exist_ok=True)\n",
    "\n",
    "BANDS  = [\"alpha\",\"theta\",\"beta\"]\n",
    "CHAN_TXT = os.path.join(ROOT, r\"eeg_rest\\subject_01_EC.channels.txt\")\n",
    "\n",
    "# --- 10–20 approximate 2D coords (x,y) in head space [-1,1] (frontal y>0, posterior y<0)\n",
    "# Minimal but sufficient set; we’ll fuzzy-map additional labels to nearest canonical\n",
    "canon_xy = {\n",
    " \"Fp1\":(-0.5, 0.95), \"Fpz\":(0.0, 0.98), \"Fp2\":(0.5, 0.95),\n",
    " \"AF7\":(-0.65, 0.75), \"AF3\":(-0.35, 0.78), \"AFz\":(0.0, 0.80), \"AF4\":(0.35,0.78), \"AF8\":(0.65,0.75),\n",
    " \"F7\":(-0.8, 0.55), \"F5\":(-0.55,0.58), \"F3\":(-0.35,0.60), \"F1\":(-0.15,0.62), \"Fz\":(0.0,0.65),\n",
    " \"F2\":(0.15,0.62), \"F4\":(0.35,0.60), \"F6\":(0.55,0.58), \"F8\":(0.8,0.55),\n",
    " \"FT7\":(-0.9, 0.35), \"FC5\":(-0.6,0.40), \"FC3\":(-0.4,0.42), \"FC1\":(-0.2,0.44), \"FCz\":(0.0,0.45),\n",
    " \"FC2\":(0.2,0.44), \"FC4\":(0.4,0.42), \"FC6\":(0.6,0.40), \"FT8\":(0.9,0.35),\n",
    " \"T7\":(-1.0, 0.05), \"C5\":(-0.6,0.05), \"C3\":(-0.4,0.05), \"C1\":(-0.2,0.05), \"Cz\":(0.0,0.05),\n",
    " \"C2\":(0.2,0.05), \"C4\":(0.4,0.05), \"C6\":(0.6,0.05), \"T8\":(1.0,0.05),\n",
    " \"TP7\":(-0.9,-0.25), \"CP5\":(-0.6,-0.25), \"CP3\":(-0.4,-0.25), \"CP1\":(-0.2,-0.25), \"CPz\":(0.0,-0.25),\n",
    " \"CP2\":(0.2,-0.25), \"CP4\":(0.4,-0.25), \"CP6\":(0.6,-0.25), \"TP8\":(0.9,-0.25),\n",
    " \"P7\":(-0.8,-0.50), \"P5\":(-0.55,-0.50), \"P3\":(-0.35,-0.50), \"P1\":(-0.15,-0.50), \"Pz\":(0.0,-0.52),\n",
    " \"P2\":(0.15,-0.50), \"P4\":(0.35,-0.50), \"P6\":(0.55,-0.50), \"P8\":(0.8,-0.50),\n",
    " \"PO7\":(-0.65,-0.70), \"PO3\":(-0.35,-0.70), \"POz\":(0.0,-0.72), \"PO4\":(0.35,-0.70), \"PO8\":(0.65,-0.70),\n",
    " \"O1\":(-0.4,-0.90), \"Oz\":(0.0,-0.92), \"O2\":(0.4,-0.90)\n",
    "}\n",
    "\n",
    "# Common aliases between 10-20 and older 10-10 naming\n",
    "alias = {\n",
    " \"T3\":\"T7\", \"T4\":\"T8\", \"T5\":\"P7\", \"T6\":\"P8\",\n",
    " \"FP1\":\"Fp1\", \"FP2\":\"Fp2\", \"FPZ\":\"Fpz\", \"OZ\":\"Oz\", \"CZ\":\"Cz\", \"PZ\":\"Pz\", \"FZ\":\"Fz\", \"POZ\":\"POz\"\n",
    "}\n",
    "def norm_key(s):\n",
    "    return re.sub(r\"[^A-Za-z0-9]\", \"\", s).upper()\n",
    "\n",
    "canon_keys = {norm_key(k):k for k in canon_xy.keys()}\n",
    "alias_keys = {norm_key(k):v for k,v in alias.items()}\n",
    "\n",
    "# Load channel names\n",
    "if os.path.exists(CHAN_TXT):\n",
    "    with open(CHAN_TXT,\"r\",encoding=\"utf-8\") as f:\n",
    "        ch_names = [ln.strip() for ln in f if ln.strip()]\n",
    "else:\n",
    "    ch_names = [f\"ch{i}\" for i in range(64)]  # fallback\n",
    "\n",
    "# Map each channel to a 2D coordinate (drop if unknown)\n",
    "coords = []\n",
    "keep_idx = []\n",
    "for i, ch in enumerate(ch_names):\n",
    "    key = norm_key(ch)\n",
    "    if key in canon_keys:\n",
    "        coords.append(canon_xy[canon_keys[key]]); keep_idx.append(i)\n",
    "    elif key in alias_keys and alias_keys[key] in canon_xy:\n",
    "        coords.append(canon_xy[alias_keys[key]]); keep_idx.append(i)\n",
    "    else:\n",
    "        # try loose matches like Fp1 -> FP1 or P3 -> P3 etc by removing trailing spaces\n",
    "        m = re.match(r\"(FP|AF|F|FT|FC|T|C|TP|CP|P|PO|O)Z?$\", key)\n",
    "        # if completely unknown, skip\n",
    "        # print(f\"[skip] no coords for {ch}\")\n",
    "        pass\n",
    "\n",
    "coords = np.array(coords, float)\n",
    "if coords.shape[0] == 0:\n",
    "    raise SystemExit(\"Could not map any channels to 10–20 positions. Update alias map or CHAN_TXT.\")\n",
    "\n",
    "def draw_head(ax):\n",
    "    # head outline\n",
    "    head = plt.Circle((0,0), 1.03, fill=False, linewidth=2)\n",
    "    nose = plt.Polygon([[ -0.12, 1.03],[0,1.15],[0.12,1.03]], fill=False)\n",
    "    ax.add_patch(head); ax.add_patch(nose)\n",
    "    ax.set_xlim(-1.15,1.15); ax.set_ylim(-1.1,1.2)\n",
    "    ax.set_aspect(\"equal\"); ax.axis(\"off\")\n",
    "\n",
    "def plot_clusters(cons_labels, band):\n",
    "    labs = np.array(cons_labels)\n",
    "    labs = labs[keep_idx]  # align to coords list\n",
    "    fig = plt.figure(figsize=(7,7))\n",
    "    ax = fig.add_subplot(111)\n",
    "    draw_head(ax)\n",
    "    # cluster 0\n",
    "    m0 = labs == 0\n",
    "    ax.scatter(coords[m0,0], coords[m0,1], s=70, label=\"Cluster 0\")\n",
    "    # cluster 1\n",
    "    m1 = labs == 1\n",
    "    ax.scatter(coords[m1,0], coords[m1,1], s=70, marker=\"s\", label=\"Cluster 1\")\n",
    "    # annotate a few landmarks to help orientation\n",
    "    for name in [\"Fpz\",\"Fz\",\"Cz\",\"Pz\",\"Oz\"]:\n",
    "        k = norm_key(name)\n",
    "        if k in canon_keys:\n",
    "            x,y = canon_xy[canon_keys[k]]\n",
    "            ax.text(x, y+0.03, name, ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    ax.set_title(f\"Scalp clusters | {band} (PLI consensus, k=2)\")\n",
    "    out = os.path.join(FIGDIR, f\"scalp__{band}__clusters.png\")\n",
    "    fig.tight_layout(); fig.savefig(out, dpi=160); plt.close(fig)\n",
    "    print(\"Saved:\", out)\n",
    "\n",
    "# Load and plot per band\n",
    "for b in BANDS:\n",
    "    lab_fp = os.path.join(TABDIR, f\"band__{b}__consensus_labels.npy\")\n",
    "    if not os.path.exists(lab_fp):\n",
    "        print(f\"[skip] missing labels for {b}\"); continue\n",
    "    cons = np.load(lab_fp)\n",
    "    plot_clusters(cons, b)\n",
    "\n",
    "print(\"\\nDone. If some points are missing, add aliases in the `alias` dict above and re-run.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c13bd250-cd4e-4805-830c-d8b169d3f844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 01: ok (1 files)\n",
      "Subject 02: ok (1 files)\n",
      "Subject 03: ok (1 files)\n",
      "Subject 04: ok (1 files)\n",
      "Subject 05: ok (1 files)\n",
      "Subject 06: ok (1 files)\n",
      "Subject 07: ok (1 files)\n",
      "Subject 08: ok (1 files)\n",
      "Subject 09: ok (1 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S011/S011R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S011/S011R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 10: ok (1 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S012/S012R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S012/S012R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 11: ok (1 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S013/S013R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S013/S013R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 12: ok (1 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S014/S014R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S014/S014R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 13: ok (1 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S015/S015R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S015/S015R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 14: ok (1 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S016/S016R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S016/S016R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 15: ok (1 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S017/S017R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S017/S017R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 16: ok (1 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S018/S018R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S018/S018R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 17: ok (1 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S019/S019R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S019/S019R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 18: ok (1 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S020/S020R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S020/S020R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 19: ok (1 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S021/S021R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S021/S021R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 20: ok (1 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S022/S022R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S022/S022R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 21: ok (1 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S023/S023R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S023/S023R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 22: ok (1 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S024/S024R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S024/S024R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 23: ok (1 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S025/S025R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S025/S025R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 24: ok (1 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S026/S026R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S026/S026R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 25: ok (1 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S027/S027R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S027/S027R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 26: ok (1 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S028/S028R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S028/S028R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 27: ok (1 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S029/S029R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S029/S029R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 28: ok (1 files)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading file 'S030/S030R02.edf' from 'https://physionet.org/files/eegmmidb/1.0.0/S030/S030R02.edf' to 'C:\\Users\\caleb\\mne_data\\MNE-eegbci-data\\files\\eegmmidb\\1.0.0'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 29: ok (1 files)\n",
      "Subject 30: ok (1 files)\n",
      "All 30 EC runs downloaded.\n"
     ]
    }
   ],
   "source": [
    "# Quick cell to download EC run (R02) for subjects 1–30\n",
    "import mne, os\n",
    "OUT = r\"C:\\Users\\caleb\\CNT_Lab\\eeg_rest\"\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "for subj in range(1,31):\n",
    "    try:\n",
    "        fns = mne.datasets.eegbci.load_data(subjects=[subj], runs=[2], update_path=True, verbose=\"ERROR\")\n",
    "        print(f\"Subject {subj:02d}: ok ({len(fns)} files)\")\n",
    "    except Exception as e:\n",
    "        print(f\"Subject {subj:02d}: {e}\")\n",
    "print(\"All 30 EC runs downloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a6503d2-b515-47c0-9a4a-b62e22bdf388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PART A: exporting missing subjects to NPY ===\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_types() is a legacy function. New code should use inst.pick(...).\n",
      "Export summary: [(1, 'exists'), (2, 'exists'), (3, 'exists'), (4, 'exists'), (5, 'exists'), (6, 'exists'), (7, 'exists'), (8, 'exists'), (9, 'exists'), (10, 'exists')] ...\n",
      "\n",
      "=== PART B: running 30-subject PLI consensus sweep ===\n",
      "[alpha] null 100/500\n",
      "[alpha] null 200/500\n",
      "[alpha] null 300/500\n",
      "[alpha] null 400/500\n",
      "[alpha] null 500/500\n",
      "[theta] null 100/500\n",
      "[theta] null 200/500\n",
      "[theta] null 300/500\n",
      "[theta] null 400/500\n",
      "[theta] null 500/500\n",
      "[beta] null 100/500\n",
      "[beta] null 200/500\n",
      "[beta] null 300/500\n",
      "[beta] null 400/500\n",
      "[beta] null 500/500\n",
      "\n",
      "=== 30-subject PLI spectral-on-coassoc — Summary ===\n",
      " band  n_subjects cluster_sizes     LOSO  null_ari_mean  p_value  intra_over_inter\n",
      "alpha          30      [35, 29] 0.968747       0.000011 0.001996          2.486306\n",
      "theta          30      [28, 36] 0.937500       0.000480 0.001996          1.936463\n",
      " beta          30      [30, 34] 1.000000      -0.000352 0.001996          5.932711\n",
      "\n",
      "Saved:\n",
      "  - Summary CSV: C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_30_subjects\\summary.csv\n",
      "  - Metrics JSON: C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_30_subjects\\metrics\n",
      "  - Tables NPY : C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_30_subjects\\tables\n",
      "  - Figures    : C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_30_subjects\\figures\n"
     ]
    }
   ],
   "source": [
    "# === CNT PLI Consensus — 30-subject Controller (single cell) ===\n",
    "# 1) EDF → NPY export (only for subjects missing NPY)\n",
    "# 2) PLI + spectral-on-coassoc (k=2) sweep for α/θ/β across 30 subjects\n",
    "# Outputs: C:\\Users\\caleb\\CNT_Lab\\artifacts\\pli_30_subjects\\{metrics, tables, figures, summary.csv}\n",
    "\n",
    "import os, glob, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- YOU CAN TWEAK THESE ----------\n",
    "ROOT       = r\"C:\\Users\\caleb\\CNT_Lab\"\n",
    "DATA_OUT   = os.path.join(ROOT, \"eeg_rest\")   # where subject_##_EC.npy live\n",
    "SUBJECTS   = list(range(1,31))                # 1..30\n",
    "FS_OUT     = 250.0                            # resample Hz\n",
    "DURATION_S = 60                               # seconds kept per subject\n",
    "HP, LP     = 1.0, 45.0                        # bandpass for clean PLI\n",
    "BANDS_HZ   = { \"alpha\": (8.0,13.0), \"theta\": (4.0,8.0), \"beta\": (13.0,30.0) }\n",
    "\n",
    "# Consensus/Null params\n",
    "K_FIXED    = 2                                 # force non-trivial split\n",
    "KNN_K      = 6                                 # try 4–7 if needed\n",
    "NULL_PERMS = 500                               # raise to 1000 for paper-grade\n",
    "\n",
    "OUT_ROOT   = os.path.join(ROOT, r\"artifacts\\pli_30_subjects\")\n",
    "# -----------------------------------------\n",
    "\n",
    "os.makedirs(DATA_OUT, exist_ok=True)\n",
    "os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "OUT_MET = os.path.join(OUT_ROOT, \"metrics\")\n",
    "OUT_TAB = os.path.join(OUT_ROOT, \"tables\")\n",
    "OUT_FIG = os.path.join(OUT_ROOT, \"figures\")\n",
    "for p in [OUT_MET, OUT_TAB, OUT_FIG]:\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "# ========== PART A: EDF → NPY (skip when exists) ==========\n",
    "print(\"=== PART A: exporting missing subjects to NPY ===\")\n",
    "try:\n",
    "    import mne\n",
    "except Exception:\n",
    "    import sys, subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"mne\", \"pooch\"])\n",
    "    import mne\n",
    "\n",
    "def export_subject_ec(subj, out_dir):\n",
    "    runs = [2]  # eyes-closed\n",
    "    try:\n",
    "        fpaths = mne.datasets.eegbci.load_data(subjects=[subj], runs=runs, update_path=True, verbose=\"ERROR\")\n",
    "    except TypeError:\n",
    "        fpaths = mne.datasets.eegbci.load_data(subject=subj, runs=runs, update_path=True, verbose=\"ERROR\")\n",
    "    raws=[]\n",
    "    for fp in fpaths:\n",
    "        raw = mne.io.read_raw_edf(fp, preload=True, verbose=\"ERROR\")\n",
    "        raw.pick_types(eeg=True, stim=False, eog=False, ecg=False, emg=False, misc=False)\n",
    "        raws.append(raw)\n",
    "    if not raws: \n",
    "        return False, \"no_raw\"\n",
    "    raw = mne.concatenate_raws(raws, verbose=\"ERROR\")\n",
    "    # montage + filter + resample\n",
    "    try:\n",
    "        raw.set_montage(\"standard_1020\", on_missing=\"ignore\", match_case=False, verbose=\"ERROR\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    raw.filter(HP, LP, fir_design=\"firwin\", verbose=\"ERROR\")\n",
    "    raw.resample(FS_OUT, npad=\"auto\", verbose=\"ERROR\")\n",
    "    # trim/tile to DURATION_S\n",
    "    n_keep = int(DURATION_S * raw.info[\"sfreq\"])\n",
    "    X = raw.get_data(picks=\"eeg\")            # [n_ch, n_t]\n",
    "    if X.shape[1] >= n_keep:\n",
    "        X = X[:, :n_keep]\n",
    "    else:\n",
    "        reps = int(np.ceil(n_keep / X.shape[1]))\n",
    "        X = np.tile(X, reps)[:, :n_keep]\n",
    "    ch_names = mne.pick_info(raw.info, mne.pick_types(raw.info, eeg=True)).ch_names\n",
    "    base = os.path.join(out_dir, f\"subject_{subj:02d}_EC\")\n",
    "    np.save(base + \".npy\", X.astype(np.float32))\n",
    "    with open(base + \".channels.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for ch in ch_names: f.write(ch + \"\\n\")\n",
    "    return True, X.shape\n",
    "\n",
    "export_log=[]\n",
    "for s in SUBJECTS:\n",
    "    base = os.path.join(DATA_OUT, f\"subject_{s:02d}_EC.npy\")\n",
    "    if os.path.exists(base):\n",
    "        export_log.append((s, \"exists\"))\n",
    "    else:\n",
    "        ok, msg = export_subject_ec(s, DATA_OUT)\n",
    "        export_log.append((s, msg))\n",
    "print(\"Export summary:\", export_log[:10], \"...\" if len(export_log)>10 else \"\")\n",
    "\n",
    "# ========== PART B: PLI spectral-on-coassoc (k=2) ==========\n",
    "from scipy.signal import butter, filtfilt, hilbert\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "def bandpass(x,fs,lo,hi,order=4):\n",
    "    b,a = butter(order, [lo/(fs/2), hi/(fs/2)], btype=\"band\"); return filtfilt(b,a,x)\n",
    "\n",
    "def pli_matrix(X,fs,lo,hi):\n",
    "    n = X.shape[0]; Y = np.zeros_like(X)\n",
    "    for c in range(n): Y[c] = bandpass(X[c],fs,lo,hi)\n",
    "    ph = np.angle(hilbert(Y, axis=1))\n",
    "    W = np.zeros((n,n), float)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            dphi = ph[i]-ph[j]\n",
    "            pli  = np.abs(np.mean(np.sign(np.sin(dphi))))\n",
    "            W[i,j] = W[j,i] = pli\n",
    "    np.fill_diagonal(W,0); return W\n",
    "\n",
    "def knn(W,k=6):\n",
    "    W=W.copy(); n=W.shape[0]\n",
    "    for i in range(n):\n",
    "        idx = np.argsort(W[i])[::-1]; keep = idx[:k]\n",
    "        mask = np.ones(n, bool); mask[keep] = False\n",
    "        W[i, mask] = 0.0\n",
    "    W = np.maximum(W, W.T); np.fill_diagonal(W,0); return W\n",
    "\n",
    "def laplacian(W):\n",
    "    d = W.sum(1); d = np.where(d<=1e-12,1.0,d); Dmh=np.diag(1.0/np.sqrt(d))\n",
    "    return np.eye(W.shape[0]) - Dmh @ W @ Dmh\n",
    "\n",
    "def spec_labels(W,k):\n",
    "    L = laplacian(W); evals,evecs = np.linalg.eigh(L)\n",
    "    U = evecs[:,1:k] if k>1 else evecs[:,:1]\n",
    "    U = U / (np.linalg.norm(U,axis=1,keepdims=True)+1e-12)\n",
    "    return KMeans(n_clusters=k, n_init=50, random_state=42).fit_predict(U)\n",
    "\n",
    "def build_coassoc(label_list):\n",
    "    n=len(label_list[0]); m=len(label_list); co=np.zeros((n,n),float)\n",
    "    for lab in label_list:\n",
    "        for i in range(n):\n",
    "            li=lab[i]\n",
    "            for j in range(n):\n",
    "                co[i,j]+=1.0 if li==lab[j] else 0.0\n",
    "    return co/m\n",
    "\n",
    "def loso_ari_via_coassoc(label_list):\n",
    "    co_full = build_coassoc(label_list)\n",
    "    cons_full= spec_labels(co_full, k=2)\n",
    "    aris=[]\n",
    "    for s in range(len(label_list)):\n",
    "        leave=[lab for i,lab in enumerate(label_list) if i!=s]\n",
    "        co_l = build_coassoc(leave)\n",
    "        cons_l= spec_labels(co_l, k=2)\n",
    "        aris.append(adjusted_rand_score(cons_full, cons_l))\n",
    "    return float(np.median(aris)), cons_full, co_full\n",
    "\n",
    "def randomize_labels_same_sizes(lab, rng):\n",
    "    n=len(lab); uniq,cnts=np.unique(lab, return_counts=True)\n",
    "    idx=np.arange(n); rng.shuffle(idx)\n",
    "    out=np.empty(n,int); start=0\n",
    "    for label,c in zip(uniq,cnts):\n",
    "        seg=idx[start:start+c]; out[seg]=label; start+=c\n",
    "    return out\n",
    "\n",
    "print(\"\\n=== PART B: running 30-subject PLI consensus sweep ===\")\n",
    "files = [os.path.join(DATA_OUT, f\"subject_{s:02d}_EC.npy\") for s in SUBJECTS if os.path.exists(os.path.join(DATA_OUT, f\"subject_{s:02d}_EC.npy\"))]\n",
    "if len(files) < 30:\n",
    "    print(f\"[warn] only {len(files)} NPY found; proceeding with available subjects.\")\n",
    "\n",
    "rows = []\n",
    "rng = np.random.default_rng(7)\n",
    "\n",
    "for band, (lo,hi) in BANDS_HZ.items():\n",
    "    # per-subject labels\n",
    "    subj_labels=[]\n",
    "    for f in files:\n",
    "        X = np.load(f)\n",
    "        W = pli_matrix(X, FS_OUT, lo, hi)\n",
    "        W = knn(W, KNN_K)\n",
    "        labs = spec_labels(W, K_FIXED)\n",
    "        subj_labels.append(labs)\n",
    "\n",
    "    # consensus via spectral-on-coassoc\n",
    "    loso, cons_real, co_real = loso_ari_via_coassoc(subj_labels)\n",
    "    np.save(os.path.join(OUT_TAB, f\"band__{band}__coassoc.npy\"), co_real)\n",
    "    np.save(os.path.join(OUT_TAB, f\"band__{band}__consensus_labels.npy\"), cons_real)\n",
    "\n",
    "    # null (label-preserving)\n",
    "    null_aris=[]\n",
    "    for p in range(NULL_PERMS):\n",
    "        nlabs=[randomize_labels_same_sizes(lab, rng) for lab in subj_labels]\n",
    "        _, cons_n, _ = loso_ari_via_coassoc(nlabs)\n",
    "        null_aris.append(adjusted_rand_score(cons_real, cons_n))\n",
    "        if (p+1) % 100 == 0:\n",
    "            print(f\"[{band}] null {p+1}/{NULL_PERMS}\")\n",
    "\n",
    "    null_aris = np.array(null_aris, float)\n",
    "    p_val = float((np.sum(null_aris >= loso) + 1) / (len(null_aris) + 1))\n",
    "\n",
    "    uniq,cnts = np.unique(cons_real, return_counts=True)\n",
    "    intra = co_real[cons_real[:,None]==cons_real[None,:]].mean()\n",
    "    inter = co_real[cons_real[:,None]!=cons_real[None,:]].mean()\n",
    "    ratio = float(intra/(inter+1e-12))\n",
    "\n",
    "    metrics = {\n",
    "        \"band\": band, \"connectivity\":\"PLI\", \"consensus_mode\":\"spectral_on_coassoc_k2\",\n",
    "        \"n_subjects\": len(files), \"loso_coassoc\": float(loso),\n",
    "        \"null_ari_mean\": float(null_aris.mean()), \"p_value\": p_val,\n",
    "        \"cluster_sizes\": cnts.tolist(), \"intra_mean\": float(intra),\n",
    "        \"inter_mean\": float(inter), \"intra_over_inter\": ratio\n",
    "    }\n",
    "    with open(os.path.join(OUT_MET, f\"band__{band}__metrics.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "\n",
    "    # quick figure\n",
    "    plt.figure(); plt.imshow(co_real, aspect='auto'); plt.title(f\"Co-association (spectral) | {band} (PLI, n={len(files)})\")\n",
    "    plt.colorbar(); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_FIG, f\"coassoc__{band}.png\"), dpi=160); plt.close()\n",
    "\n",
    "    rows.append([band, len(files), cnts.tolist(), float(loso), float(null_aris.mean()), p_val, ratio])\n",
    "\n",
    "# summary CSV + print\n",
    "df = pd.DataFrame(rows, columns=[\"band\",\"n_subjects\",\"cluster_sizes\",\"LOSO\",\"null_ari_mean\",\"p_value\",\"intra_over_inter\"])\n",
    "csv_path = os.path.join(OUT_ROOT, \"summary.csv\"); df.to_csv(csv_path, index=False)\n",
    "print(\"\\n=== 30-subject PLI spectral-on-coassoc — Summary ===\")\n",
    "print(df.to_string(index=False))\n",
    "print(\"\\nSaved:\")\n",
    "print(\"  - Summary CSV:\", csv_path)\n",
    "print(\"  - Metrics JSON:\", OUT_MET)\n",
    "print(\"  - Tables NPY :\", OUT_TAB)\n",
    "print(\"  - Figures    :\", OUT_FIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef17f73-f394-4a80-8690-27229c69a592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
