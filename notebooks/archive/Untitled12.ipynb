{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99047277-dd22-405f-8873-1d6f87fff4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_26492\\3651559310.py:10: DeprecationWarning: 'pkgutil.find_loader' is deprecated and slated for removal in Python 3.14; use importlib.util.find_spec() instead\n",
      "  if pkgutil.find_loader(p.split(\"==\")[0]) is None:\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Error while finding loader for 'transformers>=4.44.0' (<class 'ModuleNotFoundError'>: No module named 'transformers>=4')",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pkgutil.py:309\u001b[39m, in \u001b[36mfind_loader\u001b[39m\u001b[34m(fullname)\u001b[39m\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     spec = \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m    311\u001b[39m     \u001b[38;5;66;03m# This hack fixes an impedance mismatch between pkgutil and\u001b[39;00m\n\u001b[32m    312\u001b[39m     \u001b[38;5;66;03m# importlib, where the latter raises other errors for cases where\u001b[39;00m\n\u001b[32m    313\u001b[39m     \u001b[38;5;66;03m# pkgutil previously raised ImportError\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib.util>:91\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(name, package)\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'transformers>=4'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m pkgutil.find_loader(p.split(\u001b[33m\"\u001b[39m\u001b[33m==\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m0\u001b[39m]) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     11\u001b[39m             subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39m\u001b[33m-m\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minstall\u001b[39m\u001b[33m\"\u001b[39m, p, \u001b[33m\"\u001b[39m\u001b[33m-q\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mpip_install\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtransformers>=4.44.0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtorch>=2.1.0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentencepiece\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maccelerate>=0.33.0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnumpy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpandas\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscikit-learn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msentence-transformers>=3.0.0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     21\u001b[39m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# 1) Imports\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mpip_install\u001b[39m\u001b[34m(pkgs)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpip_install\u001b[39m(pkgs):\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m pkgs:\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mpkgutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfind_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m==\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     11\u001b[39m             subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39m\u001b[33m-m\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minstall\u001b[39m\u001b[33m\"\u001b[39m, p, \u001b[33m\"\u001b[39m\u001b[33m-q\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\pkgutil.py:315\u001b[39m, in \u001b[36mfind_loader\u001b[39m\u001b[34m(fullname)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m    311\u001b[39m     \u001b[38;5;66;03m# This hack fixes an impedance mismatch between pkgutil and\u001b[39;00m\n\u001b[32m    312\u001b[39m     \u001b[38;5;66;03m# importlib, where the latter raises other errors for cases where\u001b[39;00m\n\u001b[32m    313\u001b[39m     \u001b[38;5;66;03m# pkgutil previously raised ImportError\u001b[39;00m\n\u001b[32m    314\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mError while finding loader for \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg.format(fullname, \u001b[38;5;28mtype\u001b[39m(ex), ex)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mex\u001b[39;00m\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m spec.loader \u001b[38;5;28;01mif\u001b[39;00m spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Error while finding loader for 'transformers>=4.44.0' (<class 'ModuleNotFoundError'>: No module named 'transformers>=4')"
     ]
    }
   ],
   "source": [
    "# === CNT :: Gauge-Restored Agents (GRA) — Invariance Smoke Test (Single Cell) ===\n",
    "# Purpose: Measure whether a model's answers are invariant under symbol-preserving prompt transformations.\n",
    "# Outputs: ./gra_runs/gra_run_<timestamp>/{run_card.json, results.csv, figures/violins.png (optional)}\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "# 0) Minimal installs (skips if available)\n",
    "import sys, subprocess, pkgutil, os, json, time, math, random, re, textwrap\n",
    "def pip_install(pkgs):\n",
    "    for p in pkgs:\n",
    "        if pkgutil.find_loader(p.split(\"==\")[0]) is None:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", p, \"-q\"])\n",
    "pip_install([\n",
    "    \"transformers>=4.44.0\",\n",
    "    \"torch>=2.1.0\",\n",
    "    \"sentencepiece\",\n",
    "    \"accelerate>=0.33.0\",\n",
    "    \"numpy\",\n",
    "    \"pandas\",\n",
    "    \"scikit-learn\",\n",
    "    \"sentence-transformers>=3.0.0\"\n",
    "])\n",
    "\n",
    "# 1) Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 2) Config — tiny, fast defaults; you can upgrade models later\n",
    "CFG = {\n",
    "    \"qa_model_name\": \"google/flan-t5-small\",          # swap to \"base\" or bigger if you want\n",
    "    \"embed_model_name\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"temperature\": 0.0,                               # keep outputs stable for invariance tests\n",
    "    \"top_p\": 1.0,\n",
    "    \"seed\": 42,\n",
    "    \"device\": \"cuda\" if (os.environ.get(\"CUDA_VISIBLE_DEVICES\",\"\") or\n",
    "                         (os.name==\"nt\")) else \"cpu\", # simple heuristic; HF pipeline still chooses device\n",
    "    \"transform_samples_per_item\": 8,                  # how many transformed prompts per base\n",
    "    \"pass_threshold\": 0.85,                           # invariance threshold (cosine on answers)\n",
    "}\n",
    "\n",
    "random.seed(CFG[\"seed\"])\n",
    "np.random.seed(CFG[\"seed\"])\n",
    "\n",
    "# 3) Load models\n",
    "print(\"Loading models...\")\n",
    "qa_tok = AutoTokenizer.from_pretrained(CFG[\"qa_model_name\"])\n",
    "qa_model = AutoModelForSeq2SeqLM.from_pretrained(CFG[\"qa_model_name\"])\n",
    "qa_pipe = pipeline(\"text2text-generation\",\n",
    "                   model=qa_model, tokenizer=qa_tok,\n",
    "                   max_new_tokens=CFG[\"max_new_tokens\"],\n",
    "                   do_sample=False)\n",
    "\n",
    "embed_model = SentenceTransformer(CFG[\"embed_model_name\"])\n",
    "print(\"Models ready.\")\n",
    "\n",
    "# 4) Transformations: symbol-preserving (order/format/noise) — tune/add freely\n",
    "#    Each transformation should preserve meaning while perturbing surface form.\n",
    "def t_whitespace(prompt):      # normalize/expand whitespace\n",
    "    s = re.sub(r\"\\s+\", \" \", prompt.strip())\n",
    "    return \"  \" + s + \"   \"\n",
    "def t_reorder_bullets(prompt): # reorder semantically neutral list clauses if present\n",
    "    bullets = re.findall(r\"(?:^|\\n)[\\-\\*\\•]\\s.*\", prompt, flags=re.M)\n",
    "    if len(bullets) >= 2:\n",
    "        body = re.sub(r\"(?:^|\\n)[\\-\\*\\•]\\s.*\", \"\", prompt, flags=re.M).strip()\n",
    "        random.shuffle(bullets)\n",
    "        return body + \"\\n\" + \"\\n\".join(bullets)\n",
    "    return prompt\n",
    "def t_synonyms_light(prompt):\n",
    "    # simple, deterministic swaps to avoid external resources\n",
    "    swaps = {\n",
    "        \"explain\": \"clarify\",\n",
    "        \"show\": \"demonstrate\",\n",
    "        \"why\": \"for what reason\",\n",
    "        \"how\": \"by what method\",\n",
    "        \"answer\": \"response\",\n",
    "        \"result\": \"outcome\",\n",
    "        \"list\": \"enumerate\",\n",
    "        \"compare\": \"contrast\",\n",
    "        \"benefits\": \"advantages\",\n",
    "        \"risks\": \"hazards\"\n",
    "    }\n",
    "    def repl(m): \n",
    "        w = m.group(0)\n",
    "        return swaps.get(w.lower(), w)\n",
    "    return re.sub(r\"\\b(\" + \"|\".join(map(re.escape, swaps.keys())) + r\")\\b\", repl, prompt, flags=re.I)\n",
    "def t_insert_nulls(prompt):    # insert neutral hedges that don't change semantics\n",
    "    hedges = [\n",
    "        \"Note: for clarity only, \",\n",
    "        \"In practical terms, \",\n",
    "        \"Briefly, \",\n",
    "        \"For operators, \",\n",
    "        \"In essence, \",\n",
    "    ]\n",
    "    parts = prompt.split(\". \")\n",
    "    if len(parts) < 2: \n",
    "        return \"In essence, \" + prompt\n",
    "    idx = min(len(parts)-1, max(1, len(parts)//2))\n",
    "    parts[idx] = random.choice(hedges) + parts[idx]\n",
    "    return \". \".join(parts)\n",
    "def t_case_mix(prompt):\n",
    "    return prompt[:1].upper() + prompt[1:].lower()\n",
    "def t_format_q(prompt):        # wrap as Q/A format\n",
    "    return f\"Q: {prompt.strip()}\\nA:\"\n",
    "def t_numbering(prompt):       # add harmless numbering\n",
    "    lines = [l for l in prompt.split(\"\\n\") if l.strip()]\n",
    "    if len(lines) >= 2:\n",
    "        return \"\\n\".join(f\"{i+1}. {l}\" for i,l in enumerate(lines))\n",
    "    return \"1. \" + prompt\n",
    "def t_parenthetical(prompt):   # add clarifying parenthetical\n",
    "    return prompt + \" (answer succinctly, focusing on the essential meaning).\"\n",
    "\n",
    "TRANSFORMS = [t_whitespace, t_reorder_bullets, t_synonyms_light, t_insert_nulls,\n",
    "              t_case_mix, t_format_q, t_numbering, t_parenthetical]\n",
    "\n",
    "# 5) Data — sample items (replace with your own tasks!)\n",
    "#    Each item: {\"id\": str, \"prompt\": str}\n",
    "ITEMS = [\n",
    "    {\"id\": \"math_01\", \"prompt\": \"Explain the Pythagorean theorem and give a one-sentence example.\"},\n",
    "    {\"id\": \"policy_01\", \"prompt\": \"List two benefits and two risks of deploying large language models in healthcare triage.\"},\n",
    "    {\"id\": \"cnt_01\", \"prompt\": \"In one paragraph, define gauge-restored agents and why invariance matters for safety.\"},\n",
    "]\n",
    "\n",
    "# 6) Helpers\n",
    "def call_model(prompt: str) -> str:\n",
    "    out = qa_pipe(prompt, num_return_sequences=1)[0][\"generated_text\"]\n",
    "    return out.strip()\n",
    "\n",
    "def embed(texts):\n",
    "    # return normalized embeddings\n",
    "    X = embed_model.encode(texts, normalize_embeddings=True)\n",
    "    return np.array(X)\n",
    "\n",
    "def invariance_score(base_answer: str, alt_answers: list) -> float:\n",
    "    vecs = embed([base_answer] + alt_answers)\n",
    "    base = vecs[0:1]\n",
    "    alts = vecs[1:]\n",
    "    sims = cosine_similarity(base, alts).flatten()\n",
    "    return float(np.mean(sims)), sims.tolist()\n",
    "\n",
    "# 7) Run\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "outdir = Path(f\"./gra_runs/gra_run_{ts}\")\n",
    "(outdir / \"figures\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "records = []\n",
    "print(\"\\nRunning GRA invariance smoke test...\")\n",
    "for item in ITEMS:\n",
    "    prompt = item[\"prompt\"]\n",
    "    base_answer = call_model(prompt)\n",
    "    # choose N transforms (can be all or sampled)\n",
    "    k = min(CFG[\"transform_samples_per_item\"], len(TRANSFORMS))\n",
    "    tset = random.sample(TRANSFORMS, k)\n",
    "    alt_prompts = [t(prompt) for t in tset]\n",
    "    alt_answers = [call_model(p) for p in alt_prompts]\n",
    "    mean_sim, sims = invariance_score(base_answer, alt_answers)\n",
    "    passed = (mean_sim >= CFG[\"pass_threshold\"])\n",
    "    for t_name, ap, aa, s in zip([f.__name__ for f in tset], alt_prompts, alt_answers, sims):\n",
    "        records.append({\n",
    "            \"item_id\": item[\"id\"],\n",
    "            \"transform\": t_name,\n",
    "            \"base_prompt\": prompt,\n",
    "            \"alt_prompt\": ap,\n",
    "            \"base_answer\": base_answer,\n",
    "            \"alt_answer\": aa,\n",
    "            \"sim_cosine\": float(s),\n",
    "            \"pass_threshold\": CFG[\"pass_threshold\"],\n",
    "        })\n",
    "    print(f\"- {item['id']}: invariance={mean_sim:.3f}  [{'PASS' if passed else 'FAIL'}]\")\n",
    "\n",
    "# 8) Aggregate & save\n",
    "df = pd.DataFrame(records)\n",
    "if not df.empty:\n",
    "    agg = (df.groupby(\"item_id\")[\"sim_cosine\"]\n",
    "             .agg([\"mean\",\"std\",\"min\",\"max\",\"count\"])\n",
    "             .reset_index()\n",
    "             .rename(columns={\"mean\":\"invariance_mean\"}))\n",
    "else:\n",
    "    agg = pd.DataFrame(columns=[\"item_id\",\"invariance_mean\",\"std\",\"min\",\"max\",\"count\"])\n",
    "\n",
    "csv_path = outdir / \"results.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "# 9) Run card\n",
    "run_card = {\n",
    "    \"timestamp\": ts,\n",
    "    \"cfg\": CFG,\n",
    "    \"n_items\": len(ITEMS),\n",
    "    \"n_records\": int(len(df)),\n",
    "    \"summary\": agg.to_dict(orient=\"records\"),\n",
    "    \"threshold_pass_rate\": float(np.mean((agg[\"invariance_mean\"] >= CFG[\"pass_threshold\"]).astype(float))) if len(agg) else None,\n",
    "    \"notes\": {\n",
    "        \"definition\": \"Invariance = mean cosine similarity of base answer vs answers under symbol-preserving prompt transforms.\",\n",
    "        \"kill_shot\": f\"FAIL if invariance_mean < {CFG['pass_threshold']} on >=30% of items.\",\n",
    "        \"upgrade_paths\": [\n",
    "            \"Use larger QA model or task-specific heads.\",\n",
    "            \"Add backtranslation (offline MT) or synthetic paraphrasers.\",\n",
    "            \"Evaluate exactness where applicable (e.g., multiple choice).\",\n",
    "            \"Bucket transforms to find worst-case fragility.\"\n",
    "        ],\n",
    "    }\n",
    "}\n",
    "with open(outdir / \"run_card.json\", \"w\") as f:\n",
    "    json.dump(run_card, f, indent=2)\n",
    "\n",
    "# 10) Pretty print summary\n",
    "print(\"\\n=== GRA Invariance Summary ===\")\n",
    "if len(agg):\n",
    "    for row in agg.itertuples(index=False):\n",
    "        verdict = \"PASS\" if row.invariance_mean >= CFG[\"pass_threshold\"] else \"FAIL\"\n",
    "        print(f\"{row.item_id:>10} | mean={row.invariance_mean:.3f} (min={row.min:.3f}, max={row.max:.3f}, n={int(row.count)})  -> {verdict}\")\n",
    "    pass_rate = (agg[\"invariance_mean\"] >= CFG[\"pass_threshold\"]).mean()\n",
    "    print(f\"\\nOverall pass rate (items >= {CFG['pass_threshold']:.2f}): {pass_rate*100:.1f}%\")\n",
    "else:\n",
    "    print(\"No records. Check ITEMS list.\")\n",
    "\n",
    "print(f\"\\nSaved:\\n - {csv_path}\\n - {outdir/'run_card.json'}\")\n",
    "\n",
    "# (Optional) If you want a quick plot later, add matplotlib to CFG and plot distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8bf856-aa10-4acc-b0e7-0b54b6f589cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models…\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3b9e8f0b6ae4f61b99c37391bb7d72f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51cb134db9f42d9b84986bd6bee369a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a94647a507604978a234d9fbd0b213e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ffee851ed944ebbbf37983fdb0f6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508dcf7ac2ec43b1aff0588ffa3f4c8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25386e70253a4ce4907010f9edf24ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f164d5c4a749f9895fc9df0744e98a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ff12072393f420fb2f63414cb4382a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c46f5f1ca84f069134e7440a6b734c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e79fe9d7574740a0eff2e6cbda323e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8722a49048f24333884501b16b1039dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5283b0707aa946d09798179b1bdf2427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95f9bad3f9144a38a11732c351bb697a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956ad02540874e20800c155c6dc6ae78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94241794ab9d4c9d8bcd3e04f6cd0abd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8635e06ea3c4700aeb43b16f127d9e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031e300dce6c4b60a27b909103dd5d60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4381848128c8436fb3bba44e37ce1ed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models ready.\n",
      "\n",
      "Running GRA invariance smoke test...\n",
      "- math_01: invariance=0.811 [FAIL]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- policy_01: invariance=0.946 [PASS]\n",
      "- cnt_01: invariance=0.946 [PASS]\n",
      "\n",
      "=== GRA Invariance Summary ===\n",
      "    cnt_01 | mean=0.946 (min=0.693, max=1.000, n=8)  -> PASS\n",
      "   math_01 | mean=0.811 (min=0.678, max=1.000, n=8)  -> FAIL\n",
      " policy_01 | mean=0.946 (min=0.867, max=1.000, n=8)  -> PASS\n",
      "\n",
      "Saved:\n",
      " - gra_runs\\gra_run_20251015-201254\\results.csv\n",
      " - gra_runs\\gra_run_20251015-201254\\run_card.json\n"
     ]
    }
   ],
   "source": [
    "# === CNT :: GRA Invariance Smoke Test — Safe Installer (Py 3.13-ready) ===\n",
    "# This cell replaces the old installer logic that used pkgutil.find_loader.\n",
    "# It uses importlib.util.find_spec() on the *import name* and only then installs the *package spec*.\n",
    "\n",
    "import os, sys, subprocess, importlib.util, shutil, json, random, re, time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "def need(import_name: str) -> bool:\n",
    "    \"\"\"Return True if module is missing.\"\"\"\n",
    "    return importlib.util.find_spec(import_name) is None\n",
    "\n",
    "def sh(*args, **kwargs):\n",
    "    kwargs.setdefault(\"check\", True)\n",
    "    return subprocess.run(list(args), **kwargs)\n",
    "\n",
    "def pip_install(spec: str, extra_args=None, quiet=True):\n",
    "    \"\"\"Install a package spec with pip if not present (use with need()).\"\"\"\n",
    "    cmd = [sys.executable, \"-m\", \"pip\", \"install\", spec]\n",
    "    if extra_args:\n",
    "        cmd += extra_args\n",
    "    if quiet:\n",
    "        cmd += [\"-q\"]\n",
    "    # Best-effort retry once\n",
    "    try:\n",
    "        sh(*cmd)\n",
    "    except subprocess.CalledProcessError:\n",
    "        time.sleep(2)\n",
    "        sh(*cmd)\n",
    "\n",
    "# ---- Torch channel detection (Windows / CUDA vs CPU) ----\n",
    "TORCH_ARGS = []\n",
    "if os.name == \"nt\":\n",
    "    # Prefer official wheels; if CUDA GPU is present, try CUDA 12.x channel; otherwise CPU.\n",
    "    try:\n",
    "        has_nvidia = shutil.which(\"nvidia-smi\") is not None and sh(\"nvidia-smi\", stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL, check=False).returncode == 0\n",
    "    except Exception:\n",
    "        has_nvidia = False\n",
    "    if has_nvidia:\n",
    "        # If you know your CUDA (e.g., 12.1), this channel works well:\n",
    "        TORCH_ARGS = [\"--index-url\", \"https://download.pytorch.org/whl/cu121\"]\n",
    "    else:\n",
    "        TORCH_ARGS = [\"--index-url\", \"https://download.pytorch.org/whl/cpu\"]\n",
    "\n",
    "# ---- Install missing deps (import name -> package spec) ----\n",
    "REQS = [\n",
    "    # (import_name, package_spec, extra_pip_args)\n",
    "    (\"numpy\",                \"numpy\",                      None),\n",
    "    (\"pandas\",               \"pandas\",                     None),\n",
    "    (\"sklearn\",              \"scikit-learn\",               None),\n",
    "    (\"sentencepiece\",        \"sentencepiece\",              None),\n",
    "    (\"transformers\",         \"transformers>=4.44.0\",       None),\n",
    "    (\"accelerate\",           \"accelerate>=0.33.0\",         None),\n",
    "    (\"torch\",                \"torch\",                      TORCH_ARGS),   # channel decided above\n",
    "    (\"sentence_transformers\",\"sentence-transformers>=3.0\", None),\n",
    "]\n",
    "\n",
    "for import_name, spec, extra in REQS:\n",
    "    if need(import_name):\n",
    "        pip_install(spec, extra_args=extra)\n",
    "\n",
    "# ---- Now do the rest of the GRA test exactly as intended ----\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "CFG = {\n",
    "    \"qa_model_name\": \"google/flan-t5-small\",\n",
    "    \"embed_model_name\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"max_new_tokens\": 128,\n",
    "    \"seed\": 42,\n",
    "    \"transform_samples_per_item\": 8,\n",
    "    \"pass_threshold\": 0.85,\n",
    "}\n",
    "random.seed(CFG[\"seed\"])\n",
    "np.random.seed(CFG[\"seed\"])\n",
    "\n",
    "print(\"Loading models…\")\n",
    "qa_tok = AutoTokenizer.from_pretrained(CFG[\"qa_model_name\"])\n",
    "qa_model = AutoModelForSeq2SeqLM.from_pretrained(CFG[\"qa_model_name\"])\n",
    "qa_pipe = pipeline(\"text2text-generation\", model=qa_model, tokenizer=qa_tok, max_new_tokens=CFG[\"max_new_tokens\"], do_sample=False)\n",
    "embed_model = SentenceTransformer(CFG[\"embed_model_name\"])\n",
    "print(\"Models ready.\")\n",
    "\n",
    "# --- transforms (unchanged) ---\n",
    "import re, textwrap\n",
    "def t_whitespace(prompt):  return \"  \" + re.sub(r\"\\s+\", \" \", prompt.strip()) + \"   \"\n",
    "def t_reorder_bullets(prompt):\n",
    "    bullets = re.findall(r\"(?:^|\\n)[\\-\\*\\•]\\s.*\", prompt, flags=re.M)\n",
    "    if len(bullets) >= 2:\n",
    "        body = re.sub(r\"(?:^|\\n)[\\-\\*\\•]\\s.*\", \"\", prompt, flags=re.M).strip()\n",
    "        random.shuffle(bullets)\n",
    "        return body + \"\\n\" + \"\\n\".join(bullets)\n",
    "    return prompt\n",
    "def t_synonyms_light(prompt):\n",
    "    swaps = {\"explain\":\"clarify\",\"show\":\"demonstrate\",\"why\":\"for what reason\",\"how\":\"by what method\",\"answer\":\"response\",\"result\":\"outcome\",\"list\":\"enumerate\",\"compare\":\"contrast\",\"benefits\":\"advantages\",\"risks\":\"hazards\"}\n",
    "    return re.sub(r\"\\b(\" + \"|\".join(map(re.escape, swaps.keys())) + r\")\\b\", lambda m: swaps.get(m.group(0).lower(), m.group(0)), prompt, flags=re.I)\n",
    "def t_insert_nulls(prompt):\n",
    "    hedges = [\"Note: for clarity only, \",\"In practical terms, \",\"Briefly, \",\"For operators, \",\"In essence, \"]\n",
    "    parts = prompt.split(\". \")\n",
    "    if len(parts) < 2: return \"In essence, \" + prompt\n",
    "    idx = min(len(parts)-1, max(1, len(parts)//2))\n",
    "    parts[idx] = random.choice(hedges) + parts[idx]\n",
    "    return \". \".join(parts)\n",
    "def t_case_mix(prompt):   return prompt[:1].upper() + prompt[1:].lower()\n",
    "def t_format_q(prompt):   return f\"Q: {prompt.strip()}\\nA:\"\n",
    "def t_numbering(prompt):\n",
    "    lines = [l for l in prompt.split(\"\\n\") if l.strip()]\n",
    "    if len(lines) >= 2: return \"\\n\".join(f\"{i+1}. {l}\" for i,l in enumerate(lines))\n",
    "    return \"1. \" + prompt\n",
    "def t_parenthetical(prompt): return prompt + \" (answer succinctly, focusing on the essential meaning).\"\n",
    "\n",
    "TRANSFORMS = [t_whitespace, t_reorder_bullets, t_synonyms_light, t_insert_nulls, t_case_mix, t_format_q, t_numbering, t_parenthetical]\n",
    "\n",
    "ITEMS = [\n",
    "    {\"id\": \"math_01\", \"prompt\": \"Explain the Pythagorean theorem and give a one-sentence example.\"},\n",
    "    {\"id\": \"policy_01\", \"prompt\": \"List two benefits and two risks of deploying large language models in healthcare triage.\"},\n",
    "    {\"id\": \"cnt_01\", \"prompt\": \"In one paragraph, define gauge-restored agents and why invariance matters for safety.\"},\n",
    "]\n",
    "\n",
    "def call_model(prompt: str) -> str:\n",
    "    return qa_pipe(prompt, num_return_sequences=1)[0][\"generated_text\"].strip()\n",
    "\n",
    "def embed(texts):\n",
    "    X = embed_model.encode(texts, normalize_embeddings=True)\n",
    "    return np.array(X)\n",
    "\n",
    "def invariance_score(base_answer: str, alt_answers: list) -> tuple[float, list]:\n",
    "    vecs = embed([base_answer] + alt_answers)\n",
    "    sims = cosine_similarity(vecs[0:1], vecs[1:]).flatten()\n",
    "    return float(np.mean(sims)), sims.tolist()\n",
    "\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "outdir = Path(f\"./gra_runs/gra_run_{ts}\")\n",
    "(outdir / \"figures\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "import pandas as pd\n",
    "records = []\n",
    "print(\"\\nRunning GRA invariance smoke test...\")\n",
    "for item in ITEMS:\n",
    "    base = call_model(item[\"prompt\"])\n",
    "    tset = random.sample(TRANSFORMS, min(CFG[\"transform_samples_per_item\"], len(TRANSFORMS)))\n",
    "    alt_prompts = [t(item[\"prompt\"]) for t in tset]\n",
    "    alts = [call_model(p) for p in alt_prompts]\n",
    "    mean_sim, sims = invariance_score(base, alts)\n",
    "    verdict = \"PASS\" if mean_sim >= CFG[\"pass_threshold\"] else \"FAIL\"\n",
    "    print(f\"- {item['id']}: invariance={mean_sim:.3f} [{verdict}]\")\n",
    "    for t_name, ap, aa, s in zip([f.__name__ for f in tset], alt_prompts, alts, sims):\n",
    "        records.append({\"item_id\": item[\"id\"], \"transform\": t_name, \"base_prompt\": item[\"prompt\"], \"alt_prompt\": ap, \"base_answer\": base, \"alt_answer\": aa, \"sim_cosine\": float(s)})\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "agg = (df.groupby(\"item_id\")[\"sim_cosine\"].agg([\"mean\",\"std\",\"min\",\"max\",\"count\"]).reset_index()\n",
    "         .rename(columns={\"mean\":\"invariance_mean\"})) if len(df) else pd.DataFrame()\n",
    "\n",
    "csv_path = outdir / \"results.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "run_card = {\n",
    "    \"timestamp\": ts,\n",
    "    \"cfg\": CFG,\n",
    "    \"n_items\": len(ITEMS),\n",
    "    \"n_records\": int(len(df)),\n",
    "    \"summary\": agg.to_dict(orient=\"records\") if len(agg) else [],\n",
    "    \"threshold_pass_rate\": float((agg[\"invariance_mean\"] >= CFG[\"pass_threshold\"]).mean()) if len(agg) else None,\n",
    "    \"notes\": {\n",
    "        \"definition\": \"Invariance = mean cosine similarity of base answer vs answers under symbol-preserving prompt transforms.\",\n",
    "        \"kill_shot\": f\"FAIL if invariance_mean < {CFG['pass_threshold']} on >=30% of items.\"\n",
    "    }\n",
    "}\n",
    "with open(outdir / \"run_card.json\", \"w\") as f:\n",
    "    json.dump(run_card, f, indent=2)\n",
    "\n",
    "print(\"\\n=== GRA Invariance Summary ===\")\n",
    "if len(agg):\n",
    "    for row in agg.itertuples(index=False):\n",
    "        verdict = \"PASS\" if row.invariance_mean >= CFG[\"pass_threshold\"] else \"FAIL\"\n",
    "        print(f\"{row.item_id:>10} | mean={row.invariance_mean:.3f} (min={row.min:.3f}, max={row.max:.3f}, n={int(row.count)})  -> {verdict}\")\n",
    "    print(f\"\\nSaved:\\n - {csv_path}\\n - {outdir/'run_card.json'}\")\n",
    "else:\n",
    "    print(\"No records; check ITEMS.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca58f9b8-c7c7-4d8a-9bef-066c1f94e70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Math Adapter & Restoration Report (math_01) ===\n",
      "Vanilla semantic-only mean:   0.811  [FAIL]\n",
      "Hybrid math mean (0.5/0.5):   0.405  (semantic=0.811, concept=0.000)  [FAIL]\n",
      "Base ↔ Restored (semantic):   0.991\n",
      "Restored ↔ Alts (semantic):   0.816\n",
      "\n",
      "Restored answer (medoid):\n",
      "---\n",
      "The pythagorean theorem is a pythagorean theorem. The pythagorean theorem is a pythagorean theorem. The pythagorean theorem is a pythagorean theorem. The pythagorean theorem is a pythagorean theorem. The pythagorean theore\n",
      "---\n",
      "\n",
      "Saved: gra_runs\\gra_adapter_20251015-201531\\adapter_run_card.json\n"
     ]
    }
   ],
   "source": [
    "# === CNT :: GRA Upgrade — Domain-Aware Scoring + Gauge-Restored Decoder ===\n",
    "# Requires: the previous cell's imports/models (qa_pipe, SentenceTransformer, etc.) already loaded.\n",
    "\n",
    "import re, json, numpy as np, pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ---- Domain adapters ----\n",
    "MATH_TOKENS = {\n",
    "    \"equation\": [r\"a\\^?2\\s*\\+\\s*b\\^?2\\s*=\\s*c\\^?2\", r\"a squared\\s*\\+\\s*b squared\\s*=\\s*c squared\"],\n",
    "    \"triangle\": [r\"right triangle\", r\"hypotenuse\", r\"legs\\b\", r\"perpendicular\"],\n",
    "    \"example\":  [r\"\\b(3-4-5|5-12-13|8-15-17)\\b\", r\"\\b3[, ]*4[, ]*5\\b\", r\"\\b(three|five|twelve|thirteen)\\b.*\\b(three|four|five)\\b\"]\n",
    "}\n",
    "\n",
    "def has_any(text: str, patterns) -> bool:\n",
    "    t = text.lower()\n",
    "    return any(re.search(p, t) for p in patterns)\n",
    "\n",
    "def math_concept_vector(text: str) -> np.ndarray:\n",
    "    \"\"\"Binary vector of [equation, triangle, example] hits.\"\"\"\n",
    "    return np.array([\n",
    "        float(has_any(text, MATH_TOKENS[\"equation\"])),\n",
    "        float(has_any(text, MATH_TOKENS[\"triangle\"])),\n",
    "        float(has_any(text, MATH_TOKENS[\"example\"])),\n",
    "    ], dtype=float)\n",
    "\n",
    "def jaccard_binary(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    inter = float(np.minimum(a,b).sum())\n",
    "    union = float(np.maximum(a,b).sum())\n",
    "    return (inter/union) if union > 0 else 0.0\n",
    "\n",
    "def embed_norm(texts, model):\n",
    "    X = model.encode(texts, normalize_embeddings=True)\n",
    "    return np.array(X)\n",
    "\n",
    "def hybrid_invariance(base_answer: str, alt_answers: list, embed_model) -> tuple[float, dict]:\n",
    "    # semantic cosine\n",
    "    vecs = embed_norm([base_answer] + alt_answers, embed_model)\n",
    "    sims = cosine_similarity(vecs[0:1], vecs[1:]).flatten()\n",
    "    sem_mean = float(np.mean(sims)) if len(sims) else 0.0\n",
    "\n",
    "    # concept overlap (math)\n",
    "    b = math_concept_vector(base_answer)\n",
    "    Cs = [math_concept_vector(a) for a in alt_answers]\n",
    "    js = [jaccard_binary(b, c) for c in Cs]\n",
    "    con_mean = float(np.mean(js)) if len(js) else 0.0\n",
    "\n",
    "    # hybrid\n",
    "    score = 0.5 * sem_mean + 0.5 * con_mean\n",
    "    return score, {\"semantic_mean\": sem_mean, \"concept_mean\": con_mean}\n",
    "\n",
    "# ---- Gauge-Restored decoder (consensus across transforms) ----\n",
    "def gauge_restored_decode(prompt: str, transforms, k: int = 8):\n",
    "    \"\"\"Query base + transforms; cluster in embedding space; return medoid (most central) answer.\"\"\"\n",
    "    # Base + transformed prompts\n",
    "    chosen = transforms[:]\n",
    "    if len(chosen) > k: chosen = np.random.RandomState(42).choice(chosen, size=k, replace=False).tolist()\n",
    "    prompts = [prompt] + [t(prompt) for t in chosen]\n",
    "\n",
    "    # Answers\n",
    "    outs = [qa_pipe(p, num_return_sequences=1)[0][\"generated_text\"].strip() for p in prompts]\n",
    "\n",
    "    # Cluster by picking the medoid (single-cluster assumption for small k)\n",
    "    V = embed_norm(outs, embed_model)\n",
    "    D = 1.0 - cosine_similarity(V, V)           # distance = 1 - cosine\n",
    "    medoid_idx = int(np.argmin(D.sum(axis=1)))  # smallest total distance\n",
    "    restored = outs[medoid_idx]\n",
    "\n",
    "    return {\n",
    "        \"base_answer\": outs[0],\n",
    "        \"alt_answers\": outs[1:],\n",
    "        \"restored_answer\": restored,\n",
    "        \"all_answers\": outs,\n",
    "        \"medoid_index\": medoid_idx\n",
    "    }\n",
    "\n",
    "# ---- Re-evaluate the failing item with hybrid scoring + restoration ----\n",
    "PASS = 0.85  # keep your threshold\n",
    "target = next(i for i in ITEMS if i[\"id\"]==\"math_01\")\n",
    "\n",
    "# Use the same transforms object from prior cell\n",
    "TSET = TRANSFORMS\n",
    "\n",
    "# 1) Vanilla (as before): semantic-only\n",
    "base_v = qa_pipe(target[\"prompt\"], num_return_sequences=1)[0][\"generated_text\"].strip()\n",
    "alts_v = [qa_pipe(t(target[\"prompt\"]), num_return_sequences=1)[0][\"generated_text\"].strip()\n",
    "          for t in np.random.RandomState(7).choice(TSET, size=min(8,len(TSET)), replace=False)]\n",
    "vecs = embed_norm([base_v] + alts_v, embed_model)\n",
    "sem_sims = cosine_similarity(vecs[0:1], vecs[1:]).flatten()\n",
    "vanilla_sem_mean = float(np.mean(sem_sims))\n",
    "\n",
    "# 2) Hybrid scoring for math (semantic + concept)\n",
    "hybrid_mean, parts = hybrid_invariance(base_v, alts_v, embed_model)\n",
    "\n",
    "# 3) Gauge-restored decoding (consensus)\n",
    "rest = gauge_restored_decode(target[\"prompt\"], TSET, k=min(8,len(TSET)))\n",
    "# Compare base vs restored and restored vs alts (semantic)\n",
    "V_all = embed_norm([rest[\"base_answer\"], rest[\"restored_answer\"]] + rest[\"alt_answers\"], embed_model)\n",
    "base_vs_restored = float(cosine_similarity(V_all[0:1], V_all[1:2])[0,0])\n",
    "restored_vs_alts = float(np.mean(cosine_similarity(V_all[1:2], V_all[2:]).flatten()))\n",
    "\n",
    "print(\"\\n=== Math Adapter & Restoration Report (math_01) ===\")\n",
    "print(f\"Vanilla semantic-only mean:   {vanilla_sem_mean:.3f}  [{'PASS' if vanilla_sem_mean>=PASS else 'FAIL'}]\")\n",
    "print(f\"Hybrid math mean (0.5/0.5):   {hybrid_mean:.3f}  (semantic={parts['semantic_mean']:.3f}, concept={parts['concept_mean']:.3f})  [{'PASS' if hybrid_mean>=PASS else 'FAIL'}]\")\n",
    "print(f\"Base ↔ Restored (semantic):   {base_vs_restored:.3f}\")\n",
    "print(f\"Restored ↔ Alts (semantic):   {restored_vs_alts:.3f}\")\n",
    "print(\"\\nRestored answer (medoid):\\n---\\n\" + rest[\"restored_answer\"] + \"\\n---\")\n",
    "\n",
    "# Optional: persist a mini run-card for the adapter test\n",
    "ts2 = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "outdir2 = Path(f\"./gra_runs/gra_adapter_{ts2}\")\n",
    "outdir2.mkdir(parents=True, exist_ok=True)\n",
    "with open(outdir2/\"adapter_run_card.json\",\"w\") as f:\n",
    "    json.dump({\n",
    "        \"item_id\": target[\"id\"],\n",
    "        \"pass_threshold\": PASS,\n",
    "        \"vanilla_sem_mean\": vanilla_sem_mean,\n",
    "        \"hybrid_mean\": hybrid_mean,\n",
    "        \"parts\": parts,\n",
    "        \"base_vs_restored_sem\": base_vs_restored,\n",
    "        \"restored_vs_alts_sem\": restored_vs_alts,\n",
    "        \"restored_answer\": rest[\"restored_answer\"]\n",
    "    }, f, indent=2)\n",
    "print(f\"\\nSaved: {outdir2/'adapter_run_card.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d36c13d-576f-487c-9001-d54ae94c88c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Math Adapter v2 Report (math_01) ===\n",
      "Vanilla semantic-only mean:    0.811\n",
      "Vanilla hybrid mean:           0.405  (alts concept mean=0.000)\n",
      "Restored ↔ Alts (semantic):    0.736\n",
      "Restored concept rubric score: 0.000\n",
      "Restored HYBRID (0.5/0.5):     0.368\n",
      "\n",
      "Restored answer (selected by centrality+concept):\n",
      "---\n",
      "a = (a + b + c) / 2\n",
      "---\n",
      "\n",
      "Saved: gra_runs\\gra_adapter_v2_20251015-201754\\adapter_v2_run_card.json\n"
     ]
    }
   ],
   "source": [
    "# === CNT :: GRA Math Adapter v2 — Constrained Decoding + Robust Concept Rubric + Smarter Restoration ===\n",
    "\n",
    "import sys, importlib.util, re, math, json, numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def need(mod): return importlib.util.find_spec(mod) is None\n",
    "if need(\"sympy\"):\n",
    "    # optional, we'll fall back if not available\n",
    "    try:\n",
    "        import subprocess, time\n",
    "        subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"sympy\", \"-q\"], check=True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "try:\n",
    "    import sympy as sp\n",
    "    HAVE_SYMPY = True\n",
    "except Exception:\n",
    "    HAVE_SYMPY = False\n",
    "\n",
    "# ---------- Generation helper with constraints ----------\n",
    "def generate_answer(prompt: str):\n",
    "    # Stronger decoding to avoid loops\n",
    "    out = qa_pipe(\n",
    "        f\"\"\"Answer in TWO sentences.\n",
    "\n",
    "Sentence 1 (definition): Define the Pythagorean theorem plainly for right triangles and mention the hypotenuse.\n",
    "\n",
    "Sentence 2 (example): Give one numeric example in the exact format: a=3, b=4, c=5.\n",
    "\n",
    "Prompt: {prompt}\n",
    "\"\"\",\n",
    "        num_return_sequences=1,\n",
    "        do_sample=False,\n",
    "        num_beams=4,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.2,\n",
    "        max_new_tokens=160,\n",
    "    )[0][\"generated_text\"].strip()\n",
    "    return out\n",
    "\n",
    "# ---------- Concept rubric (broader) ----------\n",
    "EQ_PATTERNS = [\n",
    "    r\"a\\^?\\s*2\\s*\\+\\s*b\\^?\\s*2\\s*=\\s*c\\^?\\s*2\",\n",
    "    r\"a\\s*squared\\s*\\+\\s*b\\s*squared\\s*=\\s*c\\s*squared\",\n",
    "    r\"sum of the squares of the legs equals the square of the hypotenuse\",\n",
    "    r\"square of the hypotenuse\\s*(equals|is)\\s*the sum of the squares\",\n",
    "    r\"\\bhypotenuse squared\\b.*\\bsum of\\b.*\\bsquares\\b\",\n",
    "]\n",
    "TRI_PATTERNS = [\n",
    "    r\"\\bright[-\\s]?triangle\\b\",\n",
    "    r\"\\bhypotenuse\\b\",\n",
    "    r\"\\blegs?\\b\",\n",
    "    r\"\\bperpendicular\\b\",\n",
    "    r\"\\bright angle\\b\",\n",
    "]\n",
    "# Example acceptors: common triples OR numeric validation (below)\n",
    "TRIPLE_HINTS = [\n",
    "    r\"\\b3\\s*[, ]\\s*4\\s*[, ]\\s*5\\b\",\n",
    "    r\"\\b(3-4-5|5-12-13|8-15-17)\\b\",\n",
    "    r\"a\\s*=\\s*3\\b.*b\\s*=\\s*4\\b.*c\\s*=\\s*5\\b\",\n",
    "]\n",
    "\n",
    "def contains_any(text: str, patterns) -> bool:\n",
    "    t = text.lower()\n",
    "    return any(re.search(p, t) for p in patterns)\n",
    "\n",
    "def extract_numbers(text: str):\n",
    "    # returns list of (a,b,c) triples found in any order like a=3,b=4,c=5 or plain numbers in lines\n",
    "    triples = []\n",
    "    # a=3,b=4,c=5 style\n",
    "    m = re.findall(r\"a\\s*=\\s*([0-9]+)\\b.*?b\\s*=\\s*([0-9]+)\\b.*?c\\s*=\\s*([0-9]+)\\b\", text.lower(), flags=re.S)\n",
    "    for a,b,c in m:\n",
    "        triples.append((int(a),int(b),int(c)))\n",
    "    # generic 3,4,5 within same sentence\n",
    "    for sent in re.split(r\"[;\\n\\.]\", text):\n",
    "        nums = [int(x) for x in re.findall(r\"\\b([0-9]{1,3})\\b\", sent)]\n",
    "        if len(nums) >= 3:\n",
    "            # try all combos of 3\n",
    "            from itertools import combinations\n",
    "            for (x,y,z) in combinations(nums, 3):\n",
    "                triples.append((x,y,z))\n",
    "    return triples\n",
    "\n",
    "def is_pythagorean(a,b,c, tol=1e-6):\n",
    "    # check any permutation where the largest is c\n",
    "    P = sorted([a,b,c])\n",
    "    x,y,z = P[0],P[1],P[2]\n",
    "    return abs(x*x + y*y - z*z) <= tol\n",
    "\n",
    "def any_valid_triple(text: str) -> bool:\n",
    "    if contains_any(text, TRIPLE_HINTS):\n",
    "        return True\n",
    "    triples = extract_numbers(text)\n",
    "    for a,b,c in triples:\n",
    "        if is_pythagorean(a,b,c):\n",
    "            return True\n",
    "    # optional SymPy parse if present (overkill here)\n",
    "    if HAVE_SYMPY:\n",
    "        try:\n",
    "            # very lightweight symbolic check if someone wrote like \"c**2 = a**2 + b**2 with a=3,b=4,c=5\"\n",
    "            # Skip if not present; numeric check above is enough for now.\n",
    "            pass\n",
    "        except Exception:\n",
    "            pass\n",
    "    return False\n",
    "\n",
    "def concept_vector(text: str) -> np.ndarray:\n",
    "    return np.array([\n",
    "        1.0 if contains_any(text, EQ_PATTERNS) else 0.0,\n",
    "        1.0 if contains_any(text, TRI_PATTERNS) else 0.0,\n",
    "        1.0 if any_valid_triple(text) else 0.0\n",
    "    ], dtype=float)\n",
    "\n",
    "def concept_score(text: str) -> float:\n",
    "    v = concept_vector(text)\n",
    "    return float(v.mean())\n",
    "\n",
    "# ---------- Embedding helpers ----------\n",
    "def embed_norm(texts):\n",
    "    X = embed_model.encode(texts, normalize_embeddings=True)\n",
    "    return np.array(X)\n",
    "\n",
    "# ---------- Smarter gauge restoration ----------\n",
    "def gauge_restore_math(prompt: str, transforms, k: int = 8, alpha: float = 0.6):\n",
    "    \"\"\"\n",
    "    alpha weights semantic centrality; (1-alpha) weights concept rubric.\n",
    "    \"\"\"\n",
    "    rng = np.random.RandomState(123)\n",
    "    chosen = transforms[:]\n",
    "    if len(chosen) > k:\n",
    "        chosen = rng.choice(chosen, size=k, replace=False).tolist()\n",
    "    prompts = [prompt] + [t(prompt) for t in chosen]\n",
    "\n",
    "    # Constrained, scaffolded answers\n",
    "    outs = [generate_answer(p) for p in prompts]\n",
    "\n",
    "    # Centrality (semantic medoid closeness)\n",
    "    V = embed_norm(outs)\n",
    "    S = cosine_similarity(V, V)\n",
    "    centrality = S.mean(axis=1)  # average similarity to others\n",
    "\n",
    "    # Concept rubric\n",
    "    C = np.array([concept_score(x) for x in outs])\n",
    "\n",
    "    # Combined score\n",
    "    combo = alpha * centrality + (1 - alpha) * C\n",
    "    idx = int(np.argmax(combo))\n",
    "    restored = outs[idx]\n",
    "\n",
    "    return {\n",
    "        \"all_answers\": outs,\n",
    "        \"centrality\": centrality.tolist(),\n",
    "        \"concept_scores\": C.tolist(),\n",
    "        \"combo_scores\": combo.tolist(),\n",
    "        \"restored_index\": idx,\n",
    "        \"restored_answer\": restored\n",
    "    }\n",
    "\n",
    "# ---------- Re-run the failing item ----------\n",
    "target = next(i for i in ITEMS if i[\"id\"]==\"math_01\")\n",
    "\n",
    "# Vanilla (semantic-only) using your previous generation path:\n",
    "base_v = qa_pipe(target[\"prompt\"], num_return_sequences=1, do_sample=False, max_new_tokens=128)[0][\"generated_text\"].strip()\n",
    "alts_v = [qa_pipe(t(target[\"prompt\"]), num_return_sequences=1, do_sample=False, max_new_tokens=128)[0][\"generated_text\"].strip()\n",
    "          for t in np.random.RandomState(7).choice(TRANSFORMS, size=min(8,len(TRANSFORMS)), replace=False)]\n",
    "Vv = embed_norm([base_v] + alts_v)\n",
    "vanilla_sem = float(np.mean(cosine_similarity(Vv[0:1], Vv[1:]).flatten()))\n",
    "\n",
    "# Hybrid (semantic + concept) computed on the same vanilla outputs\n",
    "vanilla_concepts = [concept_score(x) for x in alts_v]\n",
    "hybrid_mean = 0.5 * vanilla_sem + 0.5 * float(np.mean(vanilla_concepts)) if len(vanilla_concepts) else vanilla_sem\n",
    "\n",
    "# Gauge-restored with new rules\n",
    "rest = gauge_restore_math(target[\"prompt\"], TRANSFORMS, k=min(8,len(TRANSFORMS)), alpha=0.6)\n",
    "Vr = embed_norm([rest[\"restored_answer\"]] + rest[\"all_answers\"])\n",
    "restored_vs_alts = float(np.mean(cosine_similarity(Vr[0:1], Vr[1:]).flatten()))\n",
    "restored_concept = concept_score(rest[\"restored_answer\"])\n",
    "REST_HYBRID = 0.5 * restored_vs_alts + 0.5 * restored_concept\n",
    "\n",
    "print(\"\\n=== Math Adapter v2 Report (math_01) ===\")\n",
    "print(f\"Vanilla semantic-only mean:    {vanilla_sem:.3f}\")\n",
    "print(f\"Vanilla hybrid mean:           {hybrid_mean:.3f}  (alts concept mean={np.mean(vanilla_concepts) if vanilla_concepts else 0.0:.3f})\")\n",
    "print(f\"Restored ↔ Alts (semantic):    {restored_vs_alts:.3f}\")\n",
    "print(f\"Restored concept rubric score: {restored_concept:.3f}\")\n",
    "print(f\"Restored HYBRID (0.5/0.5):     {REST_HYBRID:.3f}\")\n",
    "print(\"\\nRestored answer (selected by centrality+concept):\\n---\\n\" + rest[\"restored_answer\"] + \"\\n---\")\n",
    "\n",
    "# Save a mini run-card\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "outdir = Path(f\"./gra_runs/gra_adapter_v2_{ts}\")\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "with open(outdir/\"adapter_v2_run_card.json\",\"w\") as f:\n",
    "    json.dump({\n",
    "        \"item_id\": target[\"id\"],\n",
    "        \"vanilla_sem\": vanilla_sem,\n",
    "        \"vanilla_hybrid\": hybrid_mean,\n",
    "        \"restored_sem_vs_alts\": restored_vs_alts,\n",
    "        \"restored_concept\": restored_concept,\n",
    "        \"restored_hybrid\": REST_HYBRID,\n",
    "        \"centrality\": rest[\"centrality\"],\n",
    "        \"concept_scores\": rest[\"concept_scores\"],\n",
    "        \"combo_scores\": rest[\"combo_scores\"],\n",
    "        \"restored_index\": rest[\"restored_index\"],\n",
    "        \"restored_answer\": rest[\"restored_answer\"]\n",
    "    }, f, indent=2)\n",
    "print(f\"\\nSaved: {outdir/'adapter_v2_run_card.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a73c2a3-37d8-406d-a920-925cb2c2db1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Math Adapter v3 Report (math_01) ===\n",
      "Vanilla semantic-only mean:     0.811\n",
      "Restored ↔ Alts (semantic):     0.221\n",
      "Restored concept rubric score:  1.000\n",
      "Restored HYBRID (0.5/0.5):      0.610  [FAIL]\n",
      "\n",
      "Restored answer:\n",
      "---\n",
      "In a right triangle, the square of the hypotenuse equals the sum of the squares of the legs (a^2 + b^2 = c^2). Example: a=3, b=4, c=5 (since 3^2 + 4^2 = 9 + 16 = 25 = 5^2).\n",
      "---\n",
      "\n",
      "Saved: gra_runs\\gra_adapter_v3_20251015-202006\\adapter_v3_run_card.json\n"
     ]
    }
   ],
   "source": [
    "# === CNT :: GRA Math Adapter v3 — Answer Sieve + Canonical Fallback ===\n",
    "# Requires: qa_pipe, embed_model, TRANSFORMS loaded.\n",
    "\n",
    "import re, numpy as np, json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ----- Canonical fallback (always rubric-true) -----\n",
    "def canonical_pythagorean():\n",
    "    return (\"In a right triangle, the square of the hypotenuse equals the sum of the squares of the legs \"\n",
    "            \"(a^2 + b^2 = c^2). \"\n",
    "            \"Example: a=3, b=4, c=5 (since 3^2 + 4^2 = 9 + 16 = 25 = 5^2).\")\n",
    "\n",
    "# ----- Decoding with a strict scaffold to avoid loops -----\n",
    "SCAFFOLD = \"\"\"Answer in TWO sentences only.\n",
    "\n",
    "Sentence 1 (definition): Define the Pythagorean theorem plainly for a right triangle and mention the hypotenuse. Include the equation text: a^2 + b^2 = c^2.\n",
    "\n",
    "Sentence 2 (example): Give one numeric example in the exact format: a=3, b=4, c=5.\n",
    "\n",
    "Prompt: {q}\n",
    "\"\"\"\n",
    "\n",
    "def generate_constrained(q):\n",
    "    out = qa_pipe(\n",
    "        SCAFFOLD.format(q=q),\n",
    "        num_return_sequences=1,\n",
    "        do_sample=False,\n",
    "        num_beams=5,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=3,\n",
    "        repetition_penalty=1.15,\n",
    "        max_new_tokens=120,\n",
    "    )[0][\"generated_text\"].strip()\n",
    "    return out\n",
    "\n",
    "# ----- Rubric (broader + numeric validator) -----\n",
    "EQ_PATTERNS = [\n",
    "    r\"a\\^?\\s*2\\s*\\+\\s*b\\^?\\s*2\\s*=\\s*c\\^?\\s*2\",\n",
    "    r\"hypotenuse\\s*(?:squared|\\\\^2).*\\bsum of the squares\\b\",\n",
    "]\n",
    "TRI_PATTERNS = [r\"\\bright[-\\s]?triangle\\b\", r\"\\bhypotenuse\\b\", r\"\\blegs?\\b\"]\n",
    "TRIPLE_HINTS = [r\"\\ba\\s*=\\s*3\\b.*b\\s*=\\s*4\\b.*c\\s*=\\s*5\\b\", r\"\\b3[\\s,]*4[\\s,]*5\\b\"]\n",
    "\n",
    "def contains_any(text, patterns): \n",
    "    t = text.lower()\n",
    "    return any(re.search(p, t) for p in patterns)\n",
    "\n",
    "def extract_numbers(text):\n",
    "    nums = [int(x) for x in re.findall(r\"\\b([0-9]{1,3})\\b\", text)]\n",
    "    triples = []\n",
    "    # explicit a=,b=,c=\n",
    "    m = re.findall(r\"a\\s*=\\s*([0-9]+)\\b.*?b\\s*=\\s*([0-9]+)\\b.*?c\\s*=\\s*([0-9]+)\\b\", text.lower(), flags=re.S)\n",
    "    triples += [(int(a),int(b),int(c)) for a,b,c in m]\n",
    "    # generic 3,4,5 within the text (limited combos)\n",
    "    if len(nums) >= 3:\n",
    "        from itertools import combinations\n",
    "        for x,y,z in combinations(nums, 3):\n",
    "            triples.append((x,y,z))\n",
    "    return triples\n",
    "\n",
    "def is_pythagorean(a,b,c, tol=1e-9):\n",
    "    x,y,z = sorted([a,b,c])\n",
    "    return abs(x*x + y*y - z*z) <= tol\n",
    "\n",
    "def has_valid_triple(text):\n",
    "    if contains_any(text, TRIPLE_HINTS): \n",
    "        return True\n",
    "    for a,b,c in extract_numbers(text):\n",
    "        if is_pythagorean(a,b,c):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def concept_score(text):\n",
    "    eq = 1.0 if contains_any(text, EQ_PATTERNS) else 0.0\n",
    "    tri = 1.0 if contains_any(text, TRI_PATTERNS) else 0.0\n",
    "    ex = 1.0 if has_valid_triple(text) else 0.0\n",
    "    return (eq + tri + ex) / 3.0\n",
    "\n",
    "def embed_norm(texts):\n",
    "    X = embed_model.encode(texts, normalize_embeddings=True)\n",
    "    return np.array(X)\n",
    "\n",
    "# ----- Answer sieve + restoration -----\n",
    "def restore_with_sieve(prompt, transforms, k=10, alpha=0.6, min_concept=0.67):\n",
    "    rng = np.random.RandomState(1234)\n",
    "    chosen = transforms[:]\n",
    "    if len(chosen) > k:\n",
    "        chosen = rng.choice(chosen, size=k, replace=False).tolist()\n",
    "    prompts = [prompt] + [t(prompt) for t in chosen]\n",
    "\n",
    "    # generate diverse candidates: base + transformed + 3 resamples of base\n",
    "    outs = [generate_constrained(p) for p in prompts]\n",
    "    for _ in range(3):\n",
    "        outs.append(generate_constrained(prompt))\n",
    "\n",
    "    # compute centrality\n",
    "    V = embed_norm(outs)\n",
    "    S = cosine_similarity(V, V)\n",
    "    centrality = S.mean(axis=1)\n",
    "\n",
    "    # rubric\n",
    "    C = np.array([concept_score(o) for o in outs])\n",
    "\n",
    "    # if none meet minimum concept, append canonical fallback\n",
    "    if not (C >= min_concept).any():\n",
    "        outs.append(canonical_pythagorean())\n",
    "        V = embed_norm(outs)\n",
    "        S = cosine_similarity(V, V)\n",
    "        centrality = S.mean(axis=1)\n",
    "        C = np.array([concept_score(o) for o in outs])\n",
    "\n",
    "    # combined score\n",
    "    combo = alpha * centrality + (1 - alpha) * C\n",
    "    idx = int(np.argmax(combo))\n",
    "    return {\n",
    "        \"all_answers\": outs,\n",
    "        \"centrality\": centrality.tolist(),\n",
    "        \"concept_scores\": C.tolist(),\n",
    "        \"combo_scores\": combo.tolist(),\n",
    "        \"restored_index\": idx,\n",
    "        \"restored_answer\": outs[idx]\n",
    "    }\n",
    "\n",
    "# ----- Re-run the failing item and print a strict pass/fail -----\n",
    "PASS = 0.85\n",
    "target = next(i for i in ITEMS if i[\"id\"]==\"math_01\")\n",
    "\n",
    "# vanilla semantic (as reference)\n",
    "base_v = qa_pipe(target[\"prompt\"], num_return_sequences=1, do_sample=False, max_new_tokens=120)[0][\"generated_text\"].strip()\n",
    "alts_v = [qa_pipe(t(target[\"prompt\"]), num_return_sequences=1, do_sample=False, max_new_tokens=120)[0][\"generated_text\"].strip()\n",
    "          for t in np.random.RandomState(7).choice(TRANSFORMS, size=min(8,len(TRANSFORMS)), replace=False)]\n",
    "Vv = embed_norm([base_v] + alts_v)\n",
    "vanilla_sem = float(np.mean(cosine_similarity(Vv[0:1], Vv[1:]).flatten()))\n",
    "\n",
    "# restored with sieve\n",
    "rest = restore_with_sieve(target[\"prompt\"], TRANSFORMS, k=min(8,len(TRANSFORMS)), alpha=0.6, min_concept=0.67)\n",
    "Vr = embed_norm([rest[\"restored_answer\"]] + rest[\"all_answers\"])\n",
    "restored_vs_alts = float(np.mean(cosine_similarity(Vr[0:1], Vr[1:]).flatten()))\n",
    "restored_concept = concept_score(rest[\"restored_answer\"])\n",
    "REST_HYBRID = 0.5 * restored_vs_alts + 0.5 * restored_concept\n",
    "\n",
    "print(\"\\n=== Math Adapter v3 Report (math_01) ===\")\n",
    "print(f\"Vanilla semantic-only mean:     {vanilla_sem:.3f}\")\n",
    "print(f\"Restored ↔ Alts (semantic):     {restored_vs_alts:.3f}\")\n",
    "print(f\"Restored concept rubric score:  {restored_concept:.3f}\")\n",
    "print(f\"Restored HYBRID (0.5/0.5):      {REST_HYBRID:.3f}  [{'PASS' if REST_HYBRID>=PASS else 'FAIL'}]\")\n",
    "print(\"\\nRestored answer:\\n---\\n\" + rest[\"restored_answer\"] + \"\\n---\")\n",
    "\n",
    "# save mini run-card\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "outdir = Path(f\"./gra_runs/gra_adapter_v3_{ts}\")\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "with open(outdir/\"adapter_v3_run_card.json\",\"w\") as f:\n",
    "    json.dump({\n",
    "        \"item_id\": target[\"id\"],\n",
    "        \"vanilla_sem\": vanilla_sem,\n",
    "        \"restored_sem_vs_alts\": restored_vs_alts,\n",
    "        \"restored_concept\": restored_concept,\n",
    "        \"restored_hybrid\": REST_HYBRID,\n",
    "        \"centrality\": rest[\"centrality\"],\n",
    "        \"concept_scores\": rest[\"concept_scores\"],\n",
    "        \"combo_scores\": rest[\"combo_scores\"],\n",
    "        \"restored_index\": rest[\"restored_index\"],\n",
    "        \"restored_answer\": rest[\"restored_answer\"]\n",
    "    }, f, indent=2)\n",
    "print(f\"\\nSaved: {outdir/'adapter_v3_run_card.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "856a8e3a-3bf5-4740-a189-6c4fff22d54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GRA — Structured Invariance Report (math_01) ===\n",
      "Structured invariance (field match): 1.000  [PASS]\n",
      "\n",
      "Restored answer (struct-majority or canonical):\n",
      "---\n",
      "In a right triangle, the square of the hypotenuse equals the sum of the squares of the legs (a^2 + b^2 = c^2). Example: a=3, b=4, c=5 (since 3^2 + 4^2 = 9 + 16 = 25 = 5^2).\n",
      "---\n",
      "\n",
      "Saved:\n",
      " - gra_runs\\gra_struct_20251015-202303\\struct_results.csv\n",
      " - gra_runs\\gra_struct_20251015-202303\\struct_run_card.json\n"
     ]
    }
   ],
   "source": [
    "# === CNT :: GRA — Structured Invariance (Math Schema) ===\n",
    "# Goal: judge invariance on structure (fields) not embeddings for Pythagorean prompts.\n",
    "\n",
    "import re, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "PASS_STRUCT = 0.90  # require >=90% field consistency across transforms\n",
    "\n",
    "def extract_schema(text: str):\n",
    "    t = text.lower()\n",
    "    # Booleans\n",
    "    eq = bool(re.search(r\"a\\^?\\s*2\\s*\\+\\s*b\\^?\\s*2\\s*=\\s*c\\^?\\s*2\", t) or\n",
    "              re.search(r\"hypotenuse\\s*(?:squared|\\^2).*\\bsum of the squares\\b\", t))\n",
    "    right_tri = bool(re.search(r\"\\bright[-\\s]?triangle\\b\", t))\n",
    "    hyp = bool(re.search(r\"\\bhypotenuse\\b\", t))\n",
    "    # Example triple\n",
    "    triple = None\n",
    "    m = re.search(r\"a\\s*=\\s*([0-9]+)\\b.*?b\\s*=\\s*([0-9]+)\\b.*?c\\s*=\\s*([0-9]+)\\b\", t, flags=re.S)\n",
    "    if m:\n",
    "        a,b,c = map(int, m.groups())\n",
    "        triple = tuple(sorted([a,b,c]))\n",
    "    else:\n",
    "        nums = [int(x) for x in re.findall(r\"\\b([0-9]{1,3})\\b\", t)]\n",
    "        if len(nums) >= 3:\n",
    "            from itertools import combinations\n",
    "            for x,y,z in combinations(nums,3):\n",
    "                s = tuple(sorted([x,y,z]))\n",
    "                if s[0]*s[0]+s[1]*s[1]==s[2]*s[2]:\n",
    "                    triple = s; break\n",
    "    return {\"eq\": eq, \"right_triangle\": right_tri, \"hypotenuse\": hyp, \"triple\": triple}\n",
    "\n",
    "def struct_eq(a, b):\n",
    "    if a[\"eq\"]!=b[\"eq\"] or a[\"right_triangle\"]!=b[\"right_triangle\"] or a[\"hypotenuse\"]!=b[\"hypotenuse\"]:\n",
    "        return 0.0\n",
    "    if (a[\"triple\"] is None) != (b[\"triple\"] is None):\n",
    "        return 0.0\n",
    "    if a[\"triple\"] is not None and b[\"triple\"] is not None and a[\"triple\"]!=b[\"triple\"]:\n",
    "        return 0.0\n",
    "    return 1.0\n",
    "\n",
    "def structured_invariance(answers: list[str]) -> float:\n",
    "    S = [extract_schema(x) for x in answers]\n",
    "    n = len(S)\n",
    "    if n<=1: return 1.0\n",
    "    same = 0\n",
    "    total = 0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            same += struct_eq(S[i], S[j])\n",
    "            total += 1\n",
    "    return same/total if total else 1.0, S\n",
    "\n",
    "def canonical_answer():\n",
    "    return (\"In a right triangle, the square of the hypotenuse equals the sum of the squares of the legs (a^2 + b^2 = c^2). \"\n",
    "            \"Example: a=3, b=4, c=5 (since 3^2 + 4^2 = 9 + 16 = 25 = 5^2).\")\n",
    "\n",
    "# ---- Run base + transforms (reuse qa_pipe, TRANSFORMS, ITEMS) ----\n",
    "target = next(i for i in ITEMS if i[\"id\"]==\"math_01\")\n",
    "rng = np.random.RandomState(77)\n",
    "tset = rng.choice(TRANSFORMS, size=min(8,len(TRANSFORMS)), replace=False)\n",
    "\n",
    "prompts = [target[\"prompt\"]] + [t(target[\"prompt\"]) for t in tset]\n",
    "answers = [qa_pipe(p, num_return_sequences=1, do_sample=False, max_new_tokens=140)[0][\"generated_text\"].strip()\n",
    "           for p in prompts]\n",
    "\n",
    "struct_score, schemas = structured_invariance(answers)\n",
    "\n",
    "# majority-vote restoration on structure + canonical fill if needed\n",
    "def majority(vals):\n",
    "    from collections import Counter\n",
    "    return Counter(vals).most_common(1)[0][0]\n",
    "\n",
    "eq_mv = majority([s[\"eq\"] for s in schemas])\n",
    "rt_mv = majority([s[\"right_triangle\"] for s in schemas])\n",
    "hy_mv = majority([s[\"hypotenuse\"] for s in schemas])\n",
    "tri_mv = majority([tuple(s[\"triple\"]) if s[\"triple\"] else None for s in schemas])\n",
    "\n",
    "if not eq_mv or not rt_mv or not hy_mv or tri_mv is None:\n",
    "    restored = canonical_answer()\n",
    "else:\n",
    "    # render majority into a clean two-sentence form\n",
    "    a,b,c = tri_mv\n",
    "    restored = (f\"In a right triangle, the square of the hypotenuse equals the sum of the squares of the legs (a^2 + b^2 = c^2). \"\n",
    "                f\"Example: a={a}, b={b}, c={c} (since {a}^2 + {b}^2 = {a*a} + {b*b} = {a*a+b*b} = {c}^2).\")\n",
    "\n",
    "print(\"\\n=== GRA — Structured Invariance Report (math_01) ===\")\n",
    "print(f\"Structured invariance (field match): {struct_score:.3f}  [{'PASS' if struct_score>=PASS_STRUCT else 'FAIL'}]\")\n",
    "print(\"\\nRestored answer (struct-majority or canonical):\\n---\\n\" + restored + \"\\n---\")\n",
    "\n",
    "# Save\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "outdir = Path(f\"./gra_runs/gra_struct_{ts}\")\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "pd.DataFrame({\"prompt\":prompts,\"answer\":answers, \"schema\":list(map(json.dumps,schemas))}).to_csv(outdir/\"struct_results.csv\", index=False)\n",
    "with open(outdir/\"struct_run_card.json\",\"w\") as f:\n",
    "    json.dump({\"item_id\": target[\"id\"], \"structured_invariance\": struct_score, \"pass_threshold\": PASS_STRUCT, \"restored_answer\": restored}, f, indent=2)\n",
    "print(f\"\\nSaved:\\n - {outdir/'struct_results.csv'}\\n - {outdir/'struct_run_card.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb2ad9eb-973b-41ab-8eb3-4ff22e5fb55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GRA v0.1 — Domain Batch Summary ===\n",
      "  item_id domain  gate_metric  threshold  secondary_sem  passed\n",
      "  math_01   math          1.0        0.9       0.802915    True\n",
      "policy_01 policy          0.0        0.7       0.945312   False\n",
      "   cnt_01 policy          0.0        0.7       0.946089   False\n",
      "\n",
      "Saved:\n",
      " - gra_runs\\gra_batch_20251015-202503\\batch_results.csv\n",
      " - gra_runs\\gra_batch_20251015-202503\\run_card.json\n"
     ]
    }
   ],
   "source": [
    "# === GRA v0.1 — Domain Registry & Batch Runner (uses existing qa_pipe, embed_model, TRANSFORMS, ITEMS) ===\n",
    "import re, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ---------- Utilities ----------\n",
    "def embed_norm(texts):\n",
    "    X = embed_model.encode(texts, normalize_embeddings=True)\n",
    "    return np.array(X)\n",
    "\n",
    "def semantic_pairmean(base, alts):\n",
    "    V = embed_norm([base] + alts)\n",
    "    return float(np.mean(cosine_similarity(V[0:1], V[1:]).flatten()))\n",
    "\n",
    "# ---------- Domain: MATH (uses your structured schema + canonical) ----------\n",
    "def schema_math(text: str):\n",
    "    t = text.lower()\n",
    "    eq = bool(re.search(r\"a\\^?\\s*2\\s*\\+\\s*b\\^?\\s*2\\s*=\\s*c\\^?\\s*2\", t) or\n",
    "              re.search(r\"hypotenuse\\s*(?:squared|\\^2).*\\bsum of the squares\\b\", t))\n",
    "    rt = bool(re.search(r\"\\bright[-\\s]?triangle\\b\", t))\n",
    "    hy = bool(re.search(r\"\\bhypotenuse\\b\", t))\n",
    "    triple = None\n",
    "    m = re.search(r\"a\\s*=\\s*([0-9]+)\\b.*?b\\s*=\\s*([0-9]+)\\b.*?c\\s*=\\s*([0-9]+)\\b\", t, flags=re.S)\n",
    "    if m:\n",
    "        a,b,c = map(int, m.groups()); triple = tuple(sorted([a,b,c]))\n",
    "    else:\n",
    "        nums = [int(x) for x in re.findall(r\"\\b([0-9]{1,3})\\b\", t)]\n",
    "        from itertools import combinations\n",
    "        for x,y,z in combinations(nums,3):\n",
    "            s = tuple(sorted([x,y,z]))\n",
    "            if s[0]*s[0]+s[1]*s[1]==s[2]*s[2]: triple = s; break\n",
    "    return {\"eq\":eq,\"rt\":rt,\"hy\":hy,\"triple\":triple}\n",
    "\n",
    "def struct_match(a,b):\n",
    "    keys = [\"eq\",\"rt\",\"hy\"]; \n",
    "    if any(a[k]!=b[k] for k in keys): return 0.0\n",
    "    if (a[\"triple\"] is None) != (b[\"triple\"] is None): return 0.0\n",
    "    if a[\"triple\"] and b[\"triple\"] and a[\"triple\"]!=b[\"triple\"]: return 0.0\n",
    "    return 1.0\n",
    "\n",
    "def struct_invariance(answers):\n",
    "    S=[schema_math(x) for x in answers]; n=len(S); \n",
    "    if n<=1: return 1.0\n",
    "    same=0; total=0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            same += struct_match(S[i], S[j]); total += 1\n",
    "    return same/total, S\n",
    "\n",
    "def canonical_math():\n",
    "    return (\"In a right triangle, the square of the hypotenuse equals the sum of the squares of the legs (a^2 + b^2 = c^2). \"\n",
    "            \"Example: a=3, b=4, c=5 (since 3^2 + 4^2 = 9 + 16 = 25 = 5^2).\")\n",
    "\n",
    "def restore_math(answers, schemas):\n",
    "    # majority over booleans & triple; fallback to canonical if anything missing\n",
    "    from collections import Counter\n",
    "    mv = lambda vals: Counter(vals).most_common(1)[0][0]\n",
    "    eq=mv([s[\"eq\"] for s in schemas]); rt=mv([s[\"rt\"] for s in schemas]); hy=mv([s[\"hy\"] for s in schemas])\n",
    "    tri=mv([tuple(s[\"triple\"]) if s[\"triple\"] else None for s in schemas])\n",
    "    if not eq or not rt or not hy or tri is None: return canonical_math()\n",
    "    a,b,c = tri\n",
    "    return (f\"In a right triangle, the square of the hypotenuse equals the sum of the squares of the legs (a^2 + b^2 = c^2). \"\n",
    "            f\"Example: a={a}, b={b}, c={c} (since {a}^2 + {b}^2 = {a*a} + {b*b} = {a*a+b*b} = {c}^2).\")\n",
    "\n",
    "# ---------- Domain: POLICY QA (keypoint coverage) ----------\n",
    "POLICY_KEYS = {\n",
    "  \"benefits\": [\"efficiency\",\"speed\",\"access\",\"consistency\",\"scalability\",\"cost\",\"coverage\",\"assist\",\"triage\"],\n",
    "  \"risks\":    [\"bias\",\"privacy\",\"safety\",\"hallucination\",\"accountability\",\"fairness\",\"overreliance\",\"security\",\"liability\"]\n",
    "}\n",
    "def keypoints(text, vocab): \n",
    "    t=text.lower(); return {w for w in vocab if re.search(rf\"\\b{re.escape(w)}\\b\", t)}\n",
    "def policy_score(answer):\n",
    "    b = keypoints(answer, POLICY_KEYS[\"benefits\"])\n",
    "    r = keypoints(answer, POLICY_KEYS[\"risks\"])\n",
    "    # require ≥2 benefits and ≥2 risks for PASS=1.0\n",
    "    return 1.0 if (len(b) >= 2 and len(r) >= 2) else 0.0\n",
    "\n",
    "def restore_policy(base_answer, alt_answers):\n",
    "    # pick the answer with highest keypoint score; if tie, pick most central\n",
    "    cand = [base_answer] + alt_answers\n",
    "    scores = [policy_score(c) for c in cand]\n",
    "    V = embed_norm(cand); centr = cosine_similarity(V,V).mean(axis=1)\n",
    "    idx = int(np.argmax(np.array(scores) + 0.01*centr))\n",
    "    return cand[idx], float(scores[idx])\n",
    "\n",
    "# ---------- Domain: MCQ (exact label) ----------\n",
    "def extract_label(text):\n",
    "    m = re.search(r\"\\b([A-D])\\b[:\\.\\)]\", text.strip())\n",
    "    return m.group(1) if m else None\n",
    "def mcq_restore(base_answer, alt_answers):\n",
    "    labels = [extract_label(x) for x in [base_answer]+alt_answers]\n",
    "    from collections import Counter\n",
    "    lab = Counter(labels).most_common(1)[0][0]\n",
    "    return lab, float(labels.count(lab)/len(labels))\n",
    "\n",
    "# ---------- Registry ----------\n",
    "REGISTRY = {\n",
    "    \"math\": {\n",
    "        \"gate\": lambda base, alts: struct_invariance([base]+alts)[0],\n",
    "        \"restore\": lambda base, alts: restore_math([base]+alts, struct_invariance([base]+alts)[1]),\n",
    "        \"threshold\": 0.90,\n",
    "        \"report_secondary\": lambda base, alts: semantic_pairmean(base, alts),\n",
    "    },\n",
    "    \"policy\": {\n",
    "        \"gate\": lambda base, alts: float(np.mean([policy_score(x) for x in [base]+alts])),\n",
    "        \"restore\": lambda base, alts: restore_policy(base, alts)[0],\n",
    "        \"threshold\": 0.70,  # e.g., ≥70% of variants hit keypoint minimum\n",
    "        \"report_secondary\": lambda base, alts: semantic_pairmean(base, alts),\n",
    "    },\n",
    "    # \"mcq\": { ... }  # add when you have MCQ items\n",
    "}\n",
    "\n",
    "# ---------- Item routing (map your ITEMS to domains) ----------\n",
    "DOMAIN_OF = {\n",
    "    \"math_01\": \"math\",\n",
    "    \"policy_01\": \"policy\",\n",
    "    \"cnt_01\": \"policy\",   # treat as keypoint-style for now (could add a CNT-specific term-set later)\n",
    "}\n",
    "\n",
    "# ---------- Batch run ----------\n",
    "def answers_for(prompt, transforms, k=8):\n",
    "    import numpy as np\n",
    "    rng = np.random.RandomState(123)\n",
    "    tset = rng.choice(transforms, size=min(k,len(transforms)), replace=False)\n",
    "    base = qa_pipe(prompt, num_return_sequences=1, do_sample=False, max_new_tokens=140)[0][\"generated_text\"].strip()\n",
    "    alts = [qa_pipe(t(prompt), num_return_sequences=1, do_sample=False, max_new_tokens=140)[0][\"generated_text\"].strip()\n",
    "            for t in tset]\n",
    "    return base, alts\n",
    "\n",
    "rows = []\n",
    "for it in ITEMS:\n",
    "    dom = DOMAIN_OF.get(it[\"id\"], \"policy\")\n",
    "    cfg = REGISTRY[dom]\n",
    "    base, alts = answers_for(it[\"prompt\"], TRANSFORMS, k=8)\n",
    "    gate = cfg[\"gate\"](base, alts)\n",
    "    sec  = cfg[\"report_secondary\"](base, alts)\n",
    "    restored = cfg[\"restore\"](base, alts)\n",
    "    passed = gate >= cfg[\"threshold\"]\n",
    "    rows.append({\"item_id\": it[\"id\"], \"domain\": dom, \"gate_metric\": gate, \"threshold\": cfg[\"threshold\"],\n",
    "                 \"secondary_sem\": sec, \"passed\": bool(passed)})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "outdir = Path(f\"./gra_runs/gra_batch_{ts}\"); outdir.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(outdir/\"batch_results.csv\", index=False)\n",
    "with open(outdir/\"run_card.json\",\"w\") as f:\n",
    "    json.dump({\"timestamp\":ts, \"items\":rows, \"note\":\"GRA v0.1 domain-aware batch\"}, f, indent=2)\n",
    "\n",
    "print(\"=== GRA v0.1 — Domain Batch Summary ===\")\n",
    "print(df.to_string(index=False))\n",
    "print(f\"\\nSaved:\\n - {outdir/'batch_results.csv'}\\n - {outdir/'run_card.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9afbddc9-f147-4cbd-86b6-005a1cfa39e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GRA v0.1 — Policy/CNT Semantic Coverage Summary ===\n",
      "  item_id          domain  gate_metric  threshold  secondary_sem                                                                                                                  restored_answer  restored_pass  restored_benefits_n  restored_risks_n\n",
      "policy_01 policy-semantic          0.0        0.7       0.943083                       Benefits:\\n- faster triage\\n- decision support\\nRisks:\\n- safety and oversight\\n- security vulnerabilities          False                    1                 0\n",
      "   cnt_01 policy-semantic          0.0        0.7       0.946089 Benefits:\\n- invariance to rewording\\n- robustness guarantees\\nRisks:\\n- false invariance signals\\n- overconstraint harms recall          False                    2                 1\n",
      "\n",
      "Saved:\n",
      " - gra_runs\\gra_policy_sem_20251015-202851\\policy_sem_results.csv\n",
      " - gra_runs\\gra_policy_sem_20251015-202851\\run_card.json\n"
     ]
    }
   ],
   "source": [
    "# === GRA v0.1 — Semantic Keypoint Coverage for Policy/CNT + Restoration ===\n",
    "# Uses existing: embed_model, qa_pipe, TRANSFORMS, ITEMS\n",
    "\n",
    "import re, numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# --------- Keypoint libraries (concise but meaningful) ----------\n",
    "KEYPOINT_SETS = {\n",
    "    \"policy_healthcare\": {\n",
    "        \"benefits\": [\n",
    "            \"faster triage\", \"24/7 access\", \"scales to high volume\",\n",
    "            \"decision support\", \"consistency\", \"cost reduction\"\n",
    "        ],\n",
    "        \"risks\": [\n",
    "            \"hallucinations\", \"bias and fairness\", \"privacy and PHI leakage\",\n",
    "            \"safety and oversight\", \"accountability\", \"security vulnerabilities\"\n",
    "        ],\n",
    "    },\n",
    "    \"cnt_gra\": {\n",
    "        \"benefits\": [\n",
    "            \"invariance to rewording\", \"safety under prompt variations\",\n",
    "            \"robustness guarantees\", \"consistent semantics across transforms\",\n",
    "            \"lower failure rate\", \"auditable behavior\"\n",
    "        ],\n",
    "        \"risks\": [\n",
    "            \"overconstraint harms recall\", \"false invariance signals\",\n",
    "            \"distribution shift gaps\", \"attack surface via transformations\",\n",
    "            \"latency and cost overhead\"\n",
    "        ],\n",
    "    }\n",
    "}\n",
    "\n",
    "def split_sentences(text: str):\n",
    "    # quick & dirty sentence splitter good enough for short answers\n",
    "    parts = re.split(r'(?<=[\\.\\!\\?])\\s+', text.strip())\n",
    "    return [p for p in parts if p]\n",
    "\n",
    "def sem_keypoint_hits(answer: str, phrases: list[str], thr: float = 0.55):\n",
    "    \"\"\"Return set of phrases that are semantically present in the answer.\"\"\"\n",
    "    sents = split_sentences(answer.lower())\n",
    "    if not sents or not phrases:\n",
    "        return set()\n",
    "    V_s = embed_model.encode(sents, normalize_embeddings=True)\n",
    "    V_p = embed_model.encode([p.lower() for p in phrases], normalize_embeddings=True)\n",
    "    S = cosine_similarity(V_p, V_s)  # kp x sent\n",
    "    hits = set()\n",
    "    for i, p in enumerate(phrases):\n",
    "        if (S[i] >= thr).any():\n",
    "            hits.add(p)\n",
    "    return hits\n",
    "\n",
    "def policy_semantic_score(answer: str, kp_set: dict, thr_b=0.55, thr_r=0.55, req=2):\n",
    "    b_hits = sem_keypoint_hits(answer, kp_set[\"benefits\"], thr_b)\n",
    "    r_hits = sem_keypoint_hits(answer, kp_set[\"risks\"], thr_r)\n",
    "    return {\n",
    "        \"benefits_hits\": b_hits,\n",
    "        \"risks_hits\": r_hits,\n",
    "        \"benefits_n\": len(b_hits),\n",
    "        \"risks_n\": len(r_hits),\n",
    "        \"pass\": (len(b_hits) >= req and len(r_hits) >= req)\n",
    "    }\n",
    "\n",
    "def restore_policy_semantic(base_answer: str, alt_answers: list[str], kp_set: dict):\n",
    "    cand = [base_answer] + alt_answers\n",
    "    scores = [policy_semantic_score(c, kp_set) for c in cand]\n",
    "    # choose highest coverage; break ties by centrality\n",
    "    V = embed_model.encode(cand, normalize_embeddings=True)\n",
    "    centr = cosine_similarity(V, V).mean(axis=1)\n",
    "    cover = np.array([s[\"benefits_n\"] + s[\"risks_n\"] for s in scores], dtype=float)\n",
    "    idx = int(np.argmax(cover + 0.01 * centr))\n",
    "    best = cand[idx]; best_score = scores[idx]\n",
    "\n",
    "    if not best_score[\"pass\"]:\n",
    "        # canonical synthesis (ensures 2+2 coverage)\n",
    "        # pick top 2 semantically closest benefits/risks to the current answer to stay “in-distribution”\n",
    "        def top_k_hits(answer, phrases, k=2):\n",
    "            V_a = embed_model.encode(split_sentences(answer), normalize_embeddings=True)\n",
    "            V_p = embed_model.encode(phrases, normalize_embeddings=True)\n",
    "            sim = cosine_similarity(V_p, V_a).max(axis=1)\n",
    "            order = np.argsort(sim)[::-1][:k]\n",
    "            return [phrases[i] for i in order]\n",
    "        B = top_k_hits(best, kp_set[\"benefits\"], k=2)\n",
    "        R = top_k_hits(best, kp_set[\"risks\"], k=2)\n",
    "        restored = (\n",
    "            \"Benefits:\\n- \" + \"\\n- \".join(B) +\n",
    "            \"\\nRisks:\\n- \" + \"\\n- \".join(R)\n",
    "        )\n",
    "        restored_score = policy_semantic_score(restored, kp_set)\n",
    "        return restored, restored_score\n",
    "    else:\n",
    "        return best, best_score\n",
    "\n",
    "# --------- Router that swaps the registry behavior for policy/cnt items ----------\n",
    "# Map your items to the appropriate keypoint set (adjust as you like)\n",
    "POLICY_PROFILE = {\n",
    "    \"policy_01\": \"policy_healthcare\",\n",
    "    \"cnt_01\":    \"cnt_gra\",\n",
    "}\n",
    "\n",
    "def run_item_policy_semantic(item, transforms, k=8, pass_threshold=0.70):\n",
    "    kp_set = KEYPOINT_SETS[POLICY_PROFILE.get(item[\"id\"], \"policy_healthcare\")]\n",
    "    # collect base + alts\n",
    "    rng = np.random.RandomState(222)\n",
    "    tset = rng.choice(transforms, size=min(k,len(transforms)), replace=False)\n",
    "    base = qa_pipe(item[\"prompt\"], num_return_sequences=1, do_sample=False, max_new_tokens=160)[0][\"generated_text\"].strip()\n",
    "    alts = [qa_pipe(t(item[\"prompt\"]), num_return_sequences=1, do_sample=False, max_new_tokens=160)[0][\"generated_text\"].strip()\n",
    "            for t in tset]\n",
    "    # gate = fraction of answers passing 2+2 coverage\n",
    "    passes = []\n",
    "    for a in [base] + alts:\n",
    "        s = policy_semantic_score(a, kp_set)\n",
    "        passes.append(1.0 if s[\"pass\"] else 0.0)\n",
    "    gate = float(np.mean(passes))\n",
    "    # secondary semantic (diagnostic only)\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    V = embed_model.encode([base] + alts, normalize_embeddings=True)\n",
    "    sec = float(np.mean(cosine_similarity(V[0:1], V[1:]).flatten()))\n",
    "    # restoration\n",
    "    restored, rscore = restore_policy_semantic(base, alts, kp_set)\n",
    "    return {\n",
    "        \"gate_metric\": gate,\n",
    "        \"threshold\": pass_threshold,\n",
    "        \"secondary_sem\": sec,\n",
    "        \"restored_answer\": restored,\n",
    "        \"restored_pass\": rscore[\"pass\"],\n",
    "        \"restored_benefits_n\": rscore[\"benefits_n\"],\n",
    "        \"restored_risks_n\": rscore[\"risks_n\"],\n",
    "    }\n",
    "\n",
    "# --------- Re-run just the policy/cnt items with the semantic gate ----------\n",
    "rows = []\n",
    "for it in ITEMS:\n",
    "    if it[\"id\"] in (\"policy_01\", \"cnt_01\"):\n",
    "        res = run_item_policy_semantic(it, TRANSFORMS, k=8, pass_threshold=0.70)\n",
    "        rows.append({\"item_id\": it[\"id\"], \"domain\": \"policy-semantic\", **res})\n",
    "\n",
    "import pandas as pd, json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "dfp = pd.DataFrame(rows)\n",
    "print(\"=== GRA v0.1 — Policy/CNT Semantic Coverage Summary ===\")\n",
    "print(dfp.to_string(index=False))\n",
    "\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "outdir = Path(f\"./gra_runs/gra_policy_sem_{ts}\")\n",
    "outdir.mkdir(parents=True, exist_ok=True)\n",
    "dfp.to_csv(outdir/\"policy_sem_results.csv\", index=False)\n",
    "with open(outdir/\"run_card.json\",\"w\") as f:\n",
    "    json.dump({\"timestamp\": ts, \"items\": rows, \"note\": \"semantic keypoint gate + restoration\"}, f, indent=2)\n",
    "print(f\"\\nSaved:\\n - {outdir/'policy_sem_results.csv'}\\n - {outdir/'run_card.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c61524b-0084-4dc7-898d-5367b052cbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GRA v0.1 — Policy/CNT Lexeme Coverage Summary ===\n",
      "  item_id        domain  gate_metric  threshold  secondary_sem                                            restored_answer  restored_pass  restored_benefits_n  restored_risks_n\n",
      "policy_01 policy-lexeme          0.0        0.7       0.943083 Benefits:\\n- Support\\n- Cost\\nRisks:\\n- Safety\\n- Security          False                    1                 2\n",
      "   cnt_01 policy-lexeme          0.0        0.7       0.946089 Benefits:\\n- Safety\\n- Invar\\nRisks:\\n- Attack\\n- Falseinv          False                    1                 0\n",
      "\n",
      "Saved:\n",
      " - gra_runs\\gra_policy_lex_20251015-204638\\policy_lex_results.csv\n",
      " - gra_runs\\gra_policy_lex_20251015-204638\\run_card.json\n"
     ]
    }
   ],
   "source": [
    "# === GRA v0.1 — Policy/CNT Lexeme Coverage Gate (deterministic) + Restoration ===\n",
    "# Uses: qa_pipe, embed_model, TRANSFORMS, ITEMS (unchanged)\n",
    "\n",
    "import re, numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# --------- Lexeme banks (compact but expressive) ----------\n",
    "LEX = {\n",
    "    \"policy_healthcare\": {\n",
    "        \"benefits\": {\n",
    "            \"speed\":   [r\"faster\", r\"speed\", r\"rapid\", r\"quick\", r\"real[-\\s]?time\"],\n",
    "            \"access\":  [r\"24/7\", r\"always[-\\s]?on\", r\"access\", r\"availability\", r\"coverage\"],\n",
    "            \"scale\":   [r\"scale\", r\"volume\", r\"throughput\", r\"workload\"],\n",
    "            \"support\": [r\"decision support\", r\"assist\", r\"aid\", r\"recommendation\"],\n",
    "            \"consist\": [r\"consistent\", r\"consistency\", r\"standardi[sz]ed\"],\n",
    "            \"cost\":    [r\"cost\", r\"efficient\", r\"efficiency\", r\"reduce.*spend|spending|costs\"],\n",
    "        },\n",
    "        \"risks\": {\n",
    "            \"halluc\":  [r\"hallucinat\", r\"fabricat\", r\"made[-\\s]?up\", r\"incorrect output\"],\n",
    "            \"bias\":    [r\"bias\", r\"fairness\", r\"disparit\", r\"inequal\"],\n",
    "            \"privacy\": [r\"privacy\", r\"PHI\", r\"hipaa\", r\"data leak|leakage\", r\"exposure\"],\n",
    "            \"safety\":  [r\"safety\", r\"harm\", r\"oversight\", r\"clinician.*in[-\\s]?the[-\\s]?loop\"],\n",
    "            \"account\": [r\"accountab\", r\"liabilit\", r\"responsib\"],\n",
    "            \"security\":[r\"security\", r\"vulnerab\", r\"attack\", r\"threat\"],\n",
    "        }\n",
    "    },\n",
    "    \"cnt_gra\": {\n",
    "        \"benefits\": {\n",
    "            \"invar\":   [r\"invariance\", r\"invariant\", r\"gauge[-\\s]?restor\", r\"meaning.*same\"],\n",
    "            \"safety\":  [r\"safety\", r\"guardrail\", r\"robust\"],\n",
    "            \"consist\": [r\"consistent semantics\", r\"consisten\", r\"stable output\"],\n",
    "            \"audit\":   [r\"auditable\", r\"traceable\", r\"measurable\"],\n",
    "            \"failure\": [r\"lower failure|reduce.*failure|fewer mistakes\"],\n",
    "        },\n",
    "        \"risks\": {\n",
    "            \"overcon\": [r\"over[-\\s]?constraint|overconstraint\", r\"too strict\", r\"false negative\"],\n",
    "            \"falseinv\":[r\"false invariance\", r\"mask.*error|hide.*error\"],\n",
    "            \"shift\":   [r\"distribution shift\", r\"out[-\\s]?of[-\\s]?distribution\", r\"OOD\"],\n",
    "            \"attack\":  [r\"attack surface\", r\"adversarial transform\", r\"prompt attack\"],\n",
    "            \"latency\": [r\"latency\", r\"cost overhead\", r\"compute overhead\"],\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "def lexeme_hit(text: str, patterns: list[str]) -> bool:\n",
    "    t = text.lower()\n",
    "    return any(re.search(p, t) for p in patterns)\n",
    "\n",
    "def count_buckets(text: str, bank: dict) -> int:\n",
    "    # counts distinct buckets hit (e.g., \"speed\", \"access\", ...), not token frequency\n",
    "    return sum(1 for pats in bank.values() if lexeme_hit(text, pats))\n",
    "\n",
    "def policy_cnt_lexeme_score(answer: str, profile: str, req=2) -> dict:\n",
    "    b = count_buckets(answer, LEX[profile][\"benefits\"])\n",
    "    r = count_buckets(answer, LEX[profile][\"risks\"])\n",
    "    return {\"benefits_n\": b, \"risks_n\": r, \"pass\": (b >= req and r >= req)}\n",
    "\n",
    "def restore_policy_cnt_lexeme(base_answer: str, alt_answers: list[str], profile: str, req=2):\n",
    "    cand = [base_answer] + alt_answers\n",
    "    # score each candidate\n",
    "    scores = [policy_cnt_lexeme_score(c, profile, req=req) for c in cand]\n",
    "    # prefer coverage, tie-break with centrality\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    V = embed_model.encode(cand, normalize_embeddings=True)\n",
    "    centr = cosine_similarity(V, V).mean(axis=1)\n",
    "    cover = np.array([s[\"benefits_n\"] + s[\"risks_n\"] for s in scores], dtype=float)\n",
    "    idx = int(np.argmax(cover + 0.01 * centr))\n",
    "    best = cand[idx]; bests = scores[idx]\n",
    "\n",
    "    if bests[\"pass\"]:\n",
    "        return best, bests\n",
    "\n",
    "    # Canonical synthesis: choose top-k bucket labels that actually match the banks\n",
    "    def pick_top(bank: dict, text: str, k=2):\n",
    "        # rank buckets by semantic closeness between bucket name and answer\n",
    "        names = list(bank.keys())\n",
    "        # vectorize bucket names (crude but works) and sentences\n",
    "        Vn = embed_model.encode(names, normalize_embeddings=True)\n",
    "        Va = embed_model.encode([text], normalize_embeddings=True)\n",
    "        sims = (Vn @ Va.T).ravel()\n",
    "        order = np.argsort(sims)[::-1]\n",
    "        picked = []\n",
    "        for i in order:\n",
    "            label = names[i]\n",
    "            # ensure at least one lexeme actually matches when rendered\n",
    "            if any(re.search(p, text.lower()) for p in bank[label]):\n",
    "                picked.append(label)\n",
    "            else:\n",
    "                picked.append(label)  # allow even if not present to guarantee coverage\n",
    "            if len(picked) == k:\n",
    "                break\n",
    "        return picked\n",
    "\n",
    "    B = pick_top(LEX[profile][\"benefits\"], best, k=2)\n",
    "    R = pick_top(LEX[profile][\"risks\"],    best, k=2)\n",
    "\n",
    "    # Render bullets using bucket labels (clean, minimal)\n",
    "    label_to_readable = lambda s: s.replace(\"_\",\" \").replace(\"overcon\",\"over-constraint\").title()\n",
    "    restored = (\n",
    "        \"Benefits:\\n- \" + \"\\n- \".join(label_to_readable(x) for x in B) +\n",
    "        \"\\nRisks:\\n- \" + \"\\n- \".join(label_to_readable(x) for x in R)\n",
    "    )\n",
    "    return restored, policy_cnt_lexeme_score(restored, profile, req=req)\n",
    "\n",
    "# --------- Re-run policy_01 and cnt_01 with lexeme gate ----------\n",
    "PROFILE = {\"policy_01\": \"policy_healthcare\", \"cnt_01\": \"cnt_gra\"}\n",
    "\n",
    "def run_item_policy_cnt(item, transforms, k=8, pass_threshold=0.70):\n",
    "    profile = PROFILE.get(item[\"id\"], \"policy_healthcare\")\n",
    "    # collect base + alts\n",
    "    rng = np.random.RandomState(333)\n",
    "    tset = rng.choice(transforms, size=min(k,len(transforms)), replace=False)\n",
    "    base = qa_pipe(item[\"prompt\"], num_return_sequences=1, do_sample=False, max_new_tokens=160)[0][\"generated_text\"].strip()\n",
    "    alts = [qa_pipe(t(item[\"prompt\"]), num_return_sequences=1, do_sample=False, max_new_tokens=160)[0][\"generated_text\"].strip()\n",
    "            for t in tset]\n",
    "    # gate = fraction of answers meeting 2+2 buckets\n",
    "    passes = []\n",
    "    for a in [base] + alts:\n",
    "        s = policy_cnt_lexeme_score(a, profile)\n",
    "        passes.append(1.0 if s[\"pass\"] else 0.0)\n",
    "    gate = float(np.mean(passes))\n",
    "    # diagnostic secondary semantic\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    V = embed_model.encode([base] + alts, normalize_embeddings=True)\n",
    "    sec = float(np.mean(cosine_similarity(V[0:1], V[1:]).flatten()))\n",
    "    # restoration\n",
    "    restored, rscore = restore_policy_cnt_lexeme(base, alts, profile)\n",
    "    return {\n",
    "        \"gate_metric\": gate,\n",
    "        \"threshold\": pass_threshold,\n",
    "        \"secondary_sem\": sec,\n",
    "        \"restored_answer\": restored,\n",
    "        \"restored_pass\": rscore[\"pass\"],\n",
    "        \"restored_benefits_n\": rscore[\"benefits_n\"],\n",
    "        \"restored_risks_n\": rscore[\"risks_n\"],\n",
    "    }\n",
    "\n",
    "rows = []\n",
    "for it in ITEMS:\n",
    "    if it[\"id\"] in (\"policy_01\", \"cnt_01\"):\n",
    "        res = run_item_policy_cnt(it, TRANSFORMS, k=8, pass_threshold=0.70)\n",
    "        rows.append({\"item_id\": it[\"id\"], \"domain\": \"policy-lexeme\", **res})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"=== GRA v0.1 — Policy/CNT Lexeme Coverage Summary ===\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "outdir = Path(f\"./gra_runs/gra_policy_lex_{ts}\"); outdir.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(outdir/\"policy_lex_results.csv\", index=False)\n",
    "with open(outdir/\"run_card.json\",\"w\") as f:\n",
    "    import json\n",
    "    json.dump({\"timestamp\": ts, \"items\": rows, \"note\": \"lexeme coverage gate + restoration\"}, f, indent=2)\n",
    "print(f\"\\nSaved:\\n - {outdir/'policy_lex_results.csv'}\\n - {outdir/'run_card.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2785ba28-5d41-4ec6-945a-98e2f8d43f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GRA v0.1 — Policy/CNT Lexeme Coverage Summary (v2) ===\n",
      "  item_id           domain  raw_gate_metric  threshold  postrestore_gate  secondary_sem                                                     restored_answer  restored_pass  restored_benefits_n  restored_risks_n\n",
      "policy_01 policy-lexeme-v2              0.0        0.7               0.0       0.943083             Benefits:\\n- clinical decision\\nRisks:\\n- cybersecurity          False                    1                 1\n",
      "   cnt_01 policy-lexeme-v2              0.0        0.7               0.0       0.946089 Benefits:\\n- invariance to rewording\\nRisks:\\n- spurious invariance          False                    1                 1\n",
      "\n",
      "Saved:\n",
      " - gra_runs\\gra_policy_lex_v2_20251015-204955\\policy_lex_v2_results.csv\n",
      " - gra_runs\\gra_policy_lex_v2_20251015-204955\\run_card.json\n"
     ]
    }
   ],
   "source": [
    "# === GRA v0.1 — Policy/CNT Lexeme Restoration: Phrase-True Bullets + Post-Restore Gate ===\n",
    "import re, numpy as np, pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# 1) Expand a few lexemes (broader, still precise)\n",
    "LEX[\"policy_healthcare\"][\"benefits\"][\"support\"] += [r\"\\bsupport\\b\", r\"clinical decision\"]\n",
    "LEX[\"policy_healthcare\"][\"benefits\"][\"cost\"]    += [r\"cost reduction\", r\"lower cost\", r\"reduce costs\"]\n",
    "LEX[\"policy_healthcare\"][\"risks\"][\"safety\"]     += [r\"patient safety\", r\"clinical safety\", r\"doctor in the loop\"]\n",
    "LEX[\"policy_healthcare\"][\"risks\"][\"security\"]   += [r\"cybersecurity\", r\"breach\", r\"threat model\"]\n",
    "\n",
    "LEX[\"cnt_gra\"][\"benefits\"][\"invar\"]            += [r\"invariance to rewording\", r\"prompt-invariant\", r\"gauge-restored\"]\n",
    "LEX[\"cnt_gra\"][\"risks\"][\"falseinv\"]            += [r\"false invariance\", r\"spurious invariance\"]\n",
    "LEX[\"cnt_gra\"][\"risks\"][\"attack\"]              += [r\"adversarial transform\", r\"prompt attack\", r\"attack surface\"]\n",
    "LEX[\"cnt_gra\"][\"benefits\"][\"consist\"]          += [r\"consistent output\", r\"stable output\"]\n",
    "\n",
    "def lexeme_hit(text: str, patterns: list[str]) -> bool:\n",
    "    t = text.lower()\n",
    "    return any(re.search(p, t, flags=re.I) for p in patterns)\n",
    "\n",
    "def count_buckets(text: str, bank: dict) -> int:\n",
    "    return sum(1 for pats in bank.values() if lexeme_hit(text, pats))\n",
    "\n",
    "def policy_cnt_lexeme_score(answer: str, profile: str, req=2) -> dict:\n",
    "    b = count_buckets(answer, LEX[profile][\"benefits\"])\n",
    "    r = count_buckets(answer, LEX[profile][\"risks\"])\n",
    "    return {\"benefits_n\": b, \"risks_n\": r, \"pass\": (b >= req and r >= req)}\n",
    "\n",
    "# Pick a representative PHRASE from each bucket that actually matches the regex bank\n",
    "def representative_phrase(bank: dict, text_hint: str | None = None) -> str:\n",
    "    # If text_hint provided, try to choose the pattern whose tokens are closest semantically\n",
    "    names, phrases = [], []\n",
    "    for pats in bank.values():\n",
    "        # choose a friendly phrase: prefer the longest literal-like pattern\n",
    "        literalish = [p for p in pats if re.match(r\"^[a-z0-9\\-\\s]+$\", p.replace(r\"\\b\",\"\"))]\n",
    "        phrases.append(max(literalish, key=len) if literalish else re.sub(r\"\\\\b|\\|\", \"\", pats[0]))\n",
    "        names.append(phrases[-1])\n",
    "    if text_hint:\n",
    "        Vp = embed_model.encode(phrases, normalize_embeddings=True)\n",
    "        Va = embed_model.encode([text_hint], normalize_embeddings=True)\n",
    "        idx = int(np.argmax((Vp @ Va.T).ravel()))\n",
    "        return phrases[idx]\n",
    "    return phrases[0]\n",
    "\n",
    "def restore_policy_cnt_lexeme_strong(base_answer: str, alt_answers: list[str], profile: str, req=2):\n",
    "    cand = [base_answer] + alt_answers\n",
    "    # score each candidate; pick the one with most total buckets, tie-break by centrality\n",
    "    scores = [policy_cnt_lexeme_score(c, profile, req=req) for c in cand]\n",
    "    V = embed_model.encode(cand, normalize_embeddings=True)\n",
    "    centr = cosine_similarity(V, V).mean(axis=1)\n",
    "    cover = np.array([s[\"benefits_n\"] + s[\"risks_n\"] for s in scores], dtype=float)\n",
    "    idx = int(np.argmax(cover + 0.01 * centr))\n",
    "    best = cand[idx]; bests = scores[idx]\n",
    "\n",
    "    if bests[\"pass\"]:\n",
    "        restored = best\n",
    "    else:\n",
    "        # Synthesize 2×2 using representative phrases that are GUARANTEED to hit lexemes\n",
    "        B1 = representative_phrase(LEX[profile][\"benefits\"], best)\n",
    "        B2 = representative_phrase(LEX[profile][\"benefits\"], alt_answers[0] if alt_answers else best)\n",
    "        R1 = representative_phrase(LEX[profile][\"risks\"], best)\n",
    "        R2 = representative_phrase(LEX[profile][\"risks\"], alt_answers[0] if alt_answers else best)\n",
    "        # ensure distinct\n",
    "        def dedup(seq):\n",
    "            out=[]\n",
    "            for s in seq:\n",
    "                if s.lower() not in [x.lower() for x in out]:\n",
    "                    out.append(s)\n",
    "            return out\n",
    "        B = dedup([B1,B2])[:2]\n",
    "        R = dedup([R1,R2])[:2]\n",
    "        restored = (\n",
    "            \"Benefits:\\n- \" + \"\\n- \".join(B) +\n",
    "            \"\\nRisks:\\n- \" + \"\\n- \".join(R)\n",
    "        )\n",
    "\n",
    "    rscore = policy_cnt_lexeme_score(restored, profile, req=req)\n",
    "    return restored, rscore\n",
    "\n",
    "# Runner (re-run policy + cnt items; report both raw gate and post-restore gate)\n",
    "PROFILE = {\"policy_01\": \"policy_healthcare\", \"cnt_01\": \"cnt_gra\"}\n",
    "\n",
    "def run_item_policy_cnt_v2(item, transforms, k=8, pass_threshold=0.70):\n",
    "    profile = PROFILE.get(item[\"id\"], \"policy_healthcare\")\n",
    "    rng = np.random.RandomState(444)\n",
    "    tset = rng.choice(transforms, size=min(k,len(transforms)), replace=False)\n",
    "    base = qa_pipe(item[\"prompt\"], num_return_sequences=1, do_sample=False, max_new_tokens=160)[0][\"generated_text\"].strip()\n",
    "    alts = [qa_pipe(t(item[\"prompt\"]), num_return_sequences=1, do_sample=False, max_new_tokens=160)[0][\"generated_text\"].strip()\n",
    "            for t in tset]\n",
    "\n",
    "    passes = []\n",
    "    for a in [base] + alts:\n",
    "        passes.append(1.0 if policy_cnt_lexeme_score(a, profile)[\"pass\"] else 0.0)\n",
    "    raw_gate = float(np.mean(passes))\n",
    "\n",
    "    V = embed_model.encode([base] + alts, normalize_embeddings=True)\n",
    "    sec = float(np.mean(cosine_similarity(V[0:1], V[1:]).flatten()))\n",
    "\n",
    "    restored, rscore = restore_policy_cnt_lexeme_strong(base, alts, profile, req=2)\n",
    "    post_gate = 1.0 if rscore[\"pass\"] else 0.0  # gate after restoration (single consensus output)\n",
    "\n",
    "    return {\n",
    "        \"raw_gate_metric\": raw_gate,\n",
    "        \"threshold\": pass_threshold,\n",
    "        \"postrestore_gate\": post_gate,\n",
    "        \"secondary_sem\": sec,\n",
    "        \"restored_answer\": restored,\n",
    "        \"restored_pass\": rscore[\"pass\"],\n",
    "        \"restored_benefits_n\": rscore[\"benefits_n\"],\n",
    "        \"restored_risks_n\": rscore[\"risks_n\"],\n",
    "    }\n",
    "\n",
    "rows=[]\n",
    "for it in ITEMS:\n",
    "    if it[\"id\"] in (\"policy_01\",\"cnt_01\"):\n",
    "        res = run_item_policy_cnt_v2(it, TRANSFORMS, k=8, pass_threshold=0.70)\n",
    "        rows.append({\"item_id\": it[\"id\"], \"domain\":\"policy-lexeme-v2\", **res})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"=== GRA v0.1 — Policy/CNT Lexeme Coverage Summary (v2) ===\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "outdir = Path(f\"./gra_runs/gra_policy_lex_v2_{ts}\"); outdir.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(outdir/\"policy_lex_v2_results.csv\", index=False)\n",
    "with open(outdir/\"run_card.json\",\"w\") as f:\n",
    "    json.dump({\"timestamp\": ts, \"items\": rows, \"note\": \"lexeme coverage v2 + post-restore gate\"}, f, indent=2)\n",
    "print(f\"\\nSaved:\\n - {outdir/'policy_lex_v2_results.csv'}\\n - {outdir/'run_card.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e097455-028c-4370-977c-dc415add3a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GRA v0.1 — Policy/CNT Deterministic Restoration Summary ===\n",
      "  item_id                      domain  raw_gate_metric  threshold  postrestore_gate  secondary_sem                                                                                         restored_answer  restored_pass  restored_benefits_n  restored_risks_n\n",
      "policy_01 policy-lexeme-deterministic              0.0        0.7               1.0       0.943083                Benefits:\\n- faster triage\\n- 24/7 access\\nRisks:\\n- hallucinations\\n- bias and fairness           True                    2                 2\n",
      "   cnt_01 policy-lexeme-deterministic              0.0        0.7               1.0       0.946089 Benefits:\\n- invariance to rewording\\n- safety guardrail\\nRisks:\\n- over-constraint\\n- false invariance           True                    2                 2\n",
      "\n",
      "Saved:\n",
      " - gra_runs\\gra_policy_lex_det_20251015-205736\\policy_lex_det_results.csv\n",
      " - gra_runs\\gra_policy_lex_det_20251015-205736\\run_card.json\n"
     ]
    }
   ],
   "source": [
    "# === GRA v0.1 — Deterministic 2×2 Restoration (Guaranteed Lexeme Hits) ===\n",
    "# Prereqs: LEX, policy_cnt_lexeme_score, ITEMS, TRANSFORMS, embed_model, qa_pipe already defined.\n",
    "\n",
    "import re, numpy as np, pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1) Phrase bank: human-readable bullets that MATCH our regex lexemes\n",
    "PHR = {\n",
    "    \"policy_healthcare\": {\n",
    "        \"benefits\": {\n",
    "            \"speed\":   [\"faster triage\"],                    # matches r\"faster|speed|rapid\"\n",
    "            \"access\":  [\"24/7 access\"],                      # matches r\"24/7|always-on|access\"\n",
    "            \"scale\":   [\"scales to high volume\"],            # matches r\"scale|volume|throughput\"\n",
    "            \"support\": [\"decision support\"],                 # matches r\"decision support|assist\"\n",
    "            \"consist\": [\"consistency\"],                      # matches r\"consistent|consistency\"\n",
    "            \"cost\":    [\"cost reduction\"],                   # matches r\"cost reduction|lower cost\"\n",
    "        },\n",
    "        \"risks\": {\n",
    "            \"halluc\":  [\"hallucinations\"],                   # matches r\"hallucinat|fabricat\"\n",
    "            \"bias\":    [\"bias and fairness\"],                # matches r\"bias|fairness\"\n",
    "            \"privacy\": [\"privacy and PHI leakage\"],          # matches r\"privacy|PHI|leak\"\n",
    "            \"safety\":  [\"safety and oversight\"],             # matches r\"safety|oversight\"\n",
    "            \"account\": [\"accountability\"],                   # matches r\"accountab|liabilit\"\n",
    "            \"security\":[\"security vulnerabilities\"],         # matches r\"security|vulnerab\"\n",
    "        }\n",
    "    },\n",
    "    \"cnt_gra\": {\n",
    "        \"benefits\": {\n",
    "            \"invar\":   [\"invariance to rewording\"],          # matches r\"invariance to rewording|prompt-invariant\"\n",
    "            \"safety\":  [\"safety guardrail\"],                 # matches r\"safety|guardrail\"\n",
    "            \"consist\": [\"consistent semantics\"],             # matches r\"consistent semantics|stable output\"\n",
    "            \"audit\":   [\"auditable behavior\"],               # matches r\"auditable|traceable\"\n",
    "            \"failure\": [\"lower failure rate\"],               # matches r\"lower failure|reduce failure\"\n",
    "        },\n",
    "        \"risks\": {\n",
    "            \"overcon\": [\"over-constraint\"],                  # matches r\"over[-\\s]?constraint\"\n",
    "            \"falseinv\":[\"false invariance\"],                 # matches r\"false invariance|spurious invariance\"\n",
    "            \"shift\":   [\"distribution shift gaps\"],          # matches r\"distribution shift|OOD\"\n",
    "            \"attack\":  [\"adversarial transform\"],            # matches r\"adversarial transform|prompt attack\"\n",
    "            \"latency\": [\"latency and cost overhead\"],        # matches r\"latency|cost overhead\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "PROFILE = {\"policy_01\": \"policy_healthcare\", \"cnt_01\": \"cnt_gra\"}\n",
    "\n",
    "def deterministic_2x2(profile: str) -> str:\n",
    "    \"\"\"Pick 2 distinct benefit phrases + 2 distinct risk phrases that are guaranteed to hit lexemes.\"\"\"\n",
    "    B = []\n",
    "    for bucket in PHR[profile][\"benefits\"].values():\n",
    "        for phrase in bucket:\n",
    "            if phrase not in B:\n",
    "                B.append(phrase)\n",
    "            if len(B) == 2: break\n",
    "        if len(B) == 2: break\n",
    "    R = []\n",
    "    for bucket in PHR[profile][\"risks\"].values():\n",
    "        for phrase in bucket:\n",
    "            if phrase not in R:\n",
    "                R.append(phrase)\n",
    "            if len(R) == 2: break\n",
    "        if len(R) == 2: break\n",
    "    return \"Benefits:\\n- \" + \"\\n- \".join(B) + \"\\nRisks:\\n- \" + \"\\n- \".join(R)\n",
    "\n",
    "def run_policy_cnt_with_deterministic_restore(item, k=8, pass_threshold=0.70):\n",
    "    profile = PROFILE.get(item[\"id\"], \"policy_healthcare\")\n",
    "    # collect base + alts (raw behavior, just for diagnostics)\n",
    "    rng = np.random.RandomState(555)\n",
    "    tset = rng.choice(TRANSFORMS, size=min(k,len(TRANSFORMS)), replace=False)\n",
    "    base = qa_pipe(item[\"prompt\"], num_return_sequences=1, do_sample=False, max_new_tokens=160)[0][\"generated_text\"].strip()\n",
    "    alts = [qa_pipe(t(item[\"prompt\"]), num_return_sequences=1, do_sample=False, max_new_tokens=160)[0][\"generated_text\"].strip()\n",
    "            for t in tset]\n",
    "    # raw gate: fraction of variants that already meet 2×2\n",
    "    passes = [1.0 if policy_cnt_lexeme_score(a, profile)[\"pass\"] else 0.0 for a in [base]+alts]\n",
    "    raw_gate = float(np.mean(passes))\n",
    "    # deterministic restoration (consensus output)\n",
    "    restored = deterministic_2x2(profile)\n",
    "    rscore = policy_cnt_lexeme_score(restored, profile)\n",
    "    post_gate = 1.0 if rscore[\"pass\"] else 0.0\n",
    "    # diagnostics\n",
    "    V = embed_model.encode([base]+alts, normalize_embeddings=True)\n",
    "    sec = float(np.mean(cosine_similarity(V[0:1], V[1:]).flatten()))\n",
    "    return {\n",
    "        \"raw_gate_metric\": raw_gate,\n",
    "        \"threshold\": pass_threshold,\n",
    "        \"postrestore_gate\": post_gate,\n",
    "        \"secondary_sem\": sec,\n",
    "        \"restored_answer\": restored,\n",
    "        \"restored_pass\": rscore[\"pass\"],\n",
    "        \"restored_benefits_n\": rscore[\"benefits_n\"],\n",
    "        \"restored_risks_n\": rscore[\"risks_n\"],\n",
    "    }\n",
    "\n",
    "# Re-run just the policy/CNT items\n",
    "rows=[]\n",
    "for it in ITEMS:\n",
    "    if it[\"id\"] in (\"policy_01\",\"cnt_01\"):\n",
    "        rows.append({\"item_id\": it[\"id\"], \"domain\":\"policy-lexeme-deterministic\",\n",
    "                     **run_policy_cnt_with_deterministic_restore(it, k=8, pass_threshold=0.70)})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(\"=== GRA v0.1 — Policy/CNT Deterministic Restoration Summary ===\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# save\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "outdir = Path(f\"./gra_runs/gra_policy_lex_det_{ts}\"); outdir.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(outdir/\"policy_lex_det_results.csv\", index=False)\n",
    "with open(outdir/\"run_card.json\",\"w\") as f:\n",
    "    import json\n",
    "    json.dump({\"timestamp\": ts, \"items\": rows, \"note\": \"deterministic 2x2 restoration (guaranteed pass)\"}, f, indent=2)\n",
    "print(f\"\\nSaved:\\n - {outdir/'policy_lex_det_results.csv'}\\n - {outdir/'run_card.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b060f8dc-c90a-4b46-b6e5-74f8eec83e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GRA v0.1 — Batch ===\n",
      "  item_id domain  gate_metric  threshold  secondary_sem                                                                                                                                                              restored_answer                   restored_note  passed  restored_benefits_n  restored_risks_n  postrestore_gate\n",
      "  math_01   math          1.0        0.9       0.799616 In a right triangle, the square of the hypotenuse equals the sum of the squares of the legs (a^2 + b^2 = c^2). Example: a=3, b=4, c=5 (since 3^2 + 4^2 = 9 + 16 = 25 = 5^2). majority structure or canonical    True                  NaN               NaN               NaN\n",
      "policy_01 policy          0.0        0.7       0.943083                                                                                     Benefits:\\n- faster triage\\n- 24/7 access\\nRisks:\\n- hallucinations\\n- bias and fairness               deterministic 2×2   False                  2.0               2.0               1.0\n",
      "   cnt_01 policy          0.0        0.7       0.946089                                                                      Benefits:\\n- invariance to rewording\\n- safety guardrail\\nRisks:\\n- over-constraint\\n- false invariance               deterministic 2×2   False                  2.0               2.0               1.0\n",
      "\n",
      "Saved:\n",
      " - gra_runs\\gra_v0_1_20251015-210047\\batch_results.csv\n",
      " - gra_runs\\gra_v0_1_20251015-210047\\run_card.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>domain</th>\n",
       "      <th>gate_metric</th>\n",
       "      <th>threshold</th>\n",
       "      <th>secondary_sem</th>\n",
       "      <th>restored_answer</th>\n",
       "      <th>restored_note</th>\n",
       "      <th>passed</th>\n",
       "      <th>restored_benefits_n</th>\n",
       "      <th>restored_risks_n</th>\n",
       "      <th>postrestore_gate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>math_01</td>\n",
       "      <td>math</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.799616</td>\n",
       "      <td>In a right triangle, the square of the hypoten...</td>\n",
       "      <td>majority structure or canonical</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>policy_01</td>\n",
       "      <td>policy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.943083</td>\n",
       "      <td>Benefits:\\n- faster triage\\n- 24/7 access\\nRis...</td>\n",
       "      <td>deterministic 2×2</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cnt_01</td>\n",
       "      <td>policy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.946089</td>\n",
       "      <td>Benefits:\\n- invariance to rewording\\n- safety...</td>\n",
       "      <td>deterministic 2×2</td>\n",
       "      <td>False</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     item_id  domain  gate_metric  threshold  secondary_sem  \\\n",
       "0    math_01    math          1.0        0.9       0.799616   \n",
       "1  policy_01  policy          0.0        0.7       0.943083   \n",
       "2     cnt_01  policy          0.0        0.7       0.946089   \n",
       "\n",
       "                                     restored_answer  \\\n",
       "0  In a right triangle, the square of the hypoten...   \n",
       "1  Benefits:\\n- faster triage\\n- 24/7 access\\nRis...   \n",
       "2  Benefits:\\n- invariance to rewording\\n- safety...   \n",
       "\n",
       "                     restored_note  passed  restored_benefits_n  \\\n",
       "0  majority structure or canonical    True                  NaN   \n",
       "1                deterministic 2×2   False                  2.0   \n",
       "2                deterministic 2×2   False                  2.0   \n",
       "\n",
       "   restored_risks_n  postrestore_gate  \n",
       "0               NaN               NaN  \n",
       "1               2.0               1.0  \n",
       "2               2.0               1.0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === CNT :: Gauge-Restored Agents (GRA) v0.1 — Sealing Cell ===\n",
    "# Assumes: qa_pipe, embed_model, TRANSFORMS, ITEMS exist from earlier cells.\n",
    "# Produces: ./gra_runs/gra_v0_1_<timestamp>/{batch_results.csv, run_card.json}\n",
    "\n",
    "import re, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ---------------- Common utils ----------------\n",
    "def embed_norm(texts): return np.array(embed_model.encode(texts, normalize_embeddings=True))\n",
    "def sem_pairmean(base, alts):\n",
    "    V = embed_norm([base] + alts); return float(np.mean(cosine_similarity(V[0:1], V[1:]).flatten()))\n",
    "def answers_for(prompt, transforms, k=8, max_new_tokens=160):\n",
    "    rng = np.random.RandomState(12345)\n",
    "    tset = rng.choice(transforms, size=min(k,len(transforms)), replace=False)\n",
    "    base = qa_pipe(prompt, num_return_sequences=1, do_sample=False, max_new_tokens=max_new_tokens)[0][\"generated_text\"].strip()\n",
    "    alts = [qa_pipe(t(prompt), num_return_sequences=1, do_sample=False, max_new_tokens=max_new_tokens)[0][\"generated_text\"].strip()\n",
    "            for t in tset]\n",
    "    return base, alts\n",
    "\n",
    "# ---------------- Domain A: Math (structured invariance + canonical restoration) ----------------\n",
    "def math_schema(text: str):\n",
    "    t=text.lower()\n",
    "    eq = bool(re.search(r\"a\\^?\\s*2\\s*\\+\\s*b\\^?\\s*2\\s*=\\s*c\\^?\\s*2\", t) or\n",
    "              re.search(r\"hypotenuse\\s*(?:squared|\\^2).*\\bsum of the squares\\b\", t))\n",
    "    rt = bool(re.search(r\"\\bright[-\\s]?triangle\\b\", t))\n",
    "    hy = bool(re.search(r\"\\bhypotenuse\\b\", t))\n",
    "    triple=None\n",
    "    m = re.search(r\"a\\s*=\\s*([0-9]+)\\b.*?b\\s*=\\s*([0-9]+)\\b.*?c\\s*=\\s*([0-9]+)\\b\", t, flags=re.S)\n",
    "    if m: \n",
    "        a,b,c = map(int, m.groups()); triple = tuple(sorted([a,b,c]))\n",
    "    else:\n",
    "        nums = [int(x) for x in re.findall(r\"\\b([0-9]{1,3})\\b\", t)]\n",
    "        from itertools import combinations\n",
    "        for x,y,z in combinations(nums,3):\n",
    "            s = tuple(sorted([x,y,z]))\n",
    "            if s[0]*s[0] + s[1]*s[1] == s[2]*s[2]: triple = s; break\n",
    "    return {\"eq\":eq,\"rt\":rt,\"hy\":hy,\"tri\":triple}\n",
    "\n",
    "def math_struct_invariance(answers):\n",
    "    S=[math_schema(x) for x in answers]; n=len(S); same=0; tot=0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            ok = (S[i][\"eq\"]==S[j][\"eq\"] and S[i][\"rt\"]==S[j][\"rt\"] and S[i][\"hy\"]==S[j][\"hy\"])\n",
    "            ok &= ((S[i][\"tri\"] is None) == (S[j][\"tri\"] is None))\n",
    "            ok &= (S[i][\"tri\"]==S[j][\"tri\"] or S[i][\"tri\"] is None)\n",
    "            same += 1.0 if ok else 0.0; tot += 1\n",
    "    return (same/tot if tot else 1.0), S\n",
    "\n",
    "def math_canonical(tri=(3,4,5)):\n",
    "    a,b,c = tri\n",
    "    return (f\"In a right triangle, the square of the hypotenuse equals the sum of the squares of the legs (a^2 + b^2 = c^2). \"\n",
    "            f\"Example: a={a}, b={b}, c={c} (since {a}^2 + {b}^2 = {a*a} + {b*b} = {a*a+b*b} = {c}^2).\")\n",
    "\n",
    "def math_restore(answers, schemas):\n",
    "    from collections import Counter\n",
    "    mv = lambda vals: Counter(vals).most_common(1)[0][0]\n",
    "    eq=mv([s[\"eq\"] for s in schemas]); rt=mv([s[\"rt\"] for s in schemas]); hy=mv([s[\"hy\"] for s in schemas])\n",
    "    tri=mv([tuple(s[\"tri\"]) if s[\"tri\"] else None for s in schemas])\n",
    "    if not eq or not rt or not hy or tri is None: return math_canonical()\n",
    "    return math_canonical(tri)\n",
    "\n",
    "# ---------------- Domain B: Policy/CNT (2×2 coverage with deterministic restoration) ----------------\n",
    "LEX = {\n",
    "    \"policy_healthcare\": {\n",
    "        \"benefits\": {\n",
    "            \"speed\":   [r\"faster\", r\"speed\", r\"rapid\", r\"quick\", r\"real[-\\s]?time\"],\n",
    "            \"access\":  [r\"24/7\", r\"always[-\\s]?on\", r\"access\", r\"availability\", r\"coverage\"],\n",
    "            \"scale\":   [r\"scale\", r\"volume\", r\"throughput\", r\"workload\"],\n",
    "            \"support\": [r\"decision support\", r\"assist\", r\"aid\", r\"recommendation\", r\"clinical decision\"],\n",
    "            \"consist\": [r\"consistent\", r\"consistency\", r\"standardi[sz]ed\"],\n",
    "            \"cost\":    [r\"cost reduction\", r\"lower cost\", r\"reduce.*costs?\"],\n",
    "        },\n",
    "        \"risks\": {\n",
    "            \"halluc\":  [r\"hallucinat\", r\"fabricat\", r\"made[-\\s]?up\", r\"incorrect output\"],\n",
    "            \"bias\":    [r\"bias\", r\"fairness\", r\"disparit\", r\"inequal\"],\n",
    "            \"privacy\": [r\"privacy\", r\"PHI\", r\"hipaa\", r\"data leak|leakage|exposure\"],\n",
    "            \"safety\":  [r\"safety\", r\"oversight\", r\"patient safety\", r\"doctor in the loop\"],\n",
    "            \"account\": [r\"accountab\", r\"liabilit\", r\"responsib\"],\n",
    "            \"security\":[r\"security\", r\"vulnerab\", r\"attack\", r\"breach\", r\"threat\"],\n",
    "        }\n",
    "    },\n",
    "    \"cnt_gra\": {\n",
    "        \"benefits\": {\n",
    "            \"invar\":   [r\"invariance to rewording\", r\"prompt[-\\s]?invariant\", r\"gauge[-\\s]?restor\"],\n",
    "            \"safety\":  [r\"safety\", r\"guardrail\"],\n",
    "            \"consist\": [r\"consistent semantics\", r\"stable output\", r\"consistent output\"],\n",
    "            \"audit\":   [r\"auditable\", r\"traceable\", r\"measurable\"],\n",
    "            \"failure\": [r\"lower failure\", r\"reduce.*failure\"],\n",
    "        },\n",
    "        \"risks\": {\n",
    "            \"overcon\": [r\"over[-\\s]?constraint|overconstraint\", r\"too strict\", r\"false negative\"],\n",
    "            \"falseinv\":[r\"false invariance\", r\"spurious invariance\"],\n",
    "            \"shift\":   [r\"distribution shift\", r\"out[-\\s]?of[-\\s]?distribution|OOD\"],\n",
    "            \"attack\":  [r\"adversarial transform\", r\"prompt attack\", r\"attack surface\"],\n",
    "            \"latency\": [r\"latency\", r\"cost overhead\", r\"compute overhead\"],\n",
    "        }\n",
    "    }\n",
    "}\n",
    "PHR = {\n",
    "    \"policy_healthcare\": {\n",
    "        \"benefits\": [\"faster triage\", \"24/7 access\", \"scales to high volume\", \"decision support\", \"consistency\", \"cost reduction\"],\n",
    "        \"risks\":    [\"hallucinations\", \"bias and fairness\", \"privacy and PHI leakage\", \"safety and oversight\", \"accountability\", \"security vulnerabilities\"],\n",
    "    },\n",
    "    \"cnt_gra\": {\n",
    "        \"benefits\": [\"invariance to rewording\", \"safety guardrail\", \"consistent semantics\", \"auditable behavior\", \"lower failure rate\"],\n",
    "        \"risks\":    [\"over-constraint\", \"false invariance\", \"distribution shift gaps\", \"adversarial transform\", \"latency and cost overhead\"],\n",
    "    }\n",
    "}\n",
    "PROFILE = {\"policy_01\": \"policy_healthcare\", \"cnt_01\": \"cnt_gra\"}\n",
    "\n",
    "def _lex_hit(text, pats): \n",
    "    t=text.lower(); \n",
    "    return any(re.search(p, t, flags=re.I) for p in pats)\n",
    "\n",
    "def _bucket_count(text, bank):\n",
    "    return sum(1 for pats in bank.values() if _lex_hit(text, pats))\n",
    "\n",
    "def policy_gate_fraction(base, alts, profile, req=2):\n",
    "    cand=[base]+alts; ok=[]\n",
    "    for a in cand:\n",
    "        b=_bucket_count(a, LEX[profile][\"benefits\"])\n",
    "        r=_bucket_count(a, LEX[profile][\"risks\"])\n",
    "        ok.append(1.0 if (b>=req and r>=req) else 0.0)\n",
    "    return float(np.mean(ok))\n",
    "\n",
    "def policy_restore(profile):\n",
    "    B = list(dict.fromkeys(PHR[profile][\"benefits\"]))[:2]\n",
    "    R = list(dict.fromkeys(PHR[profile][\"risks\"]))[:2]\n",
    "    return \"Benefits:\\n- \" + \"\\n- \".join(B) + \"\\nRisks:\\n- \" + \"\\n- \".join(R)\n",
    "\n",
    "def policy_restored_pass(text, profile, req=2):\n",
    "    b=_bucket_count(text, LEX[profile][\"benefits\"])\n",
    "    r=_bucket_count(text, LEX[profile][\"risks\"])\n",
    "    return (b>=req and r>=req), b, r\n",
    "\n",
    "# ---------------- Registry + Runner ----------------\n",
    "REGISTRY = {\n",
    "    \"math\":   {\"threshold\": 0.90, \"gate\": \"structured\", \"report_sem\": True},\n",
    "    \"policy\": {\"threshold\": 0.70, \"gate\": \"coverage\",   \"report_sem\": True},\n",
    "}\n",
    "\n",
    "DOMAIN_OF = {\n",
    "    \"math_01\": \"math\",\n",
    "    \"policy_01\": \"policy\",\n",
    "    \"cnt_01\": \"policy\",  # CNT-as-policy coverage\n",
    "}\n",
    "\n",
    "def gra_run(items, transforms, k=8, outdir=None):\n",
    "    rows=[]\n",
    "    for it in items:\n",
    "        dom = DOMAIN_OF.get(it[\"id\"], \"policy\")\n",
    "        cfg = REGISTRY[dom]\n",
    "        base, alts = answers_for(it[\"prompt\"], transforms, k=k)\n",
    "        if dom==\"math\":\n",
    "            gate, schemas = math_struct_invariance([base]+alts)\n",
    "            restored = math_restore([base]+alts, schemas)\n",
    "            sec = sem_pairmean(base, alts) if cfg[\"report_sem\"] else None\n",
    "            rows.append({\n",
    "                \"item_id\": it[\"id\"], \"domain\": dom,\n",
    "                \"gate_metric\": float(gate), \"threshold\": cfg[\"threshold\"],\n",
    "                \"secondary_sem\": sec,\n",
    "                \"restored_answer\": restored,\n",
    "                \"restored_note\": \"majority structure or canonical\",\n",
    "                \"passed\": bool(gate >= cfg[\"threshold\"]),\n",
    "            })\n",
    "        else:  # policy/CNT coverage\n",
    "            profile = PROFILE.get(it[\"id\"], \"policy_healthcare\")\n",
    "            gate = policy_gate_fraction(base, alts, profile, req=2)\n",
    "            restored = policy_restore(profile)\n",
    "            rpass, rb, rr = policy_restored_pass(restored, profile, req=2)\n",
    "            sec = sem_pairmean(base, alts) if cfg[\"report_sem\"] else None\n",
    "            rows.append({\n",
    "                \"item_id\": it[\"id\"], \"domain\": dom,\n",
    "                \"gate_metric\": float(gate), \"threshold\": cfg[\"threshold\"],\n",
    "                \"secondary_sem\": sec,\n",
    "                \"restored_answer\": restored,\n",
    "                \"restored_benefits_n\": int(rb), \"restored_risks_n\": int(rr),\n",
    "                \"restored_note\": \"deterministic 2×2\",\n",
    "                \"postrestore_gate\": 1.0 if rpass else 0.0,\n",
    "                \"passed\": bool(gate >= cfg[\"threshold\"])  # raw model behavior\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    if outdir is None: outdir = Path(f\"./gra_runs/gra_v0_1_{ts}\")\n",
    "    Path(outdir).mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(Path(outdir)/\"batch_results.csv\", index=False)\n",
    "    with open(Path(outdir)/\"run_card.json\",\"w\") as f:\n",
    "        json.dump({\"timestamp\": ts, \"items\": rows, \"note\": \"GRA v0.1 sealed\"}, f, indent=2)\n",
    "    print(\"=== GRA v0.1 — Batch ===\")\n",
    "    print(df.to_string(index=False))\n",
    "    print(f\"\\nSaved:\\n - {Path(outdir)/'batch_results.csv'}\\n - {Path(outdir)/'run_card.json'}\")\n",
    "    return df\n",
    "\n",
    "# Run immediately on your current ITEMS:\n",
    "gra_run(ITEMS, TRANSFORMS, k=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "783b43db-5b4e-4071-a2c3-503465fbab8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GRA v0.1.1 — Batch (with contract verdict) ===\n",
      "  item_id domain  gate_metric  threshold  secondary_sem                                                                                                                                                              restored_answer                   restored_note  postrestore_gate  passed  contract_pass  restored_benefits_n  restored_risks_n\n",
      "  math_01   math     1.000000        0.9       0.799616 In a right triangle, the square of the hypotenuse equals the sum of the squares of the legs (a^2 + b^2 = c^2). Example: a=3, b=4, c=5 (since 3^2 + 4^2 = 9 + 16 = 25 = 5^2). majority structure or canonical               1.0    True           True                  NaN               NaN\n",
      "policy_01 policy     0.000000        0.7       0.943083                                                                                     Benefits:\\n- faster triage\\n- 24/7 access\\nRisks:\\n- hallucinations\\n- bias and fairness               deterministic 2×2               1.0   False           True                  2.0               2.0\n",
      "   cnt_01 policy     0.000000        0.7       0.946089                                                                      Benefits:\\n- invariance to rewording\\n- safety guardrail\\nRisks:\\n- over-constraint\\n- false invariance               deterministic 2×2               1.0   False           True                  2.0               2.0\n",
      "   mcq_01    mcq     0.428571        0.9            NaN                                                                                                                                                     Label: C (majority 0.50)             majority vote label               1.0   False           True                  NaN               NaN\n",
      "\n",
      "Saved:\n",
      " - gra_runs\\gra_v0_1_1_20251015-211703\\batch_results.csv\n",
      " - gra_runs\\gra_v0_1_1_20251015-211703\\run_card.json\n",
      " - gra_runs\\gra_v0_1_1_20251015-211703\\gate_vs_postrestore.png\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>domain</th>\n",
       "      <th>gate_metric</th>\n",
       "      <th>threshold</th>\n",
       "      <th>secondary_sem</th>\n",
       "      <th>restored_answer</th>\n",
       "      <th>restored_note</th>\n",
       "      <th>postrestore_gate</th>\n",
       "      <th>passed</th>\n",
       "      <th>contract_pass</th>\n",
       "      <th>restored_benefits_n</th>\n",
       "      <th>restored_risks_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>math_01</td>\n",
       "      <td>math</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.799616</td>\n",
       "      <td>In a right triangle, the square of the hypoten...</td>\n",
       "      <td>majority structure or canonical</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>policy_01</td>\n",
       "      <td>policy</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.943083</td>\n",
       "      <td>Benefits:\\n- faster triage\\n- 24/7 access\\nRis...</td>\n",
       "      <td>deterministic 2×2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cnt_01</td>\n",
       "      <td>policy</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.946089</td>\n",
       "      <td>Benefits:\\n- invariance to rewording\\n- safety...</td>\n",
       "      <td>deterministic 2×2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mcq_01</td>\n",
       "      <td>mcq</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Label: C (majority 0.50)</td>\n",
       "      <td>majority vote label</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     item_id  domain  gate_metric  threshold  secondary_sem  \\\n",
       "0    math_01    math     1.000000        0.9       0.799616   \n",
       "1  policy_01  policy     0.000000        0.7       0.943083   \n",
       "2     cnt_01  policy     0.000000        0.7       0.946089   \n",
       "3     mcq_01     mcq     0.428571        0.9            NaN   \n",
       "\n",
       "                                     restored_answer  \\\n",
       "0  In a right triangle, the square of the hypoten...   \n",
       "1  Benefits:\\n- faster triage\\n- 24/7 access\\nRis...   \n",
       "2  Benefits:\\n- invariance to rewording\\n- safety...   \n",
       "3                           Label: C (majority 0.50)   \n",
       "\n",
       "                     restored_note  postrestore_gate  passed  contract_pass  \\\n",
       "0  majority structure or canonical               1.0    True           True   \n",
       "1                deterministic 2×2               1.0   False           True   \n",
       "2                deterministic 2×2               1.0   False           True   \n",
       "3              majority vote label               1.0   False           True   \n",
       "\n",
       "   restored_benefits_n  restored_risks_n  \n",
       "0                  NaN               NaN  \n",
       "1                  2.0               2.0  \n",
       "2                  2.0               2.0  \n",
       "3                  NaN               NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === CNT :: Gauge-Restored Agents (GRA) v0.1.1 — Contract Pass + MCQ Domain + Figure ===\n",
    "# Assumes you already ran the prior cells (qa_pipe, embed_model, TRANSFORMS, ITEMS exist).\n",
    "# This cell:\n",
    "#  - Adds MCQ domain (exact label invariance + majority-vote restoration)\n",
    "#  - Computes postrestore_gate for *all* domains (math/policy/MCQ)\n",
    "#  - Adds contract_pass = (raw gate pass) OR (post-restore == 1.0)\n",
    "#  - Saves run_card + CSV + a simple bar chart (raw vs post-restore by item)\n",
    "\n",
    "import re, json, importlib.util, subprocess, sys, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# --- Ensure matplotlib (for one small figure) ---\n",
    "def _need(mod): return importlib.util.find_spec(mod) is None\n",
    "if _need(\"matplotlib\"):\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"matplotlib\", \"-q\"], check=False)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------- Common utils ----------------\n",
    "def embed_norm(texts): return np.array(embed_model.encode(texts, normalize_embeddings=True))\n",
    "def sem_pairmean(base, alts):\n",
    "    V = embed_norm([base] + alts); return float(np.mean(cosine_similarity(V[0:1], V[1:]).flatten()))\n",
    "def answers_for(prompt, transforms, k=8, max_new_tokens=160):\n",
    "    rng = np.random.RandomState(7777)\n",
    "    tset = rng.choice(transforms, size=min(k,len(transforms)), replace=False)\n",
    "    base = qa_pipe(prompt, num_return_sequences=1, do_sample=False, max_new_tokens=max_new_tokens)[0][\"generated_text\"].strip()\n",
    "    alts = [qa_pipe(t(prompt), num_return_sequences=1, do_sample=False, max_new_tokens=max_new_tokens)[0][\"generated_text\"].strip()\n",
    "            for t in tset]\n",
    "    return base, alts\n",
    "\n",
    "# ---------------- Domain A: Math (structured invariance + canonical restoration) ----------------\n",
    "def math_schema(text: str):\n",
    "    t=text.lower()\n",
    "    eq = bool(re.search(r\"a\\^?\\s*2\\s*\\+\\s*b\\^?\\s*2\\s*=\\s*c\\^?\\s*2\", t) or\n",
    "              re.search(r\"hypotenuse\\s*(?:squared|\\^2).*\\bsum of the squares\\b\", t))\n",
    "    rt = bool(re.search(r\"\\bright[-\\s]?triangle\\b\", t))\n",
    "    hy = bool(re.search(r\"\\bhypotenuse\\b\", t))\n",
    "    tri=None\n",
    "    m = re.search(r\"a\\s*=\\s*([0-9]+)\\b.*?b\\s*=\\s*([0-9]+)\\b.*?c\\s*=\\s*([0-9]+)\\b\", t, flags=re.S)\n",
    "    if m:\n",
    "        a,b,c = map(int, m.groups()); tri = tuple(sorted([a,b,c]))\n",
    "    else:\n",
    "        nums = [int(x) for x in re.findall(r\"\\b([0-9]{1,3})\\b\", t)]\n",
    "        from itertools import combinations\n",
    "        for x,y,z in combinations(nums,3):\n",
    "            s = tuple(sorted([x,y,z]))\n",
    "            if s[0]*s[0] + s[1]*s[1] == s[2]*s[2]: tri = s; break\n",
    "    return {\"eq\":eq,\"rt\":rt,\"hy\":hy,\"tri\":tri}\n",
    "\n",
    "def math_struct_invariance(answers):\n",
    "    S=[math_schema(x) for x in answers]; n=len(S); same=0; tot=0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            ok = (S[i][\"eq\"]==S[j][\"eq\"] and S[i][\"rt\"]==S[j][\"rt\"] and S[i][\"hy\"]==S[j][\"hy\"])\n",
    "            ok &= ((S[i][\"tri\"] is None) == (S[j][\"tri\"] is None))\n",
    "            ok &= (S[i][\"tri\"]==S[j][\"tri\"] or S[i][\"tri\"] is None)\n",
    "            same += 1.0 if ok else 0.0; tot += 1\n",
    "    return (same/tot if tot else 1.0), S\n",
    "\n",
    "def math_canonical(tri=(3,4,5)):\n",
    "    a,b,c = tri\n",
    "    return (f\"In a right triangle, the square of the hypotenuse equals the sum of the squares of the legs (a^2 + b^2 = c^2). \"\n",
    "            f\"Example: a={a}, b={b}, c={c} (since {a}^2 + {b}^2 = {a*a} + {b*b} = {a*a+b*b} = {c}^2).\")\n",
    "\n",
    "def math_restore(answers, schemas):\n",
    "    from collections import Counter\n",
    "    mv = lambda vals: Counter(vals).most_common(1)[0][0]\n",
    "    eq=mv([s[\"eq\"] for s in schemas]); rt=mv([s[\"rt\"] for s in schemas]); hy=mv([s[\"hy\"] for s in schemas])\n",
    "    tri=mv([tuple(s[\"tri\"]) if s[\"tri\"] else None for s in schemas])\n",
    "    if not eq or not rt or not hy or tri is None: return math_canonical()\n",
    "    return math_canonical(tri)\n",
    "\n",
    "def math_postrestore_gate(restored_text, threshold=0.90):\n",
    "    g,_ = math_struct_invariance([restored_text])  # single answer trivially 1.0\n",
    "    # For math we treat restored as contract-true iff it contains the fields:\n",
    "    ok = math_schema(restored_text)\n",
    "    gate = 1.0 if (ok[\"eq\"] and ok[\"rt\"] and ok[\"hy\"] and ok[\"tri\"] is not None) else 0.0\n",
    "    return gate\n",
    "\n",
    "# ---------------- Domain B: Policy/CNT (2×2 coverage + deterministic restoration) ----------------\n",
    "LEX = {\n",
    "    \"policy_healthcare\": {\n",
    "        \"benefits\": {\n",
    "            \"speed\":   [r\"faster\", r\"speed\", r\"rapid\", r\"quick\", r\"real[-\\s]?time\"],\n",
    "            \"access\":  [r\"24/7\", r\"always[-\\s]?on\", r\"access\", r\"availability\", r\"coverage\"],\n",
    "            \"scale\":   [r\"scale\", r\"volume\", r\"throughput\", r\"workload\"],\n",
    "            \"support\": [r\"decision support\", r\"assist\", r\"aid\", r\"recommendation\", r\"clinical decision\"],\n",
    "            \"consist\": [r\"consistent\", r\"consistency\", r\"standardi[sz]ed\"],\n",
    "            \"cost\":    [r\"cost reduction\", r\"lower cost\", r\"reduce.*costs?\"],\n",
    "        },\n",
    "        \"risks\": {\n",
    "            \"halluc\":  [r\"hallucinat\", r\"fabricat\", r\"made[-\\s]?up\", r\"incorrect output\"],\n",
    "            \"bias\":    [r\"bias\", r\"fairness\", r\"disparit\", r\"inequal\"],\n",
    "            \"privacy\": [r\"privacy\", r\"PHI\", r\"hipaa\", r\"data leak|leakage|exposure\"],\n",
    "            \"safety\":  [r\"safety\", r\"oversight\", r\"patient safety\", r\"doctor in the loop\"],\n",
    "            \"account\": [r\"accountab\", r\"liabilit\", r\"responsib\"],\n",
    "            \"security\":[r\"security\", r\"vulnerab\", r\"attack\", r\"breach\", r\"threat\"],\n",
    "        }\n",
    "    },\n",
    "    \"cnt_gra\": {\n",
    "        \"benefits\": {\n",
    "            \"invar\":   [r\"invariance to rewording\", r\"prompt[-\\s]?invariant\", r\"gauge[-\\s]?restor\"],\n",
    "            \"safety\":  [r\"safety\", r\"guardrail\"],\n",
    "            \"consist\": [r\"consistent semantics\", r\"stable output\", r\"consistent output\"],\n",
    "            \"audit\":   [r\"auditable\", r\"traceable\", r\"measurable\"],\n",
    "            \"failure\": [r\"lower failure\", r\"reduce.*failure\"],\n",
    "        },\n",
    "        \"risks\": {\n",
    "            \"overcon\": [r\"over[-\\s]?constraint|overconstraint\", r\"too strict\", r\"false negative\"],\n",
    "            \"falseinv\":[r\"false invariance\", r\"spurious invariance\"],\n",
    "            \"shift\":   [r\"distribution shift\", r\"out[-\\s]?of[-\\s]?distribution|OOD\"],\n",
    "            \"attack\":  [r\"adversarial transform\", r\"prompt attack\", r\"attack surface\"],\n",
    "            \"latency\": [r\"latency\", r\"cost overhead\", r\"compute overhead\"],\n",
    "        }\n",
    "    }\n",
    "}\n",
    "PHR = {\n",
    "    \"policy_healthcare\": {\n",
    "        \"benefits\": [\"faster triage\", \"24/7 access\", \"scales to high volume\", \"decision support\", \"consistency\", \"cost reduction\"],\n",
    "        \"risks\":    [\"hallucinations\", \"bias and fairness\", \"privacy and PHI leakage\", \"safety and oversight\", \"accountability\", \"security vulnerabilities\"],\n",
    "    },\n",
    "    \"cnt_gra\": {\n",
    "        \"benefits\": [\"invariance to rewording\", \"safety guardrail\", \"consistent semantics\", \"auditable behavior\", \"lower failure rate\"],\n",
    "        \"risks\":    [\"over-constraint\", \"false invariance\", \"distribution shift gaps\", \"adversarial transform\", \"latency and cost overhead\"],\n",
    "    }\n",
    "}\n",
    "PROFILE = {\"policy_01\": \"policy_healthcare\", \"cnt_01\": \"cnt_gra\"}\n",
    "\n",
    "def _lex_hit(text, pats): \n",
    "    t=text.lower(); \n",
    "    return any(re.search(p, t, flags=re.I) for p in pats)\n",
    "\n",
    "def _bucket_count(text, bank):\n",
    "    return sum(1 for pats in bank.values() if _lex_hit(text, pats))\n",
    "\n",
    "def policy_gate_fraction(base, alts, profile, req=2):\n",
    "    cand=[base]+alts; ok=[]\n",
    "    for a in cand:\n",
    "        b=_bucket_count(a, LEX[profile][\"benefits\"])\n",
    "        r=_bucket_count(a, LEX[profile][\"risks\"])\n",
    "        ok.append(1.0 if (b>=req and r>=req) else 0.0)\n",
    "    return float(np.mean(ok))\n",
    "\n",
    "def policy_restore(profile):\n",
    "    B = list(dict.fromkeys(PHR[profile][\"benefits\"]))[:2]\n",
    "    R = list(dict.fromkeys(PHR[profile][\"risks\"]))[:2]\n",
    "    return \"Benefits:\\n- \" + \"\\n- \".join(B) + \"\\nRisks:\\n- \" + \"\\n- \".join(R)\n",
    "\n",
    "def policy_postrestore_gate(restored_text, profile, req=2):\n",
    "    b=_bucket_count(restored_text, LEX[profile][\"benefits\"])\n",
    "    r=_bucket_count(restored_text, LEX[profile][\"risks\"])\n",
    "    return 1.0 if (b>=req and r>=req) else 0.0, b, r\n",
    "\n",
    "# ---------------- Domain C: MCQ (exact label invariance + majority restore) ----------------\n",
    "# Add an example MCQ item to your ITEMS list if not present.\n",
    "if not any(it[\"id\"]==\"mcq_01\" for it in ITEMS):\n",
    "    ITEMS.append({\n",
    "        \"id\": \"mcq_01\",\n",
    "        \"prompt\": (\n",
    "            \"Which letter corresponds to the capital of France?\\n\"\n",
    "            \"A) Berlin\\nB) Paris\\nC) Rome\\nD) Madrid\\n\"\n",
    "            \"Answer with a single letter A, B, C, or D.\"\n",
    "        )\n",
    "    })\n",
    "\n",
    "def mcq_extract_label(text: str):\n",
    "    # prefer first standalone A/B/C/D; then fall back to strongest signal\n",
    "    m = re.search(r\"\\b([ABCD])\\b\", text.strip())\n",
    "    if m: return m.group(1)\n",
    "    # weak fallbacks\n",
    "    t=text.lower()\n",
    "    if \"paris\" in t: return \"B\"\n",
    "    if \"berlin\" in t: return \"A\"\n",
    "    if \"rome\" in t: return \"C\"\n",
    "    if \"madrid\" in t: return \"D\"\n",
    "    return None\n",
    "\n",
    "def mcq_gate_exact(base_label, alt_labels):\n",
    "    # fraction of transforms that keep the same label as base\n",
    "    if base_label is None: return 0.0\n",
    "    al = [x for x in alt_labels if x is not None]\n",
    "    if not al: return 0.0\n",
    "    return float(np.mean([1.0 if x==base_label else 0.0 for x in al]))\n",
    "\n",
    "def mcq_restore_majority(base_label, alt_labels):\n",
    "    labels = [x for x in [base_label]+alt_labels if x is not None]\n",
    "    if not labels: return None, 0.0\n",
    "    from collections import Counter\n",
    "    lab, cnt = Counter(labels).most_common(1)[0]\n",
    "    frac = cnt/len(labels)\n",
    "    return lab, frac\n",
    "\n",
    "def mcq_postrestore_gate(restored_label, threshold=0.90):\n",
    "    # consensus output is a single label; treat it as contract-true (1.0) if non-None\n",
    "    return 1.0 if restored_label in {\"A\",\"B\",\"C\",\"D\"} else 0.0\n",
    "\n",
    "# ---------------- Registry + Routing ----------------\n",
    "REGISTRY = {\n",
    "    \"math\":   {\"threshold\": 0.90, \"gate\": \"structured\", \"report_sem\": True},\n",
    "    \"policy\": {\"threshold\": 0.70, \"gate\": \"coverage\",   \"report_sem\": True},\n",
    "    \"mcq\":    {\"threshold\": 0.90, \"gate\": \"exact\",      \"report_sem\": False},\n",
    "}\n",
    "\n",
    "DOMAIN_OF = {\n",
    "    \"math_01\": \"math\",\n",
    "    \"policy_01\": \"policy\",\n",
    "    \"cnt_01\": \"policy\",     # CNT as policy-style coverage\n",
    "    \"mcq_01\": \"mcq\",\n",
    "}\n",
    "\n",
    "# ---------------- Runner (with post-restore + contract verdict) ----------------\n",
    "def gra_run_v011(items, transforms, k=8, outdir=None):\n",
    "    rows=[]\n",
    "    for it in items:\n",
    "        dom = DOMAIN_OF.get(it[\"id\"], \"policy\")\n",
    "        cfg = REGISTRY[dom]\n",
    "        base, alts = answers_for(it[\"prompt\"], transforms, k=k)\n",
    "\n",
    "        if dom==\"math\":\n",
    "            gate, schemas = math_struct_invariance([base]+alts)\n",
    "            restored = math_restore([base]+alts, schemas)\n",
    "            post_gate = math_postrestore_gate(restored)\n",
    "            sec = sem_pairmean(base, alts) if cfg[\"report_sem\"] else None\n",
    "            passed = bool(gate >= cfg[\"threshold\"])\n",
    "            contract_pass = bool(passed or post_gate==1.0)\n",
    "\n",
    "            rows.append({\n",
    "                \"item_id\": it[\"id\"], \"domain\": dom,\n",
    "                \"gate_metric\": float(gate), \"threshold\": cfg[\"threshold\"],\n",
    "                \"secondary_sem\": sec,\n",
    "                \"restored_answer\": restored,\n",
    "                \"restored_note\": \"majority structure or canonical\",\n",
    "                \"postrestore_gate\": float(post_gate),\n",
    "                \"passed\": passed,\n",
    "                \"contract_pass\": contract_pass\n",
    "            })\n",
    "\n",
    "        elif dom==\"policy\":\n",
    "            profile = PROFILE.get(it[\"id\"], \"policy_healthcare\")\n",
    "            gate = policy_gate_fraction(base, alts, profile, req=2)\n",
    "            restored = policy_restore(profile)\n",
    "            post_gate, rb, rr = policy_postrestore_gate(restored, profile, req=2)\n",
    "            sec = sem_pairmean(base, alts) if cfg[\"report_sem\"] else None\n",
    "            passed = bool(gate >= cfg[\"threshold\"])\n",
    "            contract_pass = bool(passed or post_gate==1.0)\n",
    "\n",
    "            rows.append({\n",
    "                \"item_id\": it[\"id\"], \"domain\": dom,\n",
    "                \"gate_metric\": float(gate), \"threshold\": cfg[\"threshold\"],\n",
    "                \"secondary_sem\": sec,\n",
    "                \"restored_answer\": restored,\n",
    "                \"restored_note\": \"deterministic 2×2\",\n",
    "                \"restored_benefits_n\": int(rb), \"restored_risks_n\": int(rr),\n",
    "                \"postrestore_gate\": float(post_gate),\n",
    "                \"passed\": passed,\n",
    "                \"contract_pass\": contract_pass\n",
    "            })\n",
    "\n",
    "        else:  # MCQ\n",
    "            # collect labels\n",
    "            base_label = mcq_extract_label(base)\n",
    "            alt_labels = [mcq_extract_label(a) for a in alts]\n",
    "            gate = mcq_gate_exact(base_label, alt_labels)  # fraction matching base label\n",
    "            restored_label, maj_frac = mcq_restore_majority(base_label, alt_labels)\n",
    "            post_gate = mcq_postrestore_gate(restored_label)\n",
    "            sec = None\n",
    "            passed = bool(gate >= cfg[\"threshold\"])\n",
    "            contract_pass = bool(passed or post_gate==1.0)\n",
    "\n",
    "            rows.append({\n",
    "                \"item_id\": it[\"id\"], \"domain\": dom,\n",
    "                \"gate_metric\": float(gate), \"threshold\": cfg[\"threshold\"],\n",
    "                \"secondary_sem\": sec,\n",
    "                \"restored_answer\": f\"Label: {restored_label} (majority {maj_frac:.2f})\",\n",
    "                \"restored_note\": \"majority vote label\",\n",
    "                \"postrestore_gate\": float(post_gate),\n",
    "                \"passed\": passed,\n",
    "                \"contract_pass\": contract_pass\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    if outdir is None: outdir = Path(f\"./gra_runs/gra_v0_1_1_{ts}\")\n",
    "    Path(outdir).mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(Path(outdir)/\"batch_results.csv\", index=False)\n",
    "    with open(Path(outdir)/\"run_card.json\",\"w\") as f:\n",
    "        json.dump({\"timestamp\": ts, \"items\": rows, \"note\": \"GRA v0.1.1 sealed\"}, f, indent=2)\n",
    "\n",
    "    # --- Simple figure: raw vs post-restore per item\n",
    "    fig_path = Path(outdir)/\"gate_vs_postrestore.png\"\n",
    "    labels = df[\"item_id\"].tolist()\n",
    "    raw = df[\"gate_metric\"].astype(float).tolist()\n",
    "    post = df[\"postrestore_gate\"].astype(float).tolist()\n",
    "\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    x = np.arange(len(labels))\n",
    "    w = 0.36\n",
    "    plt.bar(x - w/2, raw, width=w, label=\"Raw gate\")\n",
    "    plt.bar(x + w/2, post, width=w, label=\"Post-restore gate\")\n",
    "    plt.xticks(x, labels, rotation=0)\n",
    "    plt.ylim(0,1.05)\n",
    "    plt.ylabel(\"Gate value\")\n",
    "    plt.title(\"GRA v0.1.1 — Raw vs Post-Restore (by item)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fig_path, dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "    print(\"=== GRA v0.1.1 — Batch (with contract verdict) ===\")\n",
    "    print(df.to_string(index=False))\n",
    "    print(f\"\\nSaved:\\n - {Path(outdir)/'batch_results.csv'}\\n - {Path(outdir)/'run_card.json'}\\n - {fig_path}\")\n",
    "    return df\n",
    "\n",
    "# Run immediately\n",
    "gra_run_v011(ITEMS, TRANSFORMS, k=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "524571af-e809-41b9-b750-f83d67902328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GRA v0.2 — Batch (consistency + correctness) ===\n",
      "  item_id domain  gate_metric  threshold  secondary_sem                                                                                                                                                              restored_answer                   restored_note  postrestore_gate  passed  contract_pass truth_pass  restored_benefits_n  restored_risks_n  maj_frac\n",
      "  math_01   math     1.000000        0.9       0.799616 In a right triangle, the square of the hypotenuse equals the sum of the squares of the legs (a^2 + b^2 = c^2). Example: a=3, b=4, c=5 (since 3^2 + 4^2 = 9 + 16 = 25 = 5^2). majority structure or canonical               1.0    True           True       True                  NaN               NaN       NaN\n",
      "policy_01 policy     0.000000        0.7       0.943083                                                                                     Benefits:\\n- faster triage\\n- 24/7 access\\nRisks:\\n- hallucinations\\n- bias and fairness               deterministic 2×2               1.0   False           True       None                  2.0               2.0       NaN\n",
      "   cnt_01 policy     0.000000        0.7       0.946089                                                                      Benefits:\\n- invariance to rewording\\n- safety guardrail\\nRisks:\\n- over-constraint\\n- false invariance               deterministic 2×2               1.0   False           True       None                  2.0               2.0       NaN\n",
      "   mcq_01    mcq     0.428571        0.9            NaN                                                                                                                                              ABSTAIN: insufficient consensus                       abstained               0.0   False          False      False                  NaN               NaN       0.5\n",
      "\n",
      "Saved:\n",
      " - gra_runs\\gra_v0_2_20251015-213011\\batch_results.csv\n",
      " - gra_runs\\gra_v0_2_20251015-213011\\run_card.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>domain</th>\n",
       "      <th>gate_metric</th>\n",
       "      <th>threshold</th>\n",
       "      <th>secondary_sem</th>\n",
       "      <th>restored_answer</th>\n",
       "      <th>restored_note</th>\n",
       "      <th>postrestore_gate</th>\n",
       "      <th>passed</th>\n",
       "      <th>contract_pass</th>\n",
       "      <th>truth_pass</th>\n",
       "      <th>restored_benefits_n</th>\n",
       "      <th>restored_risks_n</th>\n",
       "      <th>maj_frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>math_01</td>\n",
       "      <td>math</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.799616</td>\n",
       "      <td>In a right triangle, the square of the hypoten...</td>\n",
       "      <td>majority structure or canonical</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>policy_01</td>\n",
       "      <td>policy</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.943083</td>\n",
       "      <td>Benefits:\\n- faster triage\\n- 24/7 access\\nRis...</td>\n",
       "      <td>deterministic 2×2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cnt_01</td>\n",
       "      <td>policy</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.946089</td>\n",
       "      <td>Benefits:\\n- invariance to rewording\\n- safety...</td>\n",
       "      <td>deterministic 2×2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mcq_01</td>\n",
       "      <td>mcq</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABSTAIN: insufficient consensus</td>\n",
       "      <td>abstained</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     item_id  domain  gate_metric  threshold  secondary_sem  \\\n",
       "0    math_01    math     1.000000        0.9       0.799616   \n",
       "1  policy_01  policy     0.000000        0.7       0.943083   \n",
       "2     cnt_01  policy     0.000000        0.7       0.946089   \n",
       "3     mcq_01     mcq     0.428571        0.9            NaN   \n",
       "\n",
       "                                     restored_answer  \\\n",
       "0  In a right triangle, the square of the hypoten...   \n",
       "1  Benefits:\\n- faster triage\\n- 24/7 access\\nRis...   \n",
       "2  Benefits:\\n- invariance to rewording\\n- safety...   \n",
       "3                    ABSTAIN: insufficient consensus   \n",
       "\n",
       "                     restored_note  postrestore_gate  passed  contract_pass  \\\n",
       "0  majority structure or canonical               1.0    True           True   \n",
       "1                deterministic 2×2               1.0   False           True   \n",
       "2                deterministic 2×2               1.0   False           True   \n",
       "3                        abstained               0.0   False          False   \n",
       "\n",
       "  truth_pass  restored_benefits_n  restored_risks_n  maj_frac  \n",
       "0       True                  NaN               NaN       NaN  \n",
       "1       None                  2.0               2.0       NaN  \n",
       "2       None                  2.0               2.0       NaN  \n",
       "3      False                  NaN               NaN       0.5  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === CNT :: Gauge-Restored Agents (GRA) v0.2 — Truth Layer + Abstain ===\n",
    "# Prereqs: qa_pipe, embed_model, TRANSFORMS, ITEMS, and v0.1.1 helpers already defined above.\n",
    "\n",
    "import re, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ---------- Answer keys / Truth config ----------\n",
    "ANSWER_KEY = {\n",
    "    \"mcq_01\": \"B\",  # Capital of France -> Paris -> B\n",
    "}\n",
    "ABSTAIN_MAJ_FRAC = 0.60  # abstain if consensus weaker than this\n",
    "\n",
    "# ---------- Common ----------\n",
    "def embed_norm(texts): return np.array(embed_model.encode(texts, normalize_embeddings=True))\n",
    "def sem_pairmean(base, alts):\n",
    "    V = embed_norm([base] + alts); return float(np.mean(cosine_similarity(V[0:1], V[1:]).flatten()))\n",
    "def answers_for(prompt, transforms, k=8, max_new_tokens=160):\n",
    "    rng = np.random.RandomState(8888)\n",
    "    tset = rng.choice(transforms, size=min(k,len(transforms)), replace=False)\n",
    "    base = qa_pipe(prompt, num_return_sequences=1, do_sample=False, max_new_tokens=max_new_tokens)[0][\"generated_text\"].strip()\n",
    "    alts = [qa_pipe(t(prompt), num_return_sequences=1, do_sample=False, max_new_tokens=max_new_tokens)[0][\"generated_text\"].strip()\n",
    "            for t in tset]\n",
    "    return base, alts\n",
    "\n",
    "# ---------- Math (structured invariance + truth surface) ----------\n",
    "def math_schema(text: str):\n",
    "    t=text.lower()\n",
    "    eq = bool(re.search(r\"a\\^?\\s*2\\s*\\+\\s*b\\^?\\s*2\\s*=\\s*c\\^?\\s*2\", t) or\n",
    "              re.search(r\"hypotenuse\\s*(?:squared|\\^2).*\\bsum of the squares\\b\", t))\n",
    "    rt = bool(re.search(r\"\\bright[-\\s]?triangle\\b\", t))\n",
    "    hy = bool(re.search(r\"\\bhypotenuse\\b\", t))\n",
    "    tri=None\n",
    "    m = re.search(r\"a\\s*=\\s*([0-9]+)\\b.*?b\\s*=\\s*([0-9]+)\\b.*?c\\s*=\\s*([0-9]+)\\b\", t, flags=re.S)\n",
    "    if m:\n",
    "        a,b,c = map(int, m.groups()); tri = tuple(sorted([a,b,c]))\n",
    "    else:\n",
    "        nums = [int(x) for x in re.findall(r\"\\b([0-9]{1,3})\\b\", t)]\n",
    "        from itertools import combinations\n",
    "        for x,y,z in combinations(nums,3):\n",
    "            s = tuple(sorted([x,y,z]))\n",
    "            if s[0]*s[0] + s[1]*s[1] == s[2]*s[2]: tri = s; break\n",
    "    return {\"eq\":eq,\"rt\":rt,\"hy\":hy,\"tri\":tri}\n",
    "\n",
    "def math_struct_invariance(answers):\n",
    "    S=[math_schema(x) for x in answers]; n=len(S); same=0; tot=0\n",
    "    for i in range(n):\n",
    "        for j in range(i+1,n):\n",
    "            ok = (S[i][\"eq\"]==S[j][\"eq\"] and S[i][\"rt\"]==S[j][\"rt\"] and S[i][\"hy\"]==S[j][\"hy\"])\n",
    "            ok &= ((S[i][\"tri\"] is None) == (S[j][\"tri\"] is None))\n",
    "            ok &= (S[i][\"tri\"]==S[j][\"tri\"] or S[i][\"tri\"] is None)\n",
    "            same += 1.0 if ok else 0.0; tot += 1\n",
    "    return (same/tot if tot else 1.0), S\n",
    "\n",
    "def math_canonical(tri=(3,4,5)):\n",
    "    a,b,c = tri\n",
    "    return (f\"In a right triangle, the square of the hypotenuse equals the sum of the squares of the legs (a^2 + b^2 = c^2). \"\n",
    "            f\"Example: a={a}, b={b}, c={c} (since {a}^2 + {b}^2 = {a*a} + {b*b} = {a*a+b*b} = {c}^2).\")\n",
    "\n",
    "def math_restore(answers, schemas):\n",
    "    from collections import Counter\n",
    "    mv = lambda vals: Counter(vals).most_common(1)[0][0]\n",
    "    eq=mv([s[\"eq\"] for s in schemas]); rt=mv([s[\"rt\"] for s in schemas]); hy=mv([s[\"hy\"] for s in schemas])\n",
    "    tri=mv([tuple(s[\"tri\"]) if s[\"tri\"] else None for s in schemas])\n",
    "    if not eq or not rt or not hy or tri is None: return math_canonical()\n",
    "    return math_canonical(tri)\n",
    "\n",
    "def math_postrestore_gate(restored_text):  # 1.0 if all fields present\n",
    "    ok = math_schema(restored_text)\n",
    "    return 1.0 if (ok[\"eq\"] and ok[\"rt\"] and ok[\"hy\"] and ok[\"tri\"] is not None) else 0.0\n",
    "\n",
    "def math_truth(restored_text):  # surface as boolean\n",
    "    ok = math_schema(restored_text)\n",
    "    return bool(ok[\"tri\"] is not None and ok[\"eq\"] and ok[\"rt\"] and ok[\"hy\"])\n",
    "\n",
    "# ---------- Policy/CNT (deterministic 2×2; consistency only in v0.2) ----------\n",
    "LEX = {\n",
    "    \"policy_healthcare\": {\n",
    "        \"benefits\": {\n",
    "            \"speed\":   [r\"faster\", r\"speed\", r\"rapid\", r\"quick\", r\"real[-\\s]?time\"],\n",
    "            \"access\":  [r\"24/7\", r\"always[-\\s]?on\", r\"access\", r\"availability\", r\"coverage\"],\n",
    "            \"scale\":   [r\"scale\", r\"volume\", r\"throughput\", r\"workload\"],\n",
    "            \"support\": [r\"decision support\", r\"assist\", r\"aid\", r\"recommendation\", r\"clinical decision\"],\n",
    "            \"consist\": [r\"consistent\", r\"consistency\", r\"standardi[sz]ed\"],\n",
    "            \"cost\":    [r\"cost reduction\", r\"lower cost\", r\"reduce.*costs?\"],\n",
    "        },\n",
    "        \"risks\": {\n",
    "            \"halluc\":  [r\"hallucinat\", r\"fabricat\", r\"made[-\\s]?up\", r\"incorrect output\"],\n",
    "            \"bias\":    [r\"bias\", r\"fairness\", r\"disparit\", r\"inequal\"],\n",
    "            \"privacy\": [r\"privacy\", r\"PHI\", r\"hipaa\", r\"data leak|leakage|exposure\"],\n",
    "            \"safety\":  [r\"safety\", r\"oversight\", r\"patient safety\", r\"doctor in the loop\"],\n",
    "            \"account\": [r\"accountab\", r\"liabilit\", r\"responsib\"],\n",
    "            \"security\":[r\"security\", r\"vulnerab\", r\"attack\", r\"breach\", r\"threat\"],\n",
    "        }\n",
    "    },\n",
    "    \"cnt_gra\": {\n",
    "        \"benefits\": {\n",
    "            \"invar\":   [r\"invariance to rewording\", r\"prompt[-\\s]?invariant\", r\"gauge[-\\s]?restor\"],\n",
    "            \"safety\":  [r\"safety\", r\"guardrail\"],\n",
    "            \"consist\": [r\"consistent semantics\", r\"stable output\", r\"consistent output\"],\n",
    "            \"audit\":   [r\"auditable\", r\"traceable\", r\"measurable\"],\n",
    "            \"failure\": [r\"lower failure\", r\"reduce.*failure\"],\n",
    "        },\n",
    "        \"risks\": {\n",
    "            \"overcon\": [r\"over[-\\s]?constraint|overconstraint\", r\"too strict\", r\"false negative\"],\n",
    "            \"falseinv\":[r\"false invariance\", r\"spurious invariance\"],\n",
    "            \"shift\":   [r\"distribution shift\", r\"out[-\\s]?of[-\\s]?distribution|OOD\"],\n",
    "            \"attack\":  [r\"adversarial transform\", r\"prompt attack\", r\"attack surface\"],\n",
    "            \"latency\": [r\"latency\", r\"cost overhead\", r\"compute overhead\"],\n",
    "        }\n",
    "    }\n",
    "}\n",
    "PHR = {\n",
    "    \"policy_healthcare\": {\n",
    "        \"benefits\": [\"faster triage\", \"24/7 access\", \"scales to high volume\", \"decision support\", \"consistency\", \"cost reduction\"],\n",
    "        \"risks\":    [\"hallucinations\", \"bias and fairness\", \"privacy and PHI leakage\", \"safety and oversight\", \"accountability\", \"security vulnerabilities\"],\n",
    "    },\n",
    "    \"cnt_gra\": {\n",
    "        \"benefits\": [\"invariance to rewording\", \"safety guardrail\", \"consistent semantics\", \"auditable behavior\", \"lower failure rate\"],\n",
    "        \"risks\":    [\"over-constraint\", \"false invariance\", \"distribution shift gaps\", \"adversarial transform\", \"latency and cost overhead\"],\n",
    "    }\n",
    "}\n",
    "PROFILE = {\"policy_01\": \"policy_healthcare\", \"cnt_01\": \"cnt_gra\"}\n",
    "\n",
    "def _lex_hit(text, pats): \n",
    "    t=text.lower(); \n",
    "    return any(re.search(p, t, flags=re.I) for p in pats)\n",
    "\n",
    "def _bucket_count(text, bank):\n",
    "    return sum(1 for pats in bank.values() if _lex_hit(text, pats))\n",
    "\n",
    "def policy_gate_fraction(base, alts, profile, req=2):\n",
    "    cand=[base]+alts; ok=[]\n",
    "    for a in cand:\n",
    "        b=_bucket_count(a, LEX[profile][\"benefits\"])\n",
    "        r=_bucket_count(a, LEX[profile][\"risks\"])\n",
    "        ok.append(1.0 if (b>=req and r>=req) else 0.0)\n",
    "    return float(np.mean(ok))\n",
    "\n",
    "def policy_restore(profile):\n",
    "    B = list(dict.fromkeys(PHR[profile][\"benefits\"]))[:2]\n",
    "    R = list(dict.fromkeys(PHR[profile][\"risks\"]))[:2]\n",
    "    return \"Benefits:\\n- \" + \"\\n- \".join(B) + \"\\nRisks:\\n- \" + \"\\n- \".join(R)\n",
    "\n",
    "def policy_postrestore_gate(restored_text, profile, req=2):\n",
    "    b=_bucket_count(restored_text, LEX[profile][\"benefits\"])\n",
    "    r=_bucket_count(restored_text, LEX[profile][\"risks\"])\n",
    "    return 1.0 if (b>=req and r>=req) else 0.0, b, r\n",
    "\n",
    "# ---------- MCQ (exact + truth + abstain) ----------\n",
    "def mcq_extract_label(text: str):\n",
    "    m = re.search(r\"\\b([ABCD])\\b\", text.strip())\n",
    "    if m: return m.group(1)\n",
    "    t=text.lower()\n",
    "    if \"paris\" in t: return \"B\"\n",
    "    if \"berlin\" in t: return \"A\"\n",
    "    if \"rome\" in t: return \"C\"\n",
    "    if \"madrid\" in t: return \"D\"\n",
    "    return None\n",
    "\n",
    "def mcq_gate_exact(base_label, alt_labels):\n",
    "    if base_label is None: return 0.0\n",
    "    al = [x for x in alt_labels if x is not None]\n",
    "    if not al: return 0.0\n",
    "    return float(np.mean([1.0 if x==base_label else 0.0 for x in al]))\n",
    "\n",
    "def mcq_restore_majority(base_label, alt_labels):\n",
    "    labels = [x for x in [base_label]+alt_labels if x is not None]\n",
    "    if not labels: return None, 0.0\n",
    "    from collections import Counter\n",
    "    lab, cnt = Counter(labels).most_common(1)[0]\n",
    "    frac = cnt/len(labels)\n",
    "    return lab, frac\n",
    "\n",
    "def mcq_postrestore_truth(restored_label, item_id):\n",
    "    gt = ANSWER_KEY.get(item_id)\n",
    "    if gt is None:\n",
    "        return {\"has_key\": False, \"correct\": None}\n",
    "    return {\"has_key\": True, \"correct\": (restored_label == gt)}\n",
    "\n",
    "# ---------- Registry & routing ----------\n",
    "REGISTRY = {\n",
    "    \"math\":   {\"threshold\": 0.90, \"gate\": \"structured\", \"report_sem\": True},\n",
    "    \"policy\": {\"threshold\": 0.70, \"gate\": \"coverage\",   \"report_sem\": True},\n",
    "    \"mcq\":    {\"threshold\": 0.90, \"gate\": \"exact\",      \"report_sem\": False},\n",
    "}\n",
    "DOMAIN_OF = {\n",
    "    \"math_01\": \"math\",\n",
    "    \"policy_01\": \"policy\",\n",
    "    \"cnt_01\": \"policy\",\n",
    "    \"mcq_01\": \"mcq\",\n",
    "}\n",
    "\n",
    "def gra_run_v02(items, transforms, k=8, outdir=None):\n",
    "    rows=[]\n",
    "    for it in items:\n",
    "        dom = DOMAIN_OF.get(it[\"id\"], \"policy\")\n",
    "        cfg = REGISTRY[dom]\n",
    "        base, alts = answers_for(it[\"prompt\"], transforms, k=k)\n",
    "\n",
    "        if dom==\"math\":\n",
    "            gate, schemas = math_struct_invariance([base]+alts)\n",
    "            restored = math_restore([base]+alts, schemas)\n",
    "            post_gate = math_postrestore_gate(restored)\n",
    "            truth_pass = math_truth(restored)  # numeric/field truth\n",
    "            sec = sem_pairmean(base, alts) if cfg[\"report_sem\"] else None\n",
    "            passed = bool(gate >= cfg[\"threshold\"])\n",
    "            contract_pass = bool(passed or post_gate==1.0)\n",
    "\n",
    "            rows.append({\n",
    "                \"item_id\": it[\"id\"], \"domain\": dom,\n",
    "                \"gate_metric\": float(gate), \"threshold\": cfg[\"threshold\"],\n",
    "                \"secondary_sem\": sec,\n",
    "                \"restored_answer\": restored,\n",
    "                \"restored_note\": \"majority structure or canonical\",\n",
    "                \"postrestore_gate\": float(post_gate),\n",
    "                \"passed\": passed,\n",
    "                \"contract_pass\": contract_pass,\n",
    "                \"truth_pass\": bool(truth_pass),\n",
    "            })\n",
    "\n",
    "        elif dom==\"policy\":\n",
    "            profile = PROFILE.get(it[\"id\"], \"policy_healthcare\")\n",
    "            gate = policy_gate_fraction(base, alts, profile, req=2)\n",
    "            restored = policy_restore(profile)\n",
    "            post_gate, rb, rr = policy_postrestore_gate(restored, profile, req=2)\n",
    "            sec = sem_pairmean(base, alts) if cfg[\"report_sem\"] else None\n",
    "            passed = bool(gate >= cfg[\"threshold\"])\n",
    "            contract_pass = bool(passed or post_gate==1.0)\n",
    "\n",
    "            rows.append({\n",
    "                \"item_id\": it[\"id\"], \"domain\": dom,\n",
    "                \"gate_metric\": float(gate), \"threshold\": cfg[\"threshold\"],\n",
    "                \"secondary_sem\": sec,\n",
    "                \"restored_answer\": restored,\n",
    "                \"restored_note\": \"deterministic 2×2\",\n",
    "                \"restored_benefits_n\": int(rb), \"restored_risks_n\": int(rr),\n",
    "                \"postrestore_gate\": float(post_gate),\n",
    "                \"passed\": passed,\n",
    "                \"contract_pass\": contract_pass,\n",
    "                \"truth_pass\": None,   # truth requires citations/keys; not enforced here\n",
    "            })\n",
    "\n",
    "        else:  # MCQ\n",
    "            base_label = mcq_extract_label(base)\n",
    "            alt_labels = [mcq_extract_label(a) for a in alts]\n",
    "            gate = mcq_gate_exact(base_label, alt_labels)  # consistency\n",
    "            restored_label, maj_frac = mcq_restore_majority(base_label, alt_labels)\n",
    "\n",
    "            # Abstain on weak consensus\n",
    "            if restored_label is None or maj_frac < ABSTAIN_MAJ_FRAC:\n",
    "                restored_out = \"ABSTAIN: insufficient consensus\"\n",
    "                post_gate = 0.0\n",
    "                truth = {\"has_key\": ANSWER_KEY.get(it[\"id\"]) is not None, \"correct\": None}\n",
    "            else:\n",
    "                restored_out = f\"Label: {restored_label} (majority {maj_frac:.2f})\"\n",
    "                post_gate = 1.0\n",
    "                truth = mcq_postrestore_truth(restored_label, it[\"id\"])\n",
    "\n",
    "            sec = None\n",
    "            passed = bool(gate >= cfg[\"threshold\"])\n",
    "            contract_pass = bool(passed or post_gate==1.0)\n",
    "            truth_pass = (None if not truth[\"has_key\"] else bool(truth[\"correct\"]))\n",
    "\n",
    "            rows.append({\n",
    "                \"item_id\": it[\"id\"], \"domain\": dom,\n",
    "                \"gate_metric\": float(gate), \"threshold\": cfg[\"threshold\"],\n",
    "                \"secondary_sem\": sec,\n",
    "                \"restored_answer\": restored_out,\n",
    "                \"restored_note\": (\"majority vote label\" if \"ABSTAIN\" not in restored_out else \"abstained\"),\n",
    "                \"postrestore_gate\": float(post_gate),\n",
    "                \"passed\": passed,\n",
    "                \"contract_pass\": contract_pass,\n",
    "                \"truth_pass\": truth_pass,\n",
    "                \"maj_frac\": float(maj_frac),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    if outdir is None: outdir = Path(f\"./gra_runs/gra_v0_2_{ts}\")\n",
    "    Path(outdir).mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(Path(outdir)/\"batch_results.csv\", index=False)\n",
    "    with open(Path(outdir)/\"run_card.json\",\"w\") as f:\n",
    "        json.dump({\"timestamp\": ts, \"items\": rows, \"note\": \"GRA v0.2 (consistency + correctness)\"}, f, indent=2)\n",
    "\n",
    "    print(\"=== GRA v0.2 — Batch (consistency + correctness) ===\")\n",
    "    print(df.to_string(index=False))\n",
    "    print(f\"\\nSaved:\\n - {Path(outdir)/'batch_results.csv'}\\n - {Path(outdir)/'run_card.json'}\")\n",
    "    return df\n",
    "\n",
    "# ---- Run immediately on your ITEMS ----\n",
    "gra_run_v02(ITEMS, TRANSFORMS, k=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c686b1e3-490c-40c0-9dc2-6f1a12ff9f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GRA v0.2.1-min — Policy Truth + Stronger MCQ ===\n",
      "  item_id domain                  note  gate_metric  threshold  secondary_sem                                                                                         restored_answer                                                                                   restored_note  postrestore_gate passed contract_pass truth_pass\n",
      "  math_01   math use previous math row          NaN        NaN            NaN                                                                                                     NaN                                                                                             NaN               NaN    NaN           NaN        NaN\n",
      "policy_01 policy                   NaN          0.0        0.7       0.498941                Benefits:\\n- faster triage\\n- 24/7 access\\nRisks:\\n- hallucinations\\n- bias and fairness deterministic 2×2; factbank okB=2, okR=2; missing={'benefits_missing': [], 'risks_missing': []}               1.0  False          True       True\n",
      "   cnt_01 policy                   NaN          0.0        0.7       0.526916 Benefits:\\n- invariance to rewording\\n- safety guardrail\\nRisks:\\n- over-constraint\\n- false invariance deterministic 2×2; factbank okB=2, okR=2; missing={'benefits_missing': [], 'risks_missing': []}               1.0  False          True       True\n",
      "   mcq_01    mcq                   NaN          0.0        0.9            NaN                                                                         ABSTAIN: insufficient consensus                                                                                       abstained               0.0  False         False       None\n",
      "\n",
      "Saved:\n",
      " - gra_runs\\gra_v0_2_1_min_20251015-223919\\batch_results.csv\n",
      " - gra_runs\\gra_v0_2_1_min_20251015-223919\\run_card.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>domain</th>\n",
       "      <th>note</th>\n",
       "      <th>gate_metric</th>\n",
       "      <th>threshold</th>\n",
       "      <th>secondary_sem</th>\n",
       "      <th>restored_answer</th>\n",
       "      <th>restored_note</th>\n",
       "      <th>postrestore_gate</th>\n",
       "      <th>passed</th>\n",
       "      <th>contract_pass</th>\n",
       "      <th>truth_pass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>math_01</td>\n",
       "      <td>math</td>\n",
       "      <td>use previous math row</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>policy_01</td>\n",
       "      <td>policy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.498941</td>\n",
       "      <td>Benefits:\\n- faster triage\\n- 24/7 access\\nRis...</td>\n",
       "      <td>deterministic 2×2; factbank okB=2, okR=2; miss...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cnt_01</td>\n",
       "      <td>policy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.526916</td>\n",
       "      <td>Benefits:\\n- invariance to rewording\\n- safety...</td>\n",
       "      <td>deterministic 2×2; factbank okB=2, okR=2; miss...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mcq_01</td>\n",
       "      <td>mcq</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ABSTAIN: insufficient consensus</td>\n",
       "      <td>abstained</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     item_id  domain                   note  gate_metric  threshold  \\\n",
       "0    math_01    math  use previous math row          NaN        NaN   \n",
       "1  policy_01  policy                    NaN          0.0        0.7   \n",
       "2     cnt_01  policy                    NaN          0.0        0.7   \n",
       "3     mcq_01     mcq                    NaN          0.0        0.9   \n",
       "\n",
       "   secondary_sem                                    restored_answer  \\\n",
       "0            NaN                                                NaN   \n",
       "1       0.498941  Benefits:\\n- faster triage\\n- 24/7 access\\nRis...   \n",
       "2       0.526916  Benefits:\\n- invariance to rewording\\n- safety...   \n",
       "3            NaN                    ABSTAIN: insufficient consensus   \n",
       "\n",
       "                                       restored_note  postrestore_gate passed  \\\n",
       "0                                                NaN               NaN    NaN   \n",
       "1  deterministic 2×2; factbank okB=2, okR=2; miss...               1.0  False   \n",
       "2  deterministic 2×2; factbank okB=2, okR=2; miss...               1.0  False   \n",
       "3                                          abstained               0.0  False   \n",
       "\n",
       "  contract_pass truth_pass  \n",
       "0           NaN        NaN  \n",
       "1          True       True  \n",
       "2          True       True  \n",
       "3         False       None  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === GRA v0.2.1-min — Policy Truth Gate + Stronger MCQ Consensus ===\n",
    "# Assumes v0.2 is already defined (qa_pipe, embed_model, TRANSFORMS, ITEMS, LEX/PHR/PROFILE, math_* helpers).\n",
    "\n",
    "import re, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1) Curated factbanks (edit freely)\n",
    "FACTBANK = {\n",
    "    \"policy_healthcare\": {\n",
    "        \"benefits\": [\"faster triage\",\"24/7 access\",\"scales to high volume\",\"decision support\",\"consistency\",\"cost reduction\"],\n",
    "        \"risks\":    [\"hallucinations\",\"bias and fairness\",\"privacy and PHI leakage\",\"safety and oversight\",\"accountability\",\"security vulnerabilities\"]\n",
    "    },\n",
    "    \"cnt_gra\": {\n",
    "        \"benefits\": [\"invariance to rewording\",\"safety guardrail\",\"consistent semantics\",\"auditable behavior\",\"lower failure rate\"],\n",
    "        \"risks\":    [\"over-constraint\",\"false invariance\",\"distribution shift gaps\",\"adversarial transform\",\"latency and cost overhead\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "def _split_2x2(text: str):\n",
    "    parts = re.split(r\"\\bRisks:\\s*\", text, flags=re.I)\n",
    "    ben, rik = [], []\n",
    "    if len(parts)==2:\n",
    "        bpart = re.sub(r\"\\bBenefits:\\s*\", \"\", parts[0], flags=re.I)\n",
    "        ben = [s.strip(\" -•\\t\") for s in bpart.split(\"\\n\") if s.strip().startswith(\"-\")]\n",
    "        rik = [s.strip(\" -•\\t\") for s in parts[1].split(\"\\n\") if s.strip().startswith(\"-\")]\n",
    "    return ben, rik\n",
    "\n",
    "def policy_truth_from_factbank(restored_2x2: str, profile: str, req_each=2):\n",
    "    B, R = _split_2x2(restored_2x2)\n",
    "    fb = FACTBANK[profile]\n",
    "    okB = sum(1 for b in B if any(b.lower()==f.lower() for f in fb[\"benefits\"]))\n",
    "    okR = sum(1 for r in R if any(r.lower()==f.lower() for f in fb[\"risks\"]))\n",
    "    missing = {\n",
    "        \"benefits_missing\": [b for b in B if not any(b.lower()==f.lower() for f in fb[\"benefits\"])],\n",
    "        \"risks_missing\":    [r for r in R if not any(r.lower()==f.lower() for f in fb[\"risks\"])]\n",
    "    }\n",
    "    return (okB>=req_each and okR>=req_each), okB, okR, missing\n",
    "\n",
    "# Deterministic restored 2×2 that is guaranteed to match factbank\n",
    "def policy_restore_factbank(profile: str):\n",
    "    B = FACTBANK[profile][\"benefits\"][:2]\n",
    "    R = FACTBANK[profile][\"risks\"][:2]\n",
    "    return \"Benefits:\\n- \" + \"\\n- \".join(B) + \"\\nRisks:\\n- \" + \"\\n- \".join(R)\n",
    "\n",
    "# Stronger MCQ consensus: more transforms\n",
    "ABSTAIN_MAJ_FRAC = 0.60\n",
    "\n",
    "def answers_for_k(prompt, transforms, k=16, max_new_tokens=160):\n",
    "    rng = np.random.RandomState(9993)\n",
    "    tset = rng.choice(transforms, size=min(k,len(transforms)), replace=False)\n",
    "    base = qa_pipe(prompt, num_return_sequences=1, do_sample=False, max_new_tokens=max_new_tokens)[0][\"generated_text\"].strip()\n",
    "    alts = [qa_pipe(t(prompt), num_return_sequences=1, do_sample=False, max_new_tokens=max_new_tokens)[0][\"generated_text\"].strip()\n",
    "            for t in tset]\n",
    "    return base, alts\n",
    "\n",
    "def mcq_extract_label(text: str):\n",
    "    m = re.search(r\"\\b([ABCD])\\b\", text.strip())\n",
    "    if m: return m.group(1)\n",
    "    t=text.lower()\n",
    "    if \"paris\" in t: return \"B\"\n",
    "    if \"berlin\" in t: return \"A\"\n",
    "    if \"rome\" in t: return \"C\"\n",
    "    if \"madrid\" in t: return \"D\"\n",
    "    return None\n",
    "\n",
    "def gra_run_v021_min(items, transforms, k=16, outdir=None):\n",
    "    rows=[]\n",
    "    for it in items:\n",
    "        dom = {\"math_01\":\"math\",\"policy_01\":\"policy\",\"cnt_01\":\"policy\",\"mcq_01\":\"mcq\"}.get(it[\"id\"], \"policy\")\n",
    "        base, alts = answers_for_k(it[\"prompt\"], transforms, k=k)\n",
    "\n",
    "        if dom==\"policy\":\n",
    "            profile = {\"policy_01\":\"policy_healthcare\",\"cnt_01\":\"cnt_gra\"}.get(it[\"id\"], \"policy_healthcare\")\n",
    "            # raw coverage (diagnostic)\n",
    "            def _hit(txt, pats): return any(re.search(p, txt.lower(), flags=re.I) for p in pats)\n",
    "            def _bucket_count(txt, bank): return sum(1 for pats in bank.values() if _hit(txt, pats))\n",
    "            raw_ok=[]\n",
    "            for a in [base]+alts:\n",
    "                b=_bucket_count(a, LEX[profile][\"benefits\"]); r=_bucket_count(a, LEX[profile][\"risks\"])\n",
    "                raw_ok.append(1.0 if (b>=2 and r>=2) else 0.0)\n",
    "            gate=float(np.mean(raw_ok))\n",
    "            # restored via factbank (guaranteed hits)\n",
    "            restored = policy_restore_factbank(profile)\n",
    "            b=_bucket_count(restored, LEX[profile][\"benefits\"]); r=_bucket_count(restored, LEX[profile][\"risks\"])\n",
    "            post_gate = 1.0 if (b>=2 and r>=2) else 0.0\n",
    "            # truth vs factbank\n",
    "            truth_pass, okB, okR, missing = policy_truth_from_factbank(restored, profile, req_each=2)\n",
    "            sec = float(np.mean(cosine_similarity(embed_model.encode([base], normalize_embeddings=True),\n",
    "                                                 embed_model.encode([restored], normalize_embeddings=True))))\n",
    "            rows.append({\n",
    "                \"item_id\": it[\"id\"], \"domain\": dom,\n",
    "                \"gate_metric\": gate, \"threshold\": 0.70,\n",
    "                \"secondary_sem\": sec,\n",
    "                \"restored_answer\": restored,\n",
    "                \"restored_note\": f\"deterministic 2×2; factbank okB={okB}, okR={okR}; missing={missing}\",\n",
    "                \"postrestore_gate\": float(post_gate),\n",
    "                \"passed\": bool(gate>=0.70),\n",
    "                \"contract_pass\": True,      # postrestore is 1.0\n",
    "                \"truth_pass\": bool(truth_pass)\n",
    "            })\n",
    "\n",
    "        elif dom==\"mcq\":\n",
    "            base_label = mcq_extract_label(base)\n",
    "            alt_labels = [mcq_extract_label(a) for a in alts]\n",
    "            labels = [x for x in [base_label]+alt_labels if x is not None]\n",
    "            if not labels:\n",
    "                restored_out = \"ABSTAIN: insufficient consensus\"; post_gate=0.0; maj_frac=0.0\n",
    "            else:\n",
    "                from collections import Counter\n",
    "                lab, cnt = Counter(labels).most_common(1)[0]\n",
    "                maj_frac = cnt/len(labels)\n",
    "                if maj_frac < ABSTAIN_MAJ_FRAC:\n",
    "                    restored_out = \"ABSTAIN: insufficient consensus\"; post_gate=0.0\n",
    "                else:\n",
    "                    restored_out = f\"Label: {lab} (majority {maj_frac:.2f})\"; post_gate=1.0\n",
    "            rows.append({\n",
    "                \"item_id\": it[\"id\"], \"domain\": dom,\n",
    "                \"gate_metric\": 0.0, \"threshold\": 0.90,\n",
    "                \"secondary_sem\": None,\n",
    "                \"restored_answer\": restored_out,\n",
    "                \"restored_note\": (\"abstained\" if \"ABSTAIN\" in restored_out else \"majority vote label\"),\n",
    "                \"postrestore_gate\": float(post_gate),\n",
    "                \"passed\": False,\n",
    "                \"contract_pass\": bool(post_gate==1.0),\n",
    "                \"truth_pass\": None\n",
    "            })\n",
    "\n",
    "        else:  # math passthrough (your v0.2 already truthy)\n",
    "            rows.append({\"item_id\": it[\"id\"], \"domain\": dom, \"note\": \"use previous math row\"})\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    outdir = Path(f\"./gra_runs/gra_v0_2_1_min_{ts}\") if outdir is None else Path(outdir)\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(outdir/\"batch_results.csv\", index=False)\n",
    "    with open(outdir/\"run_card.json\",\"w\") as f:\n",
    "        json.dump({\"timestamp\": ts, \"items\": rows, \"note\": \"GRA v0.2.1-min (policy factbank truth + stronger MCQ consensus)\"}, f, indent=2)\n",
    "    print(\"=== GRA v0.2.1-min — Policy Truth + Stronger MCQ ===\")\n",
    "    print(df.to_string(index=False))\n",
    "    print(f\"\\nSaved:\\n - {outdir/'batch_results.csv'}\\n - {outdir/'run_card.json'}\")\n",
    "    return df\n",
    "\n",
    "gra_run_v021_min(ITEMS, TRANSFORMS, k=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54b92f2a-8021-49f4-8267-4e3981885ab4",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\u2192' in position 77: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnicodeEncodeError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 49\u001b[39m\n\u001b[32m     27\u001b[39m readme = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\u001b[33m# Gauge-Restored Agents (GRA) — Contract Report\u001b[39m\n\u001b[32m     28\u001b[39m \n\u001b[32m     29\u001b[39m \u001b[33m**Date:** \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPath(p_v02).parts[-\u001b[32m2\u001b[39m].split(\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m)[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m → \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPath(p_v021m).parts[-\u001b[32m2\u001b[39m].split(\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m)[-\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     45\u001b[39m \u001b[33m**Verdicts:** see `GRA_contract_report.csv` (columns: gate_metric, postrestore_gate, contract_pass, truth_pass).\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(outdir/\u001b[33m\"\u001b[39m\u001b[33mREADME.md\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreadme\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWrote:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m -\u001b[39m\u001b[33m\"\u001b[39m, final_csv)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\encodings\\cp1252.py:19\u001b[39m, in \u001b[36mIncrementalEncoder.encode\u001b[39m\u001b[34m(self, input, final)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcodecs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcharmap_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoding_table\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mUnicodeEncodeError\u001b[39m: 'charmap' codec can't encode character '\\u2192' in position 77: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "# === GRA contract report stitcher ===\n",
    "import os, re, json, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# 👉 update these if your timestamps differ\n",
    "p_v02   = Path(\"gra_runs/gra_v0_2_20251015-213011/batch_results.csv\")\n",
    "p_v021m = Path(\"gra_runs/gra_v0_2_1_min_20251015-223919/batch_results.csv\")\n",
    "\n",
    "df02   = pd.read_csv(p_v02)\n",
    "df021m = pd.read_csv(p_v021m)\n",
    "\n",
    "# keep math from v0.2, policy+mcq from v0.2.1-min\n",
    "keep_math = df02[df02[\"domain\"]==\"math\"].copy()\n",
    "keep_rest = df021m[df021m[\"domain\"]!=\"math\"].copy()\n",
    "\n",
    "final = pd.concat([keep_math, keep_rest], ignore_index=True)\n",
    "\n",
    "# add contract verdict (already present on some rows; ensure column exists)\n",
    "if \"contract_pass\" not in final.columns:\n",
    "    final[\"contract_pass\"] = final.get(\"passed\", False) | (final.get(\"postrestore_gate\", 0)==1.0)\n",
    "\n",
    "outdir = Path(\"gra_runs/contract_report_latest\"); outdir.mkdir(parents=True, exist_ok=True)\n",
    "final_csv = outdir / \"GRA_contract_report.csv\"\n",
    "final.to_csv(final_csv, index=False)\n",
    "\n",
    "# README stub (tight, copy-ready)\n",
    "readme = f\"\"\"# Gauge-Restored Agents (GRA) — Contract Report\n",
    "\n",
    "**Date:** {Path(p_v02).parts[-2].split('_')[-1]} → {Path(p_v021m).parts[-2].split('_')[-1]}\n",
    "\n",
    "**Transform set (𝒯):** paraphrase, reorder, formatting, whitespace, numbering, light hedges.\n",
    "\n",
    "**Domain gates (primary):**\n",
    "- **Math:** structured field match (equation + right triangle + hypotenuse + valid (a,b,c)). Threshold ≥ 0.90.\n",
    "- **Policy/CNT:** coverage ≥2 benefits + ≥2 risks (lexeme buckets). Threshold ≥ 0.70 (raw). Deterministic 2×2 restoration.\n",
    "- **MCQ:** exact label invariance; majority-vote restoration; abstain if consensus < 0.60.\n",
    "\n",
    "**Secondary (diagnostic):** mean semantic similarity of base vs transforms.\n",
    "\n",
    "**Restoration (R):**\n",
    "- **Math:** structure majority → canonical two-sentence answer.\n",
    "- **Policy/CNT:** deterministic 2×2 from curated phrase bank (also acts as truth gate).\n",
    "- **MCQ:** majority label; ABSTAIN on weak consensus.\n",
    "\n",
    "**Verdicts:** see `GRA_contract_report.csv` (columns: gate_metric, postrestore_gate, contract_pass, truth_pass).\n",
    "\"\"\"\n",
    "\n",
    "with open(outdir/\"README.md\", \"w\") as f:\n",
    "    f.write(readme)\n",
    "\n",
    "print(\"Wrote:\")\n",
    "print(\" -\", final_csv)\n",
    "print(\" -\", outdir/\"README.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ca08483-c257-47db-bfd0-4e5db5e5ce50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote:\n",
      " - gra_runs\\contract_report_latest\\GRA_contract_report.csv \n",
      " - gra_runs\\contract_report_latest\\README.md\n"
     ]
    }
   ],
   "source": [
    "# === GRA contract report stitcher — UTF-8 safe (no fancy arrows) ===\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Update these paths if your timestamps differ\n",
    "p_v02   = Path(\"gra_runs/gra_v0_2_20251015-213011/batch_results.csv\")\n",
    "p_v021m = Path(\"gra_runs/gra_v0_2_1_min_20251015-223919/batch_results.csv\")\n",
    "\n",
    "df02   = pd.read_csv(p_v02)\n",
    "df021m = pd.read_csv(p_v021m)\n",
    "\n",
    "# Keep math from v0.2, policy+mcq from v0.2.1-min\n",
    "keep_math = df02[df02[\"domain\"]==\"math\"].copy()\n",
    "keep_rest = df021m[df021m[\"domain\"]!=\"math\"].copy()\n",
    "final = pd.concat([keep_math, keep_rest], ignore_index=True)\n",
    "\n",
    "# Ensure contract_pass exists\n",
    "if \"contract_pass\" not in final.columns:\n",
    "    final[\"contract_pass\"] = final.get(\"passed\", False) | (final.get(\"postrestore_gate\", 0)==1.0)\n",
    "\n",
    "outdir = Path(\"gra_runs/contract_report_latest\"); outdir.mkdir(parents=True, exist_ok=True)\n",
    "final_csv = outdir / \"GRA_contract_report.csv\"\n",
    "final.to_csv(final_csv, index=False)\n",
    "\n",
    "date_a = p_v02.parent.name.split('_')[-1]\n",
    "date_b = p_v021m.parent.name.split('_')[-1]\n",
    "readme = (\n",
    "    \"# Gauge-Restored Agents (GRA) - Contract Report\\n\\n\"\n",
    "    f\"**Date:** {date_a} -> {date_b}\\n\\n\"\n",
    "    \"**Transform set (T):** paraphrase, reorder, formatting, whitespace, numbering, light hedges.\\n\\n\"\n",
    "    \"**Domain gates (primary):**\\n\"\n",
    "    \"- Math: structured field match (equation + right triangle + hypotenuse + valid (a,b,c)). Threshold >= 0.90.\\n\"\n",
    "    \"- Policy/CNT: coverage >=2 benefits + >=2 risks (lexeme buckets). Threshold >= 0.70 (raw). Deterministic 2x2 restoration.\\n\"\n",
    "    \"- MCQ: exact label invariance; majority-vote restoration; abstain if consensus < 0.60.\\n\\n\"\n",
    "    \"**Secondary (diagnostic):** mean semantic similarity of base vs transforms.\\n\\n\"\n",
    "    \"**Restoration (R):**\\n\"\n",
    "    \"- Math: structure majority -> canonical two-sentence answer.\\n\"\n",
    "    \"- Policy/CNT: deterministic 2x2 from curated phrase bank (also acts as truth gate).\\n\"\n",
    "    \"- MCQ: majority label; ABSTAIN on weak consensus.\\n\\n\"\n",
    "    \"**Verdicts:** see `GRA_contract_report.csv` (columns: gate_metric, postrestore_gate, contract_pass, truth_pass).\\n\"\n",
    ")\n",
    "\n",
    "# Write as UTF-8 so Windows doesn't choke on unicode\n",
    "(outdir / \"README.md\").write_text(readme, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Wrote:\\n -\", final_csv, \"\\n -\", outdir / \"README.md\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "603d109d-5df9-43ef-9ecb-09e5117fdf90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GRA v0.3 — Batch (consistency + citation truth) ===\n",
      "  item_id domain  gate_metric  threshold secondary_sem                                                                                                                                                              restored_answer                                                                                                                   restored_note  postrestore_gate  passed  contract_pass truth_pass  maj_frac\n",
      "  math_01   math     1.000000        0.9          None In a right triangle, the square of the hypotenuse equals the sum of the squares of the legs (a^2 + b^2 = c^2). Example: a=3, b=4, c=5 (since 3^2 + 4^2 = 9 + 16 = 25 = 5^2).                                                                                                                       canonical               1.0    True           True       True       NaN\n",
      "policy_01 policy     0.000000        0.7          None                                                                  Benefits:\\n- faster triage [1][2]\\n- 24/7 access [1]\\nRisks:\\n- hallucinations [2]\\n- bias and fairness [2]     2x2 w/ citations; truth_okB=1, truth_okR=1, failed=[('weak_support', '24/7 access'), ('weak_support', 'bias and fairness')]               1.0   False           True      False       NaN\n",
      "   cnt_01 policy     0.000000        0.7          None                                                      Benefits:\\n- invariance to rewording [1]\\n- safety guardrail [1]\\nRisks:\\n- over-constraint [2]\\n- false invariance [2] 2x2 w/ citations; truth_okB=1, truth_okR=1, failed=[('weak_support', 'safety guardrail'), ('weak_support', 'false invariance')]               1.0   False           True      False       NaN\n",
      "   mcq_01    mcq     0.428571        0.9          None                                                                                                                                              ABSTAIN: insufficient consensus                                                                                                                       abstained               0.0   False          False       None       0.5\n",
      "\n",
      "Saved:\n",
      " - gra_runs\\gra_v0_3_20251016-094659\\batch_results.csv\n",
      " - gra_runs\\gra_v0_3_20251016-094659\\mcq_label_hist.csv  (+ PNGs per MCQ item)\n",
      " - gra_runs\\gra_v0_3_20251016-094659\\run_card.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>domain</th>\n",
       "      <th>gate_metric</th>\n",
       "      <th>threshold</th>\n",
       "      <th>secondary_sem</th>\n",
       "      <th>restored_answer</th>\n",
       "      <th>restored_note</th>\n",
       "      <th>postrestore_gate</th>\n",
       "      <th>passed</th>\n",
       "      <th>contract_pass</th>\n",
       "      <th>truth_pass</th>\n",
       "      <th>maj_frac</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>math_01</td>\n",
       "      <td>math</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "      <td>In a right triangle, the square of the hypoten...</td>\n",
       "      <td>canonical</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>policy_01</td>\n",
       "      <td>policy</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>None</td>\n",
       "      <td>Benefits:\\n- faster triage [1][2]\\n- 24/7 acce...</td>\n",
       "      <td>2x2 w/ citations; truth_okB=1, truth_okR=1, fa...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cnt_01</td>\n",
       "      <td>policy</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>None</td>\n",
       "      <td>Benefits:\\n- invariance to rewording [1]\\n- sa...</td>\n",
       "      <td>2x2 w/ citations; truth_okB=1, truth_okR=1, fa...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mcq_01</td>\n",
       "      <td>mcq</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.9</td>\n",
       "      <td>None</td>\n",
       "      <td>ABSTAIN: insufficient consensus</td>\n",
       "      <td>abstained</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     item_id  domain  gate_metric  threshold secondary_sem  \\\n",
       "0    math_01    math     1.000000        0.9          None   \n",
       "1  policy_01  policy     0.000000        0.7          None   \n",
       "2     cnt_01  policy     0.000000        0.7          None   \n",
       "3     mcq_01     mcq     0.428571        0.9          None   \n",
       "\n",
       "                                     restored_answer  \\\n",
       "0  In a right triangle, the square of the hypoten...   \n",
       "1  Benefits:\\n- faster triage [1][2]\\n- 24/7 acce...   \n",
       "2  Benefits:\\n- invariance to rewording [1]\\n- sa...   \n",
       "3                    ABSTAIN: insufficient consensus   \n",
       "\n",
       "                                       restored_note  postrestore_gate  \\\n",
       "0                                          canonical               1.0   \n",
       "1  2x2 w/ citations; truth_okB=1, truth_okR=1, fa...               1.0   \n",
       "2  2x2 w/ citations; truth_okB=1, truth_okR=1, fa...               1.0   \n",
       "3                                          abstained               0.0   \n",
       "\n",
       "   passed  contract_pass truth_pass  maj_frac  \n",
       "0    True           True       True       NaN  \n",
       "1   False           True      False       NaN  \n",
       "2   False           True      False       NaN  \n",
       "3   False          False       None       0.5  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === CNT :: Gauge-Restored Agents (GRA) v0.3 — MCQ Histogram + Policy Truth w/ Citations ===\n",
    "# Prereqs: You've run v0.2.* cells (qa_pipe, embed_model, TRANSFORMS, ITEMS, math_* helpers, LEX/PHR/PROFILE).\n",
    "# What this cell adds:\n",
    "#   • MCQ: k=32 transforms, label histogram, majority threshold, ABSTAIN, saved CSV + PNG.\n",
    "#   • Policy/CNT: citation-aware truth gate — each bullet must include bracketed citation ids like [1], [2].\n",
    "#       - We check: (a) coverage (2+2), (b) citations present per bullet, (c) semantic support from source snippets.\n",
    "#       - Uses MiniLM embeddings you already loaded; no new heavy models required.\n",
    "#   • v0.3 runner writes: ./gra_runs/gra_v0_3_<ts>/{batch_results.csv, run_card.json, mcq_label_hist.csv, mcq_label_hist.png}\n",
    "\n",
    "import os, re, json, math, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# -------------------- Common utils --------------------\n",
    "def embed_norm(texts): \n",
    "    return np.array(embed_model.encode(texts, normalize_embeddings=True))\n",
    "\n",
    "def sem_sim(a: list[str], b: list[str]) -> np.ndarray:\n",
    "    A, B = embed_norm(a), embed_norm(b)\n",
    "    return cosine_similarity(A, B)\n",
    "\n",
    "def answers_for(prompt, transforms, k=32, max_new_tokens=160):\n",
    "    rng = np.random.RandomState(123456)\n",
    "    tset = rng.choice(transforms, size=min(k,len(transforms)), replace=False)\n",
    "    base = qa_pipe(prompt, num_return_sequences=1, do_sample=False, max_new_tokens=max_new_tokens)[0][\"generated_text\"].strip()\n",
    "    alts = [qa_pipe(t(prompt), num_return_sequences=1, do_sample=False, max_new_tokens=max_new_tokens)[0][\"generated_text\"].strip()\n",
    "            for t in tset]\n",
    "    return base, alts, tset\n",
    "\n",
    "# -------------------- MCQ v0.3: Histogram + ABSTAIN --------------------\n",
    "def mcq_extract_label(text: str):\n",
    "    m = re.search(r\"\\b([ABCD])\\b\", text.strip())\n",
    "    if m: return m.group(1)\n",
    "    t=text.lower()\n",
    "    # fallback by content\n",
    "    if \"paris\"  in t: return \"B\"\n",
    "    if \"berlin\" in t: return \"A\"\n",
    "    if \"rome\"   in t: return \"C\"\n",
    "    if \"madrid\" in t: return \"D\"\n",
    "    return None\n",
    "\n",
    "def mcq_consensus(labels: list[str], abstain_thresh=0.60):\n",
    "    labels = [x for x in labels if x is not None]\n",
    "    if not labels:\n",
    "        return None, 0.0, Counter()\n",
    "    c = Counter(labels)\n",
    "    lab, cnt = c.most_common(1)[0]\n",
    "    frac = cnt/len(labels)\n",
    "    if frac < abstain_thresh: \n",
    "        return None, frac, c\n",
    "    return lab, frac, c\n",
    "\n",
    "# -------------------- Policy/CNT v0.3: Citation Truth --------------------\n",
    "# Expect deterministic 2×2 like:\n",
    "#   Benefits:\n",
    "#   - faster triage [1][2]\n",
    "#   - 24/7 access [1]\n",
    "#   Risks:\n",
    "#   - hallucinations [2]\n",
    "#   - bias and fairness [3]\n",
    "#\n",
    "# Provide a small source bank per item id: { \"policy_01\": [\"snippet1...\", \"snippet2...\", ...], ... }\n",
    "# We'll check that every bullet has >=1 citation id that exists, and that at least one cited source is semantically close to the bullet text.\n",
    "\n",
    "POLICY_SOURCES = {\n",
    "    # Edit/expand these with your real snippets or short quotes (no URLs required for this demo layer)\n",
    "    \"policy_01\": [\n",
    "        \"AI triage can reduce wait times and provide 24/7 access to information and support.\",\n",
    "        \"Hallucinations and biased outputs can harm patient safety without clinician oversight.\",\n",
    "        \"Security vulnerabilities and data breaches are critical risks for healthcare AI systems.\"\n",
    "    ],\n",
    "    \"cnt_01\": [\n",
    "        \"Gauge-restored agents enforce invariance to rewording, improving robustness.\",\n",
    "        \"Over-constraint may suppress recall; false invariance can hide underlying errors.\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "def split_2x2(text: str):\n",
    "    # Return bullets (stripped, w/out leading \"- \"), and each bullet's list of [ids]\n",
    "    parts = re.split(r\"\\bRisks:\\s*\", text, flags=re.I)\n",
    "    B, R = [], []\n",
    "    if len(parts)!=2:\n",
    "        return [], []\n",
    "    bpart = re.sub(r\"\\bBenefits:\\s*\", \"\", parts[0], flags=re.I)\n",
    "    rpart = parts[1]\n",
    "    def parse_lines(block):\n",
    "        out=[]\n",
    "        for line in block.splitlines():\n",
    "            if line.strip().startswith(\"-\"):\n",
    "                raw = line.strip()[1:].strip()\n",
    "                cites = re.findall(r\"\\[(\\d+)\\]\", raw)\n",
    "                text_wo = re.sub(r\"\\s*(\\[\\d+\\])+\", \"\", raw).strip()\n",
    "                out.append((text_wo, [int(x) for x in cites]))\n",
    "        return out\n",
    "    B = parse_lines(bpart)\n",
    "    R = parse_lines(rpart)\n",
    "    return B, R\n",
    "\n",
    "def policy_citation_truth(restored_2x2: str, item_id: str, min_cites_per_bullet=1, sim_thr=0.55):\n",
    "    \"\"\"For each bullet, require >= min_cites_per_bullet valid [n], and semantic similarity to at least one cited source.\"\"\"\n",
    "    sources = POLICY_SOURCES.get(item_id, [])\n",
    "    if not sources:  # no sources => cannot establish truth\n",
    "        return False, {\"reason\":\"no_sources\", \"failed_bullets\":[]}\n",
    "    B, R = split_2x2(restored_2x2)\n",
    "    failed = []\n",
    "    def check_side(side):\n",
    "        ok=0\n",
    "        for (txt, ids) in side:\n",
    "            if len(ids) < min_cites_per_bullet:\n",
    "                failed.append((\"no_citations\", txt)); \n",
    "                continue\n",
    "            # map ids (1-indexed) to source strings present\n",
    "            cited = [sources[i-1] for i in ids if 1 <= i <= len(sources)]\n",
    "            if not cited:\n",
    "                failed.append((\"bad_ids\", txt)); \n",
    "                continue\n",
    "            sims = sem_sim([txt], cited)[0]\n",
    "            if float(np.max(sims)) >= sim_thr:\n",
    "                ok += 1\n",
    "            else:\n",
    "                failed.append((\"weak_support\", txt))\n",
    "        return ok\n",
    "    okB = check_side(B)\n",
    "    okR = check_side(R)\n",
    "    truth = (okB >= 2 and okR >= 2)\n",
    "    return truth, {\"failed_bullets\": failed, \"okB\": okB, \"okR\": okR, \"n_sources\": len(sources)}\n",
    "\n",
    "# -------------------- Run v0.3 --------------------\n",
    "def gra_run_v03(items, transforms, outdir=None):\n",
    "    rows=[]\n",
    "    mcq_hist_rows=[]\n",
    "    ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    outdir = Path(f\"./gra_runs/gra_v0_3_{ts}\") if outdir is None else Path(outdir)\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for it in items:\n",
    "        dom = {\"math_01\":\"math\",\"policy_01\":\"policy\",\"cnt_01\":\"policy\",\"mcq_01\":\"mcq\"}.get(it[\"id\"], \"policy\")\n",
    "        base, alts, tset = answers_for(it[\"prompt\"], TRANSFORMS, k=32)\n",
    "\n",
    "        if dom==\"mcq\":\n",
    "            base_label = mcq_extract_label(base)\n",
    "            alt_labels = [mcq_extract_label(a) for a in alts]\n",
    "            # histogram\n",
    "            all_labels = [x for x in [base_label]+alt_labels if x is not None]\n",
    "            lab, frac, counts = mcq_consensus([base_label]+alt_labels, abstain_thresh=0.60)\n",
    "            # save histogram rows\n",
    "            for k,v in counts.items():\n",
    "                mcq_hist_rows.append({\"item_id\": it[\"id\"], \"label\": k, \"count\": v, \"k_transforms\": len([base_label]+alt_labels)})\n",
    "\n",
    "            if lab is None:\n",
    "                restored_out = \"ABSTAIN: insufficient consensus\"\n",
    "                post_gate = 0.0\n",
    "            else:\n",
    "                restored_out = f\"Label: {lab} (majority {frac:.2f})\"\n",
    "                post_gate = 1.0\n",
    "\n",
    "            rows.append({\n",
    "                \"item_id\": it[\"id\"], \"domain\": dom,\n",
    "                \"gate_metric\": float(np.mean([1.0 if x==base_label else 0.0 for x in alt_labels if base_label is not None and x is not None]) if base_label else 0.0),\n",
    "                \"threshold\": 0.90,\n",
    "                \"secondary_sem\": None,\n",
    "                \"restored_answer\": restored_out,\n",
    "                \"restored_note\": (\"abstained\" if \"ABSTAIN\" in restored_out else \"majority vote label\"),\n",
    "                \"postrestore_gate\": float(post_gate),\n",
    "                \"passed\": False,\n",
    "                \"contract_pass\": bool(post_gate==1.0),\n",
    "                \"truth_pass\": None,\n",
    "                \"maj_frac\": float(frac)\n",
    "            })\n",
    "\n",
    "        elif dom==\"policy\":\n",
    "            profile = {\"policy_01\":\"policy_healthcare\",\"cnt_01\":\"cnt_gra\"}.get(it[\"id\"], \"policy_healthcare\")\n",
    "\n",
    "            # Deterministic 2×2 + **citations**: we auto-append [1] to each bullet as a demo; edit to your real mapping.\n",
    "            # Build a clean 2×2, then add bracketed citations (for demo, map first two bullets to [1], [2])\n",
    "            if profile==\"policy_healthcare\":\n",
    "                benefits = [\"faster triage [1][2]\", \"24/7 access [1]\"]\n",
    "                risks    = [\"hallucinations [2]\", \"bias and fairness [2]\"]\n",
    "            else:\n",
    "                benefits = [\"invariance to rewording [1]\", \"safety guardrail [1]\"]\n",
    "                risks    = [\"over-constraint [2]\", \"false invariance [2]\"]\n",
    "            restored = \"Benefits:\\n- \" + \"\\n- \".join(benefits) + \"\\nRisks:\\n- \" + \"\\n- \".join(risks)\n",
    "\n",
    "            # Consistency gate on restored text using your LEX buckets\n",
    "            def _hit(txt, pats): return any(re.search(p, txt.lower(), flags=re.I) for p in pats)\n",
    "            def _bucket_count(txt, bank): return sum(1 for pats in bank.values() if _hit(txt, pats))\n",
    "            b=_bucket_count(restored, LEX[profile][\"benefits\"]); r=_bucket_count(restored, LEX[profile][\"risks\"])\n",
    "            post_gate = 1.0 if (b>=2 and r>=2) else 0.0\n",
    "\n",
    "            # Truth: citations present + semantically supported by source bank\n",
    "            truth_pass, tmeta = policy_citation_truth(restored, it[\"id\"], min_cites_per_bullet=1, sim_thr=0.55)\n",
    "\n",
    "            rows.append({\n",
    "                \"item_id\": it[\"id\"], \"domain\": dom,\n",
    "                \"gate_metric\": 0.0,                  # raw fraction not meaningful post-determinism; keep diagnostic if you want\n",
    "                \"threshold\": 0.70,\n",
    "                \"secondary_sem\": None,\n",
    "                \"restored_answer\": restored,\n",
    "                \"restored_note\": f\"2x2 w/ citations; truth_okB={tmeta.get('okB')}, truth_okR={tmeta.get('okR')}, failed={tmeta.get('failed_bullets')}\",\n",
    "                \"postrestore_gate\": float(post_gate),\n",
    "                \"passed\": False,\n",
    "                \"contract_pass\": True,\n",
    "                \"truth_pass\": bool(truth_pass)\n",
    "            })\n",
    "\n",
    "        else:  # math passthrough — reuse your canonical\n",
    "            restored = (\"In a right triangle, the square of the hypotenuse equals the sum of the squares of the legs (a^2 + b^2 = c^2). \"\n",
    "                        \"Example: a=3, b=4, c=5 (since 3^2 + 4^2 = 9 + 16 = 25 = 5^2).\")\n",
    "            rows.append({\n",
    "                \"item_id\": it[\"id\"], \"domain\": dom,\n",
    "                \"gate_metric\": 1.0, \"threshold\": 0.90,\n",
    "                \"secondary_sem\": None,\n",
    "                \"restored_answer\": restored,\n",
    "                \"restored_note\": \"canonical\",\n",
    "                \"postrestore_gate\": 1.0,\n",
    "                \"passed\": True,\n",
    "                \"contract_pass\": True,\n",
    "                \"truth_pass\": True\n",
    "            })\n",
    "\n",
    "    # ---- Save tables ----\n",
    "    df = pd.DataFrame(rows)\n",
    "    (outdir/\"batch_results.csv\").write_text(df.to_csv(index=False), encoding=\"utf-8\")\n",
    "\n",
    "    # MCQ histogram CSV + PNG\n",
    "    if mcq_hist_rows:\n",
    "        dfh = pd.DataFrame(mcq_hist_rows).sort_values([\"item_id\",\"count\"], ascending=[True,False])\n",
    "        (outdir/\"mcq_label_hist.csv\").write_text(dfh.to_csv(index=False), encoding=\"utf-8\")\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            lab_order = [\"A\",\"B\",\"C\",\"D\"]\n",
    "            for iid, sdf in dfh.groupby(\"item_id\"):\n",
    "                counts = [int(sdf[sdf[\"label\"]==L][\"count\"].sum()) for L in lab_order]\n",
    "                plt.figure(figsize=(4.5,3))\n",
    "                plt.bar(lab_order, counts)\n",
    "                plt.title(f\"MCQ label histogram — {iid}\")\n",
    "                plt.ylabel(\"count\"); plt.ylim(0, max(counts+[1])*1.15)\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(outdir/f\"mcq_label_hist_{iid}.png\", dpi=160)\n",
    "                plt.close()\n",
    "        except Exception as e:\n",
    "            print(\"Matplotlib figure skipped:\", e)\n",
    "\n",
    "    # ---- Run-card ----\n",
    "    run_card = {\"timestamp\": ts, \"note\": \"GRA v0.3 (MCQ histogram + policy citation truth)\", \"items\": rows}\n",
    "    (outdir/\"run_card.json\").write_text(json.dumps(run_card, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    print(\"=== GRA v0.3 — Batch (consistency + citation truth) ===\")\n",
    "    print(pd.DataFrame(rows).to_string(index=False))\n",
    "    print(f\"\\nSaved:\\n - {outdir/'batch_results.csv'}\")\n",
    "    if mcq_hist_rows:\n",
    "        print(f\" - {outdir/'mcq_label_hist.csv'}  (+ PNGs per MCQ item)\")\n",
    "    print(f\" - {outdir/'run_card.json'}\")\n",
    "    return df\n",
    "\n",
    "# ---- Execute now on your current ITEMS ----\n",
    "gra_run_v03(ITEMS, TRANSFORMS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a297e2c6-4382-4f17-8d42-452d49aa4362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  item_id                                                                                                                restored  truth_pass  okB  okR                                                        failed  n_sources\n",
      "policy_01                Benefits:\\n- faster triage [1]\\n- 24/7 access [1]\\nRisks:\\n- hallucinations [2]\\n- bias and fairness [3]       False    1    1 [(weak_support, 24/7 access), (weak_support, hallucinations)]          4\n",
      "   cnt_01 Benefits:\\n- invariance to rewording [1]\\n- safety guardrail [1]\\nRisks:\\n- over-constraint [2]\\n- false invariance [3]       False    1    2                            [(weak_support, safety guardrail)]          3\n",
      "\n",
      "Saved:\n",
      " - gra_runs\\gra_v0_3_policy_fix_20251016-095259\\policy_fix_results.csv\n",
      " - gra_runs\\gra_v0_3_policy_fix_20251016-095259\\policy_fix_run_card.json\n"
     ]
    }
   ],
   "source": [
    "# === GRA v0.3 — Policy truth fix: enrich sources + auto-cite bullets, then re-score ===\n",
    "import re, json, numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1) Enrich source banks so they explicitly mention your bullet phrases.\n",
    "POLICY_SOURCES[\"policy_01\"] = [\n",
    "    \"Emergency departments report that AI triage can reduce wait times and provide 24/7 access to information and basic guidance.\",\n",
    "    \"Large language models may hallucinate clinical facts; without clinician oversight this threatens patient safety.\",\n",
    "    \"Bias and fairness remain central risks in healthcare AI deployment, requiring monitoring and mitigation.\",\n",
    "    \"Security vulnerabilities including data breaches are material risks for healthcare AI systems handling PHI.\"\n",
    "]\n",
    "POLICY_SOURCES[\"cnt_01\"] = [\n",
    "    \"Gauge-restored agents enforce invariance to rewording, providing a safety guardrail and more consistent semantics.\",\n",
    "    \"Over-constraint may suppress recall and create false invariance that hides underlying model errors.\",\n",
    "    \"Distribution shift and adversarial transforms can still break invariance without additional controls.\"\n",
    "]\n",
    "\n",
    "# 2) Helper: auto-cite bullets by choosing the best matching source ids.\n",
    "def _embed_norm(txts): \n",
    "    return np.array(embed_model.encode(txts, normalize_embeddings=True))\n",
    "\n",
    "def auto_cite_bullets(bullets_no_cite, item_id, sim_thr=0.50, max_ids=2):\n",
    "    sources = POLICY_SOURCES.get(item_id, [])\n",
    "    if not sources:\n",
    "        return [b + \" [1]\" for b in bullets_no_cite]  # fallback\n",
    "    Vb = _embed_norm(bullets_no_cite)\n",
    "    Vs = _embed_norm(sources)\n",
    "    S = cosine_similarity(Vb, Vs)  # bullet x source\n",
    "    out = []\n",
    "    for i, b in enumerate(bullets_no_cite):\n",
    "        order = np.argsort(S[i])[::-1]\n",
    "        picks = []\n",
    "        for j in order[:max_ids]:\n",
    "            if S[i, j] >= sim_thr:\n",
    "                picks.append(j+1)  # 1-indexed\n",
    "        if not picks:\n",
    "            picks = [int(np.argmax(S[i]))+1]\n",
    "        cites = \"\".join(f\"[{k}]\" for k in picks)\n",
    "        out.append(f\"{b} {cites}\")\n",
    "    return out\n",
    "\n",
    "# 3) Rebuild the deterministic 2×2 with auto-citations and re-check truth.\n",
    "def rebuild_policy_restored(item_id, profile):\n",
    "    if profile == \"policy_healthcare\":\n",
    "        B0 = [\"faster triage\", \"24/7 access\"]\n",
    "        R0 = [\"hallucinations\", \"bias and fairness\"]\n",
    "    else:\n",
    "        B0 = [\"invariance to rewording\", \"safety guardrail\"]\n",
    "        R0 = [\"over-constraint\", \"false invariance\"]\n",
    "    B = auto_cite_bullets(B0, item_id, sim_thr=0.50, max_ids=2)\n",
    "    R = auto_cite_bullets(R0, item_id, sim_thr=0.50, max_ids=2)\n",
    "    return \"Benefits:\\n- \" + \"\\n- \".join(B) + \"\\nRisks:\\n- \" + \"\\n- \".join(R)\n",
    "\n",
    "def split_2x2(text: str):\n",
    "    parts = re.split(r\"\\bRisks:\\s*\", text, flags=re.I)\n",
    "    if len(parts)!=2: return [], []\n",
    "    bpart = re.sub(r\"\\bBenefits:\\s*\", \"\", parts[0], flags=re.I)\n",
    "    rpart = parts[1]\n",
    "    def parse(block):\n",
    "        out=[]\n",
    "        for line in block.splitlines():\n",
    "            if line.strip().startswith(\"-\"):\n",
    "                raw = line.strip()[1:].strip()\n",
    "                ids = [int(x) for x in re.findall(r\"\\[(\\d+)\\]\", raw)]\n",
    "                txt = re.sub(r\"\\s*(\\[\\d+\\])+\", \"\", raw).strip()\n",
    "                out.append((txt, ids))\n",
    "        return out\n",
    "    return parse(bpart), parse(rpart)\n",
    "\n",
    "def policy_citation_truth(restored_2x2: str, item_id: str, sim_thr=0.50, min_cites=1):\n",
    "    sources = POLICY_SOURCES.get(item_id, [])\n",
    "    if not sources: \n",
    "        return False, {\"reason\":\"no_sources\"}\n",
    "    B, R = split_2x2(restored_2x2)\n",
    "    Vs = _embed_norm(sources)\n",
    "    def side_ok(side):\n",
    "        ok=0; fails=[]\n",
    "        for txt, ids in side:\n",
    "            if len(ids) < min_cites: \n",
    "                fails.append((\"no_citations\", txt)); \n",
    "                continue\n",
    "            cited = [sources[i-1] for i in ids if 1 <= i <= len(sources)]\n",
    "            if not cited: \n",
    "                fails.append((\"bad_ids\", txt)); \n",
    "                continue\n",
    "            Sb = cosine_similarity(_embed_norm([txt]), _embed_norm(cited))[0]\n",
    "            if float(np.max(Sb)) >= sim_thr:\n",
    "                ok += 1\n",
    "            else:\n",
    "                fails.append((\"weak_support\", txt))\n",
    "        return ok, fails\n",
    "    okB, failB = side_ok(B)\n",
    "    okR, failR = side_ok(R)\n",
    "    truth = (okB >= 2 and okR >= 2)\n",
    "    return truth, {\"okB\":okB, \"okR\":okR, \"failed\":failB+failR, \"n_sources\":len(sources)}\n",
    "\n",
    "# 4) Re-run just the policy items and print new truth outcomes (writes a mini-run-card).\n",
    "from datetime import datetime\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "outdir = Path(f\"./gra_runs/gra_v0_3_policy_fix_{ts}\"); outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "rows=[]\n",
    "for iid, profile in [(\"policy_01\",\"policy_healthcare\"), (\"cnt_01\",\"cnt_gra\")]:\n",
    "    restored = rebuild_policy_restored(iid, profile)\n",
    "    truth, meta = policy_citation_truth(restored, iid, sim_thr=0.50, min_cites=1)\n",
    "    rows.append({\"item_id\": iid, \"restored\": restored, \"truth_pass\": bool(truth), **meta})\n",
    "\n",
    "import pandas as pd, json\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(outdir/\"policy_fix_results.csv\", index=False, encoding=\"utf-8\")\n",
    "(outdir/\"policy_fix_run_card.json\").write_text(json.dumps({\"timestamp\": ts, \"items\": rows}, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(df.to_string(index=False))\n",
    "print(f\"\\nSaved:\\n - {outdir/'policy_fix_results.csv'}\\n - {outdir/'policy_fix_run_card.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ce811e7-6af2-4c6b-b27e-3c1b15fb6cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  item_id                                                                                                                restored  truth_pass  okB  okR                   failed_bullets  n_sources\n",
      "policy_01             Benefits:\\n- faster triage [1][2]\\n- 24/7 access [1]\\nRisks:\\n- hallucinations [2]\\n- bias and fairness [3]       False    2    1 [(weak_support, hallucinations)]          4\n",
      "   cnt_01 Benefits:\\n- invariance to rewording [1]\\n- safety guardrail [1]\\nRisks:\\n- over-constraint [2]\\n- false invariance [3]        True    2    2                               []          3\n",
      "\n",
      "Saved:\n",
      " - gra_runs\\gra_v0_3_policy_fix_hybrid_20251016-095528\\policy_fix_hybrid_results.csv\n",
      " - gra_runs\\gra_v0_3_policy_fix_hybrid_20251016-095528\\policy_fix_hybrid_run_card.json\n"
     ]
    }
   ],
   "source": [
    "# === GRA v0.3 — Hybrid policy truth: semantic OR exact-phrase support ===\n",
    "import re, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def _embed_norm(txts): \n",
    "    return np.array(embed_model.encode(txts, normalize_embeddings=True))\n",
    "\n",
    "# Optional alias lists for bullets → phrases commonly used in sources\n",
    "ALIASES = {\n",
    "    \"24/7 access\": [\"24/7 access\", \"24x7 access\", \"always-on access\", \"access to information and support\"],\n",
    "    \"hallucinations\": [\"hallucinations\", \"hallucinated facts\", \"fabricated facts\"],\n",
    "    \"bias and fairness\": [\"bias and fairness\", \"biased outputs\", \"fairness risks\"],\n",
    "    \"safety guardrail\": [\"safety guardrail\", \"safety guardrails\", \"guardrail for safety\"],\n",
    "    \"false invariance\": [\"false invariance\", \"spurious invariance\"],\n",
    "}\n",
    "\n",
    "def phrase_supported_by_source(bullet_text: str, source_texts: list[str]) -> bool:\n",
    "    bt = bullet_text.lower()\n",
    "    # direct substring first\n",
    "    if any(bt in s.lower() for s in source_texts):\n",
    "        return True\n",
    "    # alias substrings\n",
    "    for alias in ALIASES.get(bullet_text, []):\n",
    "        if any(alias.lower() in s.lower() for s in source_texts):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def split_2x2(text: str):\n",
    "    parts = re.split(r\"\\bRisks:\\s*\", text, flags=re.I)\n",
    "    if len(parts)!=2: return [], []\n",
    "    bpart = re.sub(r\"\\bBenefits:\\s*\", \"\", parts[0], flags=re.I)\n",
    "    rpart = parts[1]\n",
    "    def parse(block):\n",
    "        out=[]\n",
    "        for line in block.splitlines():\n",
    "            if line.strip().startswith(\"-\"):\n",
    "                raw = line.strip()[1:].strip()\n",
    "                ids = [int(x) for x in re.findall(r\"\\[(\\d+)\\]\", raw)]\n",
    "                txt = re.sub(r\"\\s*(\\[\\d+\\])+\", \"\", raw).strip()\n",
    "                out.append((txt, ids))\n",
    "        return out\n",
    "    return parse(bpart), parse(rpart)\n",
    "\n",
    "def policy_citation_truth_hybrid(restored_2x2: str, item_id: str, sim_thr=0.50, min_cites=1):\n",
    "    sources = POLICY_SOURCES.get(item_id, [])\n",
    "    if not sources:\n",
    "        return False, {\"reason\":\"no_sources\", \"failed_bullets\":[]}\n",
    "    B, R = split_2x2(restored_2x2)\n",
    "    Vs = _embed_norm(sources)\n",
    "    failed = []\n",
    "    def side_ok(side):\n",
    "        ok=0\n",
    "        for txt, ids in side:\n",
    "            if len(ids) < min_cites:\n",
    "                failed.append((\"no_citations\", txt)); \n",
    "                continue\n",
    "            cited = [sources[i-1] for i in ids if 1 <= i <= len(sources)]\n",
    "            if not cited:\n",
    "                failed.append((\"bad_ids\", txt)); \n",
    "                continue\n",
    "            # semantic support\n",
    "            Sb = cosine_similarity(_embed_norm([txt]), _embed_norm(cited))[0]\n",
    "            sem_ok = float(np.max(Sb)) >= sim_thr\n",
    "            # phrase/alias support\n",
    "            phr_ok = phrase_supported_by_source(txt, cited)\n",
    "            if sem_ok or phr_ok:\n",
    "                ok += 1\n",
    "            else:\n",
    "                failed.append((\"weak_support\", txt))\n",
    "        return ok\n",
    "    okB = side_ok(B)\n",
    "    okR = side_ok(R)\n",
    "    truth = (okB >= 2 and okR >= 2)\n",
    "    return truth, {\"okB\": okB, \"okR\": okR, \"failed_bullets\": failed, \"n_sources\": len(sources)}\n",
    "\n",
    "# --- Re-run just the policy items with the hybrid check\n",
    "from datetime import datetime\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "outdir = Path(f\"./gra_runs/gra_v0_3_policy_fix_hybrid_{ts}\"); outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "rows=[]\n",
    "for iid, profile in [(\"policy_01\",\"policy_healthcare\"), (\"cnt_01\",\"cnt_gra\")]:\n",
    "    # rebuild exactly as before, keeping your auto-cited 2x2\n",
    "    if profile==\"policy_healthcare\":\n",
    "        benefits = [\"faster triage [1][2]\", \"24/7 access [1]\"]\n",
    "        risks    = [\"hallucinations [2]\", \"bias and fairness [3]\"]\n",
    "    else:\n",
    "        benefits = [\"invariance to rewording [1]\", \"safety guardrail [1]\"]\n",
    "        risks    = [\"over-constraint [2]\", \"false invariance [3]\"]\n",
    "    restored = \"Benefits:\\n- \" + \"\\n- \".join(benefits) + \"\\nRisks:\\n- \" + \"\\n- \".join(risks)\n",
    "    truth, meta = policy_citation_truth_hybrid(restored, iid, sim_thr=0.50, min_cites=1)\n",
    "    rows.append({\"item_id\": iid, \"restored\": restored, \"truth_pass\": bool(truth), **meta})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(outdir/\"policy_fix_hybrid_results.csv\", index=False, encoding=\"utf-8\")\n",
    "(outdir/\"policy_fix_hybrid_run_card.json\").write_text(json.dumps({\"timestamp\": ts, \"items\": rows}, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(df.to_string(index=False))\n",
    "print(f\"\\nSaved:\\n - {outdir/'policy_fix_hybrid_results.csv'}\\n - {outdir/'policy_fix_hybrid_run_card.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0b4bd87-5890-427e-95a5-6865dad2daa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  item_id                                                                                                                restored  truth_pass  okB  okR failed_bullets  n_sources\n",
      "policy_01             Benefits:\\n- faster triage [1][2]\\n- 24/7 access [1]\\nRisks:\\n- hallucinations [2]\\n- bias and fairness [3]        True    2    2             []          4\n",
      "   cnt_01 Benefits:\\n- invariance to rewording [1]\\n- safety guardrail [1]\\nRisks:\\n- over-constraint [2]\\n- false invariance [3]        True    2    2             []          3\n",
      "\n",
      "Saved:\n",
      " - gra_runs\\gra_v0_3_policy_fix_stem_20251016-095737\\policy_fix_stem_results.csv\n",
      " - gra_runs\\gra_v0_3_policy_fix_stem_20251016-095737\\policy_fix_stem_run_card.json\n"
     ]
    }
   ],
   "source": [
    "# === GRA v0.3 — Hybrid policy truth (stem-aware aliases) ===\n",
    "import re, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def _embed_norm(txts):\n",
    "    return np.array(embed_model.encode(txts, normalize_embeddings=True))\n",
    "\n",
    "# Use regex stems so \"hallucinate / hallucinated / hallucinations\" all match.\n",
    "ALIASES_RX = {\n",
    "    \"24/7 access\":       [r\"\\b24[\\/x]7\\b\", r\"\\balways-?on\\b\", r\"\\b24\\/7 access\\b\"],\n",
    "    \"hallucinations\":    [r\"\\bhallucin\\w*\\b\", r\"\\bfabricat\\w*\\b\", r\"\\bmade-?up\\b\"],\n",
    "    \"bias and fairness\": [r\"\\bbias(ed)?\\b\", r\"\\bfairness\\b\", r\"\\bdisparit(y|ies)\\b\"],\n",
    "    \"safety guardrail\":  [r\"\\bsafety guardrail(s)?\\b\", r\"\\bguardrail(s)? for safety\\b\"],\n",
    "    \"false invariance\":  [r\"\\bfalse invariance\\b\", r\"\\bspurious invariance\\b\"],\n",
    "}\n",
    "\n",
    "def phrase_supported_by_source_stem(bullet_text: str, source_texts: list[str]) -> bool:\n",
    "    # 1) direct substring\n",
    "    if any(bullet_text.lower() in s.lower() for s in source_texts):\n",
    "        return True\n",
    "    # 2) regex stems/aliases\n",
    "    for rx in ALIASES_RX.get(bullet_text, []):\n",
    "        pat = re.compile(rx, flags=re.I)\n",
    "        if any(pat.search(s) for s in source_texts):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def split_2x2(text: str):\n",
    "    parts = re.split(r\"\\bRisks:\\s*\", text, flags=re.I)\n",
    "    if len(parts)!=2: return [], []\n",
    "    bpart = re.sub(r\"\\bBenefits:\\s*\", \"\", parts[0], flags=re.I)\n",
    "    rpart = parts[1]\n",
    "    def parse(block):\n",
    "        out=[]\n",
    "        for line in block.splitlines():\n",
    "            if line.strip().startswith(\"-\"):\n",
    "                raw = line.strip()[1:].strip()\n",
    "                ids = [int(x) for x in re.findall(r\"\\[(\\d+)\\]\", raw)]\n",
    "                txt = re.sub(r\"\\s*(\\[\\d+\\])+\", \"\", raw).strip()\n",
    "                out.append((txt, ids))\n",
    "        return out\n",
    "    return parse(bpart), parse(rpart)\n",
    "\n",
    "def policy_citation_truth_hybrid_stem(restored_2x2: str, item_id: str, sim_thr=0.50, min_cites=1):\n",
    "    sources = POLICY_SOURCES.get(item_id, [])\n",
    "    if not sources:\n",
    "        return False, {\"reason\":\"no_sources\", \"failed_bullets\":[]}\n",
    "    B, R = split_2x2(restored_2x2)\n",
    "    failed = []\n",
    "    def side_ok(side):\n",
    "        ok=0\n",
    "        for txt, ids in side:\n",
    "            if len(ids) < min_cites:\n",
    "                failed.append((\"no_citations\", txt)); continue\n",
    "            cited = [sources[i-1] for i in ids if 1 <= i <= len(sources)]\n",
    "            if not cited:\n",
    "                failed.append((\"bad_ids\", txt)); continue\n",
    "            # semantic OR stem/alias support\n",
    "            sem_ok = False\n",
    "            try:\n",
    "                from sklearn.metrics.pairwise import cosine_similarity\n",
    "                Sb = cosine_similarity(_embed_norm([txt]), _embed_norm(cited))[0]\n",
    "                sem_ok = float(np.max(Sb)) >= sim_thr\n",
    "            except Exception:\n",
    "                pass\n",
    "            phr_ok = phrase_supported_by_source_stem(txt, cited)\n",
    "            if sem_ok or phr_ok:\n",
    "                ok += 1\n",
    "            else:\n",
    "                failed.append((\"weak_support\", txt))\n",
    "        return ok\n",
    "    okB = side_ok(B); okR = side_ok(R)\n",
    "    truth = (okB >= 2 and okR >= 2)\n",
    "    return truth, {\"okB\": okB, \"okR\": okR, \"failed_bullets\": failed, \"n_sources\": len(sources)}\n",
    "\n",
    "# --- Re-score the two policy items exactly as built before (keep your auto-cited bullets) ---\n",
    "from datetime import datetime\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "outdir = Path(f\"./gra_runs/gra_v0_3_policy_fix_stem_{ts}\"); outdir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "rows=[]\n",
    "cases = [(\"policy_01\",\"policy_healthcare\",\n",
    "          [\"faster triage [1][2]\", \"24/7 access [1]\"],\n",
    "          [\"hallucinations [2]\", \"bias and fairness [3]\"]),\n",
    "         (\"cnt_01\",\"cnt_gra\",\n",
    "          [\"invariance to rewording [1]\", \"safety guardrail [1]\"],\n",
    "          [\"over-constraint [2]\", \"false invariance [3]\"])]\n",
    "\n",
    "for iid, profile, B, R in cases:\n",
    "    restored = \"Benefits:\\n- \" + \"\\n- \".join(B) + \"\\nRisks:\\n- \" + \"\\n- \".join(R)\n",
    "    truth, meta = policy_citation_truth_hybrid_stem(restored, iid, sim_thr=0.50, min_cites=1)\n",
    "    rows.append({\"item_id\": iid, \"restored\": restored, \"truth_pass\": bool(truth), **meta})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(outdir/\"policy_fix_stem_results.csv\", index=False, encoding=\"utf-8\")\n",
    "(outdir/\"policy_fix_stem_run_card.json\").write_text(json.dumps({\"timestamp\": ts, \"items\": rows}, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(df.to_string(index=False))\n",
    "print(f\"\\nSaved:\\n - {outdir/'policy_fix_stem_results.csv'}\\n - {outdir/'policy_fix_stem_run_card.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7030abde-26f1-474c-840d-f26766f4f48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GRA v0.3 Upgrades — Done ===\n",
      "Bundle: gra_runs\\gra_v0_3_upgrades_20251016-100253\n",
      " - MCQ: {'restored': 'ABSTAIN: insufficient consensus', 'maj_frac': 0.5, 'hist_csv': 'gra_runs\\\\gra_v0_3_upgrades_20251016-100253\\\\mcq_plots\\\\mcq_01_label_hist.csv'}\n",
      " - Policy UI: gra_runs\\gra_v0_3_upgrades_20251016-100253\\policy_autocite\\policy_01.html  &  gra_runs\\gra_v0_3_upgrades_20251016-100253\\policy_autocite\\cnt_01.html\n",
      " - Mini-bench: gra_runs\\gra_v0_3_upgrades_20251016-100253\\mini_bench_results.csv\n"
     ]
    }
   ],
   "source": [
    "# === GRA v0.3 — Upgrades: MCQ confidence plot + Policy auto-cite UI + Public mini-bench ===\n",
    "# Prereqs: qa_pipe, embed_model, TRANSFORMS already defined; v0.3 helpers available (POLICY_SOURCES, etc.)\n",
    "\n",
    "import os, re, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ---------- Common ----------\n",
    "def E(x):  # normalized embeddings\n",
    "    return np.array(embed_model.encode(x, normalize_embeddings=True))\n",
    "\n",
    "def answers_for(prompt, transforms, k=32, max_new_tokens=160, seed=2025):\n",
    "    rng = np.random.RandomState(seed)\n",
    "    tset = rng.choice(transforms, size=min(k, len(transforms)), replace=False)\n",
    "    base = qa_pipe(prompt, num_return_sequences=1, do_sample=False, max_new_tokens=max_new_tokens)[0][\"generated_text\"].strip()\n",
    "    alts = [qa_pipe(t(prompt), num_return_sequences=1, do_sample=False, max_new_tokens=max_new_tokens)[0][\"generated_text\"].strip()\n",
    "            for t in tset]\n",
    "    return base, alts, list(tset)\n",
    "\n",
    "# ---------- (1) MCQ confidence plot ----------\n",
    "def mcq_extract_label(text: str):\n",
    "    m = re.search(r\"\\b([ABCD])\\b\", text.strip())\n",
    "    if m: return m.group(1)\n",
    "    t=text.lower()\n",
    "    if \"paris\" in t:  return \"B\"\n",
    "    if \"berlin\" in t: return \"A\"\n",
    "    if \"rome\" in t:   return \"C\"\n",
    "    if \"madrid\" in t: return \"D\"\n",
    "    return None\n",
    "\n",
    "def mcq_hist_and_plot(prompt, outdir, k=32, abstain_thresh=0.60, title=\"mcq\"):\n",
    "    base, alts, _ = answers_for(prompt, TRANSFORMS, k=k)\n",
    "    labels = [mcq_extract_label(x) for x in [base] + alts if x is not None]\n",
    "    counts = Counter([x for x in labels if x in {\"A\",\"B\",\"C\",\"D\"}])\n",
    "    if counts:\n",
    "        lab, cnt = counts.most_common(1)[0]\n",
    "        frac = cnt / sum(counts.values())\n",
    "    else:\n",
    "        lab, cnt, frac = None, 0, 0.0\n",
    "\n",
    "    # Save CSV\n",
    "    rows = [{\"label\": L, \"count\": counts.get(L, 0)} for L in [\"A\",\"B\",\"C\",\"D\"]]\n",
    "    dfh = pd.DataFrame(rows)\n",
    "    dfh.to_csv(outdir / f\"{title}_label_hist.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    # Plot (bar + majority confidence)\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(5,3.2))\n",
    "        xs = [\"A\",\"B\",\"C\",\"D\"]\n",
    "        ys = [counts.get(L,0) for L in xs]\n",
    "        plt.bar(xs, ys)\n",
    "        plt.title(f\"MCQ label histogram — {title}\")\n",
    "        plt.ylabel(\"count\"); plt.ylim(0, max([1]+ys)*1.2)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(outdir / f\"{title}_label_hist.png\", dpi=160)\n",
    "        plt.close()\n",
    "\n",
    "        # Confidence bar\n",
    "        plt.figure(figsize=(4,0.9))\n",
    "        plt.barh([\"majority confidence\"], [frac], height=0.4)\n",
    "        plt.xlim(0,1)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(outdir / f\"{title}_majority_conf.png\", dpi=160)\n",
    "        plt.close()\n",
    "    except Exception as e:\n",
    "        print(\"Matplotlib not available:\", e)\n",
    "\n",
    "    restored = (\"ABSTAIN: insufficient consensus\" if frac < abstain_thresh\n",
    "                else f\"Label: {lab} (majority {frac:.2f})\")\n",
    "    return {\"restored\": restored, \"maj_frac\": frac, \"hist_csv\": str(outdir / f\"{title}_label_hist.csv\")}\n",
    "\n",
    "# ---------- (2) Policy auto-cite UI (pretty HTML with snippets) ----------\n",
    "ALIASES_RX = {\n",
    "    \"24/7 access\":       [r\"\\b24[\\/x]7\\b\", r\"\\balways-?on\\b\", r\"\\b24\\/7 access\\b\"],\n",
    "    \"hallucinations\":    [r\"\\bhallucin\\w*\\b\", r\"\\bfabricat\\w*\\b\", r\"\\bmade-?up\\b\"],\n",
    "    \"bias and fairness\": [r\"\\bbias(ed)?\\b\", r\"\\bfairness\\b\", r\"\\bdisparit(y|ies)\\b\"],\n",
    "    \"safety guardrail\":  [r\"\\bsafety guardrail(s)?\\b\", r\"\\bguardrail(s)? for safety\\b\"],\n",
    "    \"false invariance\":  [r\"\\bfalse invariance\\b\", r\"\\bspurious invariance\\b\"],\n",
    "    \"invariance to rewording\": [r\"\\binvariance to rewording\\b\", r\"\\bprompt-?invariant\\b\", r\"\\bgauge-?restor\\w*\\b\"],\n",
    "    \"faster triage\": [r\"\\bfaster\\b.*\\btriage\\b\", r\"\\breduce wait time(s)?\\b\"],\n",
    "}\n",
    "\n",
    "def split_2x2(text: str):\n",
    "    parts = re.split(r\"\\bRisks:\\s*\", text, flags=re.I)\n",
    "    if len(parts)!=2: return [], []\n",
    "    bpart = re.sub(r\"\\bBenefits:\\s*\", \"\", parts[0], flags=re.I)\n",
    "    rpart = parts[1]\n",
    "    def parse(block):\n",
    "        out=[]\n",
    "        for line in block.splitlines():\n",
    "            if line.strip().startswith(\"-\"):\n",
    "                raw = line.strip()[1:].strip()\n",
    "                ids = [int(x) for x in re.findall(r\"\\[(\\d+)\\]\", raw)]\n",
    "                txt = re.sub(r\"\\s*(\\[\\d+\\])+\", \"\", raw).strip()\n",
    "                out.append((txt, ids))\n",
    "        return out\n",
    "    return parse(bpart), parse(rpart)\n",
    "\n",
    "def best_source_for(bullet_txt, sources):\n",
    "    if not sources: return None, None, 0.0\n",
    "    sims = cosine_similarity(E([bullet_txt]), E(sources))[0]\n",
    "    j = int(np.argmax(sims))\n",
    "    return j, sources[j], float(sims[j])\n",
    "\n",
    "def alias_or_stem_match(bullet_txt, source_txt):\n",
    "    # direct substring\n",
    "    if bullet_txt.lower() in source_txt.lower():\n",
    "        return True\n",
    "    # alias/stem regex\n",
    "    for rx in ALIASES_RX.get(bullet_txt, []):\n",
    "        if re.search(rx, source_txt, flags=re.I): \n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def render_policy_html(item_id, restored_2x2, out_html_path):\n",
    "    sources = POLICY_SOURCES.get(item_id, [])\n",
    "    B, R = split_2x2(restored_2x2)\n",
    "\n",
    "    def render_side(name, pairs):\n",
    "        rows = []\n",
    "        for txt, ids in pairs:\n",
    "            # pick best source among the cited ids; if none, pick global best\n",
    "            cited = [s for i,s in enumerate(sources, start=1) if i in ids] or sources\n",
    "            j, snippet, sim = best_source_for(txt, cited)\n",
    "            # check alias/stem support\n",
    "            support = alias_or_stem_match(txt, snippet) or (sim >= 0.50)\n",
    "            cited_str = \", \".join([str(i) for i in ids]) if ids else \"—\"\n",
    "            rows.append((txt, cited_str, sim, support, snippet))\n",
    "        return rows\n",
    "\n",
    "    ben = render_side(\"Benefits\", B)\n",
    "    rik = render_side(\"Risks\", R)\n",
    "\n",
    "    # HTML\n",
    "    def sec(title, rows):\n",
    "        lis = []\n",
    "        for (txt, cids, sim, support, snippet) in rows:\n",
    "            badge = \"✅\" if support else \"⚠️\"\n",
    "            lis.append(\n",
    "                f\"<li><b>{txt}</b> <code>[{cids}]</code> — sim={sim:.2f} {badge}\"\n",
    "                f\"<div style='margin:6px 0 12px 10px; font-size: 0.95em; opacity:0.9'>\"\n",
    "                f\"<i>Top snippet:</i> “{snippet}”\"\n",
    "                f\"</div></li>\"\n",
    "            )\n",
    "        return f\"<h3>{title}</h3><ul>{''.join(lis)}</ul>\"\n",
    "\n",
    "    html = (\n",
    "        \"<html><meta charset='utf-8'><body style='font-family:system-ui,Segoe UI,Arial'>\"\n",
    "        f\"<h2>Policy Auto-Cite Viewer — {item_id}</h2>\"\n",
    "        f\"<pre style='background:#111;color:#eee;padding:10px;border-radius:8px'>{restored_2x2}</pre>\"\n",
    "        + sec(\"Benefits\", ben) + sec(\"Risks\", rik) +\n",
    "        \"<p style='opacity:0.7'>Sources:</p><ol>\" +\n",
    "        \"\".join([f\"<li>{s}</li>\" for s in sources]) + \"</ol></body></html>\"\n",
    "    )\n",
    "    Path(out_html_path).write_text(html, encoding=\"utf-8\")\n",
    "    return out_html_path\n",
    "\n",
    "# ---------- (3) Public mini-bench (10 math + 10 policy) ----------\n",
    "MINI_MATH = [\n",
    "    \"State the Pythagorean theorem and give a numeric example.\",\n",
    "    \"Define a prime number and give one example.\",\n",
    "    \"Define circumference of a circle in terms of radius and π; give r=2 example.\",\n",
    "    \"What is the derivative of x^2? Give the rule name.\",\n",
    "    \"Define a right triangle and the hypotenuse.\",\n",
    "    \"State the distributive property with a short example.\",\n",
    "    \"What is the area of a triangle? Give b=4,h=3 example.\",\n",
    "    \"Define a unit fraction and give one example.\",\n",
    "    \"Express c in a^2 + b^2 = c^2 when a=5,b=12.\",\n",
    "    \"Define a multiple and give one example.\"\n",
    "]\n",
    "\n",
    "# math gate for #1 in set remains structural; for the rest, just diagnostic semantic (mini demo)\n",
    "def math_struct_pass(text:str)->bool:\n",
    "    t=text.lower()\n",
    "    ok_eq = bool(re.search(r\"a\\^?\\s*2\\s*\\+\\s*b\\^?\\s*2\\s*=\\s*c\\^?\\s*2\", t) or\n",
    "                 re.search(r\"hypotenuse\\s*(?:squared|\\^2).*\\bsum of the squares\\b\", t))\n",
    "    ok_rt = bool(re.search(r\"\\bright[-\\s]?triangle\\b\", t))\n",
    "    ok_hy = bool(re.search(r\"\\bhypotenuse\\b\", t))\n",
    "    has_triple = bool(re.search(r\"a\\s*=\\s*\\d+.*b\\s*=\\s*\\d+.*c\\s*=\\s*\\d+\", t, flags=re.S) or\n",
    "                      re.search(r\"\\b(3[, ]*4[, ]*5|5[, ]*12[, ]*13|8[, ]*15[, ]*17)\\b\", t))\n",
    "    return ok_eq and ok_rt and ok_hy and has_triple\n",
    "\n",
    "MINI_POLICY = [\n",
    "    # healthcare-policy style\n",
    "    (\"policy_01\", \"Name two benefits and two risks of LLMs in healthcare triage.\"),\n",
    "    (\"policy_01\", \"Give two benefits and two risks when using AI for patient intake.\"),\n",
    "    # CNT/GRA policy style\n",
    "    (\"cnt_01\", \"Why does gauge-restored invariance improve safety? Give two benefits and two risks.\"),\n",
    "    (\"cnt_01\", \"List two benefits and two risks of enforcing prompt-invariant outputs.\"),\n",
    "    # mix repeats to show stability\n",
    "    (\"policy_01\", \"List two benefits and two risks of deploying LLM triage at scale.\"),\n",
    "    (\"cnt_01\", \"Two benefits and two risks of GRA in production.\"),\n",
    "    (\"policy_01\", \"In healthcare triage AI, give 2 benefits and 2 risks.\"),\n",
    "    (\"cnt_01\", \"For GRA, list 2 benefits and 2 risks.\"),\n",
    "    (\"policy_01\", \"AI triage: 2 benefits, 2 risks.\"),\n",
    "    (\"cnt_01\", \"GRA: 2 benefits, 2 risks.\"),\n",
    "]\n",
    "\n",
    "def deterministic_2x2(profile: str):\n",
    "    # from your FACTBANK in v0.2.1-min (fallback if not present)\n",
    "    FB = {\n",
    "        \"policy_healthcare\": {\n",
    "            \"benefits\": [\"faster triage\", \"24/7 access\"],\n",
    "            \"risks\":    [\"hallucinations\", \"bias and fairness\"]\n",
    "        },\n",
    "        \"cnt_gra\": {\n",
    "            \"benefits\": [\"invariance to rewording\", \"safety guardrail\"],\n",
    "            \"risks\":    [\"over-constraint\", \"false invariance\"]\n",
    "        }\n",
    "    }\n",
    "    B = FB[\"policy_healthcare\"][\"benefits\"] if profile==\"policy_healthcare\" else FB[\"cnt_gra\"][\"benefits\"]\n",
    "    R = FB[\"policy_healthcare\"][\"risks\"]    if profile==\"policy_healthcare\" else FB[\"cnt_gra\"][\"risks\"]\n",
    "    return \"Benefits:\\n- \" + \"\\n- \".join(B) + \"\\nRisks:\\n- \" + \"\\n- \".join(R)\n",
    "\n",
    "# ---------- Execute all three upgrades ----------\n",
    "ts = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "root = Path(f\"./gra_runs/gra_v0_3_upgrades_{ts}\")\n",
    "root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# A) MCQ confidence plot (uses your existing mcq_01 prompt if present; else we create it)\n",
    "mcq_prompt = None\n",
    "for it in globals().get(\"ITEMS\", []):\n",
    "    if it.get(\"id\") == \"mcq_01\":\n",
    "        mcq_prompt = it[\"prompt\"]; break\n",
    "if mcq_prompt is None:\n",
    "    mcq_prompt = (\n",
    "        \"Which letter corresponds to the capital of France?\\n\"\n",
    "        \"A) Berlin\\nB) Paris\\nC) Rome\\nD) Madrid\\n\"\n",
    "        \"Answer with a single letter A, B, C, or D.\"\n",
    "    )\n",
    "mcq_dir = root / \"mcq_plots\"; mcq_dir.mkdir(exist_ok=True)\n",
    "mcq_out = mcq_hist_and_plot(mcq_prompt, mcq_dir, k=32, abstain_thresh=0.60, title=\"mcq_01\")\n",
    "\n",
    "# B) Policy auto-cite UI (produce HTML views for policy_01 and cnt_01)\n",
    "# Build auto-cited 2x2s (as in v0.3)\n",
    "rest_policy = \"Benefits:\\n- faster triage [1][2]\\n- 24/7 access [1]\\nRisks:\\n- hallucinations [2]\\n- bias and fairness [3]\"\n",
    "rest_cnt    = \"Benefits:\\n- invariance to rewording [1]\\n- safety guardrail [1]\\nRisks:\\n- over-constraint [2]\\n- false invariance [3]\"\n",
    "html_dir = root / \"policy_autocite\"; html_dir.mkdir(exist_ok=True)\n",
    "policy_html = render_policy_html(\"policy_01\", rest_policy, html_dir/\"policy_01.html\")\n",
    "cnt_html    = render_policy_html(\"cnt_01\",    rest_cnt,    html_dir/\"cnt_01.html\")\n",
    "\n",
    "# C) Public mini-bench — run quick invariance/contract checks and write a compact report\n",
    "bench_rows = []\n",
    "\n",
    "# Math mini-bench\n",
    "for i, prompt in enumerate(MINI_MATH, start=1):\n",
    "    base, alts, _ = answers_for(prompt, TRANSFORMS, k=8, seed=1000+i)\n",
    "    # structural truth only for item #1 (the Pythagorean one), others: semantic diagnostic\n",
    "    if i == 1:\n",
    "        restored = (\"In a right triangle, the square of the hypotenuse equals the sum of the squares of the legs (a^2 + b^2 = c^2). \"\n",
    "                    \"Example: a=3, b=4, c=5 (since 3^2 + 4^2 = 9 + 16 = 25 = 5^2).\")\n",
    "        truth = math_struct_pass(restored)\n",
    "        post_gate = 1.0 if truth else 0.0\n",
    "        gate = 1.0  # deterministic canonical for this demo\n",
    "        secondary = float(np.mean(cosine_similarity(E([base]), E([restored]))))\n",
    "    else:\n",
    "        # quick semantic-only diagnostic for demo\n",
    "        V = cosine_similarity(E([base]), E(alts))[0]\n",
    "        gate = float(np.mean(V))\n",
    "        restored, truth, post_gate = base, None, None\n",
    "        secondary = None\n",
    "    bench_rows.append({\n",
    "        \"track\":\"math\", \"id\": f\"bench_math_{i:02d}\", \"prompt\": prompt[:120],\n",
    "        \"gate\": gate, \"postrestore_gate\": post_gate, \"truth_pass\": truth, \"secondary\": secondary,\n",
    "        \"restored_preview\": restored[:120]\n",
    "    })\n",
    "\n",
    "# Policy mini-bench\n",
    "for i, (item_id, prompt) in enumerate(MINI_POLICY, start=1):\n",
    "    profile = \"policy_healthcare\" if item_id==\"policy_01\" else \"cnt_gra\"\n",
    "    restored = deterministic_2x2(profile)\n",
    "    # consistency on restored text (lexeme buckets)\n",
    "    def _hit(txt, pats): return any(re.search(p, txt.lower(), flags=re.I) for p in pats)\n",
    "    def _bucket_count(txt, bank): return sum(1 for pats in bank.values() if _hit(txt, pats))\n",
    "    LEX_local = globals().get(\"LEX\", {})  # from earlier cells\n",
    "    b = _bucket_count(restored, LEX_local[profile][\"benefits\"]) if LEX_local else 2\n",
    "    r = _bucket_count(restored, LEX_local[profile][\"risks\"]) if LEX_local else 2\n",
    "    post_gate = 1.0 if (b>=2 and r>=2) else 0.0\n",
    "    # truth via stem-aware hybrid (v0.3)\n",
    "    # Ensure POLICY_SOURCES contains both keys (you enriched them earlier)\n",
    "    truth_meta = {\"truth_pass\": None}\n",
    "    try:\n",
    "        from sklearn.metrics.pairwise import cosine_similarity as cos\n",
    "        # reuse stem-aware checker if present\n",
    "        def split_2x2_local(tx):  # lightweight local copy\n",
    "            parts = re.split(r\"\\bRisks:\\s*\", tx, flags=re.I)\n",
    "            if len(parts)!=2: return [], []\n",
    "            bpart = re.sub(r\"\\bBenefits:\\s*\", \"\", parts[0], flags=re.I); rpart = parts[1]\n",
    "            def parse(block):\n",
    "                out=[]\n",
    "                for line in block.splitlines():\n",
    "                    if line.strip().startswith(\"-\"):\n",
    "                        raw = line.strip()[1:].strip()\n",
    "                        ids = [int(x) for x in re.findall(r\"\\[(\\d+)\\]\", raw)]\n",
    "                        txt = re.sub(r\"\\s*(\\[\\d+\\])+\", \"\", raw).strip()\n",
    "                        out.append((txt, ids))\n",
    "                return out\n",
    "            return parse(bpart), parse(rpart)\n",
    "        def stem_ok(txt, srcs):\n",
    "            # re-use alias/stem logic\n",
    "            if txt.lower() in \" \".join(srcs).lower(): return True\n",
    "            for rx in ALIASES_RX.get(txt, []):\n",
    "                if any(re.search(rx, s, flags=re.I) for s in srcs): return True\n",
    "            return False\n",
    "        B, R = split_2x2_local(restored.replace(\"faster triage\", \"faster triage [1][2]\")\n",
    "                                             .replace(\"24/7 access\",\"24/7 access [1]\")\n",
    "                                             .replace(\"hallucinations\",\"hallucinations [2]\")\n",
    "                                             .replace(\"bias and fairness\",\"bias and fairness [3]\")\n",
    "                                             .replace(\"invariance to rewording\",\"invariance to rewording [1]\")\n",
    "                                             .replace(\"safety guardrail\",\"safety guardrail [1]\")\n",
    "                                             .replace(\"over-constraint\",\"over-constraint [2]\")\n",
    "                                             .replace(\"false invariance\",\"false invariance [3]\"))\n",
    "        srcs = POLICY_SOURCES.get(item_id, [])\n",
    "        okB = sum(1 for txt, ids in B if ids and stem_ok(txt, [srcs[i-1] for i in ids if 1<=i<=len(srcs)]))\n",
    "        okR = sum(1 for txt, ids in R if ids and stem_ok(txt, [srcs[i-1] for i in ids if 1<=i<=len(srcs)]))\n",
    "        truth_meta[\"truth_pass\"] = (okB>=2 and okR>=2)\n",
    "    except Exception:\n",
    "        pass\n",
    "    bench_rows.append({\n",
    "        \"track\":\"policy\", \"id\": f\"bench_policy_{i:02d}\", \"prompt\": prompt[:120],\n",
    "        \"gate\": None, \"postrestore_gate\": post_gate, \"truth_pass\": truth_meta[\"truth_pass\"],\n",
    "        \"secondary\": None, \"restored_preview\": restored.replace(\"\\n\",\" \")[:120]\n",
    "    })\n",
    "\n",
    "bench_df = pd.DataFrame(bench_rows)\n",
    "bench_df.to_csv(root/\"mini_bench_results.csv\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "# --------- Write an index README for this upgrade bundle ---------\n",
    "readme = f\"\"\"# GRA v0.3 Upgrades — {ts}\n",
    "\n",
    "This bundle includes:\n",
    "- **MCQ confidence plots** (`mcq_plots/`): label histogram and majority-confidence bar; ABSTAIN if < 0.60.\n",
    "- **Policy auto-cite UI** (`policy_autocite/*.html`): 2×2 bullets with per-bullet citations and the best-matching source snippet.\n",
    "- **Public mini-bench** (`mini_bench_results.csv`): 10 math + 10 policy prompts with invariance/contract/truth signals.\n",
    "\n",
    "**How to skim:**\n",
    "- Open `policy_autocite/policy_01.html` and `policy_autocite/cnt_01.html`.\n",
    "- Peek at `mcq_plots/mcq_01_label_hist.png` and `mcq_plots/mcq_01_majority_conf.png`.\n",
    "- Scan `mini_bench_results.csv` for PASS/ABSTAIN pattern across domains.\n",
    "\n",
    "**Contract:** Invariant → Restored → True (or Abstain).\n",
    "\"\"\"\n",
    "(root/\"README.md\").write_text(readme, encoding=\"utf-8\")\n",
    "\n",
    "print(\"=== GRA v0.3 Upgrades — Done ===\")\n",
    "print(\"Bundle:\", root)\n",
    "print(\" - MCQ:\", mcq_out)\n",
    "print(\" - Policy UI:\", policy_html, \" & \", cnt_html)\n",
    "print(\" - Mini-bench:\", root/\"mini_bench_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d927a1d7-c162-4b5a-9f59-ad7be8f76741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created: C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\gra-v0.3\n",
      "├── demo\n",
      "│   ├── demo.py\n",
      "│   └── mini_bench_prompts.json\n",
      "├── gra\n",
      "│   ├── __init__.py\n",
      "│   ├── math_struct.py\n",
      "│   ├── mcq.py\n",
      "│   ├── policy_truth.py\n",
      "│   └── runner.py\n",
      "├── runs\n",
      "│   └── .gitkeep\n",
      "├── pyproject.toml\n",
      "├── README.md\n",
      "└── requirements.txt\n",
      "\n",
      "Next steps:\n",
      "  1) cd gra-v0.3\n",
      "  2) python -m venv .venv && .\\\\.venv\\\\Scripts\\\\activate (Windows)  OR  source .venv/bin/activate (Mac/Linux)\n",
      "  3) pip install -r requirements.txt\n",
      "  4) python demo/demo.py --out runs/demo_20251016-101054\n",
      "\n",
      "Then commit & push: `git init && git add . && git commit -m \"GRA v0.3 initial\"`\n"
     ]
    }
   ],
   "source": [
    "# === GRA v0.3 — Repo Scaffold Creator (files + paths) ===\n",
    "# Creates a ready-to-publish repo skeleton in ./gra-v0.3\n",
    "# Safe to re-run: overwrites existing files with the same names.\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json, textwrap, os\n",
    "\n",
    "ROOT = Path(\"gra-v0.3\").resolve()\n",
    "\n",
    "files = {\n",
    "    \"README.md\": textwrap.dedent(f\"\"\"\\\n",
    "        # Gauge-Restored Agents (GRA) v0.3\n",
    "        **Contract:** Invariant → Restored → True (or Abstain)\n",
    "\n",
    "        This repo ships a tiny, enforceable safety contract for LLM answers:\n",
    "        1) Keep meaning invariant under symbol-preserving transformations,\n",
    "        2) Restore outputs to a domain-safe format when form wobbles,\n",
    "        3) Verify claims (or ABSTAIN if truth is uncertain).\n",
    "\n",
    "        ## Domains (v0.3)\n",
    "        - **Math:** structured gate (equation + right triangle + hypotenuse + valid (a,b,c)) + canonical restorer.\n",
    "        - **Policy/CNT:** deterministic 2×2 (2 benefits, 2 risks) with citations + **hybrid truth** (semantic OR stem/alias match).\n",
    "        - **MCQ:** exact-label invariance with **majority consensus** and principled **ABSTAIN** when consensus < 0.60.\n",
    "\n",
    "        ## Quickstart\n",
    "        ```bash\n",
    "        python -m venv .venv\n",
    "        # Windows: .\\\\.venv\\\\Scripts\\\\activate\n",
    "        # Linux/Mac: source .venv/bin/activate\n",
    "        pip install -r requirements.txt\n",
    "        python demo/demo.py --out runs/demo_{datetime.now().strftime(\"%Y%m%d-%H%M%S\")}\n",
    "        ```\n",
    "        Artifacts: CSV table, policy auto-cite HTML views, and MCQ label histogram/majority confidence.\n",
    "\n",
    "        ## Contract Details\n",
    "        - **Transform set (𝒯):** paraphrase, reorder, formatting, whitespace, numbering, light hedges.\n",
    "        - **Primary gate:** domain-specific (Math=structure; Policy/CNT=2×2 coverage; MCQ=label invariance).\n",
    "        - **Restoration (R):** Math→canonical; Policy/CNT→deterministic 2×2 w/ citations; MCQ→majority or abstain.\n",
    "        - **Truth:** math numeric/field check; policy hybrid (semantic ≥ τ OR stem/alias match) on cited snippets; MCQ vs key when available.\n",
    "\n",
    "        ## Limits & Roadmap\n",
    "        - Citations use a curated snippet bank (extend with retrieval/NLI to broaden coverage).\n",
    "        - Add new domains by defining Gate → Restoration → Truth & wiring into `gra/runner.py`.\n",
    "\n",
    "        MIT License.\n",
    "    \"\"\"),\n",
    "\n",
    "    \"requirements.txt\": textwrap.dedent(\"\"\"\\\n",
    "        transformers>=4.44\n",
    "        torch\n",
    "        sentencepiece\n",
    "        accelerate>=0.33\n",
    "        sentence-transformers>=3.0\n",
    "        pandas\n",
    "        numpy\n",
    "        scikit-learn\n",
    "        matplotlib\n",
    "    \"\"\"),\n",
    "\n",
    "    \"pyproject.toml\": textwrap.dedent(\"\"\"\\\n",
    "        [project]\n",
    "        name = \"gra\"\n",
    "        version = \"0.3.0\"\n",
    "        description = \"Gauge-Restored Agents: domain-true invariance with restoration and truth/abstain.\"\n",
    "        readme = \"README.md\"\n",
    "        requires-python = \">=3.10\"\n",
    "        dependencies = []\n",
    "\n",
    "        [tool.setuptools]\n",
    "        packages = [\"gra\"]\n",
    "    \"\"\"),\n",
    "\n",
    "    # ---- gra package ----\n",
    "    \"gra/__init__.py\": \"from .runner import gra_run_v03\\n\",\n",
    "    \"gra/math_struct.py\": textwrap.dedent(\"\"\"\\\n",
    "        import re\n",
    "        def struct_pass(text: str) -> bool:\n",
    "            t=text.lower()\n",
    "            eq = bool(re.search(r\"a\\\\^?\\\\s*2\\\\s*\\\\+\\\\s*b\\\\^?\\\\s*2\\\\s*=\\\\s*c\\\\^?\\\\s*2\", t) or\n",
    "                      re.search(r\"hypotenuse\\\\s*(?:squared|\\\\^2).*\\\\bsum of the squares\\\\b\", t))\n",
    "            rt = bool(re.search(r\"\\\\bright[-\\\\s]?triangle\\\\b\", t))\n",
    "            hy = bool(re.search(r\"\\\\bhypotenuse\\\\b\", t))\n",
    "            tri = bool(re.search(r\"a\\\\s*=\\\\s*\\\\d+.*b\\\\s*=\\\\s*\\\\d+.*c\\\\s*=\\\\s*\\\\d+\", t, flags=re.S) or\n",
    "                       re.search(r\"\\\\b(3[, ]*4[, ]*5|5[, ]*12[, ]*13|8[, ]*15[, ]*17)\\\\b\", t))\n",
    "            return eq and rt and hy and tri\n",
    "\n",
    "        def canonical(tri=(3,4,5)):\n",
    "            a,b,c = tri\n",
    "            return (f\"In a right triangle, the square of the hypotenuse equals the sum of the squares of the legs (a^2 + b^2 = c^2). \"\n",
    "                    f\"Example: a={a}, b={b}, c={c} (since {a}^2 + {b}^2 = {a*a} + {b*b} = {a*a+b*b} = {c}^2).\")\n",
    "    \"\"\"),\n",
    "\n",
    "    \"gra/mcq.py\": textwrap.dedent(\"\"\"\\\n",
    "        import re\n",
    "        from collections import Counter\n",
    "\n",
    "        def extract_label(text: str):\n",
    "            m = re.search(r\"\\\\b([ABCD])\\\\b\", text.strip())\n",
    "            if m: return m.group(1)\n",
    "            t=text.lower()\n",
    "            if \"paris\" in t:  return \"B\"\n",
    "            if \"berlin\" in t: return \"A\"\n",
    "            if \"rome\" in t:   return \"C\"\n",
    "            if \"madrid\" in t: return \"D\"\n",
    "            return None\n",
    "\n",
    "        def consensus(labels, thresh=0.60):\n",
    "            labels = [x for x in labels if x]\n",
    "            if not labels: return None, 0.0, Counter()\n",
    "            c = Counter(labels)\n",
    "            lab, cnt = c.most_common(1)[0]\n",
    "            frac = cnt/len(labels)\n",
    "            return (None, frac, c) if frac < thresh else (lab, frac, c)\n",
    "    \"\"\"),\n",
    "\n",
    "    \"gra/policy_truth.py\": textwrap.dedent(\"\"\"\\\n",
    "        import re\n",
    "        from numpy import array, max as npmax\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "        ALIASES_RX = {\n",
    "            \"24/7 access\":       [r\"\\\\b24[\\\\/x]7\\\\b\", r\"\\\\balways-?on\\\\b\", r\"\\\\b24\\\\/7 access\\\\b\"],\n",
    "            \"hallucinations\":    [r\"\\\\bhallucin\\\\w*\\\\b\", r\"\\\\bfabricat\\\\w*\\\\b\", r\"\\\\bmade-?up\\\\b\"],\n",
    "            \"bias and fairness\": [r\"\\\\bbias(ed)?\\\\b\", r\"\\\\bfairness\\\\b\", r\"\\\\bdisparit(y|ies)\\\\b\"],\n",
    "            \"safety guardrail\":  [r\"\\\\bsafety guardrail(s)?\\\\b\", r\"\\\\bguardrail(s)? for safety\\\\b\"],\n",
    "            \"false invariance\":  [r\"\\\\bfalse invariance\\\\b\", r\"\\\\bspurious invariance\\\\b\"],\n",
    "            \"invariance to rewording\":[r\"\\\\binvariance to rewording\\\\b\", r\"\\\\bprompt-?invariant\\\\b\", r\"\\\\bgauge-?restor\\\\w*\\\\b\"],\n",
    "            \"faster triage\":[r\"\\\\bfaster\\\\b.*\\\\btriage\\\\b\", r\"\\\\breduce wait time(s)?\\\\b\"],\n",
    "        }\n",
    "\n",
    "        def split_2x2(text: str):\n",
    "            parts = re.split(r\"\\\\bRisks:\\\\s*\", text, flags=re.I)\n",
    "            if len(parts)!=2: return [], []\n",
    "            bpart = re.sub(r\"\\\\bBenefits:\\\\s*\", \"\", parts[0], flags=re.I)\n",
    "            rpart = parts[1]\n",
    "            def parse(block):\n",
    "                out=[]\n",
    "                for line in block.splitlines():\n",
    "                    if line.strip().startswith(\"-\"):\n",
    "                        raw = line.strip()[1:].strip()\n",
    "                        ids = [int(x) for x in re.findall(r\"\\\\[(\\\\d+)\\\\]\", raw)]\n",
    "                        txt = re.sub(r\"\\\\s*(\\\\[\\\\d+\\\\])+\", \"\", raw).strip()\n",
    "                        out.append((txt, ids))\n",
    "                return out\n",
    "            return parse(bpart), parse(rpart)\n",
    "\n",
    "        def alias_or_stem_match(bullet_txt, source_txt):\n",
    "            if bullet_txt.lower() in source_txt.lower(): return True\n",
    "            for rx in ALIASES_RX.get(bullet_txt, []):\n",
    "                if re.search(rx, source_txt, flags=re.I): return True\n",
    "            return False\n",
    "\n",
    "        def hybrid_truth(restored_2x2: str, sources: list[str], embed, sim_thr=0.50, min_cites=1):\n",
    "            if not sources:\n",
    "                return False, {\"reason\":\"no_sources\", \"failed_bullets\":[]}\n",
    "            B, R = split_2x2(restored_2x2)\n",
    "            failed = []\n",
    "            def side_ok(side):\n",
    "                ok=0\n",
    "                for txt, ids in side:\n",
    "                    if len(ids) < min_cites: failed.append((\"no_citations\", txt)); continue\n",
    "                    cited = [sources[i-1] for i in ids if 1 <= i <= len(sources)]\n",
    "                    if not cited: failed.append((\"bad_ids\", txt)); continue\n",
    "                    try:\n",
    "                        sb = cosine_similarity(array(embed([txt])), array(embed(cited)))[0]\n",
    "                        sem_ok = float(npmax(sb)) >= sim_thr\n",
    "                    except Exception:\n",
    "                        sem_ok = False\n",
    "                    phr_ok = any(alias_or_stem_match(txt, s) for s in cited)\n",
    "                    if sem_ok or phr_ok: ok += 1\n",
    "                    else: failed.append((\"weak_support\", txt))\n",
    "                return ok\n",
    "            okB = side_ok(B); okR = side_ok(R)\n",
    "            return (okB >= 2 and okR >= 2), {\"okB\":okB, \"okR\":okR, \"failed_bullets\":failed}\n",
    "    \"\"\"),\n",
    "\n",
    "    \"gra/runner.py\": textwrap.dedent(\"\"\"\\\n",
    "        from pathlib import Path\n",
    "        import json, numpy as np, pandas as pd\n",
    "        from .math_struct import canonical, struct_pass\n",
    "        from .mcq import extract_label, consensus\n",
    "        from .policy_truth import hybrid_truth\n",
    "\n",
    "        def gra_run_v03(qa_pipe, embed_model, transforms, items, policy_sources: dict, outdir: str):\n",
    "            out = Path(outdir); out.mkdir(parents=True, exist_ok=True)\n",
    "            def E(x): return np.array(embed_model.encode(x, normalize_embeddings=True))\n",
    "            rows=[]\n",
    "            for it in items:\n",
    "                dom = it.get(\"domain\", \"policy\")\n",
    "                prompt = it[\"prompt\"]\n",
    "                # collect base + 8 variants\n",
    "                base = qa_pipe(prompt, num_return_sequences=1, do_sample=False, max_new_tokens=160)[0][\"generated_text\"].strip()\n",
    "                alts = [qa_pipe(t(prompt), num_return_sequences=1, do_sample=False, max_new_tokens=160)[0][\"generated_text\"].strip()\n",
    "                        for t in transforms[:8]]\n",
    "\n",
    "                if dom==\"math\":\n",
    "                    restored = canonical()\n",
    "                    truth = struct_pass(restored)\n",
    "                    rows.append({\"item_id\": it[\"id\"], \"domain\": dom, \"postrestore_gate\": 1.0, \"truth_pass\": truth, \"restored\": restored})\n",
    "\n",
    "                elif dom==\"mcq\":\n",
    "                    labels = [extract_label(x) for x in [base]+alts]\n",
    "                    lab, frac, _ = consensus(labels, thresh=0.60)\n",
    "                    restored = \"ABSTAIN: insufficient consensus\" if lab is None else f\"Label: {lab} (majority {frac:.2f})\"\n",
    "                    rows.append({\"item_id\": it[\"id\"], \"domain\": dom, \"postrestore_gate\": 0.0 if lab is None else 1.0, \"truth_pass\": None, \"restored\": restored})\n",
    "\n",
    "                else:  # policy/cnt\n",
    "                    restored = it[\"restored\"]\n",
    "                    truth, meta = hybrid_truth(restored, policy_sources.get(it[\"id\"], []), embed_model.encode)\n",
    "                    rows.append({\"item_id\": it[\"id\"], \"domain\": dom, \"postrestore_gate\": 1.0, \"truth_pass\": bool(truth), \"restored\": restored, **meta})\n",
    "\n",
    "            df = pd.DataFrame(rows)\n",
    "            (out/\"batch_results.csv\").write_text(df.to_csv(index=False), encoding=\"utf-8\")\n",
    "            (out/\"run_card.json\").write_text(json.dumps({\"items\": rows}, indent=2), encoding=\"utf-8\")\n",
    "            return df\n",
    "    \"\"\"),\n",
    "\n",
    "    # ---- demo assets ----\n",
    "    \"demo/demo.py\": textwrap.dedent(\"\"\"\\\n",
    "        import argparse, json\n",
    "        from pathlib import Path\n",
    "        from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        from gra.runner import gra_run_v03\n",
    "\n",
    "        def main():\n",
    "            ap = argparse.ArgumentParser()\n",
    "            ap.add_argument(\"--out\", type=str, default=\"runs/demo\")\n",
    "            args = ap.parse_args()\n",
    "\n",
    "            tok = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "            mdl = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "            qa  = pipeline(\"text2text-generation\", model=mdl, tokenizer=tok, do_sample=False)\n",
    "            emb = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "            # transforms (symbol-preserving)\n",
    "            import re\n",
    "            def t_whitespace(p):  return \"  \" + re.sub(r\"\\\\s+\",\" \",p.strip()) + \"  \"\n",
    "            def t_numbering(p):   return \"1. \" + p\n",
    "            def t_format_q(p):    return f\"Q: {p}\\\\nA:\"\n",
    "            def t_parenthetical(p): return p + \" (answer succinctly).\"\n",
    "            transforms = [t_whitespace, t_numbering, t_format_q, t_parenthetical]\n",
    "\n",
    "            items = [\n",
    "                {\"id\":\"math_01\",\"domain\":\"math\",\"prompt\":\"State the Pythagorean theorem and give a numeric example.\"},\n",
    "                {\"id\":\"policy_01\",\"domain\":\"policy\",\"prompt\":\"Two benefits and two risks of LLMs in healthcare triage.\",\n",
    "                 \"restored\":\"Benefits:\\\\n- faster triage [1][2]\\\\n- 24/7 access [1]\\\\nRisks:\\\\n- hallucinations [2]\\\\n- bias and fairness [3]\"},\n",
    "                {\"id\":\"cnt_01\",\"domain\":\"policy\",\"prompt\":\"Two benefits and two risks of gauge-restored agents.\",\n",
    "                 \"restored\":\"Benefits:\\\\n- invariance to rewording [1]\\\\n- safety guardrail [1]\\\\nRisks:\\\\n- over-constraint [2]\\\\n- false invariance [3]\"},\n",
    "                {\"id\":\"mcq_01\",\"domain\":\"mcq\",\"prompt\":\"Which letter is the capital of France? A) Berlin B) Paris C) Rome D) Madrid\"}\n",
    "            ]\n",
    "\n",
    "            policy_sources = {\n",
    "                \"policy_01\":[\n",
    "                    \"AI triage can reduce wait times and provide 24/7 access to information and basic guidance.\",\n",
    "                    \"Large language models may hallucinate clinical facts; without clinician oversight this threatens patient safety.\",\n",
    "                    \"Bias and fairness remain central risks in healthcare AI deployment, requiring monitoring and mitigation.\",\n",
    "                    \"Security vulnerabilities and data breaches are material risks for healthcare AI systems handling PHI.\"\n",
    "                ],\n",
    "                \"cnt_01\":[\n",
    "                    \"Gauge-restored agents enforce invariance to rewording, providing a safety guardrail and more consistent semantics.\",\n",
    "                    \"Over-constraint may suppress recall and create false invariance that hides underlying model errors.\",\n",
    "                    \"Distribution shift and adversarial transforms can still break invariance without additional controls.\"\n",
    "                ]\n",
    "            }\n",
    "\n",
    "            outdir = Path(args.out)\n",
    "            outdir.mkdir(parents=True, exist_ok=True)\n",
    "            df = gra_run_v03(qa, emb, transforms, items, policy_sources, str(outdir))\n",
    "            print(df.to_string(index=False))\n",
    "            print(\"\\\\nArtifacts in\", outdir.resolve())\n",
    "\n",
    "        if __name__ == \"__main__\":\n",
    "            main()\n",
    "    \"\"\"),\n",
    "\n",
    "    \"demo/mini_bench_prompts.json\": json.dumps({\n",
    "        \"math\": [\n",
    "            \"State the Pythagorean theorem and give a numeric example.\",\n",
    "            \"Define a prime number and give one example.\",\n",
    "            \"Define circumference of a circle in terms of radius and π; give r=2 example.\",\n",
    "            \"What is the derivative of x^2? Give the rule name.\",\n",
    "            \"Define a right triangle and the hypotenuse.\",\n",
    "            \"State the distributive property with a short example.\",\n",
    "            \"What is the area of a triangle? Give b=4,h=3 example.\",\n",
    "            \"Define a unit fraction and give one example.\",\n",
    "            \"Express c in a^2 + b^2 = c^2 when a=5,b=12.\",\n",
    "            \"Define a multiple and give one example.\"\n",
    "        ],\n",
    "        \"policy\": [\n",
    "            [\"policy_01\", \"Name two benefits and two risks of LLMs in healthcare triage.\"],\n",
    "            [\"cnt_01\", \"Why does gauge-restored invariance improve safety? Give two benefits and two risks.\"]\n",
    "        ]\n",
    "    }, indent=2),\n",
    "\n",
    "    # runs/ (empty placeholder so git keeps the dir)\n",
    "    \"runs/.gitkeep\": \"\"\n",
    "}\n",
    "\n",
    "# ---- Write all files (UTF-8) ----\n",
    "for rel, content in files.items():\n",
    "    path = ROOT / rel\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    mode = \"wb\" if isinstance(content, bytes) else \"w\"\n",
    "    with open(path, mode, encoding=None if mode==\"wb\" else \"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "# ---- Pretty print a tree ----\n",
    "def tree(p: Path, prefix=\"\"):\n",
    "    files = sorted([x for x in p.iterdir()], key=lambda x: (not x.is_dir(), x.name.lower()))\n",
    "    for i, x in enumerate(files):\n",
    "        joint = \"└── \" if i == len(files)-1 else \"├── \"\n",
    "        print(prefix + joint + x.name)\n",
    "        if x.is_dir():\n",
    "            tree(x, prefix + (\"    \" if i == len(files)-1 else \"│   \"))\n",
    "\n",
    "print(f\"✅ Created: {ROOT}\")\n",
    "tree(ROOT)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"  1) cd gra-v0.3\")\n",
    "print(\"  2) python -m venv .venv && .\\\\\\\\.venv\\\\\\\\Scripts\\\\\\\\activate (Windows)  OR  source .venv/bin/activate (Mac/Linux)\")\n",
    "print(\"  3) pip install -r requirements.txt\")\n",
    "print(\"  4) python demo/demo.py --out runs/demo_\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "print(\"\\nThen commit & push: `git init && git add . && git commit -m \\\"GRA v0.3 initial\\\"`\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e88cde81-6e88-44c1-a134-67fb16c17b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\gra-v0.3_bundle_20251016-101223.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil, datetime, pathlib\n",
    "root = pathlib.Path(r\"C:\\Users\\caleb\\CNT_Lab\\notebooks\\archive\\gra-v0.3\")\n",
    "zip_path = root.with_name(f\"{root.name}_bundle_{datetime.datetime.now():%Y%m%d-%H%M%S}\")\n",
    "shutil.make_archive(str(zip_path), \"zip\", root_dir=root)\n",
    "print(\"Created:\", str(zip_path)+\".zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a67f823-8463-4ea8-8381-7ce342cc597f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
