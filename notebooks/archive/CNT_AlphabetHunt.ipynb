{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00d16b83-223e-46aa-a40f-577239665373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â· Installing scikit-learn â€¦\n"
     ]
    },
    {
     "ename": "UnsupportedOperation",
     "evalue": "fileno",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnsupportedOperation\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mÂ· Installing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m â€¦\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m             subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39m\u001b[33m-m\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minstall\u001b[39m\u001b[33m\"\u001b[39m, p], stdout=sys.stdout)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43m_ensure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnumpy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscipy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpandas\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmatplotlib\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtqdm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscikit-learn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmne\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjoblib\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mumap-learn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhmmlearn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrequests\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     26\u001b[39m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# 1) Imports (after install)\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36m_ensure\u001b[39m\u001b[34m(pkgs)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m importlib.util.find_spec(mod) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mÂ· Installing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m â€¦\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-m\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpip\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstall\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py:414\u001b[39m, in \u001b[36mcheck_call\u001b[39m\u001b[34m(*popenargs, **kwargs)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_call\u001b[39m(*popenargs, **kwargs):\n\u001b[32m    405\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Run command with arguments.  Wait for command to complete.  If\u001b[39;00m\n\u001b[32m    406\u001b[39m \u001b[33;03m    the exit code was zero then return, otherwise raise\u001b[39;00m\n\u001b[32m    407\u001b[39m \u001b[33;03m    CalledProcessError.  The CalledProcessError object will have the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    412\u001b[39m \u001b[33;03m    check_call([\"ls\", \"-l\"])\u001b[39;00m\n\u001b[32m    413\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m     retcode = \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    415\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m retcode:\n\u001b[32m    416\u001b[39m         cmd = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33margs\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py:395\u001b[39m, in \u001b[36mcall\u001b[39m\u001b[34m(timeout, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall\u001b[39m(*popenargs, timeout=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    388\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Run command with arguments.  Wait for command to complete or\u001b[39;00m\n\u001b[32m    389\u001b[39m \u001b[33;03m    for timeout seconds, then return the returncode attribute.\u001b[39;00m\n\u001b[32m    390\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    393\u001b[39m \u001b[33;03m    retcode = call([\"ls\", \"-l\"])\u001b[39;00m\n\u001b[32m    394\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[32m    396\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    397\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m p.wait(timeout=timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py:1005\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[39m\n\u001b[32m    986\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUser ID cannot be negative, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muid\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    988\u001b[39m \u001b[38;5;66;03m# Input and output objects. The general principle is like\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[38;5;66;03m# this:\u001b[39;00m\n\u001b[32m    990\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1000\u001b[39m \u001b[38;5;66;03m# are -1 when not using PIPEs. The child objects are -1\u001b[39;00m\n\u001b[32m   1001\u001b[39m \u001b[38;5;66;03m# when not redirecting.\u001b[39;00m\n\u001b[32m   1003\u001b[39m (p2cread, p2cwrite,\n\u001b[32m   1004\u001b[39m  c2pread, c2pwrite,\n\u001b[32m-> \u001b[39m\u001b[32m1005\u001b[39m  errread, errwrite) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstdin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[38;5;66;03m# From here on, raising exceptions may cause file descriptor leakage\u001b[39;00m\n\u001b[32m   1008\u001b[39m \n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# We wrap OS handles *before* launching the child, otherwise a\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# quickly terminating child could make our fds unwrappable\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# (see #8458).\u001b[39;00m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _mswindows:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py:1400\u001b[39m, in \u001b[36mPopen._get_handles\u001b[39m\u001b[34m(self, stdin, stdout, stderr)\u001b[39m\n\u001b[32m   1397\u001b[39m     c2pwrite = msvcrt.get_osfhandle(stdout)\n\u001b[32m   1398\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1399\u001b[39m     \u001b[38;5;66;03m# Assuming file-like object\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1400\u001b[39m     c2pwrite = msvcrt.get_osfhandle(\u001b[43mstdout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfileno\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1401\u001b[39m c2pwrite = \u001b[38;5;28mself\u001b[39m._make_inheritable(c2pwrite)\n\u001b[32m   1403\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stderr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\cnt_genome\\.venv\\Lib\\site-packages\\ipykernel\\iostream.py:371\u001b[39m, in \u001b[36mOutStream.fileno\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    369\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._original_stdstream_copy\n\u001b[32m    370\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mfileno\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m io.UnsupportedOperation(msg)\n",
      "\u001b[31mUnsupportedOperation\u001b[39m: fileno"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ðŸ§  Cognitive Alphabet Finder â€” One Mega Cell\n",
    "# Paste this cell in JupyterLab and run.\n",
    "# ============================================\n",
    "\n",
    "import os, sys, math, json, time, glob, gc, warnings, subprocess, importlib.util\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# -----------------------\n",
    "# 0) (Optional) Auto-Install Dependencies\n",
    "# -----------------------\n",
    "AUTO_INSTALL = True  # set False if you manage packages yourself\n",
    "\n",
    "def _ensure(pkgs):\n",
    "    if not AUTO_INSTALL:\n",
    "        return\n",
    "    for p in pkgs:\n",
    "        mod = p.split(\"==\")[0].split(\"[\")[0]\n",
    "        if importlib.util.find_spec(mod) is None:\n",
    "            print(f\"Â· Installing {p} â€¦\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", p], stdout=sys.stdout)\n",
    "\n",
    "_ensure([\n",
    "    \"numpy\", \"scipy\", \"pandas\", \"matplotlib\", \"tqdm\",\n",
    "    \"scikit-learn\", \"mne\", \"joblib\", \"umap-learn\", \"hmmlearn\", \"requests\"\n",
    "])\n",
    "\n",
    "# -----------------------\n",
    "# 1) Imports (after install)\n",
    "# -----------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from joblib import dump\n",
    "import requests\n",
    "import mne\n",
    "\n",
    "try:\n",
    "    import umap\n",
    "    HAS_UMAP = True\n",
    "except Exception:\n",
    "    HAS_UMAP = False\n",
    "\n",
    "try:\n",
    "    from hmmlearn.hmm import GaussianHMM\n",
    "    HAS_HMM = True\n",
    "except Exception:\n",
    "    HAS_HMM = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# -----------------------\n",
    "# 2) Configuration\n",
    "# -----------------------\n",
    "BANDS = {\n",
    "    \"delta\": (1.0, 4.0),\n",
    "    \"theta\": (4.0, 8.0),\n",
    "    \"alpha\": (8.0, 13.0),\n",
    "    \"beta\":  (13.0, 30.0),\n",
    "    \"gamma\": (30.0, 45.0),  # conservative gamma upper bound for typical EEG\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Data locations\n",
    "    data_dir: str = \"./brainwaves\"\n",
    "    output_dir: str = \"./cog_alphabet\"\n",
    "\n",
    "    # Either provide URLs here or just drop files into data_dir\n",
    "    dataset_urls: list = field(default_factory=lambda: [\n",
    "        # Examples (put your own links here):\n",
    "        # \"https://example.org/path/to/subject01.edf\",\n",
    "        # \"https://example.org/path/to/recording.fif\",\n",
    "        # \"https://example.org/path/to/session.vhdr\",\n",
    "        # \"https://example.org/path/to/export.csv\",\n",
    "    ])\n",
    "\n",
    "    # File types to consider\n",
    "    exts: tuple = (\".edf\", \".bdf\", \".fif\", \".vhdr\", \".eeg\", \".set\", \".fdt\", \".csv\", \".tsv\")\n",
    "\n",
    "    # Preprocessing\n",
    "    target_sfreq: float = 250.0      # resample here for speed/consistency\n",
    "    notch: list = field(default_factory=lambda: [50.0, 60.0])  # one or both; removed if not needed\n",
    "    l_freq: float = 0.5              # highpass\n",
    "    h_freq: float = 45.0             # lowpass\n",
    "    montage: str = \"standard_1020\"   # applied where EEG channel names match\n",
    "\n",
    "    # Epoching\n",
    "    epoch_len_s: float = 2.0         # window length (s)\n",
    "    step_s: float = 0.5              # hop (s) ; overlap = epoch_len - step\n",
    "\n",
    "    # Modeling\n",
    "    k_range: tuple = (3, 9)          # search K in [low, high]\n",
    "    use_umap: bool = True            # for 2-D viz\n",
    "    use_hmm: bool = False            # refine sequence with HMM temporal smoothing\n",
    "    random_state: int = 42\n",
    "    n_jobs: int = 1                  # MNE filters can use n_jobs\n",
    "\n",
    "    # Housekeeping\n",
    "    max_files: int = None            # limit for quick tests\n",
    "    verbose: bool = True\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# -----------------------\n",
    "# 3) Utilities\n",
    "# -----------------------\n",
    "def safe_mkdir(p):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def download_files(urls, dest):\n",
    "    if not urls:\n",
    "        return []\n",
    "    safe_mkdir(dest)\n",
    "    paths = []\n",
    "    for url in urls:\n",
    "        fname = os.path.join(dest, url.split(\"/\")[-1].split(\"?\")[0])\n",
    "        if os.path.exists(fname) and os.path.getsize(fname) > 0:\n",
    "            print(f\"âœ“ Exists: {fname}\")\n",
    "            paths.append(fname)\n",
    "            continue\n",
    "        print(f\"â†“ Downloading: {url}\")\n",
    "        try:\n",
    "            with requests.get(url, stream=True, timeout=60) as r:\n",
    "                r.raise_for_status()\n",
    "                total = int(r.headers.get(\"content-length\", 0))\n",
    "                with open(fname, \"wb\") as f, tqdm(\n",
    "                    total=total, unit=\"B\", unit_scale=True, desc=os.path.basename(fname)\n",
    "                ) as bar:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk); bar.update(len(chunk))\n",
    "            paths.append(fname)\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed: {url} ({e})\")\n",
    "    return paths\n",
    "\n",
    "def find_files(root, exts):\n",
    "    files = []\n",
    "    for ext in exts:\n",
    "        files.extend(glob.glob(os.path.join(root, f\"**/*{ext}\"), recursive=True))\n",
    "    return sorted(list(dict.fromkeys(files)))\n",
    "\n",
    "def _load_csv_tsv(path):\n",
    "    df = pd.read_csv(path) if path.endswith(\".csv\") else pd.read_csv(path, sep=\"\\t\")\n",
    "    # Heuristics: assume either columns are channels (time in first col),\n",
    "    # or there is an explicit 'time' column in seconds.\n",
    "    if \"time\" in df.columns:\n",
    "        time_s = df[\"time\"].to_numpy()\n",
    "        dt = np.median(np.diff(time_s))\n",
    "        sfreq = 1.0 / dt\n",
    "        ch_names = [c for c in df.columns if c != \"time\"]\n",
    "        data = df[ch_names].to_numpy().T\n",
    "    else:\n",
    "        # Assume uniform sampling; ask user to edit if wrong.\n",
    "        # Guess sampling rate by assuming 250 Hz if not specified.\n",
    "        sfreq = cfg.target_sfreq\n",
    "        ch_names = list(df.columns)\n",
    "        data = df[ch_names].to_numpy().T\n",
    "        time_s = np.arange(data.shape[1]) / sfreq\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=[\"eeg\"] * len(ch_names))\n",
    "    raw = mne.io.RawArray(data, info, verbose=False)\n",
    "    return raw\n",
    "\n",
    "def load_raw_any(path):\n",
    "    path_low = path.lower()\n",
    "    try:\n",
    "        if path_low.endswith(\".edf\") or path_low.endswith(\".bdf\"):\n",
    "            raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "        elif path_low.endswith(\".fif\"):\n",
    "            raw = mne.io.read_raw_fif(path, preload=True, verbose=False)\n",
    "        elif path_low.endswith(\".vhdr\") or path_low.endswith(\".eeg\"):\n",
    "            raw = mne.io.read_raw_brainvision(path, preload=True, verbose=False)\n",
    "        elif path_low.endswith(\".set\") or path_low.endswith(\".fdt\"):\n",
    "            raw = mne.io.read_raw_eeglab(path, preload=True, verbose=False)\n",
    "        elif path_low.endswith(\".csv\") or path_low.endswith(\".tsv\"):\n",
    "            raw = _load_csv_tsv(path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported format\")\n",
    "        # Ensure basic channel picking across modalities\n",
    "        picks = mne.pick_types(\n",
    "            raw.info, eeg=True, meg=True, seeg=True, ecog=True, fnirs=True, exclude=\"bads\"\n",
    "        )\n",
    "        if len(picks) == 0:\n",
    "            raise RuntimeError(\"No EEG/MEG/iEEG/fNIRS channels found.\")\n",
    "        raw.pick(picks)\n",
    "        return raw\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Could not load {os.path.basename(path)}: {e}\")\n",
    "        return None\n",
    "\n",
    "def basic_clean(raw: mne.io.BaseRaw):\n",
    "    raw = raw.copy()\n",
    "    # Notch (if line noise present)\n",
    "    if cfg.notch:\n",
    "        try:\n",
    "            raw.notch_filter(cfg.notch, n_jobs=cfg.n_jobs, verbose=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Bandpass\n",
    "    raw.filter(cfg.l_freq, cfg.h_freq, n_jobs=cfg.n_jobs, verbose=False)\n",
    "    # Resample\n",
    "    if abs(raw.info[\"sfreq\"] - cfg.target_sfreq) > 1e-3:\n",
    "        raw.resample(cfg.target_sfreq, npad=\"auto\", verbose=False)\n",
    "    # Reference EEG if present\n",
    "    ch_types = set(raw.get_channel_types())\n",
    "    if \"eeg\" in ch_types:\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False)\n",
    "        raw.apply_proj()\n",
    "        # Try to set montage to improve spatial consistency (won't fail if names unfamiliar)\n",
    "        try:\n",
    "            montage = mne.channels.make_standard_montage(cfg.montage)\n",
    "            raw.set_montage(montage, match_case=False, match_alias=True, on_missing=\"warn\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return raw\n",
    "\n",
    "def make_epochs(raw: mne.io.BaseRaw):\n",
    "    dur = cfg.epoch_len_s\n",
    "    hop = cfg.step_s\n",
    "    overlap = max(0.0, dur - hop)\n",
    "    epochs = mne.make_fixed_length_epochs(\n",
    "        raw, duration=dur, overlap=overlap, preload=True, verbose=False\n",
    "    )\n",
    "    # Start times per epoch (in seconds, referenced to raw start)\n",
    "    starts = epochs.events[:, 0] / epochs.info[\"sfreq\"]\n",
    "    ends = starts + dur\n",
    "    return epochs, starts, ends\n",
    "\n",
    "# -----------------------\n",
    "# 4) Feature Extraction\n",
    "# -----------------------\n",
    "from scipy.signal import welch\n",
    "\n",
    "def band_indices(freqs, low, high):\n",
    "    return np.where((freqs >= low) & (freqs < high))[0]\n",
    "\n",
    "def hjorth_params(x):  # x: (..., n_times)\n",
    "    # Returns (activity, mobility, complexity) along last axis\n",
    "    diff1 = np.diff(x, axis=-1)\n",
    "    var0 = np.var(x, axis=-1, ddof=0) + 1e-12\n",
    "    var1 = np.var(diff1, axis=-1, ddof=0) + 1e-12\n",
    "    mobility = np.sqrt(var1 / var0)\n",
    "    diff2 = np.diff(diff1, axis=-1)\n",
    "    var2 = np.var(diff2, axis=-1, ddof=0) + 1e-12\n",
    "    complexity = np.sqrt((var2 / var1) / (var1 / var0))\n",
    "    return var0, mobility, complexity\n",
    "\n",
    "def spectral_features(epochs: mne.Epochs, bands: dict):\n",
    "    \"\"\"Compute epoch-wise features: relative band powers (+ dispersion), spectral entropy, centroid, Hjorth.\"\"\"\n",
    "    X = epochs.get_data()  # shape: (n_epochs, n_channels, n_times)\n",
    "    n_ep, n_ch, n_t = X.shape\n",
    "    sf = epochs.info[\"sfreq\"]\n",
    "    nperseg = min(int(sf * 2), n_t)  # ~2s segments (or full)\n",
    "    noverlap = nperseg // 2\n",
    "\n",
    "    # PSD via Welch\n",
    "    freqs, psd = welch(\n",
    "        X, fs=sf, nperseg=nperseg, noverlap=noverlap, axis=-1, average=\"median\"\n",
    "    )  # -> (n_ep, n_ch, n_freq)\n",
    "\n",
    "    # Power in analysis band (cfg.l_freq .. cfg.h_freq)\n",
    "    aidx = band_indices(freqs, cfg.l_freq, cfg.h_freq)\n",
    "    tot_pow = np.maximum(psd[:, :, aidx].sum(axis=-1), 1e-12)  # (n_ep, n_ch)\n",
    "\n",
    "    # Relative bandpowers and dispersion across channels\n",
    "    feats = {}\n",
    "    for band, (lo, hi) in bands.items():\n",
    "        idx = band_indices(freqs, lo, hi)\n",
    "        bp = psd[:, :, idx].sum(axis=-1) / tot_pow  # (n_ep, n_ch)\n",
    "        feats[f\"{band}_rel_med\"] = np.median(bp, axis=1)\n",
    "        feats[f\"{band}_rel_iqr\"] = np.subtract(*np.percentile(bp, [75, 25], axis=1))\n",
    "        feats[f\"{band}_rel_std\"] = np.std(bp, axis=1)\n",
    "\n",
    "    # Ratios (common EEG heuristics)\n",
    "    alpha = feats[\"alpha_rel_med\"]\n",
    "    theta = feats[\"theta_rel_med\"]\n",
    "    beta  = feats[\"beta_rel_med\"]\n",
    "    feats[\"theta_over_alpha\"] = theta / np.maximum(alpha, 1e-6)\n",
    "    feats[\"beta_over_alpha\"]  = beta  / np.maximum(alpha, 1e-6)\n",
    "    feats[\"(alpha+theta)_over_beta\"] = (alpha + theta) / np.maximum(beta, 1e-6)\n",
    "\n",
    "    # Spectral entropy (normalized) & centroid\n",
    "    p = psd[:, :, aidx]\n",
    "    p_norm = p / np.maximum(p.sum(axis=-1, keepdims=True), 1e-12)\n",
    "    H = -np.sum(p_norm * np.log2(p_norm + 1e-12), axis=-1)  # (n_ep, n_ch)\n",
    "    Hn = H / np.log2(p.shape[-1])\n",
    "    feats[\"spec_entropy_med\"] = np.median(Hn, axis=1)\n",
    "    feats[\"spec_entropy_std\"] = np.std(Hn, axis=1)\n",
    "\n",
    "    f_a = freqs[aidx].reshape(1, 1, -1)\n",
    "    centroid = (p * f_a).sum(axis=-1) / np.maximum(p.sum(axis=-1), 1e-12)\n",
    "    feats[\"spec_centroid_med\"] = np.median(centroid, axis=1)\n",
    "    feats[\"spec_centroid_std\"] = np.std(centroid, axis=1)\n",
    "\n",
    "    # Hjorth parameters on the time series\n",
    "    act, mob, comp = hjorth_params(X)\n",
    "    feats[\"hjorth_activity_med\"]   = np.median(act, axis=1)\n",
    "    feats[\"hjorth_mobility_med\"]   = np.median(mob, axis=1)\n",
    "    feats[\"hjorth_complexity_med\"] = np.median(comp, axis=1)\n",
    "\n",
    "    # Stack into DataFrame\n",
    "    feat_df = pd.DataFrame({k: v if v.ndim == 1 else v.reshape(v.shape[0], -1)\n",
    "                            for k, v in feats.items()})\n",
    "    return feat_df\n",
    "\n",
    "# -----------------------\n",
    "# 5) Modeling: Clustering + (optional) HMM\n",
    "# -----------------------\n",
    "def choose_k_and_cluster(Z, k_low, k_high, random_state):\n",
    "    best = {\"k\": None, \"sil\": -1.0, \"model\": None, \"labels\": None}\n",
    "    for k in range(k_low, k_high + 1):\n",
    "        km = KMeans(n_clusters=k, n_init=\"auto\", random_state=random_state)\n",
    "        labels = km.fit_predict(Z)\n",
    "        sil = silhouette_score(Z, labels) if k > 1 else -1.0\n",
    "        if sil > best[\"sil\"]:\n",
    "            best = {\"k\": k, \"sil\": sil, \"model\": km, \"labels\": labels}\n",
    "    return best\n",
    "\n",
    "def hmm_smooth(labels, Z, k, random_state):\n",
    "    if not HAS_HMM:\n",
    "        print(\"HMM not available; skipping temporal smoothing.\")\n",
    "        return labels\n",
    "    try:\n",
    "        hmm = GaussianHMM(\n",
    "            n_components=k, covariance_type=\"diag\",\n",
    "            random_state=random_state, n_iter=200\n",
    "        )\n",
    "        hmm.fit(Z)\n",
    "        labels_hmm = hmm.predict(Z)\n",
    "        return labels_hmm\n",
    "    except Exception as e:\n",
    "        print(f\"HMM smoothing failed: {e}\")\n",
    "        return labels\n",
    "\n",
    "def summarize_alphabet(df_feat, labels):\n",
    "    \"\"\"Return a per-state median feature table and a JSON-ready mapping.\"\"\"\n",
    "    F = df_feat.copy()\n",
    "    F[\"state\"] = labels\n",
    "    med = F.groupby(\"state\").median(numeric_only=True).sort_index()\n",
    "    # For each state, list top features (largest z-scores vs overall)\n",
    "    z = (med - df_feat.median()) / (df_feat.std() + 1e-9)\n",
    "    summary = {}\n",
    "    for s in med.index:\n",
    "        tops = z.loc[s].abs().sort_values(ascending=False).head(8).index.tolist()\n",
    "        summary[int(s)] = {\n",
    "            \"name\": f\"State-{int(s)}\",\n",
    "            \"top_features\": tops\n",
    "        }\n",
    "    return med, summary\n",
    "\n",
    "# -----------------------\n",
    "# 6) Orchestration: Run Pipeline\n",
    "# -----------------------\n",
    "def run_pipeline():\n",
    "    t0 = time.time()\n",
    "    safe_mkdir(cfg.output_dir)\n",
    "\n",
    "    # (a) Acquire files\n",
    "    dled = download_files(cfg.dataset_urls, cfg.data_dir)\n",
    "    files = find_files(cfg.data_dir, cfg.exts)\n",
    "    if cfg.max_files is not None:\n",
    "        files = files[:cfg.max_files]\n",
    "\n",
    "    if len(files) == 0:\n",
    "        print(f\"âœ± No files found in {cfg.data_dir}. Add data or URLs and re-run.\")\n",
    "        return\n",
    "\n",
    "    # (b) Loop files â†’ clean â†’ epoch â†’ features\n",
    "    all_feat = []\n",
    "    all_meta = []\n",
    "    print(f\"â€¢ Found {len(files)} file(s). Processing â€¦\")\n",
    "    for i, fp in enumerate(files, 1):\n",
    "        print(f\"[{i}/{len(files)}] {os.path.basename(fp)}\")\n",
    "        raw = load_raw_any(fp)\n",
    "        if raw is None:\n",
    "            continue\n",
    "        raw = basic_clean(raw)\n",
    "        epochs, starts, ends = make_epochs(raw)\n",
    "        if len(epochs) == 0:\n",
    "            print(\"   (no epochs)\")\n",
    "            continue\n",
    "        # Extract features\n",
    "        feat = spectral_features(epochs, BANDS)\n",
    "        # Meta\n",
    "        meta = pd.DataFrame({\n",
    "            \"file\": os.path.basename(fp),\n",
    "            \"epoch_idx\": np.arange(len(epochs)),\n",
    "            \"t_start_s\": starts,\n",
    "            \"t_end_s\": ends,\n",
    "            \"sfreq\": epochs.info[\"sfreq\"],\n",
    "            \"n_channels\": [len(epochs.ch_names)] * len(epochs)\n",
    "        })\n",
    "        all_feat.append(feat)\n",
    "        all_meta.append(meta)\n",
    "        del raw, epochs, feat, meta\n",
    "        gc.collect()\n",
    "\n",
    "    if not all_feat:\n",
    "        print(\"âœ± No usable epochs extracted. Check file formats and preprocessing settings.\")\n",
    "        return\n",
    "\n",
    "    feat_df = pd.concat(all_feat, axis=0, ignore_index=True)\n",
    "    meta_df = pd.concat(all_meta, axis=0, ignore_index=True)\n",
    "\n",
    "    # Save raw features & metadata\n",
    "    feat_path = os.path.join(cfg.output_dir, \"features.csv\")\n",
    "    meta_path = os.path.join(cfg.output_dir, \"metadata.csv\")\n",
    "    feat_df.to_csv(feat_path, index=False)\n",
    "    meta_df.to_csv(meta_path, index=False)\n",
    "    print(f\"âœ“ Saved features â†’ {feat_path}\")\n",
    "    print(f\"âœ“ Saved metadata â†’ {meta_path}\")\n",
    "\n",
    "    # (c) Scale + reduce\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(feat_df.values)\n",
    "    pca = PCA(n_components=min(20, Xs.shape[1]), random_state=cfg.random_state)\n",
    "    Z = pca.fit_transform(Xs)\n",
    "\n",
    "    # (d) Cluster (select K by silhouette)\n",
    "    best = choose_k_and_cluster(Z, cfg.k_range[0], cfg.k_range[1], cfg.random_state)\n",
    "    labels = best[\"labels\"]\n",
    "    K = best[\"k\"]\n",
    "    print(f\"â˜… Selected K={K} (silhouette={best['sil']:.3f})\")\n",
    "\n",
    "    # (e) Optional temporal smoothing via HMM\n",
    "    if cfg.use_hmm:\n",
    "        labels = hmm_smooth(labels, Z, K, cfg.random_state)\n",
    "        print(\"âœ“ Applied HMM smoothing.\")\n",
    "\n",
    "    # (f) Summarize â€œAlphabetâ€\n",
    "    centers = pd.DataFrame(best[\"model\"].cluster_centers_, columns=[f\"PC{i+1}\" for i in range(Z.shape[1])])\n",
    "    med_table, alpha_map = summarize_alphabet(feat_df, labels)\n",
    "    alpha_json_path = os.path.join(cfg.output_dir, \"cognitive_alphabet.json\")\n",
    "    with open(alpha_json_path, \"w\") as f:\n",
    "        json.dump(alpha_map, f, indent=2)\n",
    "    print(f\"âœ“ Saved alphabet map â†’ {alpha_json_path}\")\n",
    "\n",
    "    # (g) Save models\n",
    "    dump(best[\"model\"], os.path.join(cfg.output_dir, \"kmeans.joblib\"))\n",
    "    dump(pca, os.path.join(cfg.output_dir, \"pca.joblib\"))\n",
    "    dump(scaler, os.path.join(cfg.output_dir, \"scaler.joblib\"))\n",
    "\n",
    "    # (h) Visualization\n",
    "    emb2 = None\n",
    "    try:\n",
    "        if cfg.use_umap and HAS_UMAP:\n",
    "            reducer = umap.UMAP(n_components=2, random_state=cfg.random_state, n_neighbors=30, min_dist=0.1)\n",
    "            emb2 = reducer.fit_transform(Z)\n",
    "        else:\n",
    "            # Fall back to PCA 2D for visualization\n",
    "            reducer2 = PCA(n_components=2, random_state=cfg.random_state)\n",
    "            emb2 = reducer2.fit_transform(Z)\n",
    "    except Exception as e:\n",
    "        print(f\"(viz) embedding failed: {e}\")\n",
    "\n",
    "    if emb2 is not None:\n",
    "        plt.figure(figsize=(7.2, 6.2))\n",
    "        scatter = plt.scatter(emb2[:, 0], emb2[:, 1], c=labels, s=8)\n",
    "        plt.title(f\"Cognitive Alphabet (K={K})\")\n",
    "        plt.xlabel(\"Dim 1\")\n",
    "        plt.ylabel(\"Dim 2\")\n",
    "        plt.tight_layout()\n",
    "        fig_path = os.path.join(cfg.output_dir, \"embedding.png\")\n",
    "        plt.savefig(fig_path, dpi=160)\n",
    "        plt.close()\n",
    "        print(f\"âœ“ Saved embedding plot â†’ {fig_path}\")\n",
    "\n",
    "    # Feature signature heatmap\n",
    "    try:\n",
    "        plt.figure(figsize=(min(14, 2 + 0.5*len(med_table.columns)), 0.6 + 0.3*K + 2))\n",
    "        M = (med_table - feat_df.median()) / (feat_df.std() + 1e-9)  # z vs overall\n",
    "        im = plt.imshow(M.values, aspect=\"auto\")\n",
    "        plt.colorbar(im, fraction=0.025, pad=0.03, label=\"z-score vs overall\")\n",
    "        plt.yticks(range(M.shape[0]), [f\"S{int(s)}\" for s in med_table.index])\n",
    "        plt.xticks(range(M.shape[1]), M.columns, rotation=90)\n",
    "        plt.title(\"State Feature Signatures\")\n",
    "        plt.tight_layout()\n",
    "        heat_path = os.path.join(cfg.output_dir, \"state_feature_signatures.png\")\n",
    "        plt.savefig(heat_path, dpi=160)\n",
    "        plt.close()\n",
    "        print(f\"âœ“ Saved signatures heatmap â†’ {heat_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"(viz) heatmap failed: {e}\")\n",
    "\n",
    "    # (i) Save assignments\n",
    "    assign = meta_df.copy()\n",
    "    assign[\"state\"] = labels\n",
    "    assign_path = os.path.join(cfg.output_dir, \"state_assignments.csv\")\n",
    "    assign.to_csv(assign_path, index=False)\n",
    "    print(f\"âœ“ Saved state assignments â†’ {assign_path}\")\n",
    "\n",
    "    # (j) Lightweight report\n",
    "    report = {\n",
    "        \"files_processed\": len(files),\n",
    "        \"n_epochs\": int(len(assign)),\n",
    "        \"alphabet_size\": int(K),\n",
    "        \"silhouette\": float(best[\"sil\"]),\n",
    "        \"bands\": BANDS,\n",
    "        \"epoch_length_s\": cfg.epoch_len_s,\n",
    "        \"step_s\": cfg.step_s,\n",
    "        \"target_sfreq\": cfg.target_sfreq,\n",
    "        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    }\n",
    "    with open(os.path.join(cfg.output_dir, \"report.json\"), \"w\") as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    print(f\"âœ“ Saved report.json\")\n",
    "    print(f\"â± Done in {time.time() - t0:.1f}s\")\n",
    "\n",
    "# -----------------------\n",
    "# 7) Run\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸ§­ Cognitive Alphabet Finder â€” startingâ€¦\")\n",
    "    print(f\"Data directory: {cfg.data_dir}\")\n",
    "    print(f\"Output directory: {cfg.output_dir}\")\n",
    "    run_pipelin# === CNT Mechanism Pack v1 â€” Letter Faces (topomaps), Microstates, and Î±â†”Î² coupling ===\n",
    "import os, re, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import mne\n",
    "from scipy.signal import welch, hilbert\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# ------------------ Paths (edit if needed) ------------------\n",
    "RUN   = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")     # promoted v0.2 run (S001)\n",
    "REP   = RUN.parent / \"cog_alphabet_report_hybrid_v1\" / \"mechanism\"\n",
    "REP.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Which subject/run to analyze:\n",
    "SUBJ = \"S001\"\n",
    "RAW_DIR = RUN / \"brainwaves_rebuilt\"   # where S001R01..R03.edf were fetched during rebuild\n",
    "DECODE_CSV = None                      # (optional) for other subjects: set to their decode CSV with states\n",
    "\n",
    "# v0.2 processing params\n",
    "TARGET_SF=250.0; L_FREQ,H_FREQ=0.5,80.0; EPOCH_LEN,STEP=2.0,0.5\n",
    "ALPHA=(8,13); BETA=(13,30)\n",
    "\n",
    "# ------------------ helpers ------------------\n",
    "def normalize_chan_names(raw):\n",
    "    \"\"\"Map odd labels to 10-20 style & strip punctuation; best-effort.\"\"\"\n",
    "    maps = {\"FP1\":\"Fp1\",\"FP2\":\"Fp2\",\"FZ\":\"Fz\",\"CZ\":\"Cz\",\"PZ\":\"Pz\",\"OZ\":\"Oz\",\"POZ\":\"POz\",\"CPZ\":\"CPz\",\"AFZ\":\"AFz\"}\n",
    "    mapping = {}\n",
    "    for ch in raw.ch_names:\n",
    "        base = re.sub(r'[^A-Za-z0-9]', '', ch).upper()\n",
    "        if base in maps: mapping[ch] = maps[base]\n",
    "        elif base.startswith(\"FP\"): mapping[ch] = \"Fp\"+base[2:]\n",
    "        elif base.endswith(\"Z\") and len(base)>1: mapping[ch] = base[:-1]+\"z\"\n",
    "        else: mapping[ch] = base\n",
    "    mne.rename_channels(raw.info, mapping)\n",
    "\n",
    "def load_raw(p):\n",
    "    raw = mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"]-TARGET_SF)>1e-6: raw.resample(TARGET_SF, npad=\"auto\", verbose=False)\n",
    "    ny = raw.info[\"sfreq\"]/2.0\n",
    "    raw.filter(L_FREQ, min(H_FREQ, ny-1.0), verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "        try:\n",
    "            normalize_chan_names(raw)\n",
    "            raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"),\n",
    "                            match_case=False, match_alias=True, on_missing=\"ignore\")\n",
    "        except Exception: pass\n",
    "    return raw\n",
    "\n",
    "def make_epochs(raw):\n",
    "    ov = max(0.0, EPOCH_LEN-STEP)\n",
    "    return mne.make_fixed_length_epochs(raw, duration=EPOCH_LEN, overlap=ov, preload=True, verbose=False)\n",
    "\n",
    "def epoch_labels_from_run(run_dir, file_name):\n",
    "    meta = pd.read_csv(Path(run_dir,\"metadata.csv\"))\n",
    "    lab  = pd.read_csv(Path(run_dir,\"state_assignments.csv\"))[\"state\"].to_numpy()\n",
    "    meta = meta.assign(state=lab)\n",
    "    sub = meta[meta[\"file\"]==file_name].copy().reset_index(drop=True)\n",
    "    return sub  # columns: t_start_s, t_end_s, state, ...\n",
    "\n",
    "def band_power_epoch(raw, starts, ends, band):\n",
    "    sf = raw.info[\"sfreq\"]\n",
    "    lo,hi = band\n",
    "    bp = []\n",
    "    for t0,t1 in zip(starts, ends):\n",
    "        s=int(round(t0*sf)); e=int(round(t1*sf))\n",
    "        X = raw.get_data(start=s, stop=e)          # (n_ch, n_t)\n",
    "        nper = min(int(sf*2), X.shape[1]); nov = nper//2\n",
    "        freqs, P = welch(X, fs=sf, nperseg=nper, noverlap=nov, axis=-1, average=\"median\")\n",
    "        idx = (freqs>=lo)&(freqs<hi)\n",
    "        bp.append(P[:, idx].sum(-1))              # (n_ch,)\n",
    "    return np.stack(bp,0)                          # (n_ep, n_ch)\n",
    "\n",
    "def plot_topomap(ax, raw, values, title):\n",
    "    ax.set_title(title, fontsize=11, pad=4)\n",
    "    try:\n",
    "        mne.viz.plot_topomap(values, raw.info, axes=ax, show=False)\n",
    "    except Exception:\n",
    "        ax.axis(\"off\"); ax.text(0.5,0.5,\"topomap unavailable\", ha=\"center\", va=\"center\", fontsize=9)\n",
    "\n",
    "def letter_topomaps(raw, sub_df, out_prefix):\n",
    "    \"\"\"Per-letter Î±/Î² maps (avg band power per channel across epochs in that letter).\"\"\"\n",
    "    starts = sub_df[\"t_start_s\"].to_numpy()\n",
    "    ends   = sub_df[\"t_end_s\"].to_numpy()\n",
    "    L      = sub_df[\"state\"].to_numpy().astype(int)\n",
    "    if len(L)==0: return None\n",
    "    BPa = band_power_epoch(raw, starts, ends, ALPHA)  # (n_ep, n_ch)\n",
    "    BPb = band_power_epoch(raw, starts, ends, BETA)\n",
    "\n",
    "    maps = {}\n",
    "    for s in np.unique(L):\n",
    "        sel = (L==s)\n",
    "        if np.sum(sel)<1: continue\n",
    "        a_map = BPa[sel].mean(0); b_map = BPb[sel].mean(0)\n",
    "        maps[int(s)] = (a_map, b_map)\n",
    "\n",
    "    # plot grid\n",
    "    if not maps: return None\n",
    "    ks = sorted(maps.keys())\n",
    "    fig, axes = plt.subplots(len(ks), 2, figsize=(6, 2.6*len(ks)), constrained_layout=True)\n",
    "    if len(ks)==1: axes = np.array([axes])\n",
    "    for r,s in enumerate(ks):\n",
    "        a_map, b_map = maps[s]\n",
    "        plot_topomap(axes[r,0], raw, a_map, f\"S{s} â€” Î± (8â€“13 Hz)\")\n",
    "        plot_topomap(axes[r,1], raw, b_map, f\"S{s} â€” Î² (13â€“30 Hz)\")\n",
    "    fig.suptitle(f\"Letter Topographies â€” {out_prefix}\", fontsize=14, weight=\"bold\")\n",
    "    out_png = REP / f\"{out_prefix}_letter_topomaps.png\"\n",
    "    fig.savefig(out_png, dpi=160); plt.close(fig)\n",
    "    print(\"Saved:\", out_png)\n",
    "    return maps\n",
    "\n",
    "def gfp_peaks(raw):\n",
    "    \"\"\"Return indices of Global Field Power peaks.\"\"\"\n",
    "    X = raw.get_data()                      # (n_ch, n_t)\n",
    "    Xc = X - X.mean(axis=0, keepdims=True)  # center across channels\n",
    "    gfp = Xc.std(axis=0)\n",
    "    # pick local maxima (simple)\n",
    "    peaks = np.where((gfp[1:-1] > gfp[:-2]) & (gfp[1:-1] > gfp[2:]))[0] + 1\n",
    "    # thin peaks (every ~10 ms) for independence\n",
    "    sf = raw.info[\"sfreq\"]; min_gap = int(0.01*sf)\n",
    "    keep = []\n",
    "    last = -min_gap\n",
    "    for p in peaks:\n",
    "        if p - last >= min_gap:\n",
    "            keep.append(p); last = p\n",
    "    return np.array(keep, int)\n",
    "\n",
    "def microstates_from_run(raw, k=4, max_samples=5000):\n",
    "    \"\"\"K-means on normalized topographies at GFP peaks (R02 or R03).\"\"\"\n",
    "    idx = gfp_peaks(raw)\n",
    "    if len(idx)==0: return None, None, None\n",
    "    if len(idx)>max_samples:\n",
    "        rng = np.random.default_rng(42)\n",
    "        idx = rng.choice(idx, size=max_samples, replace=False)\n",
    "    X = raw.get_data()[:, idx]           # (n_ch, n_samp)\n",
    "    # normalize each map to unit norm, sign-flip invariant (use absolute)\n",
    "    Xn = X / (np.linalg.norm(X, axis=0, keepdims=True) + 1e-12)\n",
    "    Xn = np.abs(Xn)\n",
    "    # kmeans\n",
    "    from sklearn.cluster import KMeans\n",
    "    km = KMeans(n_clusters=k, n_init=\"auto\", random_state=42).fit(Xn.T)\n",
    "    labels = km.labels_\n",
    "    centers = km.cluster_centers_.T     # (n_ch, k)\n",
    "    return centers, labels, idx\n",
    "\n",
    "def letter_microstate_contingency(raw, sub_df, centers, labels, idx):\n",
    "    \"\"\"Map each GFP peak to its epoch letter; count (letter x microstate).\"\"\"\n",
    "    if centers is None: return None\n",
    "    sf = raw.info[\"sfreq\"]\n",
    "    # epoch index for each GFP time\n",
    "    starts = (sub_df[\"t_start_s\"].to_numpy()*sf).astype(int)\n",
    "    ends   = (sub_df[\"t_end_s\"].to_numpy()*sf).astype(int)\n",
    "    L      = sub_df[\"state\"].to_numpy().astype(int)\n",
    "    # build timeâ†’letter map\n",
    "    t2l = np.full(int(ends.max())+1, -1, int)\n",
    "    for s,e,l in zip(starts, ends, L):\n",
    "        t2l[s:e] = l\n",
    "    ms = pd.DataFrame({\"letter\": t2l[idx], \"micro\": labels})\n",
    "    ms = ms[ms[\"letter\"]>=0]\n",
    "    if len(ms)==0: return None\n",
    "    tab = (ms.groupby([\"letter\",\"micro\"]).size()\n",
    "             .rename(\"count\").reset_index()\n",
    "             .pivot(index=\"letter\", columns=\"micro\", values=\"count\").fillna(0.0))\n",
    "    # normalize rows\n",
    "    tab = tab.div(tab.sum(axis=1), axis=0)\n",
    "    return tab\n",
    "\n",
    "def letter_alpha_beta_coupling(raw, sub_df, posterior=(\"O1\",\"Oz\",\"O2\"), central=(\"C3\",\"Cz\",\"C4\")):\n",
    "    \"\"\"Per-letter correlation between Î± envelope (posterior) and Î² envelope (central).\"\"\"\n",
    "    sf = raw.info[\"sfreq\"]\n",
    "    # picks\n",
    "    names = [ch.upper().strip() for ch in raw.ch_names]\n",
    "    pidx = [i for i,ch in enumerate(raw.ch_names) if ch.upper().strip() in [p.upper() for p in posterior]]\n",
    "    cidx = [i for i,ch in enumerate(raw.ch_names) if ch.upper().strip() in [p.upper() for p in central]]\n",
    "    if len(pidx)<1: pidx = mne.pick_types(raw.info, eeg=True)\n",
    "    if len(cidx)<1: cidx = mne.pick_types(raw.info, eeg=True)\n",
    "    # filter bands\n",
    "    def band_env(data, band):\n",
    "        lo,hi = band\n",
    "        Xf = mne.filter.filter_data(data, sfreq=sf, l_freq=lo, h_freq=hi,\n",
    "                                    method=\"iir\", iir_params=dict(order=4, ftype=\"butter\"),\n",
    "                                    verbose=False)\n",
    "        env = np.abs(hilbert(Xf, axis=-1))\n",
    "        return env\n",
    "    starts = (sub_df[\"t_start_s\"].to_numpy()*sf).astype(int)\n",
    "    ends   = (sub_df[\"t_end_s\"].to_numpy()*sf).astype(int)\n",
    "    L      = sub_df[\"state\"].to_numpy().astype(int)\n",
    "    rows=[]\n",
    "    for s,e,l in zip(starts, ends, L):\n",
    "        X = raw.get_data(start=s, stop=e)\n",
    "        a_env = band_env(X[pidx], ALPHA).mean(0)\n",
    "        b_env = band_env(X[cidx], BETA ).mean(0)\n",
    "        if a_env.std()<1e-9 or b_env.std()<1e-9: continue\n",
    "        r = float(np.corrcoef(a_env, b_env)[0,1])\n",
    "        rows.append({\"letter\":int(l), \"r_alpha_beta\": r})\n",
    "    if not rows: return None\n",
    "    df = pd.DataFrame(rows)\n",
    "    summ = df.groupby(\"letter\")[\"r_alpha_beta\"].agg([\"mean\",\"median\",\"count\"]).reset_index()\n",
    "    return summ, df\n",
    "\n",
    "# ------------------ RUN ANALYSIS (S001) ------------------\n",
    "# R02 (EC) topographies\n",
    "p_ec = RAW_DIR / f\"{SUBJ}R02.edf\"\n",
    "raw_ec = load_raw(p_ec)\n",
    "sub_ec = epoch_labels_from_run(RUN, f\"{SUBJ}R02.edf\")\n",
    "maps_ec = letter_topomaps(raw_ec, sub_ec, f\"{SUBJ}_R02\")\n",
    "\n",
    "# R03 (task) topographies\n",
    "p_task = RAW_DIR / f\"{SUBJ}R03.edf\"\n",
    "raw_task = load_raw(p_task)\n",
    "sub_task = epoch_labels_from_run(RUN, f\"{SUBJ}R03.edf\")\n",
    "maps_task = letter_topomaps(raw_task, sub_task, f\"{SUBJ}_R03\")\n",
    "\n",
    "# Microstates (R02 and R03)\n",
    "for tag, raw0, sub0 in [(\"R02\", raw_ec, sub_ec), (\"R03\", raw_task, sub_task)]:\n",
    "    C, lab, idx = microstates_from_run(raw0, k=4, max_samples=6000)\n",
    "    if C is None:\n",
    "        print(f\"[{tag}] microstates unavailable.\")\n",
    "        continue\n",
    "    # plot centroids\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10,2.8), constrained_layout=True)\n",
    "    for i in range(4):\n",
    "        plot_topomap(axes[i], raw0, C[:, i], f\"Î¼{i}\")\n",
    "    fig.suptitle(f\"Microstate Centroids â€” {SUBJ} {tag}\", fontsize=14, weight=\"bold\")\n",
    "    out_png = REP / f\"{SUBJ}_{tag}_microstates.png\"\n",
    "    fig.savefig(out_png, dpi=160); plt.close(fig); print(\"Saved:\", out_png)\n",
    "    # contingency letter x microstate\n",
    "    tab = letter_microstate_contingency(raw0, sub0, C, lab, idx)\n",
    "    if tab is not None:\n",
    "        out_csv = REP / f\"{SUBJ}_{tag}_letter_microstate.csv\"\n",
    "        tab.to_csv(out_csv)\n",
    "        print(\"Saved:\", out_csv)\n",
    "\n",
    "# Î±â†”Î² coupling (R03)\n",
    "coupl_summ, coupl_df = letter_alpha_beta_coupling(raw_task, sub_task)\n",
    "if coupl_summ is not None:\n",
    "    coupl_summ.to_csv(REP/f\"{SUBJ}_R03_alpha_beta_coupling_summary.csv\", index=False)\n",
    "    coupl_df.to_csv(REP/f\"{SUBJ}_R03_alpha_beta_coupling_by_epoch.csv\", index=False)\n",
    "    print(\"Saved:\", REP/f\"{SUBJ}_R03_alpha_beta_coupling_summary.csv\")\n",
    "\n",
    "# ------------------ (Optional) Other subject via DECODE_CSV ------------------\n",
    "if DECODE_CSV is not None:\n",
    "    # Example: analyze S003 using its decode CSV + corresponding EDFs in generalization_data\n",
    "    df = pd.read_csv(DECODE_CSV)\n",
    "    subj = df[\"subject\"].iloc[0]\n",
    "    for run in [\"R02\", \"R03\"]:\n",
    "        fn = next((f for f in df[\"file\"].unique() if run in f), None)\n",
    "        if not fn: continue\n",
    "        raw1 = load_raw(GEN / fn)\n",
    "        # align epochs & letters from decode CSV\n",
    "        sub1 = df[df[\"file\"]==fn][[\"t_start_s\",\"t_end_s\",\"state\"]].copy().reset_index(drop=True)\n",
    "        letter_topomaps(raw1, sub1, f\"{subj}_{run}\")\n",
    "        C, lab, idx = microstates_from_run(raw1, k=4, max_samples=6000)\n",
    "        if C is not None:\n",
    "            fig, axes = plt.subplots(1, 4, figsize=(10,2.8), constrained_layout=True)\n",
    "            for i in range(4): plot_topomap(axes[i], raw1, C[:, i], f\"Î¼{i}\")\n",
    "            fig.suptitle(f\"Microstate Centroids â€” {subj} {run}\", fontsize=14, weight=\"bold\")\n",
    "            fig.savefig(REP/f\"{subj}_{run}_microstates.png\", dpi=160); plt.close(fig)\n",
    "            tab = letter_microstate_contingency(raw1, sub1, C, lab, idx)\n",
    "            if tab is not None:\n",
    "                tab.to_csv(REP/f\"{subj}_{run}_letter_microstate.csv\")\n",
    "e()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1206a9a3-74c2-4e07-91ed-2bb5001c6eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO_INSTALL = True  # keep True if you want auto-installs\n",
    "\n",
    "def _ensure(pkgs):\n",
    "    if not AUTO_INSTALL:\n",
    "        return\n",
    "    # Prefer Jupyterâ€™s %pip (works inside notebooks)\n",
    "    ip = None\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        ip = get_ipython()\n",
    "    except Exception:\n",
    "        ip = None\n",
    "\n",
    "    for p in pkgs:\n",
    "        mod = p.split(\"==\")[0].split(\"[\")[0]\n",
    "        if importlib.util.find_spec(mod) is not None:\n",
    "            continue\n",
    "        print(f\"Â· Installing {p} â€¦\")\n",
    "        try:\n",
    "            if ip is not None and hasattr(ip, \"run_line_magic\"):\n",
    "                ip.run_line_magic(\"pip\", f\"install {p}\")\n",
    "            else:\n",
    "                # No stdout/stderr redirection â€” avoids .fileno() issue\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", p])\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Install failed for {p}: {e}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5bcf5c8-f001-410f-85f8-8de68e8d4235",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO_INSTALL = True  # keep True if you want auto-installs\n",
    "\n",
    "def _ensure(pkgs):\n",
    "    if not AUTO_INSTALL:\n",
    "        return\n",
    "    # Prefer Jupyterâ€™s %pip (works inside notebooks)\n",
    "    ip = None\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        ip = get_ipython()\n",
    "    except Exception:\n",
    "        ip = None\n",
    "\n",
    "    for p in pkgs:\n",
    "        mod = p.split(\"==\")[0].split(\"[\")[0]\n",
    "        if importlib.util.find_spec(mod) is not None:\n",
    "            continue\n",
    "        print(f\"Â· Installing {p} â€¦\")\n",
    "        try:\n",
    "            if ip is not None and hasattr(ip, \"run_line_magic\"):\n",
    "                ip.run_line_magic(\"pip\", f\"install {p}\")\n",
    "            else:\n",
    "                # No stdout/stderr redirection â€” avoids .fileno() issue\n",
    "                subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", p])\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Install failed for {p}: {e}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53da6f33-817d-4182-a58a-16c35303981a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecomposition\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PCA\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcluster\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KMeans\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ðŸ§  Cognitive Alphabet Finder â€” One Mega Cell\n",
    "# Paste this cell in JupyterLab and run.\n",
    "# ============================================\n",
    "\n",
    "import os, sys, math, json, time, glob, gc, warnings, subprocess, importlib.util\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# -----------------------\n",
    "# 0) (Optional) Auto-Install Dependencies\n",
    "# -----------------------\n",
    "AUTO_INSTALL = False  # set False if you manage packages yourself\n",
    "\n",
    "def _ensure(pkgs):\n",
    "    if not AUTO_INSTALL:\n",
    "        return\n",
    "    for p in pkgs:\n",
    "        mod = p.split(\"==\")[0].split(\"[\")[0]\n",
    "        if importlib.util.find_spec(mod) is None:\n",
    "            print(f\"Â· Installing {p} â€¦\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", p], stdout=sys.stdout)\n",
    "\n",
    "_ensure([\n",
    "    \"numpy\", \"scipy\", \"pandas\", \"matplotlib\", \"tqdm\",\n",
    "    \"scikit-learn\", \"mne\", \"joblib\", \"umap-learn\", \"hmmlearn\", \"requests\"\n",
    "])\n",
    "\n",
    "# -----------------------\n",
    "# 1) Imports (after install)\n",
    "# -----------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from joblib import dump\n",
    "import requests\n",
    "import mne\n",
    "\n",
    "try:\n",
    "    import umap\n",
    "    HAS_UMAP = True\n",
    "except Exception:\n",
    "    HAS_UMAP = False\n",
    "\n",
    "try:\n",
    "    from hmmlearn.hmm import GaussianHMM\n",
    "    HAS_HMM = True\n",
    "except Exception:\n",
    "    HAS_HMM = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# -----------------------\n",
    "# 2) Configuration\n",
    "# -----------------------\n",
    "BANDS = {\n",
    "    \"delta\": (1.0, 4.0),\n",
    "    \"theta\": (4.0, 8.0),\n",
    "    \"alpha\": (8.0, 13.0),\n",
    "    \"beta\":  (13.0, 30.0),\n",
    "    \"gamma\": (30.0, 45.0),  # conservative gamma upper bound for typical EEG\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Data locations\n",
    "    data_dir: str = \"./brainwaves\"\n",
    "    output_dir: str = \"./cog_alphabet\"\n",
    "\n",
    "    # Either provide URLs here or just drop files into data_dir\n",
    "    dataset_urls: list = field(default_factory=lambda: [\n",
    "        # Examples (put your own links here):\n",
    "        # \"https://example.org/path/to/subject01.edf\",\n",
    "        # \"https://example.org/path/to/recording.fif\",\n",
    "        # \"https://example.org/path/to/session.vhdr\",\n",
    "        # \"https://example.org/path/to/export.csv\",\n",
    "    ])\n",
    "\n",
    "    # File types to consider\n",
    "    exts: tuple = (\".edf\", \".bdf\", \".fif\", \".vhdr\", \".eeg\", \".set\", \".fdt\", \".csv\", \".tsv\")\n",
    "\n",
    "    # Preprocessing\n",
    "    target_sfreq: float = 250.0      # resample here for speed/consistency\n",
    "    notch: list = field(default_factory=lambda: [50.0, 60.0])  # one or both; removed if not needed\n",
    "    l_freq: float = 0.5              # highpass\n",
    "    h_freq: float = 45.0             # lowpass\n",
    "    montage: str = \"standard_1020\"   # applied where EEG channel names match\n",
    "\n",
    "    # Epoching\n",
    "    epoch_len_s: float = 2.0         # window length (s)\n",
    "    step_s: float = 0.5              # hop (s) ; overlap = epoch_len - step\n",
    "\n",
    "    # Modeling\n",
    "    k_range: tuple = (3, 9)          # search K in [low, high]\n",
    "    use_umap: bool = True            # for 2-D viz\n",
    "    use_hmm: bool = False            # refine sequence with HMM temporal smoothing\n",
    "    random_state: int = 42\n",
    "    n_jobs: int = 1                  # MNE filters can use n_jobs\n",
    "\n",
    "    # Housekeeping\n",
    "    max_files: int = None            # limit for quick tests\n",
    "    verbose: bool = True\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# -----------------------\n",
    "# 3) Utilities\n",
    "# -----------------------\n",
    "def safe_mkdir(p):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def download_files(urls, dest):\n",
    "    if not urls:\n",
    "        return []\n",
    "    safe_mkdir(dest)\n",
    "    paths = []\n",
    "    for url in urls:\n",
    "        fname = os.path.join(dest, url.split(\"/\")[-1].split(\"?\")[0])\n",
    "        if os.path.exists(fname) and os.path.getsize(fname) > 0:\n",
    "            print(f\"âœ“ Exists: {fname}\")\n",
    "            paths.append(fname)\n",
    "            continue\n",
    "        print(f\"â†“ Downloading: {url}\")\n",
    "        try:\n",
    "            with requests.get(url, stream=True, timeout=60) as r:\n",
    "                r.raise_for_status()\n",
    "                total = int(r.headers.get(\"content-length\", 0))\n",
    "                with open(fname, \"wb\") as f, tqdm(\n",
    "                    total=total, unit=\"B\", unit_scale=True, desc=os.path.basename(fname)\n",
    "                ) as bar:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk); bar.update(len(chunk))\n",
    "            paths.append(fname)\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed: {url} ({e})\")\n",
    "    return paths\n",
    "\n",
    "def find_files(root, exts):\n",
    "    files = []\n",
    "    for ext in exts:\n",
    "        files.extend(glob.glob(os.path.join(root, f\"**/*{ext}\"), recursive=True))\n",
    "    return sorted(list(dict.fromkeys(files)))\n",
    "\n",
    "def _load_csv_tsv(path):\n",
    "    df = pd.read_csv(path) if path.endswith(\".csv\") else pd.read_csv(path, sep=\"\\t\")\n",
    "    # Heuristics: assume either columns are channels (time in first col),\n",
    "    # or there is an explicit 'time' column in seconds.\n",
    "    if \"time\" in df.columns:\n",
    "        time_s = df[\"time\"].to_numpy()\n",
    "        dt = np.median(np.diff(time_s))\n",
    "        sfreq = 1.0 / dt\n",
    "        ch_names = [c for c in df.columns if c != \"time\"]\n",
    "        data = df[ch_names].to_numpy().T\n",
    "    else:\n",
    "        # Assume uniform sampling; ask user to edit if wrong.\n",
    "        # Guess sampling rate by assuming 250 Hz if not specified.\n",
    "        sfreq = cfg.target_sfreq\n",
    "        ch_names = list(df.columns)\n",
    "        data = df[ch_names].to_numpy().T\n",
    "        time_s = np.arange(data.shape[1]) / sfreq\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=[\"eeg\"] * len(ch_names))\n",
    "    raw = mne.io.RawArray(data, info, verbose=False)\n",
    "    return raw\n",
    "\n",
    "def load_raw_any(path):\n",
    "    path_low = path.lower()\n",
    "    try:\n",
    "        if path_low.endswith(\".edf\") or path_low.endswith(\".bdf\"):\n",
    "            raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "        elif path_low.endswith(\".fif\"):\n",
    "            raw = mne.io.read_raw_fif(path, preload=True, verbose=False)\n",
    "        elif path_low.endswith(\".vhdr\") or path_low.endswith(\".eeg\"):\n",
    "            raw = mne.io.read_raw_brainvision(path, preload=True, verbose=False)\n",
    "        elif path_low.endswith(\".set\") or path_low.endswith(\".fdt\"):\n",
    "            raw = mne.io.read_raw_eeglab(path, preload=True, verbose=False)\n",
    "        elif path_low.endswith(\".csv\") or path_low.endswith(\".tsv\"):\n",
    "            raw = _load_csv_tsv(path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported format\")\n",
    "        # Ensure basic channel picking across modalities\n",
    "        picks = mne.pick_types(\n",
    "            raw.info, eeg=True, meg=True, seeg=True, ecog=True, fnirs=True, exclude=\"bads\"\n",
    "        )\n",
    "        if len(picks) == 0:\n",
    "            raise RuntimeError(\"No EEG/MEG/iEEG/fNIRS channels found.\")\n",
    "        raw.pick(picks)\n",
    "        return raw\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Could not load {os.path.basename(path)}: {e}\")\n",
    "        return None\n",
    "\n",
    "def basic_clean(raw: mne.io.BaseRaw):\n",
    "    raw = raw.copy()\n",
    "    # Notch (if line noise present)\n",
    "    if cfg.notch:\n",
    "        try:\n",
    "            raw.notch_filter(cfg.notch, n_jobs=cfg.n_jobs, verbose=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Bandpass\n",
    "    raw.filter(cfg.l_freq, cfg.h_freq, n_jobs=cfg.n_jobs, verbose=False)\n",
    "    # Resample\n",
    "    if abs(raw.info[\"sfreq\"] - cfg.target_sfreq) > 1e-3:\n",
    "        raw.resample(cfg.target_sfreq, npad=\"auto\", verbose=False)\n",
    "    # Reference EEG if present\n",
    "    ch_types = set(raw.get_channel_types())\n",
    "    if \"eeg\" in ch_types:\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False)\n",
    "        raw.apply_proj()\n",
    "        # Try to set montage to improve spatial consistency (won't fail if names unfamiliar)\n",
    "        try:\n",
    "            montage = mne.channels.make_standard_montage(cfg.montage)\n",
    "            raw.set_montage(montage, match_case=False, match_alias=True, on_missing=\"warn\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return raw\n",
    "\n",
    "def make_epochs(raw: mne.io.BaseRaw):\n",
    "    dur = cfg.epoch_len_s\n",
    "    hop = cfg.step_s\n",
    "    overlap = max(0.0, dur - hop)\n",
    "    epochs = mne.make_fixed_length_epochs(\n",
    "        raw, duration=dur, overlap=overlap, preload=True, verbose=False\n",
    "    )\n",
    "    # Start times per epoch (in seconds, referenced to raw start)\n",
    "    starts = epochs.events[:, 0] / epochs.info[\"sfreq\"]\n",
    "    ends = starts + dur\n",
    "    return epochs, starts, ends\n",
    "\n",
    "# -----------------------\n",
    "# 4) Feature Extraction\n",
    "# -----------------------\n",
    "from scipy.signal import welch\n",
    "\n",
    "def band_indices(freqs, low, high):\n",
    "    return np.where((freqs >= low) & (freqs < high))[0]\n",
    "\n",
    "def hjorth_params(x):  # x: (..., n_times)\n",
    "    # Returns (activity, mobility, complexity) along last axis\n",
    "    diff1 = np.diff(x, axis=-1)\n",
    "    var0 = np.var(x, axis=-1, ddof=0) + 1e-12\n",
    "    var1 = np.var(diff1, axis=-1, ddof=0) + 1e-12\n",
    "    mobility = np.sqrt(var1 / var0)\n",
    "    diff2 = np.diff(diff1, axis=-1)\n",
    "    var2 = np.var(diff2, axis=-1, ddof=0) + 1e-12\n",
    "    complexity = np.sqrt((var2 / var1) / (var1 / var0))\n",
    "    return var0, mobility, complexity\n",
    "\n",
    "def spectral_features(epochs: mne.Epochs, bands: dict):\n",
    "    \"\"\"Compute epoch-wise features: relative band powers (+ dispersion), spectral entropy, centroid, Hjorth.\"\"\"\n",
    "    X = epochs.get_data()  # shape: (n_epochs, n_channels, n_times)\n",
    "    n_ep, n_ch, n_t = X.shape\n",
    "    sf = epochs.info[\"sfreq\"]\n",
    "    nperseg = min(int(sf * 2), n_t)  # ~2s segments (or full)\n",
    "    noverlap = nperseg // 2\n",
    "\n",
    "    # PSD via Welch\n",
    "    freqs, psd = welch(\n",
    "        X, fs=sf, nperseg=nperseg, noverlap=noverlap, axis=-1, average=\"median\"\n",
    "    )  # -> (n_ep, n_ch, n_freq)\n",
    "\n",
    "    # Power in analysis band (cfg.l_freq .. cfg.h_freq)\n",
    "    aidx = band_indices(freqs, cfg.l_freq, cfg.h_freq)\n",
    "    tot_pow = np.maximum(psd[:, :, aidx].sum(axis=-1), 1e-12)  # (n_ep, n_ch)\n",
    "\n",
    "    # Relative bandpowers and dispersion across channels\n",
    "    feats = {}\n",
    "    for band, (lo, hi) in bands.items():\n",
    "        idx = band_indices(freqs, lo, hi)\n",
    "        bp = psd[:, :, idx].sum(axis=-1) / tot_pow  # (n_ep, n_ch)\n",
    "        feats[f\"{band}_rel_med\"] = np.median(bp, axis=1)\n",
    "        feats[f\"{band}_rel_iqr\"] = np.subtract(*np.percentile(bp, [75, 25], axis=1))\n",
    "        feats[f\"{band}_rel_std\"] = np.std(bp, axis=1)\n",
    "\n",
    "    # Ratios (common EEG heuristics)\n",
    "    alpha = feats[\"alpha_rel_med\"]\n",
    "    theta = feats[\"theta_rel_med\"]\n",
    "    beta  = feats[\"beta_rel_med\"]\n",
    "    feats[\"theta_over_alpha\"] = theta / np.maximum(alpha, 1e-6)\n",
    "    feats[\"beta_over_alpha\"]  = beta  / np.maximum(alpha, 1e-6)\n",
    "    feats[\"(alpha+theta)_over_beta\"] = (alpha + theta) / np.maximum(beta, 1e-6)\n",
    "\n",
    "    # Spectral entropy (normalized) & centroid\n",
    "    p = psd[:, :, aidx]\n",
    "    p_norm = p / np.maximum(p.sum(axis=-1, keepdims=True), 1e-12)\n",
    "    H = -np.sum(p_norm * np.log2(p_norm + 1e-12), axis=-1)  # (n_ep, n_ch)\n",
    "    Hn = H / np.log2(p.shape[-1])\n",
    "    feats[\"spec_entropy_med\"] = np.median(Hn, axis=1)\n",
    "    feats[\"spec_entropy_std\"] = np.std(Hn, axis=1)\n",
    "\n",
    "    f_a = freqs[aidx].reshape(1, 1, -1)\n",
    "    centroid = (p * f_a).sum(axis=-1) / np.maximum(p.sum(axis=-1), 1e-12)\n",
    "    feats[\"spec_centroid_med\"] = np.median(centroid, axis=1)\n",
    "    feats[\"spec_centroid_std\"] = np.std(centroid, axis=1)\n",
    "\n",
    "    # Hjorth parameters on the time series\n",
    "    act, mob, comp = hjorth_params(X)\n",
    "    feats[\"hjorth_activity_med\"]   = np.median(act, axis=1)\n",
    "    feats[\"hjorth_mobility_med\"]   = np.median(mob, axis=1)\n",
    "    feats[\"hjorth_complexity_med\"] = np.median(comp, axis=1)\n",
    "\n",
    "    # Stack into DataFrame\n",
    "    feat_df = pd.DataFrame({k: v if v.ndim == 1 else v.reshape(v.shape[0], -1)\n",
    "                            for k, v in feats.items()})\n",
    "    return feat_df\n",
    "\n",
    "# -----------------------\n",
    "# 5) Modeling: Clustering + (optional) HMM\n",
    "# -----------------------\n",
    "def choose_k_and_cluster(Z, k_low, k_high, random_state):\n",
    "    best = {\"k\": None, \"sil\": -1.0, \"model\": None, \"labels\": None}\n",
    "    for k in range(k_low, k_high + 1):\n",
    "        km = KMeans(n_clusters=k, n_init=\"auto\", random_state=random_state)\n",
    "        labels = km.fit_predict(Z)\n",
    "        sil = silhouette_score(Z, labels) if k > 1 else -1.0\n",
    "        if sil > best[\"sil\"]:\n",
    "            best = {\"k\": k, \"sil\": sil, \"model\": km, \"labels\": labels}\n",
    "    return best\n",
    "\n",
    "def hmm_smooth(labels, Z, k, random_state):\n",
    "    if not HAS_HMM:\n",
    "        print(\"HMM not available; skipping temporal smoothing.\")\n",
    "        return labels\n",
    "    try:\n",
    "        hmm = GaussianHMM(\n",
    "            n_components=k, covariance_type=\"diag\",\n",
    "            random_state=random_state, n_iter=200\n",
    "        )\n",
    "        hmm.fit(Z)\n",
    "        labels_hmm = hmm.predict(Z)\n",
    "        return labels_hmm\n",
    "    except Exception as e:\n",
    "        print(f\"HMM smoothing failed: {e}\")\n",
    "        return labels\n",
    "\n",
    "def summarize_alphabet(df_feat, labels):\n",
    "    \"\"\"Return a per-state median feature table and a JSON-ready mapping.\"\"\"\n",
    "    F = df_feat.copy()\n",
    "    F[\"state\"] = labels\n",
    "    med = F.groupby(\"state\").median(numeric_only=True).sort_index()\n",
    "    # For each state, list top features (largest z-scores vs overall)\n",
    "    z = (med - df_feat.median()) / (df_feat.std() + 1e-9)\n",
    "    summary = {}\n",
    "    for s in med.index:\n",
    "        tops = z.loc[s].abs().sort_values(ascending=False).head(8).index.tolist()\n",
    "        summary[int(s)] = {\n",
    "            \"name\": f\"State-{int(s)}\",\n",
    "            \"top_features\": tops\n",
    "        }\n",
    "    return med, summary\n",
    "\n",
    "# -----------------------\n",
    "# 6) Orchestration: Run Pipeline\n",
    "# -----------------------\n",
    "def run_pipeline():\n",
    "    t0 = time.time()\n",
    "    safe_mkdir(cfg.output_dir)\n",
    "\n",
    "    # (a) Acquire files\n",
    "    dled = download_files(cfg.dataset_urls, cfg.data_dir)\n",
    "    files = find_files(cfg.data_dir, cfg.exts)\n",
    "    if cfg.max_files is not None:\n",
    "        files = files[:cfg.max_files]\n",
    "\n",
    "    if len(files) == 0:\n",
    "        print(f\"âœ± No files found in {cfg.data_dir}. Add data or URLs and re-run.\")\n",
    "        return\n",
    "\n",
    "    # (b) Loop files â†’ clean â†’ epoch â†’ features\n",
    "    all_feat = []\n",
    "    all_meta = []\n",
    "    print(f\"â€¢ Found {len(files)} file(s). Processing â€¦\")\n",
    "    for i, fp in enumerate(files, 1):\n",
    "        print(f\"[{i}/{len(files)}] {os.path.basename(fp)}\")\n",
    "        raw = load_raw_any(fp)\n",
    "        if raw is None:\n",
    "            continue\n",
    "        raw = basic_clean(raw)\n",
    "        epochs, starts, ends = make_epochs(raw)\n",
    "        if len(epochs) == 0:\n",
    "            print(\"   (no epochs)\")\n",
    "            continue\n",
    "        # Extract features\n",
    "        feat = spectral_features(epochs, BANDS)\n",
    "        # Meta\n",
    "        meta = pd.DataFrame({\n",
    "            \"file\": os.path.basename(fp),\n",
    "            \"epoch_idx\": np.arange(len(epochs)),\n",
    "            \"t_start_s\": starts,\n",
    "            \"t_end_s\": ends,\n",
    "            \"sfreq\": epochs.info[\"sfreq\"],\n",
    "            \"n_channels\": [len(epochs.ch_names)] * len(epochs)\n",
    "        })\n",
    "        all_feat.append(feat)\n",
    "        all_meta.append(meta)\n",
    "        del raw, epochs, feat, meta\n",
    "        gc.collect()\n",
    "\n",
    "    if not all_feat:\n",
    "        print(\"âœ± No usable epochs extracted. Check file formats and preprocessing settings.\")\n",
    "        return\n",
    "\n",
    "    feat_df = pd.concat(all_feat, axis=0, ignore_index=True)\n",
    "    meta_df = pd.concat(all_meta, axis=0, ignore_index=True)\n",
    "\n",
    "    # Save raw features & metadata\n",
    "    feat_path = os.path.join(cfg.output_dir, \"features.csv\")\n",
    "    meta_path = os.path.join(cfg.output_dir, \"metadata.csv\")\n",
    "    feat_df.to_csv(feat_path, index=False)\n",
    "    meta_df.to_csv(meta_path, index=False)\n",
    "    print(f\"âœ“ Saved features â†’ {feat_path}\")\n",
    "    print(f\"âœ“ Saved metadata â†’ {meta_path}\")\n",
    "\n",
    "    # (c) Scale + reduce\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(feat_df.values)\n",
    "    pca = PCA(n_components=min(20, Xs.shape[1]), random_state=cfg.random_state)\n",
    "    Z = pca.fit_transform(Xs)\n",
    "\n",
    "    # (d) Cluster (select K by silhouette)\n",
    "    best = choose_k_and_cluster(Z, cfg.k_range[0], cfg.k_range[1], cfg.random_state)\n",
    "    labels = best[\"labels\"]\n",
    "    K = best[\"k\"]\n",
    "    print(f\"â˜… Selected K={K} (silhouette={best['sil']:.3f})\")\n",
    "\n",
    "    # (e) Optional temporal smoothing via HMM\n",
    "    if cfg.use_hmm:\n",
    "        labels = hmm_smooth(labels, Z, K, cfg.random_state)\n",
    "        print(\"âœ“ Applied HMM smoothing.\")\n",
    "\n",
    "    # (f) Summarize â€œAlphabetâ€\n",
    "    centers = pd.DataFrame(best[\"model\"].cluster_centers_, columns=[f\"PC{i+1}\" for i in range(Z.shape[1])])\n",
    "    med_table, alpha_map = summarize_alphabet(feat_df, labels)\n",
    "    alpha_json_path = os.path.join(cfg.output_dir, \"cognitive_alphabet.json\")\n",
    "    with open(alpha_json_path, \"w\") as f:\n",
    "        json.dump(alpha_map, f, indent=2)\n",
    "    print(f\"âœ“ Saved alphabet map â†’ {alpha_json_path}\")\n",
    "\n",
    "    # (g) Save models\n",
    "    dump(best[\"model\"], os.path.join(cfg.output_dir, \"kmeans.joblib\"))\n",
    "    dump(pca, os.path.join(cfg.output_dir, \"pca.joblib\"))\n",
    "    dump(scaler, os.path.join(cfg.output_dir, \"scaler.joblib\"))\n",
    "\n",
    "    # (h) Visualization\n",
    "    emb2 = None\n",
    "    try:\n",
    "        if cfg.use_umap and HAS_UMAP:\n",
    "            reducer = umap.UMAP(n_components=2, random_state=cfg.random_state, n_neighbors=30, min_dist=0.1)\n",
    "            emb2 = reducer.fit_transform(Z)\n",
    "        else:\n",
    "            # Fall back to PCA 2D for visualization\n",
    "            reducer2 = PCA(n_components=2, random_state=cfg.random_state)\n",
    "            emb2 = reducer2.fit_transform(Z)\n",
    "    except Exception as e:\n",
    "        print(f\"(viz) embedding failed: {e}\")\n",
    "\n",
    "    if emb2 is not None:\n",
    "        plt.figure(figsize=(7.2, 6.2))\n",
    "        scatter = plt.scatter(emb2[:, 0], emb2[:, 1], c=labels, s=8)\n",
    "        plt.title(f\"Cognitive Alphabet (K={K})\")\n",
    "        plt.xlabel(\"Dim 1\")\n",
    "        plt.ylabel(\"Dim 2\")\n",
    "        plt.tight_layout()\n",
    "        fig_path = os.path.join(cfg.output_dir, \"embedding.png\")\n",
    "        plt.savefig(fig_path, dpi=160)\n",
    "        plt.close()\n",
    "        print(f\"âœ“ Saved embedding plot â†’ {fig_path}\")\n",
    "\n",
    "    # Feature signature heatmap\n",
    "    try:\n",
    "        plt.figure(figsize=(min(14, 2 + 0.5*len(med_table.columns)), 0.6 + 0.3*K + 2))\n",
    "        M = (med_table - feat_df.median()) / (feat_df.std() + 1e-9)  # z vs overall\n",
    "        im = plt.imshow(M.values, aspect=\"auto\")\n",
    "        plt.colorbar(im, fraction=0.025, pad=0.03, label=\"z-score vs overall\")\n",
    "        plt.yticks(range(M.shape[0]), [f\"S{int(s)}\" for s in med_table.index])\n",
    "        plt.xticks(range(M.shape[1]), M.columns, rotation=90)\n",
    "        plt.title(\"State Feature Signatures\")\n",
    "        plt.tight_layout()\n",
    "        heat_path = os.path.join(cfg.output_dir, \"state_feature_signatures.png\")\n",
    "        plt.savefig(heat_path, dpi=160)\n",
    "        plt.close()\n",
    "        print(f\"âœ“ Saved signatures heatmap â†’ {heat_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"(viz) heatmap failed: {e}\")\n",
    "\n",
    "    # (i) Save assignments\n",
    "    assign = meta_df.copy()\n",
    "    assign[\"state\"] = labels\n",
    "    assign_path = os.path.join(cfg.output_dir, \"state_assignments.csv\")\n",
    "    assign.to_csv(assign_path, index=False)\n",
    "    print(f\"âœ“ Saved state assignments â†’ {assign_path}\")\n",
    "\n",
    "    # (j) Lightweight report\n",
    "    report = {\n",
    "        \"files_processed\": len(files),\n",
    "        \"n_epochs\": int(len(assign)),\n",
    "        \"alphabet_size\": int(K),\n",
    "        \"silhouette\": float(best[\"sil\"]),\n",
    "        \"bands\": BANDS,\n",
    "        \"epoch_length_s\": cfg.epoch_len_s,\n",
    "        \"step_s\": cfg.step_s,\n",
    "        \"target_sfreq\": cfg.target_sfreq,\n",
    "        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    }\n",
    "    with open(os.path.join(cfg.output_dir, \"report.json\"), \"w\") as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    print(f\"âœ“ Saved report.json\")\n",
    "    print(f\"â± Done in {time.time() - t0:.1f}s\")\n",
    "\n",
    "# -----------------------\n",
    "# 7) Run\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸ§­ Cognitive Alphabet Finder â€” startingâ€¦\")\n",
    "    print(f\"Data directory: {cfg.data_dir}\")\n",
    "    print(f\"Output directory: {cfg.output_dir}\")\n",
    "    run_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6cfb45a-ae38-44f0-a0a6-5fbbe2b8680f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (25.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (80.9.0)\n",
      "Requirement already satisfied: wheel in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (0.45.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.2-cp313-cp313-win_amd64.whl.metadata (11 kB)\n",
      "Collecting mne\n",
      "  Using cached mne-1.10.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting umap-learn\n",
      "  Downloading umap_learn-0.5.9.post2-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting hmmlearn\n",
      "  Downloading hmmlearn-0.3.3-cp313-cp313-win_amd64.whl.metadata (3.1 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (1.16.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (3.10.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: requests in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (2.32.5)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: decorator in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from mne) (5.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from mne) (3.1.6)\n",
      "Collecting lazy-loader>=0.3 (from mne)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from mne) (25.0)\n",
      "Collecting pooch>=1.5 (from mne)\n",
      "  Using cached pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting numba>=0.51.2 (from umap-learn)\n",
      "  Using cached numba-0.62.0-cp313-cp313-win_amd64.whl.metadata (2.9 kB)\n",
      "Collecting pynndescent>=0.5 (from umap-learn)\n",
      "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from matplotlib) (4.60.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from requests) (2025.8.3)\n",
      "Collecting llvmlite<0.46,>=0.45.0dev0 (from numba>=0.51.2->umap-learn)\n",
      "  Using cached llvmlite-0.45.0-cp313-cp313-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from pooch>=1.5->mne) (4.4.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from jinja2->mne) (3.0.3)\n",
      "Using cached scikit_learn-1.7.2-cp313-cp313-win_amd64.whl (8.7 MB)\n",
      "Using cached mne-1.10.1-py3-none-any.whl (7.4 MB)\n",
      "Downloading umap_learn-0.5.9.post2-py3-none-any.whl (90 kB)\n",
      "Downloading hmmlearn-0.3.3-cp313-cp313-win_amd64.whl (127 kB)\n",
      "Using cached joblib-1.5.2-py3-none-any.whl (308 kB)\n",
      "Using cached lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Using cached numba-0.62.0-cp313-cp313-win_amd64.whl (2.7 MB)\n",
      "Using cached llvmlite-0.45.0-cp313-cp313-win_amd64.whl (37.9 MB)\n",
      "Using cached pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, llvmlite, lazy-loader, joblib, scikit-learn, pooch, numba, pynndescent, mne, hmmlearn, umap-learn\n",
      "\n",
      "   --- ------------------------------------  1/11 [llvmlite]\n",
      "   --- ------------------------------------  1/11 [llvmlite]\n",
      "   --- ------------------------------------  1/11 [llvmlite]\n",
      "   --- ------------------------------------  1/11 [llvmlite]\n",
      "   --- ------------------------------------  1/11 [llvmlite]\n",
      "   ---------- -----------------------------  3/11 [joblib]\n",
      "   ---------- -----------------------------  3/11 [joblib]\n",
      "   ---------- -----------------------------  3/11 [joblib]\n",
      "   ---------- -----------------------------  3/11 [joblib]\n",
      "   ---------- -----------------------------  3/11 [joblib]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   -------------- -------------------------  4/11 [scikit-learn]\n",
      "   ------------------ ---------------------  5/11 [pooch]\n",
      "   ------------------ ---------------------  5/11 [pooch]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   --------------------- ------------------  6/11 [numba]\n",
      "   ------------------------- --------------  7/11 [pynndescent]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   ----------------------------- ----------  8/11 [mne]\n",
      "   -------------------------------- -------  9/11 [hmmlearn]\n",
      "   -------------------------------- -------  9/11 [hmmlearn]\n",
      "   ------------------------------------ --- 10/11 [umap-learn]\n",
      "   ---------------------------------------- 11/11 [umap-learn]\n",
      "\n",
      "Successfully installed hmmlearn-0.3.3 joblib-1.5.2 lazy-loader-0.4 llvmlite-0.45.0 mne-1.10.1 numba-0.62.0 pooch-1.8.2 pynndescent-0.5.13 scikit-learn-1.7.2 threadpoolctl-3.6.0 umap-learn-0.5.9.post2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# 1) Make sure pip works in *this* kernel\n",
    "%pip install -U pip setuptools wheel\n",
    "\n",
    "# 2) Install scikit-learn (and friends, in case they were skipped)\n",
    "%pip install -U scikit-learn mne umap-learn hmmlearn joblib numpy scipy pandas matplotlib tqdm requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e01e2f3-5c71-4dd6-b4eb-40eae6694574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\cnt_genome\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.13.5 (tags/v3.13.5:6cb20a2, Jun 11 2025, 16:15:46) [MSC v.1943 64 bit (AMD64)]\n",
      "sklearn: 1.7.2\n",
      "mne: 1.10.1\n",
      "umap: 0.5.9.post2\n",
      "hmmlearn: 0.3.3\n",
      "numba: 0.62.0\n"
     ]
    }
   ],
   "source": [
    "import sys, sklearn, mne, umap, hmmlearn, numba\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"sklearn:\", sklearn.__version__)\n",
    "print(\"mne:\", mne.__version__)\n",
    "print(\"umap:\", umap.__version__)\n",
    "print(\"hmmlearn:\", hmmlearn.__version__)\n",
    "print(\"numba:\", numba.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96cc2afa-2a51-40d3-8600-3786229fdb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting jupyterlab_widgets\n",
      "  Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from ipywidgets) (9.5.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\caleb\\cnt_genome\\.venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Downloading widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --------------------------------- ------ 1.8/2.2 MB 9.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 8.0 MB/s  0:00:00\n",
      "Installing collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   ---------------------------------------- 3/3 [ipywidgets]\n",
      "\n",
      "Successfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U ipywidgets jupyterlab_widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa608b94-c95f-4177-8d3e-ed83cd692cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote ./brainwaves/sim_alpha.csv and sim_theta.csv\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWrote ./brainwaves/sim_alpha.csv and sim_theta.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# 2) Tune a couple knobs for a quick run\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mcfg\u001b[49m.data_dir = \u001b[33m\"\u001b[39m\u001b[33m./brainwaves\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     28\u001b[39m cfg.output_dir = \u001b[33m\"\u001b[39m\u001b[33m./cog_alphabet_demo\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m cfg.k_range = (\u001b[32m2\u001b[39m, \u001b[32m4\u001b[39m)      \u001b[38;5;66;03m# we expect ~2 states\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'cfg' is not defined"
     ]
    }
   ],
   "source": [
    "import os, numpy as np, pandas as pd\n",
    "\n",
    "# 1) Make tiny synthetic EEG with two \"cognitive letters\" (theta-heavy vs alpha-heavy)\n",
    "os.makedirs(\"./brainwaves\", exist_ok=True)\n",
    "sf = 250\n",
    "T = 60  # seconds\n",
    "t = np.arange(sf*T)/sf\n",
    "\n",
    "def synth(ch=4, freq=10, noise=0.2):\n",
    "    sig = []\n",
    "    for _ in range(ch):\n",
    "        sig.append(np.sin(2*np.pi*freq*t) + 0.5*np.sin(2*np.pi*(freq/2)*t) + noise*np.random.randn(len(t)))\n",
    "    return np.vstack(sig).T\n",
    "\n",
    "# write 2 files: one alpha-leaning (~10 Hz), one theta-leaning (~6 Hz)\n",
    "df_alpha = pd.DataFrame(synth(ch=6, freq=10), columns=[f\"Ch{i}\" for i in range(6)])\n",
    "df_alpha.insert(0, \"time\", t)\n",
    "df_alpha.to_csv(\"./brainwaves/sim_alpha.csv\", index=False)\n",
    "\n",
    "df_theta = pd.DataFrame(synth(ch=6, freq=6), columns=[f\"Ch{i}\" for i in range(6)])\n",
    "df_theta.insert(0, \"time\", t)\n",
    "df_theta.to_csv(\"./brainwaves/sim_theta.csv\", index=False)\n",
    "\n",
    "print(\"Wrote ./brainwaves/sim_alpha.csv and sim_theta.csv\")\n",
    "\n",
    "# 2) Tune a couple knobs for a quick run\n",
    "cfg.data_dir = \"./brainwaves\"\n",
    "cfg.output_dir = \"./cog_alphabet_demo\"\n",
    "cfg.k_range = (2, 4)      # we expect ~2 states\n",
    "cfg.use_hmm = True        # optional smoothing\n",
    "cfg.epoch_len_s = 2.0\n",
    "cfg.step_s = 0.5\n",
    "AUTO_INSTALL = False       # disable pip attempts inside the mega cell\n",
    "\n",
    "# 3) Go!\n",
    "run_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4863c038-6f4f-4f5d-8607-47bf7969d5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â· Installing scikit-learn â€¦\n"
     ]
    },
    {
     "ename": "UnsupportedOperation",
     "evalue": "fileno",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnsupportedOperation\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mÂ· Installing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m â€¦\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m             subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39m\u001b[33m-m\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minstall\u001b[39m\u001b[33m\"\u001b[39m, p], stdout=sys.stdout)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43m_ensure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnumpy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscipy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpandas\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmatplotlib\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtqdm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscikit-learn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmne\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjoblib\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mumap-learn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhmmlearn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrequests\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     26\u001b[39m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# 1) Imports (after install)\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36m_ensure\u001b[39m\u001b[34m(pkgs)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m importlib.util.find_spec(mod) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mÂ· Installing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m â€¦\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-m\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpip\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstall\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py:414\u001b[39m, in \u001b[36mcheck_call\u001b[39m\u001b[34m(*popenargs, **kwargs)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_call\u001b[39m(*popenargs, **kwargs):\n\u001b[32m    405\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Run command with arguments.  Wait for command to complete.  If\u001b[39;00m\n\u001b[32m    406\u001b[39m \u001b[33;03m    the exit code was zero then return, otherwise raise\u001b[39;00m\n\u001b[32m    407\u001b[39m \u001b[33;03m    CalledProcessError.  The CalledProcessError object will have the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    412\u001b[39m \u001b[33;03m    check_call([\"ls\", \"-l\"])\u001b[39;00m\n\u001b[32m    413\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m     retcode = \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    415\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m retcode:\n\u001b[32m    416\u001b[39m         cmd = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33margs\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py:395\u001b[39m, in \u001b[36mcall\u001b[39m\u001b[34m(timeout, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall\u001b[39m(*popenargs, timeout=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    388\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Run command with arguments.  Wait for command to complete or\u001b[39;00m\n\u001b[32m    389\u001b[39m \u001b[33;03m    for timeout seconds, then return the returncode attribute.\u001b[39;00m\n\u001b[32m    390\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    393\u001b[39m \u001b[33;03m    retcode = call([\"ls\", \"-l\"])\u001b[39;00m\n\u001b[32m    394\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[32m    396\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    397\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m p.wait(timeout=timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py:1005\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[39m\n\u001b[32m    986\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUser ID cannot be negative, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muid\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    988\u001b[39m \u001b[38;5;66;03m# Input and output objects. The general principle is like\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[38;5;66;03m# this:\u001b[39;00m\n\u001b[32m    990\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1000\u001b[39m \u001b[38;5;66;03m# are -1 when not using PIPEs. The child objects are -1\u001b[39;00m\n\u001b[32m   1001\u001b[39m \u001b[38;5;66;03m# when not redirecting.\u001b[39;00m\n\u001b[32m   1003\u001b[39m (p2cread, p2cwrite,\n\u001b[32m   1004\u001b[39m  c2pread, c2pwrite,\n\u001b[32m-> \u001b[39m\u001b[32m1005\u001b[39m  errread, errwrite) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstdin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[38;5;66;03m# From here on, raising exceptions may cause file descriptor leakage\u001b[39;00m\n\u001b[32m   1008\u001b[39m \n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# We wrap OS handles *before* launching the child, otherwise a\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# quickly terminating child could make our fds unwrappable\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# (see #8458).\u001b[39;00m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _mswindows:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py:1400\u001b[39m, in \u001b[36mPopen._get_handles\u001b[39m\u001b[34m(self, stdin, stdout, stderr)\u001b[39m\n\u001b[32m   1397\u001b[39m     c2pwrite = msvcrt.get_osfhandle(stdout)\n\u001b[32m   1398\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1399\u001b[39m     \u001b[38;5;66;03m# Assuming file-like object\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1400\u001b[39m     c2pwrite = msvcrt.get_osfhandle(\u001b[43mstdout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfileno\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1401\u001b[39m c2pwrite = \u001b[38;5;28mself\u001b[39m._make_inheritable(c2pwrite)\n\u001b[32m   1403\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stderr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\cnt_genome\\.venv\\Lib\\site-packages\\ipykernel\\iostream.py:371\u001b[39m, in \u001b[36mOutStream.fileno\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    369\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._original_stdstream_copy\n\u001b[32m    370\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mfileno\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m io.UnsupportedOperation(msg)\n",
      "\u001b[31mUnsupportedOperation\u001b[39m: fileno"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ðŸ§  Cognitive Alphabet Finder â€” One Mega Cell\n",
    "# Paste this cell in JupyterLab and run.\n",
    "# ============================================\n",
    "\n",
    "import os, sys, math, json, time, glob, gc, warnings, subprocess, importlib.util\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# -----------------------\n",
    "# 0) (Optional) Auto-Install Dependencies\n",
    "# -----------------------\n",
    "AUTO_INSTALL = True  # set False if you manage packages yourself\n",
    "\n",
    "def _ensure(pkgs):\n",
    "    if not AUTO_INSTALL:\n",
    "        return\n",
    "    for p in pkgs:\n",
    "        mod = p.split(\"==\")[0].split(\"[\")[0]\n",
    "        if importlib.util.find_spec(mod) is None:\n",
    "            print(f\"Â· Installing {p} â€¦\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", p], stdout=sys.stdout)\n",
    "\n",
    "_ensure([\n",
    "    \"numpy\", \"scipy\", \"pandas\", \"matplotlib\", \"tqdm\",\n",
    "    \"scikit-learn\", \"mne\", \"joblib\", \"umap-learn\", \"hmmlearn\", \"requests\"\n",
    "])\n",
    "\n",
    "# -----------------------\n",
    "# 1) Imports (after install)\n",
    "# -----------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from joblib import dump\n",
    "import requests\n",
    "import mne\n",
    "\n",
    "try:\n",
    "    import umap\n",
    "    HAS_UMAP = True\n",
    "except Exception:\n",
    "    HAS_UMAP = False\n",
    "\n",
    "try:\n",
    "    from hmmlearn.hmm import GaussianHMM\n",
    "    HAS_HMM = True\n",
    "except Exception:\n",
    "    HAS_HMM = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# -----------------------\n",
    "# 2) Configuration\n",
    "# -----------------------\n",
    "BANDS = {\n",
    "    \"delta\": (1.0, 4.0),\n",
    "    \"theta\": (4.0, 8.0),\n",
    "    \"alpha\": (8.0, 13.0),\n",
    "    \"beta\":  (13.0, 30.0),\n",
    "    \"gamma\": (30.0, 45.0),  # conservative gamma upper bound for typical EEG\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Data locations\n",
    "    data_dir: str = \"./brainwaves\"\n",
    "    output_dir: str = \"./cog_alphabet\"\n",
    "\n",
    "    # Either provide URLs here or just drop files into data_dir\n",
    "    dataset_urls: list = field(default_factory=lambda: [\n",
    "        # Examples (put your own links here):\n",
    "        # \"https://example.org/path/to/subject01.edf\",\n",
    "        # \"https://example.org/path/to/recording.fif\",\n",
    "        # \"https://example.org/path/to/session.vhdr\",\n",
    "        # \"https://example.org/path/to/export.csv\",\n",
    "    ])\n",
    "\n",
    "    # File types to consider\n",
    "    exts: tuple = (\".edf\", \".bdf\", \".fif\", \".vhdr\", \".eeg\", \".set\", \".fdt\", \".csv\", \".tsv\")\n",
    "\n",
    "    # Preprocessing\n",
    "    target_sfreq: float = 250.0      # resample here for speed/consistency\n",
    "    notch: list = field(default_factory=lambda: [50.0, 60.0])  # one or both; removed if not needed\n",
    "    l_freq: float = 0.5              # highpass\n",
    "    h_freq: float = 45.0             # lowpass\n",
    "    montage: str = \"standard_1020\"   # applied where EEG channel names match\n",
    "\n",
    "    # Epoching\n",
    "    epoch_len_s: float = 2.0         # window length (s)\n",
    "    step_s: float = 0.5              # hop (s) ; overlap = epoch_len - step\n",
    "\n",
    "    # Modeling\n",
    "    k_range: tuple = (3, 9)          # search K in [low, high]\n",
    "    use_umap: bool = True            # for 2-D viz\n",
    "    use_hmm: bool = False            # refine sequence with HMM temporal smoothing\n",
    "    random_state: int = 42\n",
    "    n_jobs: int = 1                  # MNE filters can use n_jobs\n",
    "\n",
    "    # Housekeeping\n",
    "    max_files: int = None            # limit for quick tests\n",
    "    verbose: bool = True\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# -----------------------\n",
    "# 3) Utilities\n",
    "# -----------------------\n",
    "def safe_mkdir(p):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def download_files(urls, dest):\n",
    "    if not urls:\n",
    "        return []\n",
    "    safe_mkdir(dest)\n",
    "    paths = []\n",
    "    for url in urls:\n",
    "        fname = os.path.join(dest, url.split(\"/\")[-1].split(\"?\")[0])\n",
    "        if os.path.exists(fname) and os.path.getsize(fname) > 0:\n",
    "            print(f\"âœ“ Exists: {fname}\")\n",
    "            paths.append(fname)\n",
    "            continue\n",
    "        print(f\"â†“ Downloading: {url}\")\n",
    "        try:\n",
    "            with requests.get(url, stream=True, timeout=60) as r:\n",
    "                r.raise_for_status()\n",
    "                total = int(r.headers.get(\"content-length\", 0))\n",
    "                with open(fname, \"wb\") as f, tqdm(\n",
    "                    total=total, unit=\"B\", unit_scale=True, desc=os.path.basename(fname)\n",
    "                ) as bar:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk); bar.update(len(chunk))\n",
    "            paths.append(fname)\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed: {url} ({e})\")\n",
    "    return paths\n",
    "\n",
    "def find_files(root, exts):\n",
    "    files = []\n",
    "    for ext in exts:\n",
    "        files.extend(glob.glob(os.path.join(root, f\"**/*{ext}\"), recursive=True))\n",
    "    return sorted(list(dict.fromkeys(files)))\n",
    "\n",
    "def _load_csv_tsv(path):\n",
    "    df = pd.read_csv(path) if path.endswith(\".csv\") else pd.read_csv(path, sep=\"\\t\")\n",
    "    # Heuristics: assume either columns are channels (time in first col),\n",
    "    # or there is an explicit 'time' column in seconds.\n",
    "    if \"time\" in df.columns:\n",
    "        time_s = df[\"time\"].to_numpy()\n",
    "        dt = np.median(np.diff(time_s))\n",
    "        sfreq = 1.0 / dt\n",
    "        ch_names = [c for c in df.columns if c != \"time\"]\n",
    "        data = df[ch_names].to_numpy().T\n",
    "    else:\n",
    "        # Assume uniform sampling; ask user to edit if wrong.\n",
    "        # Guess sampling rate by assuming 250 Hz if not specified.\n",
    "        sfreq = cfg.target_sfreq\n",
    "        ch_names = list(df.columns)\n",
    "        data = df[ch_names].to_numpy().T\n",
    "        time_s = np.arange(data.shape[1]) / sfreq\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=[\"eeg\"] * len(ch_names))\n",
    "    raw = mne.io.RawArray(data, info, verbose=False)\n",
    "    return raw\n",
    "\n",
    "def load_raw_any(path):\n",
    "    path_low = path.lower()\n",
    "    try:\n",
    "        if path_low.endswith(\".edf\") or path_low.endswith(\".bdf\"):\n",
    "            raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "        elif path_low.endswith(\".fif\"):\n",
    "            raw = mne.io.read_raw_fif(path, preload=True, verbose=False)\n",
    "        elif path_low.endswith(\".vhdr\") or path_low.endswith(\".eeg\"):\n",
    "            raw = mne.io.read_raw_brainvision(path, preload=True, verbose=False)\n",
    "        elif path_low.endswith(\".set\") or path_low.endswith(\".fdt\"):\n",
    "            raw = mne.io.read_raw_eeglab(path, preload=True, verbose=False)\n",
    "        elif path_low.endswith(\".csv\") or path_low.endswith(\".tsv\"):\n",
    "            raw = _load_csv_tsv(path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported format\")\n",
    "        # Ensure basic channel picking across modalities\n",
    "        picks = mne.pick_types(\n",
    "            raw.info, eeg=True, meg=True, seeg=True, ecog=True, fnirs=True, exclude=\"bads\"\n",
    "        )\n",
    "        if len(picks) == 0:\n",
    "            raise RuntimeError(\"No EEG/MEG/iEEG/fNIRS channels found.\")\n",
    "        raw.pick(picks)\n",
    "        return raw\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Could not load {os.path.basename(path)}: {e}\")\n",
    "        return None\n",
    "\n",
    "def basic_clean(raw: mne.io.BaseRaw):\n",
    "    raw = raw.copy()\n",
    "    # Notch (if line noise present)\n",
    "    if cfg.notch:\n",
    "        try:\n",
    "            raw.notch_filter(cfg.notch, n_jobs=cfg.n_jobs, verbose=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Bandpass\n",
    "    raw.filter(cfg.l_freq, cfg.h_freq, n_jobs=cfg.n_jobs, verbose=False)\n",
    "    # Resample\n",
    "    if abs(raw.info[\"sfreq\"] - cfg.target_sfreq) > 1e-3:\n",
    "        raw.resample(cfg.target_sfreq, npad=\"auto\", verbose=False)\n",
    "    # Reference EEG if present\n",
    "    ch_types = set(raw.get_channel_types())\n",
    "    if \"eeg\" in ch_types:\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False)\n",
    "        raw.apply_proj()\n",
    "        # Try to set montage to improve spatial consistency (won't fail if names unfamiliar)\n",
    "        try:\n",
    "            montage = mne.channels.make_standard_montage(cfg.montage)\n",
    "            raw.set_montage(montage, match_case=False, match_alias=True, on_missing=\"warn\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return raw\n",
    "\n",
    "def make_epochs(raw: mne.io.BaseRaw):\n",
    "    dur = cfg.epoch_len_s\n",
    "    hop = cfg.step_s\n",
    "    overlap = max(0.0, dur - hop)\n",
    "    epochs = mne.make_fixed_length_epochs(\n",
    "        raw, duration=dur, overlap=overlap, preload=True, verbose=False\n",
    "    )\n",
    "    # Start times per epoch (in seconds, referenced to raw start)\n",
    "    starts = epochs.events[:, 0] / epochs.info[\"sfreq\"]\n",
    "    ends = starts + dur\n",
    "    return epochs, starts, ends\n",
    "\n",
    "# -----------------------\n",
    "# 4) Feature Extraction\n",
    "# -----------------------\n",
    "from scipy.signal import welch\n",
    "\n",
    "def band_indices(freqs, low, high):\n",
    "    return np.where((freqs >= low) & (freqs < high))[0]\n",
    "\n",
    "def hjorth_params(x):  # x: (..., n_times)\n",
    "    # Returns (activity, mobility, complexity) along last axis\n",
    "    diff1 = np.diff(x, axis=-1)\n",
    "    var0 = np.var(x, axis=-1, ddof=0) + 1e-12\n",
    "    var1 = np.var(diff1, axis=-1, ddof=0) + 1e-12\n",
    "    mobility = np.sqrt(var1 / var0)\n",
    "    diff2 = np.diff(diff1, axis=-1)\n",
    "    var2 = np.var(diff2, axis=-1, ddof=0) + 1e-12\n",
    "    complexity = np.sqrt((var2 / var1) / (var1 / var0))\n",
    "    return var0, mobility, complexity\n",
    "\n",
    "def spectral_features(epochs: mne.Epochs, bands: dict):\n",
    "    \"\"\"Compute epoch-wise features: relative band powers (+ dispersion), spectral entropy, centroid, Hjorth.\"\"\"\n",
    "    X = epochs.get_data()  # shape: (n_epochs, n_channels, n_times)\n",
    "    n_ep, n_ch, n_t = X.shape\n",
    "    sf = epochs.info[\"sfreq\"]\n",
    "    nperseg = min(int(sf * 2), n_t)  # ~2s segments (or full)\n",
    "    noverlap = nperseg // 2\n",
    "\n",
    "    # PSD via Welch\n",
    "    freqs, psd = welch(\n",
    "        X, fs=sf, nperseg=nperseg, noverlap=noverlap, axis=-1, average=\"median\"\n",
    "    )  # -> (n_ep, n_ch, n_freq)\n",
    "\n",
    "    # Power in analysis band (cfg.l_freq .. cfg.h_freq)\n",
    "    aidx = band_indices(freqs, cfg.l_freq, cfg.h_freq)\n",
    "    tot_pow = np.maximum(psd[:, :, aidx].sum(axis=-1), 1e-12)  # (n_ep, n_ch)\n",
    "\n",
    "    # Relative bandpowers and dispersion across channels\n",
    "    feats = {}\n",
    "    for band, (lo, hi) in bands.items():\n",
    "        idx = band_indices(freqs, lo, hi)\n",
    "        bp = psd[:, :, idx].sum(axis=-1) / tot_pow  # (n_ep, n_ch)\n",
    "        feats[f\"{band}_rel_med\"] = np.median(bp, axis=1)\n",
    "        feats[f\"{band}_rel_iqr\"] = np.subtract(*np.percentile(bp, [75, 25], axis=1))\n",
    "        feats[f\"{band}_rel_std\"] = np.std(bp, axis=1)\n",
    "\n",
    "    # Ratios (common EEG heuristics)\n",
    "    alpha = feats[\"alpha_rel_med\"]\n",
    "    theta = feats[\"theta_rel_med\"]\n",
    "    beta  = feats[\"beta_rel_med\"]\n",
    "    feats[\"theta_over_alpha\"] = theta / np.maximum(alpha, 1e-6)\n",
    "    feats[\"beta_over_alpha\"]  = beta  / np.maximum(alpha, 1e-6)\n",
    "    feats[\"(alpha+theta)_over_beta\"] = (alpha + theta) / np.maximum(beta, 1e-6)\n",
    "\n",
    "    # Spectral entropy (normalized) & centroid\n",
    "    p = psd[:, :, aidx]\n",
    "    p_norm = p / np.maximum(p.sum(axis=-1, keepdims=True), 1e-12)\n",
    "    H = -np.sum(p_norm * np.log2(p_norm + 1e-12), axis=-1)  # (n_ep, n_ch)\n",
    "    Hn = H / np.log2(p.shape[-1])\n",
    "    feats[\"spec_entropy_med\"] = np.median(Hn, axis=1)\n",
    "    feats[\"spec_entropy_std\"] = np.std(Hn, axis=1)\n",
    "\n",
    "    f_a = freqs[aidx].reshape(1, 1, -1)\n",
    "    centroid = (p * f_a).sum(axis=-1) / np.maximum(p.sum(axis=-1), 1e-12)\n",
    "    feats[\"spec_centroid_med\"] = np.median(centroid, axis=1)\n",
    "    feats[\"spec_centroid_std\"] = np.std(centroid, axis=1)\n",
    "\n",
    "    # Hjorth parameters on the time series\n",
    "    act, mob, comp = hjorth_params(X)\n",
    "    feats[\"hjorth_activity_med\"]   = np.median(act, axis=1)\n",
    "    feats[\"hjorth_mobility_med\"]   = np.median(mob, axis=1)\n",
    "    feats[\"hjorth_complexity_med\"] = np.median(comp, axis=1)\n",
    "\n",
    "    # Stack into DataFrame\n",
    "    feat_df = pd.DataFrame({k: v if v.ndim == 1 else v.reshape(v.shape[0], -1)\n",
    "                            for k, v in feats.items()})\n",
    "    return feat_df\n",
    "\n",
    "# -----------------------\n",
    "# 5) Modeling: Clustering + (optional) HMM\n",
    "# -----------------------\n",
    "def choose_k_and_cluster(Z, k_low, k_high, random_state):\n",
    "    best = {\"k\": None, \"sil\": -1.0, \"model\": None, \"labels\": None}\n",
    "    for k in range(k_low, k_high + 1):\n",
    "        km = KMeans(n_clusters=k, n_init=\"auto\", random_state=random_state)\n",
    "        labels = km.fit_predict(Z)\n",
    "        sil = silhouette_score(Z, labels) if k > 1 else -1.0\n",
    "        if sil > best[\"sil\"]:\n",
    "            best = {\"k\": k, \"sil\": sil, \"model\": km, \"labels\": labels}\n",
    "    return best\n",
    "\n",
    "def hmm_smooth(labels, Z, k, random_state):\n",
    "    if not HAS_HMM:\n",
    "        print(\"HMM not available; skipping temporal smoothing.\")\n",
    "        return labels\n",
    "    try:\n",
    "        hmm = GaussianHMM(\n",
    "            n_components=k, covariance_type=\"diag\",\n",
    "            random_state=random_state, n_iter=200\n",
    "        )\n",
    "        hmm.fit(Z)\n",
    "        labels_hmm = hmm.predict(Z)\n",
    "        return labels_hmm\n",
    "    except Exception as e:\n",
    "        print(f\"HMM smoothing failed: {e}\")\n",
    "        return labels\n",
    "\n",
    "def summarize_alphabet(df_feat, labels):\n",
    "    \"\"\"Return a per-state median feature table and a JSON-ready mapping.\"\"\"\n",
    "    F = df_feat.copy()\n",
    "    F[\"state\"] = labels\n",
    "    med = F.groupby(\"state\").median(numeric_only=True).sort_index()\n",
    "    # For each state, list top features (largest z-scores vs overall)\n",
    "    z = (med - df_feat.median()) / (df_feat.std() + 1e-9)\n",
    "    summary = {}\n",
    "    for s in med.index:\n",
    "        tops = z.loc[s].abs().sort_values(ascending=False).head(8).index.tolist()\n",
    "        summary[int(s)] = {\n",
    "            \"name\": f\"State-{int(s)}\",\n",
    "            \"top_features\": tops\n",
    "        }\n",
    "    return med, summary\n",
    "\n",
    "# -----------------------\n",
    "# 6) Orchestration: Run Pipeline\n",
    "# -----------------------\n",
    "def run_pipeline():\n",
    "    t0 = time.time()\n",
    "    safe_mkdir(cfg.output_dir)\n",
    "\n",
    "    # (a) Acquire files\n",
    "    dled = download_files(cfg.dataset_urls, cfg.data_dir)\n",
    "    files = find_files(cfg.data_dir, cfg.exts)\n",
    "    if cfg.max_files is not None:\n",
    "        files = files[:cfg.max_files]\n",
    "\n",
    "    if len(files) == 0:\n",
    "        print(f\"âœ± No files found in {cfg.data_dir}. Add data or URLs and re-run.\")\n",
    "        return\n",
    "\n",
    "    # (b) Loop files â†’ clean â†’ epoch â†’ features\n",
    "    all_feat = []\n",
    "    all_meta = []\n",
    "    print(f\"â€¢ Found {len(files)} file(s). Processing â€¦\")\n",
    "    for i, fp in enumerate(files, 1):\n",
    "        print(f\"[{i}/{len(files)}] {os.path.basename(fp)}\")\n",
    "        raw = load_raw_any(fp)\n",
    "        if raw is None:\n",
    "            continue\n",
    "        raw = basic_clean(raw)\n",
    "        epochs, starts, ends = make_epochs(raw)\n",
    "        if len(epochs) == 0:\n",
    "            print(\"   (no epochs)\")\n",
    "            continue\n",
    "        # Extract features\n",
    "        feat = spectral_features(epochs, BANDS)\n",
    "        # Meta\n",
    "        meta = pd.DataFrame({\n",
    "            \"file\": os.path.basename(fp),\n",
    "            \"epoch_idx\": np.arange(len(epochs)),\n",
    "            \"t_start_s\": starts,\n",
    "            \"t_end_s\": ends,\n",
    "            \"sfreq\": epochs.info[\"sfreq\"],\n",
    "            \"n_channels\": [len(epochs.ch_names)] * len(epochs)\n",
    "        })\n",
    "        all_feat.append(feat)\n",
    "        all_meta.append(meta)\n",
    "        del raw, epochs, feat, meta\n",
    "        gc.collect()\n",
    "\n",
    "    if not all_feat:\n",
    "        print(\"âœ± No usable epochs extracted. Check file formats and preprocessing settings.\")\n",
    "        return\n",
    "\n",
    "    feat_df = pd.concat(all_feat, axis=0, ignore_index=True)\n",
    "    meta_df = pd.concat(all_meta, axis=0, ignore_index=True)\n",
    "\n",
    "    # Save raw features & metadata\n",
    "    feat_path = os.path.join(cfg.output_dir, \"features.csv\")\n",
    "    meta_path = os.path.join(cfg.output_dir, \"metadata.csv\")\n",
    "    feat_df.to_csv(feat_path, index=False)\n",
    "    meta_df.to_csv(meta_path, index=False)\n",
    "    print(f\"âœ“ Saved features â†’ {feat_path}\")\n",
    "    print(f\"âœ“ Saved metadata â†’ {meta_path}\")\n",
    "\n",
    "    # (c) Scale + reduce\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(feat_df.values)\n",
    "    pca = PCA(n_components=min(20, Xs.shape[1]), random_state=cfg.random_state)\n",
    "    Z = pca.fit_transform(Xs)\n",
    "\n",
    "    # (d) Cluster (select K by silhouette)\n",
    "    best = choose_k_and_cluster(Z, cfg.k_range[0], cfg.k_range[1], cfg.random_state)\n",
    "    labels = best[\"labels\"]\n",
    "    K = best[\"k\"]\n",
    "    print(f\"â˜… Selected K={K} (silhouette={best['sil']:.3f})\")\n",
    "\n",
    "    # (e) Optional temporal smoothing via HMM\n",
    "    if cfg.use_hmm:\n",
    "        labels = hmm_smooth(labels, Z, K, cfg.random_state)\n",
    "        print(\"âœ“ Applied HMM smoothing.\")\n",
    "\n",
    "    # (f) Summarize â€œAlphabetâ€\n",
    "    centers = pd.DataFrame(best[\"model\"].cluster_centers_, columns=[f\"PC{i+1}\" for i in range(Z.shape[1])])\n",
    "    med_table, alpha_map = summarize_alphabet(feat_df, labels)\n",
    "    alpha_json_path = os.path.join(cfg.output_dir, \"cognitive_alphabet.json\")\n",
    "    with open(alpha_json_path, \"w\") as f:\n",
    "        json.dump(alpha_map, f, indent=2)\n",
    "    print(f\"âœ“ Saved alphabet map â†’ {alpha_json_path}\")\n",
    "\n",
    "    # (g) Save models\n",
    "    dump(best[\"model\"], os.path.join(cfg.output_dir, \"kmeans.joblib\"))\n",
    "    dump(pca, os.path.join(cfg.output_dir, \"pca.joblib\"))\n",
    "    dump(scaler, os.path.join(cfg.output_dir, \"scaler.joblib\"))\n",
    "\n",
    "    # (h) Visualization\n",
    "    emb2 = None\n",
    "    try:\n",
    "        if cfg.use_umap and HAS_UMAP:\n",
    "            reducer = umap.UMAP(n_components=2, random_state=cfg.random_state, n_neighbors=30, min_dist=0.1)\n",
    "            emb2 = reducer.fit_transform(Z)\n",
    "        else:\n",
    "            # Fall back to PCA 2D for visualization\n",
    "            reducer2 = PCA(n_components=2, random_state=cfg.random_state)\n",
    "            emb2 = reducer2.fit_transform(Z)\n",
    "    except Exception as e:\n",
    "        print(f\"(viz) embedding failed: {e}\")\n",
    "\n",
    "    if emb2 is not None:\n",
    "        plt.figure(figsize=(7.2, 6.2))\n",
    "        scatter = plt.scatter(emb2[:, 0], emb2[:, 1], c=labels, s=8)\n",
    "        plt.title(f\"Cognitive Alphabet (K={K})\")\n",
    "        plt.xlabel(\"Dim 1\")\n",
    "        plt.ylabel(\"Dim 2\")\n",
    "        plt.tight_layout()\n",
    "        fig_path = os.path.join(cfg.output_dir, \"embedding.png\")\n",
    "        plt.savefig(fig_path, dpi=160)\n",
    "        plt.close()\n",
    "        print(f\"âœ“ Saved embedding plot â†’ {fig_path}\")\n",
    "\n",
    "    # Feature signature heatmap\n",
    "    try:\n",
    "        plt.figure(figsize=(min(14, 2 + 0.5*len(med_table.columns)), 0.6 + 0.3*K + 2))\n",
    "        M = (med_table - feat_df.median()) / (feat_df.std() + 1e-9)  # z vs overall\n",
    "        im = plt.imshow(M.values, aspect=\"auto\")\n",
    "        plt.colorbar(im, fraction=0.025, pad=0.03, label=\"z-score vs overall\")\n",
    "        plt.yticks(range(M.shape[0]), [f\"S{int(s)}\" for s in med_table.index])\n",
    "        plt.xticks(range(M.shape[1]), M.columns, rotation=90)\n",
    "        plt.title(\"State Feature Signatures\")\n",
    "        plt.tight_layout()\n",
    "        heat_path = os.path.join(cfg.output_dir, \"state_feature_signatures.png\")\n",
    "        plt.savefig(heat_path, dpi=160)\n",
    "        plt.close()\n",
    "        print(f\"âœ“ Saved signatures heatmap â†’ {heat_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"(viz) heatmap failed: {e}\")\n",
    "\n",
    "    # (i) Save assignments\n",
    "    assign = meta_df.copy()\n",
    "    assign[\"state\"] = labels\n",
    "    assign_path = os.path.join(cfg.output_dir, \"state_assignments.csv\")\n",
    "    assign.to_csv(assign_path, index=False)\n",
    "    print(f\"âœ“ Saved state assignments â†’ {assign_path}\")\n",
    "\n",
    "    # (j) Lightweight report\n",
    "    report = {\n",
    "        \"files_processed\": len(files),\n",
    "        \"n_epochs\": int(len(assign)),\n",
    "        \"alphabet_size\": int(K),\n",
    "        \"silhouette\": float(best[\"sil\"]),\n",
    "        \"bands\": BANDS,\n",
    "        \"epoch_length_s\": cfg.epoch_len_s,\n",
    "        \"step_s\": cfg.step_s,\n",
    "        \"target_sfreq\": cfg.target_sfreq,\n",
    "        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    }\n",
    "    with open(os.path.join(cfg.output_dir, \"report.json\"), \"w\") as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    print(f\"âœ“ Saved report.json\")\n",
    "    print(f\"â± Done in {time.time() - t0:.1f}s\")\n",
    "\n",
    "# -----------------------\n",
    "# 7) Run\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸ§­ Cognitive Alphabet Finder â€” startingâ€¦\")\n",
    "    print(f\"Data directory: {cfg.data_dir}\")\n",
    "    print(f\"Output directory: {cfg.output_dir}\")\n",
    "    run_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8224c77f-a800-43e3-9ab3-0dfd0bc4f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable the auto-installer for this session\n",
    "AUTO_INSTALL = False\n",
    "def _ensure(pkgs):\n",
    "    print(\"Skipping auto-install (packages already installed).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c355670d-1833-45d5-a0fd-f745c2c02f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â· Installing scikit-learn â€¦\n"
     ]
    },
    {
     "ename": "UnsupportedOperation",
     "evalue": "fileno",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnsupportedOperation\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mÂ· Installing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m â€¦\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m             subprocess.check_call([sys.executable, \u001b[33m\"\u001b[39m\u001b[33m-m\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minstall\u001b[39m\u001b[33m\"\u001b[39m, p], stdout=sys.stdout)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43m_ensure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnumpy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscipy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpandas\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmatplotlib\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtqdm\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mscikit-learn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmne\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mjoblib\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mumap-learn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhmmlearn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrequests\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     26\u001b[39m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# 1) Imports (after install)\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# -----------------------\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36m_ensure\u001b[39m\u001b[34m(pkgs)\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m importlib.util.find_spec(mod) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mÂ· Installing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m â€¦\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m     \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-m\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpip\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstall\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m=\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py:414\u001b[39m, in \u001b[36mcheck_call\u001b[39m\u001b[34m(*popenargs, **kwargs)\u001b[39m\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_call\u001b[39m(*popenargs, **kwargs):\n\u001b[32m    405\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Run command with arguments.  Wait for command to complete.  If\u001b[39;00m\n\u001b[32m    406\u001b[39m \u001b[33;03m    the exit code was zero then return, otherwise raise\u001b[39;00m\n\u001b[32m    407\u001b[39m \u001b[33;03m    CalledProcessError.  The CalledProcessError object will have the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    412\u001b[39m \u001b[33;03m    check_call([\"ls\", \"-l\"])\u001b[39;00m\n\u001b[32m    413\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m     retcode = \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    415\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m retcode:\n\u001b[32m    416\u001b[39m         cmd = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33margs\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py:395\u001b[39m, in \u001b[36mcall\u001b[39m\u001b[34m(timeout, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall\u001b[39m(*popenargs, timeout=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m    388\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Run command with arguments.  Wait for command to complete or\u001b[39;00m\n\u001b[32m    389\u001b[39m \u001b[33;03m    for timeout seconds, then return the returncode attribute.\u001b[39;00m\n\u001b[32m    390\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    393\u001b[39m \u001b[33;03m    retcode = call([\"ls\", \"-l\"])\u001b[39;00m\n\u001b[32m    394\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpopenargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m p:\n\u001b[32m    396\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    397\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m p.wait(timeout=timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py:1005\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[39m\n\u001b[32m    986\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUser ID cannot be negative, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muid\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    988\u001b[39m \u001b[38;5;66;03m# Input and output objects. The general principle is like\u001b[39;00m\n\u001b[32m    989\u001b[39m \u001b[38;5;66;03m# this:\u001b[39;00m\n\u001b[32m    990\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1000\u001b[39m \u001b[38;5;66;03m# are -1 when not using PIPEs. The child objects are -1\u001b[39;00m\n\u001b[32m   1001\u001b[39m \u001b[38;5;66;03m# when not redirecting.\u001b[39;00m\n\u001b[32m   1003\u001b[39m (p2cread, p2cwrite,\n\u001b[32m   1004\u001b[39m  c2pread, c2pwrite,\n\u001b[32m-> \u001b[39m\u001b[32m1005\u001b[39m  errread, errwrite) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstdin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[38;5;66;03m# From here on, raising exceptions may cause file descriptor leakage\u001b[39;00m\n\u001b[32m   1008\u001b[39m \n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# We wrap OS handles *before* launching the child, otherwise a\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# quickly terminating child could make our fds unwrappable\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# (see #8458).\u001b[39;00m\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _mswindows:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\subprocess.py:1400\u001b[39m, in \u001b[36mPopen._get_handles\u001b[39m\u001b[34m(self, stdin, stdout, stderr)\u001b[39m\n\u001b[32m   1397\u001b[39m     c2pwrite = msvcrt.get_osfhandle(stdout)\n\u001b[32m   1398\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1399\u001b[39m     \u001b[38;5;66;03m# Assuming file-like object\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1400\u001b[39m     c2pwrite = msvcrt.get_osfhandle(\u001b[43mstdout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfileno\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1401\u001b[39m c2pwrite = \u001b[38;5;28mself\u001b[39m._make_inheritable(c2pwrite)\n\u001b[32m   1403\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stderr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\cnt_genome\\.venv\\Lib\\site-packages\\ipykernel\\iostream.py:371\u001b[39m, in \u001b[36mOutStream.fileno\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    369\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._original_stdstream_copy\n\u001b[32m    370\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mfileno\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m371\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m io.UnsupportedOperation(msg)\n",
      "\u001b[31mUnsupportedOperation\u001b[39m: fileno"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ðŸ§  Cognitive Alphabet Finder â€” One Mega Cell\n",
    "# Paste this cell in JupyterLab and run.\n",
    "# ============================================\n",
    "\n",
    "import os, sys, math, json, time, glob, gc, warnings, subprocess, importlib.util\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "# -----------------------\n",
    "# 0) (Optional) Auto-Install Dependencies\n",
    "# -----------------------\n",
    "AUTO_INSTALL = True  # set False if you manage packages yourself\n",
    "\n",
    "def _ensure(pkgs):\n",
    "    if not AUTO_INSTALL:\n",
    "        return\n",
    "    for p in pkgs:\n",
    "        mod = p.split(\"==\")[0].split(\"[\")[0]\n",
    "        if importlib.util.find_spec(mod) is None:\n",
    "            print(f\"Â· Installing {p} â€¦\")\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", p], stdout=sys.stdout)\n",
    "\n",
    "_ensure([\n",
    "    \"numpy\", \"scipy\", \"pandas\", \"matplotlib\", \"tqdm\",\n",
    "    \"scikit-learn\", \"mne\", \"joblib\", \"umap-learn\", \"hmmlearn\", \"requests\"\n",
    "])\n",
    "\n",
    "# -----------------------\n",
    "# 1) Imports (after install)\n",
    "# -----------------------\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from joblib import dump\n",
    "import requests\n",
    "import mne\n",
    "\n",
    "try:\n",
    "    import umap\n",
    "    HAS_UMAP = True\n",
    "except Exception:\n",
    "    HAS_UMAP = False\n",
    "\n",
    "try:\n",
    "    from hmmlearn.hmm import GaussianHMM\n",
    "    HAS_HMM = True\n",
    "except Exception:\n",
    "    HAS_HMM = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# -----------------------\n",
    "# 2) Configuration\n",
    "# -----------------------\n",
    "BANDS = {\n",
    "    \"delta\": (1.0, 4.0),\n",
    "    \"theta\": (4.0, 8.0),\n",
    "    \"alpha\": (8.0, 13.0),\n",
    "    \"beta\":  (13.0, 30.0),\n",
    "    \"gamma\": (30.0, 45.0),  # conservative gamma upper bound for typical EEG\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Data locations\n",
    "    data_dir: str = \"./brainwaves\"\n",
    "    output_dir: str = \"./cog_alphabet\"\n",
    "\n",
    "    # Either provide URLs here or just drop files into data_dir\n",
    "    dataset_urls: list = field(default_factory=lambda: [\n",
    "        # Examples (put your own links here):\n",
    "        # \"https://example.org/path/to/subject01.edf\",\n",
    "        # \"https://example.org/path/to/recording.fif\",\n",
    "        # \"https://example.org/path/to/session.vhdr\",\n",
    "        # \"https://example.org/path/to/export.csv\",\n",
    "    ])\n",
    "\n",
    "    # File types to consider\n",
    "    exts: tuple = (\".edf\", \".bdf\", \".fif\", \".vhdr\", \".eeg\", \".set\", \".fdt\", \".csv\", \".tsv\")\n",
    "\n",
    "    # Preprocessing\n",
    "    target_sfreq: float = 250.0      # resample here for speed/consistency\n",
    "    notch: list = field(default_factory=lambda: [50.0, 60.0])  # one or both; removed if not needed\n",
    "    l_freq: float = 0.5              # highpass\n",
    "    h_freq: float = 45.0             # lowpass\n",
    "    montage: str = \"standard_1020\"   # applied where EEG channel names match\n",
    "\n",
    "    # Epoching\n",
    "    epoch_len_s: float = 2.0         # window length (s)\n",
    "    step_s: float = 0.5              # hop (s) ; overlap = epoch_len - step\n",
    "\n",
    "    # Modeling\n",
    "    k_range: tuple = (3, 9)          # search K in [low, high]\n",
    "    use_umap: bool = True            # for 2-D viz\n",
    "    use_hmm: bool = False            # refine sequence with HMM temporal smoothing\n",
    "    random_state: int = 42\n",
    "    n_jobs: int = 1                  # MNE filters can use n_jobs\n",
    "\n",
    "    # Housekeeping\n",
    "    max_files: int = None            # limit for quick tests\n",
    "    verbose: bool = True\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# -----------------------\n",
    "# 3) Utilities\n",
    "# -----------------------\n",
    "def safe_mkdir(p):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def download_files(urls, dest):\n",
    "    if not urls:\n",
    "        return []\n",
    "    safe_mkdir(dest)\n",
    "    paths = []\n",
    "    for url in urls:\n",
    "        fname = os.path.join(dest, url.split(\"/\")[-1].split(\"?\")[0])\n",
    "        if os.path.exists(fname) and os.path.getsize(fname) > 0:\n",
    "            print(f\"âœ“ Exists: {fname}\")\n",
    "            paths.append(fname)\n",
    "            continue\n",
    "        print(f\"â†“ Downloading: {url}\")\n",
    "        try:\n",
    "            with requests.get(url, stream=True, timeout=60) as r:\n",
    "                r.raise_for_status()\n",
    "                total = int(r.headers.get(\"content-length\", 0))\n",
    "                with open(fname, \"wb\") as f, tqdm(\n",
    "                    total=total, unit=\"B\", unit_scale=True, desc=os.path.basename(fname)\n",
    "                ) as bar:\n",
    "                    for chunk in r.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            f.write(chunk); bar.update(len(chunk))\n",
    "            paths.append(fname)\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed: {url} ({e})\")\n",
    "    return paths\n",
    "\n",
    "def find_files(root, exts):\n",
    "    files = []\n",
    "    for ext in exts:\n",
    "        files.extend(glob.glob(os.path.join(root, f\"**/*{ext}\"), recursive=True))\n",
    "    return sorted(list(dict.fromkeys(files)))\n",
    "\n",
    "def _load_csv_tsv(path):\n",
    "    df = pd.read_csv(path) if path.endswith(\".csv\") else pd.read_csv(path, sep=\"\\t\")\n",
    "    # Heuristics: assume either columns are channels (time in first col),\n",
    "    # or there is an explicit 'time' column in seconds.\n",
    "    if \"time\" in df.columns:\n",
    "        time_s = df[\"time\"].to_numpy()\n",
    "        dt = np.median(np.diff(time_s))\n",
    "        sfreq = 1.0 / dt\n",
    "        ch_names = [c for c in df.columns if c != \"time\"]\n",
    "        data = df[ch_names].to_numpy().T\n",
    "    else:\n",
    "        # Assume uniform sampling; ask user to edit if wrong.\n",
    "        # Guess sampling rate by assuming 250 Hz if not specified.\n",
    "        sfreq = cfg.target_sfreq\n",
    "        ch_names = list(df.columns)\n",
    "        data = df[ch_names].to_numpy().T\n",
    "        time_s = np.arange(data.shape[1]) / sfreq\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sfreq, ch_types=[\"eeg\"] * len(ch_names))\n",
    "    raw = mne.io.RawArray(data, info, verbose=False)\n",
    "    return raw\n",
    "\n",
    "def load_raw_any(path):\n",
    "    path_low = path.lower()\n",
    "    try:\n",
    "        if path_low.endswith(\".edf\") or path_low.endswith(\".bdf\"):\n",
    "            raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "        elif path_low.endswith(\".fif\"):\n",
    "            raw = mne.io.read_raw_fif(path, preload=True, verbose=False)\n",
    "        elif path_low.endswith(\".vhdr\") or path_low.endswith(\".eeg\"):\n",
    "            raw = mne.io.read_raw_brainvision(path, preload=True, verbose=False)\n",
    "        elif path_low.endswith(\".set\") or path_low.endswith(\".fdt\"):\n",
    "            raw = mne.io.read_raw_eeglab(path, preload=True, verbose=False)\n",
    "        elif path_low.endswith(\".csv\") or path_low.endswith(\".tsv\"):\n",
    "            raw = _load_csv_tsv(path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported format\")\n",
    "        # Ensure basic channel picking across modalities\n",
    "        picks = mne.pick_types(\n",
    "            raw.info, eeg=True, meg=True, seeg=True, ecog=True, fnirs=True, exclude=\"bads\"\n",
    "        )\n",
    "        if len(picks) == 0:\n",
    "            raise RuntimeError(\"No EEG/MEG/iEEG/fNIRS channels found.\")\n",
    "        raw.pick(picks)\n",
    "        return raw\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Could not load {os.path.basename(path)}: {e}\")\n",
    "        return None\n",
    "\n",
    "def basic_clean(raw: mne.io.BaseRaw):\n",
    "    raw = raw.copy()\n",
    "    # Notch (if line noise present)\n",
    "    if cfg.notch:\n",
    "        try:\n",
    "            raw.notch_filter(cfg.notch, n_jobs=cfg.n_jobs, verbose=False)\n",
    "        except Exception:\n",
    "            pass\n",
    "    # Bandpass\n",
    "    raw.filter(cfg.l_freq, cfg.h_freq, n_jobs=cfg.n_jobs, verbose=False)\n",
    "    # Resample\n",
    "    if abs(raw.info[\"sfreq\"] - cfg.target_sfreq) > 1e-3:\n",
    "        raw.resample(cfg.target_sfreq, npad=\"auto\", verbose=False)\n",
    "    # Reference EEG if present\n",
    "    ch_types = set(raw.get_channel_types())\n",
    "    if \"eeg\" in ch_types:\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False)\n",
    "        raw.apply_proj()\n",
    "        # Try to set montage to improve spatial consistency (won't fail if names unfamiliar)\n",
    "        try:\n",
    "            montage = mne.channels.make_standard_montage(cfg.montage)\n",
    "            raw.set_montage(montage, match_case=False, match_alias=True, on_missing=\"warn\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return raw\n",
    "\n",
    "def make_epochs(raw: mne.io.BaseRaw):\n",
    "    dur = cfg.epoch_len_s\n",
    "    hop = cfg.step_s\n",
    "    overlap = max(0.0, dur - hop)\n",
    "    epochs = mne.make_fixed_length_epochs(\n",
    "        raw, duration=dur, overlap=overlap, preload=True, verbose=False\n",
    "    )\n",
    "    # Start times per epoch (in seconds, referenced to raw start)\n",
    "    starts = epochs.events[:, 0] / epochs.info[\"sfreq\"]\n",
    "    ends = starts + dur\n",
    "    return epochs, starts, ends\n",
    "\n",
    "# -----------------------\n",
    "# 4) Feature Extraction\n",
    "# -----------------------\n",
    "from scipy.signal import welch\n",
    "\n",
    "def band_indices(freqs, low, high):\n",
    "    return np.where((freqs >= low) & (freqs < high))[0]\n",
    "\n",
    "def hjorth_params(x):  # x: (..., n_times)\n",
    "    # Returns (activity, mobility, complexity) along last axis\n",
    "    diff1 = np.diff(x, axis=-1)\n",
    "    var0 = np.var(x, axis=-1, ddof=0) + 1e-12\n",
    "    var1 = np.var(diff1, axis=-1, ddof=0) + 1e-12\n",
    "    mobility = np.sqrt(var1 / var0)\n",
    "    diff2 = np.diff(diff1, axis=-1)\n",
    "    var2 = np.var(diff2, axis=-1, ddof=0) + 1e-12\n",
    "    complexity = np.sqrt((var2 / var1) / (var1 / var0))\n",
    "    return var0, mobility, complexity\n",
    "\n",
    "def spectral_features(epochs: mne.Epochs, bands: dict):\n",
    "    \"\"\"Compute epoch-wise features: relative band powers (+ dispersion), spectral entropy, centroid, Hjorth.\"\"\"\n",
    "    X = epochs.get_data()  # shape: (n_epochs, n_channels, n_times)\n",
    "    n_ep, n_ch, n_t = X.shape\n",
    "    sf = epochs.info[\"sfreq\"]\n",
    "    nperseg = min(int(sf * 2), n_t)  # ~2s segments (or full)\n",
    "    noverlap = nperseg // 2\n",
    "\n",
    "    # PSD via Welch\n",
    "    freqs, psd = welch(\n",
    "        X, fs=sf, nperseg=nperseg, noverlap=noverlap, axis=-1, average=\"median\"\n",
    "    )  # -> (n_ep, n_ch, n_freq)\n",
    "\n",
    "    # Power in analysis band (cfg.l_freq .. cfg.h_freq)\n",
    "    aidx = band_indices(freqs, cfg.l_freq, cfg.h_freq)\n",
    "    tot_pow = np.maximum(psd[:, :, aidx].sum(axis=-1), 1e-12)  # (n_ep, n_ch)\n",
    "\n",
    "    # Relative bandpowers and dispersion across channels\n",
    "    feats = {}\n",
    "    for band, (lo, hi) in bands.items():\n",
    "        idx = band_indices(freqs, lo, hi)\n",
    "        bp = psd[:, :, idx].sum(axis=-1) / tot_pow  # (n_ep, n_ch)\n",
    "        feats[f\"{band}_rel_med\"] = np.median(bp, axis=1)\n",
    "        feats[f\"{band}_rel_iqr\"] = np.subtract(*np.percentile(bp, [75, 25], axis=1))\n",
    "        feats[f\"{band}_rel_std\"] = np.std(bp, axis=1)\n",
    "\n",
    "    # Ratios (common EEG heuristics)\n",
    "    alpha = feats[\"alpha_rel_med\"]\n",
    "    theta = feats[\"theta_rel_med\"]\n",
    "    beta  = feats[\"beta_rel_med\"]\n",
    "    feats[\"theta_over_alpha\"] = theta / np.maximum(alpha, 1e-6)\n",
    "    feats[\"beta_over_alpha\"]  = beta  / np.maximum(alpha, 1e-6)\n",
    "    feats[\"(alpha+theta)_over_beta\"] = (alpha + theta) / np.maximum(beta, 1e-6)\n",
    "\n",
    "    # Spectral entropy (normalized) & centroid\n",
    "    p = psd[:, :, aidx]\n",
    "    p_norm = p / np.maximum(p.sum(axis=-1, keepdims=True), 1e-12)\n",
    "    H = -np.sum(p_norm * np.log2(p_norm + 1e-12), axis=-1)  # (n_ep, n_ch)\n",
    "    Hn = H / np.log2(p.shape[-1])\n",
    "    feats[\"spec_entropy_med\"] = np.median(Hn, axis=1)\n",
    "    feats[\"spec_entropy_std\"] = np.std(Hn, axis=1)\n",
    "\n",
    "    f_a = freqs[aidx].reshape(1, 1, -1)\n",
    "    centroid = (p * f_a).sum(axis=-1) / np.maximum(p.sum(axis=-1), 1e-12)\n",
    "    feats[\"spec_centroid_med\"] = np.median(centroid, axis=1)\n",
    "    feats[\"spec_centroid_std\"] = np.std(centroid, axis=1)\n",
    "\n",
    "    # Hjorth parameters on the time series\n",
    "    act, mob, comp = hjorth_params(X)\n",
    "    feats[\"hjorth_activity_med\"]   = np.median(act, axis=1)\n",
    "    feats[\"hjorth_mobility_med\"]   = np.median(mob, axis=1)\n",
    "    feats[\"hjorth_complexity_med\"] = np.median(comp, axis=1)\n",
    "\n",
    "    # Stack into DataFrame\n",
    "    feat_df = pd.DataFrame({k: v if v.ndim == 1 else v.reshape(v.shape[0], -1)\n",
    "                            for k, v in feats.items()})\n",
    "    return feat_df\n",
    "\n",
    "# -----------------------\n",
    "# 5) Modeling: Clustering + (optional) HMM\n",
    "# -----------------------\n",
    "def choose_k_and_cluster(Z, k_low, k_high, random_state):\n",
    "    best = {\"k\": None, \"sil\": -1.0, \"model\": None, \"labels\": None}\n",
    "    for k in range(k_low, k_high + 1):\n",
    "        km = KMeans(n_clusters=k, n_init=\"auto\", random_state=random_state)\n",
    "        labels = km.fit_predict(Z)\n",
    "        sil = silhouette_score(Z, labels) if k > 1 else -1.0\n",
    "        if sil > best[\"sil\"]:\n",
    "            best = {\"k\": k, \"sil\": sil, \"model\": km, \"labels\": labels}\n",
    "    return best\n",
    "\n",
    "def hmm_smooth(labels, Z, k, random_state):\n",
    "    if not HAS_HMM:\n",
    "        print(\"HMM not available; skipping temporal smoothing.\")\n",
    "        return labels\n",
    "    try:\n",
    "        hmm = GaussianHMM(\n",
    "            n_components=k, covariance_type=\"diag\",\n",
    "            random_state=random_state, n_iter=200\n",
    "        )\n",
    "        hmm.fit(Z)\n",
    "        labels_hmm = hmm.predict(Z)\n",
    "        return labels_hmm\n",
    "    except Exception as e:\n",
    "        print(f\"HMM smoothing failed: {e}\")\n",
    "        return labels\n",
    "\n",
    "def summarize_alphabet(df_feat, labels):\n",
    "    \"\"\"Return a per-state median feature table and a JSON-ready mapping.\"\"\"\n",
    "    F = df_feat.copy()\n",
    "    F[\"state\"] = labels\n",
    "    med = F.groupby(\"state\").median(numeric_only=True).sort_index()\n",
    "    # For each state, list top features (largest z-scores vs overall)\n",
    "    z = (med - df_feat.median()) / (df_feat.std() + 1e-9)\n",
    "    summary = {}\n",
    "    for s in med.index:\n",
    "        tops = z.loc[s].abs().sort_values(ascending=False).head(8).index.tolist()\n",
    "        summary[int(s)] = {\n",
    "            \"name\": f\"State-{int(s)}\",\n",
    "            \"top_features\": tops\n",
    "        }\n",
    "    return med, summary\n",
    "\n",
    "# -----------------------\n",
    "# 6) Orchestration: Run Pipeline\n",
    "# -----------------------\n",
    "def run_pipeline():\n",
    "    t0 = time.time()\n",
    "    safe_mkdir(cfg.output_dir)\n",
    "\n",
    "    # (a) Acquire files\n",
    "    dled = download_files(cfg.dataset_urls, cfg.data_dir)\n",
    "    files = find_files(cfg.data_dir, cfg.exts)\n",
    "    if cfg.max_files is not None:\n",
    "        files = files[:cfg.max_files]\n",
    "\n",
    "    if len(files) == 0:\n",
    "        print(f\"âœ± No files found in {cfg.data_dir}. Add data or URLs and re-run.\")\n",
    "        return\n",
    "\n",
    "    # (b) Loop files â†’ clean â†’ epoch â†’ features\n",
    "    all_feat = []\n",
    "    all_meta = []\n",
    "    print(f\"â€¢ Found {len(files)} file(s). Processing â€¦\")\n",
    "    for i, fp in enumerate(files, 1):\n",
    "        print(f\"[{i}/{len(files)}] {os.path.basename(fp)}\")\n",
    "        raw = load_raw_any(fp)\n",
    "        if raw is None:\n",
    "            continue\n",
    "        raw = basic_clean(raw)\n",
    "        epochs, starts, ends = make_epochs(raw)\n",
    "        if len(epochs) == 0:\n",
    "            print(\"   (no epochs)\")\n",
    "            continue\n",
    "        # Extract features\n",
    "        feat = spectral_features(epochs, BANDS)\n",
    "        # Meta\n",
    "        meta = pd.DataFrame({\n",
    "            \"file\": os.path.basename(fp),\n",
    "            \"epoch_idx\": np.arange(len(epochs)),\n",
    "            \"t_start_s\": starts,\n",
    "            \"t_end_s\": ends,\n",
    "            \"sfreq\": epochs.info[\"sfreq\"],\n",
    "            \"n_channels\": [len(epochs.ch_names)] * len(epochs)\n",
    "        })\n",
    "        all_feat.append(feat)\n",
    "        all_meta.append(meta)\n",
    "        del raw, epochs, feat, meta\n",
    "        gc.collect()\n",
    "\n",
    "    if not all_feat:\n",
    "        print(\"âœ± No usable epochs extracted. Check file formats and preprocessing settings.\")\n",
    "        return\n",
    "\n",
    "    feat_df = pd.concat(all_feat, axis=0, ignore_index=True)\n",
    "    meta_df = pd.concat(all_meta, axis=0, ignore_index=True)\n",
    "\n",
    "    # Save raw features & metadata\n",
    "    feat_path = os.path.join(cfg.output_dir, \"features.csv\")\n",
    "    meta_path = os.path.join(cfg.output_dir, \"metadata.csv\")\n",
    "    feat_df.to_csv(feat_path, index=False)\n",
    "    meta_df.to_csv(meta_path, index=False)\n",
    "    print(f\"âœ“ Saved features â†’ {feat_path}\")\n",
    "    print(f\"âœ“ Saved metadata â†’ {meta_path}\")\n",
    "\n",
    "    # (c) Scale + reduce\n",
    "    scaler = StandardScaler()\n",
    "    Xs = scaler.fit_transform(feat_df.values)\n",
    "    pca = PCA(n_components=min(20, Xs.shape[1]), random_state=cfg.random_state)\n",
    "    Z = pca.fit_transform(Xs)\n",
    "\n",
    "    # (d) Cluster (select K by silhouette)\n",
    "    best = choose_k_and_cluster(Z, cfg.k_range[0], cfg.k_range[1], cfg.random_state)\n",
    "    labels = best[\"labels\"]\n",
    "    K = best[\"k\"]\n",
    "    print(f\"â˜… Selected K={K} (silhouette={best['sil']:.3f})\")\n",
    "\n",
    "    # (e) Optional temporal smoothing via HMM\n",
    "    if cfg.use_hmm:\n",
    "        labels = hmm_smooth(labels, Z, K, cfg.random_state)\n",
    "        print(\"âœ“ Applied HMM smoothing.\")\n",
    "\n",
    "    # (f) Summarize â€œAlphabetâ€\n",
    "    centers = pd.DataFrame(best[\"model\"].cluster_centers_, columns=[f\"PC{i+1}\" for i in range(Z.shape[1])])\n",
    "    med_table, alpha_map = summarize_alphabet(feat_df, labels)\n",
    "    alpha_json_path = os.path.join(cfg.output_dir, \"cognitive_alphabet.json\")\n",
    "    with open(alpha_json_path, \"w\") as f:\n",
    "        json.dump(alpha_map, f, indent=2)\n",
    "    print(f\"âœ“ Saved alphabet map â†’ {alpha_json_path}\")\n",
    "\n",
    "    # (g) Save models\n",
    "    dump(best[\"model\"], os.path.join(cfg.output_dir, \"kmeans.joblib\"))\n",
    "    dump(pca, os.path.join(cfg.output_dir, \"pca.joblib\"))\n",
    "    dump(scaler, os.path.join(cfg.output_dir, \"scaler.joblib\"))\n",
    "\n",
    "    # (h) Visualization\n",
    "    emb2 = None\n",
    "    try:\n",
    "        if cfg.use_umap and HAS_UMAP:\n",
    "            reducer = umap.UMAP(n_components=2, random_state=cfg.random_state, n_neighbors=30, min_dist=0.1)\n",
    "            emb2 = reducer.fit_transform(Z)\n",
    "        else:\n",
    "            # Fall back to PCA 2D for visualization\n",
    "            reducer2 = PCA(n_components=2, random_state=cfg.random_state)\n",
    "            emb2 = reducer2.fit_transform(Z)\n",
    "    except Exception as e:\n",
    "        print(f\"(viz) embedding failed: {e}\")\n",
    "\n",
    "    if emb2 is not None:\n",
    "        plt.figure(figsize=(7.2, 6.2))\n",
    "        scatter = plt.scatter(emb2[:, 0], emb2[:, 1], c=labels, s=8)\n",
    "        plt.title(f\"Cognitive Alphabet (K={K})\")\n",
    "        plt.xlabel(\"Dim 1\")\n",
    "        plt.ylabel(\"Dim 2\")\n",
    "        plt.tight_layout()\n",
    "        fig_path = os.path.join(cfg.output_dir, \"embedding.png\")\n",
    "        plt.savefig(fig_path, dpi=160)\n",
    "        plt.close()\n",
    "        print(f\"âœ“ Saved embedding plot â†’ {fig_path}\")\n",
    "\n",
    "    # Feature signature heatmap\n",
    "    try:\n",
    "        plt.figure(figsize=(min(14, 2 + 0.5*len(med_table.columns)), 0.6 + 0.3*K + 2))\n",
    "        M = (med_table - feat_df.median()) / (feat_df.std() + 1e-9)  # z vs overall\n",
    "        im = plt.imshow(M.values, aspect=\"auto\")\n",
    "        plt.colorbar(im, fraction=0.025, pad=0.03, label=\"z-score vs overall\")\n",
    "        plt.yticks(range(M.shape[0]), [f\"S{int(s)}\" for s in med_table.index])\n",
    "        plt.xticks(range(M.shape[1]), M.columns, rotation=90)\n",
    "        plt.title(\"State Feature Signatures\")\n",
    "        plt.tight_layout()\n",
    "        heat_path = os.path.join(cfg.output_dir, \"state_feature_signatures.png\")\n",
    "        plt.savefig(heat_path, dpi=160)\n",
    "        plt.close()\n",
    "        print(f\"âœ“ Saved signatures heatmap â†’ {heat_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"(viz) heatmap failed: {e}\")\n",
    "\n",
    "    # (i) Save assignments\n",
    "    assign = meta_df.copy()\n",
    "    assign[\"state\"] = labels\n",
    "    assign_path = os.path.join(cfg.output_dir, \"state_assignments.csv\")\n",
    "    assign.to_csv(assign_path, index=False)\n",
    "    print(f\"âœ“ Saved state assignments â†’ {assign_path}\")\n",
    "\n",
    "    # (j) Lightweight report\n",
    "    report = {\n",
    "        \"files_processed\": len(files),\n",
    "        \"n_epochs\": int(len(assign)),\n",
    "        \"alphabet_size\": int(K),\n",
    "        \"silhouette\": float(best[\"sil\"]),\n",
    "        \"bands\": BANDS,\n",
    "        \"epoch_length_s\": cfg.epoch_len_s,\n",
    "        \"step_s\": cfg.step_s,\n",
    "        \"target_sfreq\": cfg.target_sfreq,\n",
    "        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    }\n",
    "    with open(os.path.join(cfg.output_dir, \"report.json\"), \"w\") as f:\n",
    "        json.dump(report, f, indent=2)\n",
    "    print(f\"âœ“ Saved report.json\")\n",
    "    print(f\"â± Done in {time.time() - t0:.1f}s\")\n",
    "\n",
    "# -----------------------\n",
    "# 7) Run\n",
    "# -----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸ§­ Cognitive Alphabet Finder â€” startingâ€¦\")\n",
    "    print(f\"Data directory: {cfg.data_dir}\")\n",
    "    print(f\"Output directory: {cfg.output_dir}\")\n",
    "    run_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "982a2ded-3f66-43f8-a3e8-e5f0387c536b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§­ Cognitive Alphabet Finder â€” ready.\n",
      "Data dir: ./brainwaves | Output: ./cog_alphabet\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# ðŸ§  Cognitive Alphabet Finder â€” Mega Cell (No auto-pip, widget-free, Win/Jupyter-safe)\n",
    "# ============================================\n",
    "import os, sys, json, time, glob, gc, math, warnings, importlib.util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")  # headless-safe for Jupyter/Windows\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- TQDM in plain-text mode (no ipywidgets) ----\n",
    "os.environ[\"TQDM_NOTEBOOK\"] = \"0\"\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except Exception:\n",
    "    class tqdm:  # tiny fallback\n",
    "        def __init__(self, iterable=None, total=None, **kw): self.it=iterable or range(total or 0)\n",
    "        def __iter__(self): return iter(self.it)\n",
    "        def update(self, *a, **k): pass\n",
    "        def close(self): pass\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# ---- Optional URL downloads (no widgets, no fancy stdout) ----\n",
    "try:\n",
    "    import requests\n",
    "except Exception:\n",
    "    requests = None\n",
    "\n",
    "# ---- Try MNE (formats/IO/filters); fail clearly if missing ----\n",
    "if importlib.util.find_spec(\"mne\") is None:\n",
    "    raise ImportError(\"mne is required. In a notebook cell, run: %pip install mne\")\n",
    "import mne\n",
    "\n",
    "# ---- Try sklearn; otherwise provide NumPy shims (StandardScaler, PCA, KMeans, silhouette) ----\n",
    "_SK_OK = importlib.util.find_spec(\"sklearn\") is not None\n",
    "if _SK_OK:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn.metrics import silhouette_score\n",
    "else:\n",
    "    # Lightweight shims\n",
    "    print(\"âš ï¸  scikit-learn not found â€” using NumPy shims for StandardScaler, PCA, KMeans, silhouette_score.\")\n",
    "    class StandardScaler:\n",
    "        def __init__(self, with_mean=True, with_std=True): self.with_mean=with_mean; self.with_std=with_std\n",
    "        def fit(self, X):\n",
    "            X = np.asarray(X, float); self.mean_ = X.mean(0) if self.with_mean else np.zeros(X.shape[1])\n",
    "            s = X.std(0, ddof=0) if self.with_std else np.ones(X.shape[1]); self.scale_ = np.where(s==0,1.0,s); return self\n",
    "        def transform(self, X): X = np.asarray(X, float); return (X - self.mean_) / self.scale_\n",
    "        def fit_transform(self, X): return self.fit(X).transform(X)\n",
    "    class PCA:\n",
    "        def __init__(self, n_components=None, random_state=None): self.n_components=n_components\n",
    "        def fit(self, X):\n",
    "            X = np.asarray(X, float); Xm = X - X.mean(0, keepdims=True)\n",
    "            U,S,Vt = np.linalg.svd(Xm, full_matrices=False); self.components_ = Vt; return self\n",
    "        def transform(self, X):\n",
    "            X = np.asarray(X, float); Xm = X - X.mean(0, keepdims=True)\n",
    "            k = self.n_components or self.components_.shape[0]; return Xm @ self.components_[:k].T\n",
    "        def fit_transform(self, X): self.fit(X); return self.transform(X)\n",
    "    class KMeans:\n",
    "        def __init__(self, n_clusters=8, n_init=\"auto\", max_iter=300, tol=1e-4, random_state=None):\n",
    "            self.n_clusters=int(n_clusters); self.max_iter=max_iter; self.tol=tol; self.random_state=random_state or 42\n",
    "        def fit(self, X):\n",
    "            X = np.asarray(X, float); rng = np.random.default_rng(self.random_state)\n",
    "            # k-means++ init\n",
    "            C = [X[rng.integers(0,len(X))]]\n",
    "            for _ in range(1,self.n_clusters):\n",
    "                d2 = np.min(((X[:,None,:]-np.array(C)[None,:,:])**2).sum(2), axis=1)\n",
    "                probs = d2/(d2.sum()+1e-12); C.append(X[rng.choice(len(X), p=probs)])\n",
    "            C = np.array(C)\n",
    "            for _ in range(self.max_iter):\n",
    "                D2 = ((X[:,None,:]-C[None,:,:])**2).sum(2); labels = D2.argmin(1)\n",
    "                C_new = np.stack([X[labels==j].mean(0) if np.any(labels==j) else C[j] for j in range(self.n_clusters)],0)\n",
    "                if np.linalg.norm(C_new - C) < self.tol: C = C_new; break\n",
    "                C = C_new\n",
    "            self.cluster_centers_, self.labels_ = C, labels; return self\n",
    "        def fit_predict(self, X): return self.fit(X).labels_\n",
    "    def silhouette_score(X, labels):\n",
    "        X = np.asarray(X,float); L = np.asarray(labels); n=len(X)\n",
    "        D = np.sqrt(((X[:,None,:]-X[None,:,:])**2).sum(2))\n",
    "        s = []\n",
    "        for i in range(n):\n",
    "            Li = L[i]; same = L==Li; other = L!=Li\n",
    "            a = D[i, same & (np.arange(n)!=i)]\n",
    "            a = a.mean() if a.size else 0.0\n",
    "            b = np.inf\n",
    "            for Lj in np.unique(L[other]):\n",
    "                b = min(b, D[i, L==Lj].mean())\n",
    "            s.append((b-a)/max(a,b) if max(a,b)>0 else 0.0)\n",
    "        return float(np.mean(s))\n",
    "\n",
    "# ---- Optional HMM smoothing if hmmlearn is available ----\n",
    "_HAS_HMM = importlib.util.find_spec(\"hmmlearn\") is not None\n",
    "if _HAS_HMM:\n",
    "    from hmmlearn.hmm import GaussianHMM\n",
    "\n",
    "# =======================\n",
    "# CONFIG (edit here)\n",
    "# =======================\n",
    "class CFG:\n",
    "    # Files: either put local files under data_dir, or put URLs in dataset_urls\n",
    "    data_dir   = \"./brainwaves\"\n",
    "    output_dir = \"./cog_alphabet\"\n",
    "    dataset_urls = [\n",
    "        # Example: \"https://example.org/subj01.edf\",\n",
    "    ]\n",
    "    exts = (\".edf\", \".bdf\", \".fif\", \".vhdr\", \".eeg\", \".set\", \".fdt\", \".csv\", \".tsv\")\n",
    "\n",
    "    # Preprocessing / Epoching\n",
    "    target_sfreq = 250.0\n",
    "    notch = [50.0, 60.0]      # will try both; if fails, it skips\n",
    "    l_freq, h_freq = 0.5, 45.0\n",
    "    montage = \"standard_1020\"\n",
    "    epoch_len_s, step_s = 2.0, 0.5\n",
    "\n",
    "    # Modeling\n",
    "    bands = {\n",
    "        \"delta\": (1.0, 4.0),\n",
    "        \"theta\": (4.0, 8.0),\n",
    "        \"alpha\": (8.0, 13.0),\n",
    "        \"beta\":  (13.0, 30.0),\n",
    "        \"gamma\": (30.0, 45.0),\n",
    "    }\n",
    "    k_range = (3, 9)\n",
    "    use_hmm = False\n",
    "    random_state = 42\n",
    "\n",
    "    # Misc\n",
    "    max_files = None\n",
    "    verbose = True\n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "# =======================\n",
    "# UTILITIES\n",
    "# =======================\n",
    "def _mkdir(p): os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def _download_all(urls, dest):\n",
    "    if not urls: return []\n",
    "    if requests is None:\n",
    "        print(\"requests not installed; skipping downloads.\"); return []\n",
    "    _mkdir(dest); paths=[]\n",
    "    for url in urls:\n",
    "        fname = os.path.join(dest, url.split(\"/\")[-1].split(\"?\")[0])\n",
    "        if os.path.exists(fname) and os.path.getsize(fname)>0:\n",
    "            print(f\"âœ“ Exists: {fname}\"); paths.append(fname); continue\n",
    "        print(f\"â†“ Downloading: {url}\")\n",
    "        try:\n",
    "            r = requests.get(url, stream=True, timeout=60); r.raise_for_status()\n",
    "            with open(fname, \"wb\") as f:\n",
    "                for chunk in r.iter_content(8192):\n",
    "                    if chunk: f.write(chunk)\n",
    "            paths.append(fname)\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed: {url} ({e})\")\n",
    "    return paths\n",
    "\n",
    "def _find_files(root, exts):\n",
    "    out=[]\n",
    "    for ext in exts: out += glob.glob(os.path.join(root, f\"**/*{ext}\"), recursive=True)\n",
    "    # de-dup while preserving order\n",
    "    seen=set(); uniq=[]\n",
    "    for p in out:\n",
    "        if p not in seen: uniq.append(p); seen.add(p)\n",
    "    return uniq\n",
    "\n",
    "def _load_csv_or_tsv(path):\n",
    "    df = pd.read_csv(path) if path.endswith(\".csv\") else pd.read_csv(path, sep=\"\\t\")\n",
    "    if \"time\" in df.columns:\n",
    "        t = df[\"time\"].to_numpy(); dt = np.median(np.diff(t)); sfreq = 1.0/max(dt, 1e-12)\n",
    "        ch_names=[c for c in df.columns if c!=\"time\"]; data = df[ch_names].to_numpy().T\n",
    "    else:\n",
    "        sfreq = cfg.target_sfreq\n",
    "        ch_names=list(df.columns); data=df[ch_names].to_numpy().T\n",
    "    info = mne.create_info(ch_names, sfreq, ch_types=[\"eeg\"]*len(ch_names))\n",
    "    raw = mne.io.RawArray(data, info, verbose=False)\n",
    "    return raw\n",
    "\n",
    "def load_raw_any(path):\n",
    "    p = path.lower()\n",
    "    try:\n",
    "        if p.endswith(\".edf\") or p.endswith(\".bdf\"): raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "        elif p.endswith(\".fif\"): raw = mne.io.read_raw_fif(path, preload=True, verbose=False)\n",
    "        elif p.endswith(\".vhdr\") or p.endswith(\".eeg\"): raw = mne.io.read_raw_brainvision(path, preload=True, verbose=False)\n",
    "        elif p.endswith(\".set\") or p.endswith(\".fdt\"): raw = mne.io.read_raw_eeglab(path, preload=True, verbose=False)\n",
    "        elif p.endswith(\".csv\") or p.endswith(\".tsv\"): raw = _load_csv_or_tsv(path)\n",
    "        else: raise ValueError(\"Unsupported format\")\n",
    "        picks = mne.pick_types(raw.info, eeg=True, meg=True, seeg=True, ecog=True, fnirs=True, exclude=\"bads\")\n",
    "        if len(picks)==0: raise RuntimeError(\"No EEG/MEG/iEEG channels found.\")\n",
    "        raw.pick(picks)\n",
    "        return raw\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— Could not load {os.path.basename(path)}: {e}\"); return None\n",
    "\n",
    "def basic_clean(raw):\n",
    "    raw = raw.copy()\n",
    "    # Notch\n",
    "    try:\n",
    "        if cfg.notch: raw.notch_filter(cfg.notch, verbose=False)\n",
    "    except Exception: pass\n",
    "    # Bandpass + resample\n",
    "    raw.filter(cfg.l_freq, cfg.h_freq, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"]-cfg.target_sfreq)>1e-3:\n",
    "        raw.resample(cfg.target_sfreq, npad=\"auto\", verbose=False)\n",
    "    # Reference & montage if EEG present\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "        try:\n",
    "            raw.set_montage(mne.channels.make_standard_montage(cfg.montage),\n",
    "                            match_case=False, match_alias=True, on_missing=\"warn\")\n",
    "        except Exception: pass\n",
    "    return raw\n",
    "\n",
    "def make_epochs(raw):\n",
    "    dur, hop = cfg.epoch_len_s, cfg.step_s\n",
    "    overlap = max(0.0, dur - hop)\n",
    "    epochs = mne.make_fixed_length_epochs(raw, duration=dur, overlap=overlap, preload=True, verbose=False)\n",
    "    starts = epochs.events[:,0]/epochs.info[\"sfreq\"]; ends = starts + dur\n",
    "    return epochs, starts, ends\n",
    "\n",
    "# =======================\n",
    "# FEATURES\n",
    "# =======================\n",
    "from scipy.signal import welch\n",
    "\n",
    "def _band_idx(freqs, lo, hi): return np.where((freqs>=lo)&(freqs<hi))[0]\n",
    "\n",
    "def hjorth_params(x):  # (..., n_times)\n",
    "    d1 = np.diff(x, axis=-1)\n",
    "    var0 = np.var(x, axis=-1) + 1e-12\n",
    "    var1 = np.var(d1, axis=-1) + 1e-12\n",
    "    mob = np.sqrt(var1/var0)\n",
    "    d2 = np.diff(d1, axis=-1)\n",
    "    var2 = np.var(d2, axis=-1) + 1e-12\n",
    "    comp = np.sqrt((var2/var1)/(var1/var0))\n",
    "    return var0, mob, comp\n",
    "\n",
    "def spectral_features(epochs, bands):\n",
    "    X = epochs.get_data()                   # (n_ep, n_ch, n_t)\n",
    "    n_ep, n_ch, n_t = X.shape\n",
    "    sf = epochs.info[\"sfreq\"]\n",
    "    nperseg = min(int(sf*2), n_t); noverlap = nperseg//2\n",
    "    freqs, psd = welch(X, fs=sf, nperseg=nperseg, noverlap=noverlap, axis=-1, average=\"median\")\n",
    "    aidx = _band_idx(freqs, cfg.l_freq, cfg.h_freq)\n",
    "    tot = np.maximum(psd[:,:,aidx].sum(-1), 1e-12)\n",
    "\n",
    "    feats = {}\n",
    "    for name, (lo,hi) in bands.items():\n",
    "        idx = _band_idx(freqs, lo, hi)\n",
    "        bp = psd[:,:,idx].sum(-1)/tot\n",
    "        feats[f\"{name}_rel_med\"] = np.median(bp, axis=1)\n",
    "        feats[f\"{name}_rel_iqr\"] = np.subtract(*np.percentile(bp, [75,25], axis=1))\n",
    "        feats[f\"{name}_rel_std\"] = np.std(bp, axis=1)\n",
    "\n",
    "    alpha = feats[\"alpha_rel_med\"]; theta = feats[\"theta_rel_med\"]; beta = feats[\"beta_rel_med\"]\n",
    "    feats[\"theta_over_alpha\"] = theta/np.maximum(alpha,1e-6)\n",
    "    feats[\"beta_over_alpha\"]  = beta /np.maximum(alpha,1e-6)\n",
    "    feats[\"(alpha+theta)_over_beta\"] = (alpha+theta)/np.maximum(beta,1e-6)\n",
    "\n",
    "    p = psd[:,:,aidx]; p_norm = p/np.maximum(p.sum(-1, keepdims=True), 1e-12)\n",
    "    H = -np.sum(p_norm*np.log2(p_norm+1e-12), axis=-1); Hn = H/np.log2(p.shape[-1])\n",
    "    feats[\"spec_entropy_med\"] = np.median(Hn, axis=1)\n",
    "    feats[\"spec_entropy_std\"] = np.std(Hn, axis=1)\n",
    "\n",
    "    f = freqs[aidx].reshape(1,1,-1)\n",
    "    centroid = (p*f).sum(-1)/np.maximum(p.sum(-1),1e-12)\n",
    "    feats[\"spec_centroid_med\"] = np.median(centroid, axis=1)\n",
    "    feats[\"spec_centroid_std\"] = np.std(centroid, axis=1)\n",
    "\n",
    "    act, mob, comp = hjorth_params(X)\n",
    "    feats[\"hjorth_activity_med\"]   = np.median(act, axis=1)\n",
    "    feats[\"hjorth_mobility_med\"]   = np.median(mob, axis=1)\n",
    "    feats[\"hjorth_complexity_med\"] = np.median(comp, axis=1)\n",
    "\n",
    "    return pd.DataFrame({k:(v if v.ndim==1 else v.reshape(v.shape[0],-1)) for k,v in feats.items()})\n",
    "\n",
    "# =======================\n",
    "# MODELING\n",
    "# =======================\n",
    "def _choose_k(Z, k_low, k_high, rs):\n",
    "    best = {\"k\":None, \"sil\":-1.0, \"model\":None, \"labels\":None}\n",
    "    for k in range(k_low, k_high+1):\n",
    "        km = KMeans(n_clusters=k, n_init=\"auto\" if _SK_OK else 1, random_state=rs)\n",
    "        labels = km.fit_predict(Z)\n",
    "        sil = silhouette_score(Z, labels) if k>1 else -1.0\n",
    "        if sil > best[\"sil\"]:\n",
    "            best = {\"k\":k, \"sil\":sil, \"model\":km, \"labels\":labels}\n",
    "    return best\n",
    "\n",
    "def _hmm_smooth(labels, Z, k, rs):\n",
    "    if not _HAS_HMM: \n",
    "        print(\"HMM not available; skipping.\"); \n",
    "        return labels\n",
    "    try:\n",
    "        hmm = GaussianHMM(n_components=k, covariance_type=\"diag\", random_state=rs, n_iter=200)\n",
    "        hmm.fit(Z); return hmm.predict(Z)\n",
    "    except Exception as e:\n",
    "        print(f\"HMM failed: {e}\"); return labels\n",
    "\n",
    "def _summarize(df_feat, labels):\n",
    "    F = df_feat.copy(); F[\"state\"] = labels\n",
    "    med = F.groupby(\"state\").median(numeric_only=True).sort_index()\n",
    "    z = (med - df_feat.median())/(df_feat.std()+1e-9)\n",
    "    summary = {}\n",
    "    for s in med.index:\n",
    "        top = z.loc[s].abs().sort_values(ascending=False).head(8).index.tolist()\n",
    "        summary[int(s)] = {\"name\": f\"State-{int(s)}\", \"top_features\": top}\n",
    "    return med, summary\n",
    "\n",
    "# =======================\n",
    "# ORCHESTRATION\n",
    "# =======================\n",
    "def run_pipeline():\n",
    "    t0=time.time(); _mkdir(cfg.output_dir)\n",
    "\n",
    "    # Acquire\n",
    "    _download_all(cfg.dataset_urls, cfg.data_dir)\n",
    "    files = _find_files(cfg.data_dir, cfg.exts)\n",
    "    if cfg.max_files: files = files[:cfg.max_files]\n",
    "    if not files:\n",
    "        print(f\"âœ± No files in {cfg.data_dir}. Add data or URLs in cfg.dataset_urls.\"); return\n",
    "\n",
    "    # Process\n",
    "    all_feat=[]; all_meta=[]\n",
    "    print(f\"â€¢ Found {len(files)} file(s). Processing â€¦\")\n",
    "    for i,fp in enumerate(files,1):\n",
    "        print(f\"[{i}/{len(files)}] {os.path.basename(fp)}\")\n",
    "        raw = load_raw_any(fp)\n",
    "        if raw is None: continue\n",
    "        raw = basic_clean(raw)\n",
    "        epochs, starts, ends = make_epochs(raw)\n",
    "        if len(epochs)==0: print(\"   (no epochs)\"); continue\n",
    "        feat = spectral_features(epochs, cfg.bands)\n",
    "        meta = pd.DataFrame({\n",
    "            \"file\": os.path.basename(fp),\n",
    "            \"epoch_idx\": np.arange(len(epochs)),\n",
    "            \"t_start_s\": starts, \"t_end_s\": ends,\n",
    "            \"sfreq\": epochs.info[\"sfreq\"],\n",
    "            \"n_channels\": [len(epochs.ch_names)]*len(epochs)\n",
    "        })\n",
    "        all_feat.append(feat); all_meta.append(meta)\n",
    "        del raw, epochs, feat, meta; gc.collect()\n",
    "\n",
    "    if not all_feat:\n",
    "        print(\"âœ± No usable epochs extracted. Check formats/preprocessing.\"); return\n",
    "\n",
    "    feat_df = pd.concat(all_feat, 0, ignore_index=True)\n",
    "    meta_df = pd.concat(all_meta, 0, ignore_index=True)\n",
    "    feat_df.to_csv(os.path.join(cfg.output_dir,\"features.csv\"), index=False)\n",
    "    meta_df.to_csv(os.path.join(cfg.output_dir,\"metadata.csv\"), index=False)\n",
    "    print(\"âœ“ Saved features/metadata.\")\n",
    "\n",
    "    # Scale & reduce\n",
    "    scaler = StandardScaler(); Xs = scaler.fit_transform(feat_df.values)\n",
    "    pca = PCA(n_components=min(20, Xs.shape[1]), random_state=cfg.random_state)\n",
    "    Z = pca.fit_transform(Xs)\n",
    "\n",
    "    # Cluster\n",
    "    best = _choose_k(Z, cfg.k_range[0], cfg.k_range[1], cfg.random_state)\n",
    "    labels = best[\"labels\"]; K = best[\"k\"]\n",
    "    print(f\"â˜… Selected K={K} (silhouette={best['sil']:.3f})\")\n",
    "\n",
    "    # Optional HMM smoothing\n",
    "    if cfg.use_hmm: \n",
    "        labels = _hmm_smooth(labels, Z, K, cfg.random_state); print(\"âœ“ HMM smoothing applied.\")\n",
    "\n",
    "    # Summaries\n",
    "    centers = pd.DataFrame(best[\"model\"].cluster_centers_, columns=[f\"PC{i+1}\" for i in range(Z.shape[1])])\n",
    "    med_table, alpha_map = _summarize(feat_df, labels)\n",
    "    with open(os.path.join(cfg.output_dir,\"cognitive_alphabet.json\"), \"w\") as f: json.dump(alpha_map, f, indent=2)\n",
    "    centers.to_csv(os.path.join(cfg.output_dir,\"kmeans_centers_pc.csv\"), index=False)\n",
    "\n",
    "    # Save models (in pure Python objects if joblib missing)\n",
    "    try:\n",
    "        from joblib import dump\n",
    "        dump(best[\"model\"], os.path.join(cfg.output_dir,\"kmeans.joblib\"))\n",
    "        dump(pca, os.path.join(cfg.output_dir,\"pca.joblib\"))\n",
    "        dump(scaler, os.path.join(cfg.output_dir,\"scaler.joblib\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Plots\n",
    "    # 2D embedding (use PCA 2D to avoid extra deps)\n",
    "    try:\n",
    "        p2 = PCA(n_components=2, random_state=cfg.random_state)\n",
    "        E2 = p2.fit_transform(Z)\n",
    "        plt.figure(figsize=(7.2,6.0))\n",
    "        plt.scatter(E2[:,0], E2[:,1], c=labels, s=8)\n",
    "        plt.title(f\"Cognitive Alphabet (K={K})\"); plt.xlabel(\"Dim 1\"); plt.ylabel(\"Dim 2\"); plt.tight_layout()\n",
    "        plt.savefig(os.path.join(cfg.output_dir,\"embedding.png\"), dpi=160); plt.close()\n",
    "        print(\"âœ“ Saved embedding.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"(viz) embedding failed: {e}\")\n",
    "\n",
    "    try:\n",
    "        M = (med_table - feat_df.median())/(feat_df.std()+1e-9)\n",
    "        plt.figure(figsize=(min(14, 2+0.5*M.shape[1]), 0.6+0.3*K+2))\n",
    "        im = plt.imshow(M.values, aspect=\"auto\")\n",
    "        plt.colorbar(im, fraction=0.025, pad=0.03, label=\"z-score vs overall\")\n",
    "        plt.yticks(range(M.shape[0]), [f\"S{int(s)}\" for s in med_table.index])\n",
    "        plt.xticks(range(M.shape[1]), M.columns, rotation=90)\n",
    "        plt.title(\"State Feature Signatures\"); plt.tight_layout()\n",
    "        plt.savefig(os.path.join(cfg.output_dir,\"state_feature_signatures.png\"), dpi=160); plt.close()\n",
    "        print(\"âœ“ Saved state_feature_signatures.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"(viz) heatmap failed: {e}\")\n",
    "\n",
    "    assign = meta_df.copy(); assign[\"state\"] = labels\n",
    "    assign.to_csv(os.path.join(cfg.output_dir,\"state_assignments.csv\"), index=False)\n",
    "    with open(os.path.join(cfg.output_dir,\"report.json\"), \"w\") as f:\n",
    "        json.dump({\n",
    "            \"files_processed\": len(files),\n",
    "            \"n_epochs\": int(len(assign)),\n",
    "            \"alphabet_size\": int(K),\n",
    "            \"silhouette\": float(best[\"sil\"]),\n",
    "            \"bands\": cfg.bands,\n",
    "            \"epoch_length_s\": cfg.epoch_len_s, \"step_s\": cfg.step_s,\n",
    "            \"target_sfreq\": cfg.target_sfreq, \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, f, indent=2)\n",
    "    print(f\"â± Done in {time.time()-t0:.1f}s. Outputs â†’ {cfg.output_dir}\")\n",
    "\n",
    "# =======================\n",
    "# QUICK SELF-TEST (optional)\n",
    "# =======================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"ðŸ§­ Cognitive Alphabet Finder â€” ready.\")\n",
    "    print(f\"Data dir: {cfg.data_dir} | Output: {cfg.output_dir}\")\n",
    "    # Uncomment to run immediately:\n",
    "    # run_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "085480b2-07b6-4921-98d7-4fa41f08c60c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â†“ Downloading: https://physionet.org/files/eegmmidb/1.0.0/S001/S001R01.edf\n",
      "â†“ Downloading: https://physionet.org/files/eegmmidb/1.0.0/S001/S001R02.edf\n",
      "â†“ Downloading: https://physionet.org/files/eegmmidb/1.0.0/S001/S001R03.edf\n",
      "â€¢ Found 5 file(s). Processing â€¦\n",
      "[1/5] S001R01.edf\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[2/5] S001R02.edf\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[3/5] S001R03.edf\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[4/5] sim_alpha.csv\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[5/5] sim_theta.csv\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "concat() takes 1 positional argument but 2 positional arguments (and 1 keyword-only argument) were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m cfg.random_state = \u001b[32m42\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Run it\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 350\u001b[39m, in \u001b[36mrun_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_feat:\n\u001b[32m    348\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mâœ± No usable epochs extracted. Check formats/preprocessing.\u001b[39m\u001b[33m\"\u001b[39m); \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m350\u001b[39m feat_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_feat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    351\u001b[39m meta_df = pd.concat(all_meta, \u001b[32m0\u001b[39m, ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    352\u001b[39m feat_df.to_csv(os.path.join(cfg.output_dir,\u001b[33m\"\u001b[39m\u001b[33mfeatures.csv\u001b[39m\u001b[33m\"\u001b[39m), index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: concat() takes 1 positional argument but 2 positional arguments (and 1 keyword-only argument) were given"
     ]
    }
   ],
   "source": [
    "# === Real data run: PhysioNet EEG Motor Movement/Imagery (EEGMMI) ===\n",
    "# Source: https://physionet.org/content/eegmmidb/1.0.0/\n",
    "\n",
    "cfg.dataset_urls = [\n",
    "    \"https://physionet.org/files/eegmmidb/1.0.0/S001/S001R01.edf\",  # baseline, eyes open/closed\n",
    "    \"https://physionet.org/files/eegmmidb/1.0.0/S001/S001R02.edf\",  # motor imagery block\n",
    "    \"https://physionet.org/files/eegmmidb/1.0.0/S001/S001R03.edf\",  # motor imagery block\n",
    "    # Add more if you want (S001R04.edf ... S001R14.edf), or other subjects (S002/..., S003/...)\n",
    "]\n",
    "\n",
    "# Use the URLs (will be saved into cfg.data_dir)\n",
    "cfg.data_dir   = \"./brainwaves\"          # where downloads go\n",
    "cfg.output_dir = \"./cog_alphabet_physio\" # where outputs go\n",
    "\n",
    "# Slightly widen the band for richer gamma; keep notch at 50/60 Hz.\n",
    "cfg.h_freq = 45.0\n",
    "cfg.bands[\"gamma\"] = (30.0, 45.0)\n",
    "\n",
    "# Epoching/modelling knobs (reasonable defaults)\n",
    "cfg.epoch_len_s = 2.0\n",
    "cfg.step_s      = 0.5\n",
    "cfg.k_range     = (3, 7)    # a compact alphabet\n",
    "cfg.use_hmm     = True      # smooth temporal flips if hmmlearn is installed\n",
    "cfg.random_state = 42\n",
    "\n",
    "# Run it\n",
    "run_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0516cf09-e49f-4a79-acf6-298e31b4908d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Exists: ./brainwaves\\S001R01.edf\n",
      "âœ“ Exists: ./brainwaves\\S001R02.edf\n",
      "âœ“ Exists: ./brainwaves\\S001R03.edf\n",
      "â€¢ Found 5 file(s). Processing â€¦\n",
      "[1/5] S001R01.edf\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[2/5] S001R02.edf\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[3/5] S001R03.edf\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[4/5] sim_alpha.csv\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[5/5] sim_theta.csv\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "âœ“ Saved features/metadata.\n",
      "â˜… Selected K=3 (silhouette=0.447)\n",
      "âœ“ HMM smoothing applied.\n",
      "âœ“ Saved embedding.png\n",
      "âœ“ Saved state_feature_signatures.png\n",
      "â± Done in 4.8s. Outputs â†’ ./cog_alphabet_physio\n"
     ]
    }
   ],
   "source": [
    "# === Hotfix + Real-Data Run (one cell) ===\n",
    "# Assumes you've already executed the mega cell so cfg, helpers, and imports exist.\n",
    "\n",
    "# 0) Guard: ensure mega cell ran\n",
    "needed = [\"cfg\",\"_download_all\",\"_find_files\",\"load_raw_any\",\"basic_clean\",\"make_epochs\",\n",
    "          \"spectral_features\",\"_choose_k\",\"_hmm_smooth\",\"_summarize\",\"StandardScaler\",\"PCA\",\"plt\",\"pd\",\"np\"]\n",
    "missing = [n for n in needed if n not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(\"Missing from this session: \" + \", \".join(missing) +\n",
    "                       \". Run the 'Cognitive Alphabet Finder â€” Mega Cell' first, then re-run this cell.\")\n",
    "\n",
    "# 1) Point to real PhysioNet EEG-MMI EDFs (small, fast set). You can add more URLs below.\n",
    "cfg.dataset_urls = [\n",
    "    \"https://physionet.org/files/eegmmidb/1.0.0/S001/S001R01.edf\",\n",
    "    \"https://physionet.org/files/eegmmidb/1.0.0/S001/S001R02.edf\",\n",
    "    \"https://physionet.org/files/eegmmidb/1.0.0/S001/S001R03.edf\",\n",
    "]\n",
    "cfg.data_dir   = \"./brainwaves\"\n",
    "cfg.output_dir = \"./cog_alphabet_physio\"\n",
    "# Optional: pull more runs for the same subject\n",
    "# cfg.dataset_urls += [f\"https://physionet.org/files/eegmmidb/1.0.0/S001/S001R{r:02d}.edf\" for r in range(4, 15)]\n",
    "\n",
    "# 2) Tuning knobs\n",
    "cfg.h_freq = 45.0\n",
    "cfg.bands[\"gamma\"] = (30.0, 45.0)\n",
    "cfg.epoch_len_s = 2.0\n",
    "cfg.step_s      = 0.5\n",
    "cfg.k_range     = (3, 7)\n",
    "cfg.use_hmm     = True\n",
    "cfg.random_state = 42\n",
    "\n",
    "# 3) Hotfix: pandas 2.x concat signature + rerunable pipeline\n",
    "def run_pipeline():\n",
    "    import time, json, gc, os\n",
    "    t0=time.time(); os.makedirs(cfg.output_dir, exist_ok=True)\n",
    "\n",
    "    # Acquire\n",
    "    _download_all(cfg.dataset_urls, cfg.data_dir)\n",
    "    files = _find_files(cfg.data_dir, cfg.exts)\n",
    "    if cfg.max_files: files = files[:cfg.max_files]\n",
    "    if not files:\n",
    "        print(f\"âœ± No files in {cfg.data_dir}. Add data or URLs in cfg.dataset_urls.\"); return\n",
    "\n",
    "    # Process\n",
    "    all_feat=[]; all_meta=[]\n",
    "    print(f\"â€¢ Found {len(files)} file(s). Processing â€¦\")\n",
    "    for i,fp in enumerate(files,1):\n",
    "        print(f\"[{i}/{len(files)}] {os.path.basename(fp)}\")\n",
    "        raw = load_raw_any(fp)\n",
    "        if raw is None: continue\n",
    "        raw = basic_clean(raw)\n",
    "        epochs, starts, ends = make_epochs(raw)\n",
    "        if len(epochs)==0:\n",
    "            print(\"   (no epochs)\"); continue\n",
    "        feat = spectral_features(epochs, cfg.bands)\n",
    "        meta = pd.DataFrame({\n",
    "            \"file\": os.path.basename(fp),\n",
    "            \"epoch_idx\": np.arange(len(epochs)),\n",
    "            \"t_start_s\": starts, \"t_end_s\": ends,\n",
    "            \"sfreq\": epochs.info[\"sfreq\"],\n",
    "            \"n_channels\": [len(epochs.ch_names)]*len(epochs)\n",
    "        })\n",
    "        all_feat.append(feat); all_meta.append(meta)\n",
    "        del raw, epochs, feat, meta; gc.collect()\n",
    "\n",
    "    if not all_feat:\n",
    "        print(\"âœ± No usable epochs extracted. Check formats/preprocessing.\"); return\n",
    "\n",
    "    # âœ… pandas 2.x-safe concat\n",
    "    feat_df = pd.concat(all_feat, axis=0, ignore_index=True)\n",
    "    meta_df = pd.concat(all_meta, axis=0, ignore_index=True)\n",
    "    feat_df.to_csv(os.path.join(cfg.output_dir,\"features.csv\"), index=False)\n",
    "    meta_df.to_csv(os.path.join(cfg.output_dir,\"metadata.csv\"), index=False)\n",
    "    print(\"âœ“ Saved features/metadata.\")\n",
    "\n",
    "    # Scale & reduce\n",
    "    scaler = StandardScaler(); Xs = scaler.fit_transform(feat_df.values)\n",
    "    pca = PCA(n_components=min(20, Xs.shape[1]), random_state=cfg.random_state)\n",
    "    Z = pca.fit_transform(Xs)\n",
    "\n",
    "    # Cluster\n",
    "    best = _choose_k(Z, cfg.k_range[0], cfg.k_range[1], cfg.random_state)\n",
    "    labels = best[\"labels\"]; K = best[\"k\"]\n",
    "    print(f\"â˜… Selected K={K} (silhouette={best['sil']:.3f})\")\n",
    "\n",
    "    # Optional HMM smoothing\n",
    "    if cfg.use_hmm:\n",
    "        labels = _hmm_smooth(labels, Z, K, cfg.random_state)\n",
    "        print(\"âœ“ HMM smoothing applied.\")\n",
    "\n",
    "    # Summaries\n",
    "    centers = pd.DataFrame(best[\"model\"].cluster_centers_, columns=[f\"PC{i+1}\" for i in range(Z.shape[1])])\n",
    "    med_table, alpha_map = _summarize(feat_df, labels)\n",
    "    with open(os.path.join(cfg.output_dir,\"cognitive_alphabet.json\"), \"w\") as f: json.dump(alpha_map, f, indent=2)\n",
    "    centers.to_csv(os.path.join(cfg.output_dir,\"kmeans_centers_pc.csv\"), index=False)\n",
    "\n",
    "    # Save models (best-effort)\n",
    "    try:\n",
    "        from joblib import dump\n",
    "        dump(best[\"model\"], os.path.join(cfg.output_dir,\"kmeans.joblib\"))\n",
    "        dump(pca, os.path.join(cfg.output_dir,\"pca.joblib\"))\n",
    "        dump(scaler, os.path.join(cfg.output_dir,\"scaler.joblib\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Plots\n",
    "    try:\n",
    "        p2 = PCA(n_components=2, random_state=cfg.random_state)\n",
    "        E2 = p2.fit_transform(Z)\n",
    "        plt.figure(figsize=(7.2,6.0))\n",
    "        plt.scatter(E2[:,0], E2[:,1], c=labels, s=8)\n",
    "        plt.title(f\"Cognitive Alphabet (K={K})\")\n",
    "        plt.xlabel(\"Dim 1\"); plt.ylabel(\"Dim 2\"); plt.tight_layout()\n",
    "        plt.savefig(os.path.join(cfg.output_dir,\"embedding.png\"), dpi=160); plt.close()\n",
    "        print(\"âœ“ Saved embedding.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"(viz) embedding failed: {e}\")\n",
    "\n",
    "    try:\n",
    "        M = (med_table - feat_df.median())/(feat_df.std()+1e-9)\n",
    "        plt.figure(figsize=(min(14, 2+0.5*M.shape[1]), 0.6+0.3*K+2))\n",
    "        im = plt.imshow(M.values, aspect=\"auto\")\n",
    "        plt.colorbar(im, fraction=0.025, pad=0.03, label=\"z-score vs overall\")\n",
    "        plt.yticks(range(M.shape[0]), [f\"S{int(s)}\" for s in med_table.index])\n",
    "        plt.xticks(range(M.shape[1]), M.columns, rotation=90)\n",
    "        plt.title(\"State Feature Signatures\"); plt.tight_layout()\n",
    "        plt.savefig(os.path.join(cfg.output_dir,\"state_feature_signatures.png\"), dpi=160); plt.close()\n",
    "        print(\"âœ“ Saved state_feature_signatures.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"(viz) heatmap failed: {e}\")\n",
    "\n",
    "    assign = meta_df.copy(); assign[\"state\"] = labels\n",
    "    assign.to_csv(os.path.join(cfg.output_dir,\"state_assignments.csv\"), index=False)\n",
    "    with open(os.path.join(cfg.output_dir,\"report.json\"), \"w\") as f:\n",
    "        json.dump({\n",
    "            \"files_processed\": len(files),\n",
    "            \"n_epochs\": int(len(assign)),\n",
    "            \"alphabet_size\": int(K),\n",
    "            \"silhouette\": float(best[\"sil\"]),\n",
    "            \"bands\": cfg.bands,\n",
    "            \"epoch_length_s\": cfg.epoch_len_s, \"step_s\": cfg.step_s,\n",
    "            \"target_sfreq\": cfg.target_sfreq, \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        }, f, indent=2)\n",
    "    print(f\"â± Done in {time.time()-t0:.1f}s. Outputs â†’ {cfg.output_dir}\")\n",
    "\n",
    "# 4) Go.\n",
    "run_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2417a1a5-4258-4091-ae3a-c725a7030cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Cognitive Alphabet (K = 3) ===\n",
      "State 0: top features â†’ beta_over_alpha, beta_rel_med, hjorth_complexity_med, theta_over_alpha, theta_rel_med, spec_entropy_med, (alpha+theta)_over_beta, delta_rel_med\n",
      "State 1: top features â†’ beta_over_alpha, theta_over_alpha, hjorth_complexity_med, beta_rel_med, gamma_rel_iqr, gamma_rel_std, (alpha+theta)_over_beta, delta_rel_med\n",
      "State 2: top features â†’ alpha_rel_std, alpha_rel_med, alpha_rel_iqr, (alpha+theta)_over_beta, beta_over_alpha, theta_over_alpha, theta_rel_med, delta_rel_med\n",
      "\n",
      "=== Global Dwell Fractions ===\n",
      "       fraction\n",
      "state          \n",
      "0      0.413074\n",
      "1      0.415855\n",
      "2      0.171071\n",
      "\n",
      "=== Per-file State Composition (fractions) ===\n",
      "state              0      1      2\n",
      "file                              \n",
      "S001R01.edf    0.496  0.504  0.000\n",
      "S001R02.edf    0.000  0.000  1.000\n",
      "S001R03.edf    0.494  0.490  0.016\n",
      "sim_alpha.csv  0.496  0.504  0.000\n",
      "sim_theta.csv  0.496  0.504  0.000\n",
      "Saved ./cog_alphabet_physio\\timeline_S001R01.png\n",
      "Saved ./cog_alphabet_physio\\timeline_S001R02.png\n",
      "Saved ./cog_alphabet_physio\\timeline_S001R03.png\n",
      "Saved ./cog_alphabet_physio\\timeline_sim_alpha.png\n",
      "Saved ./cog_alphabet_physio\\timeline_sim_theta.png\n",
      "\n",
      "=== Transition Matrix (row-normalized) ===\n",
      "       S0     S1     S2\n",
      "S0  0.000  1.000  0.000\n",
      "S1  0.990  0.003  0.007\n",
      "S2  0.016  0.000  0.984\n"
     ]
    }
   ],
   "source": [
    "# === Insight Cell: read results, summarize states, per-file timelines ===\n",
    "import os, json, pandas as pd, numpy as np, matplotlib.pyplot as plt\n",
    "\n",
    "OD = \"./cog_alphabet_physio\"\n",
    "feat = pd.read_csv(os.path.join(OD, \"features.csv\"))\n",
    "meta = pd.read_csv(os.path.join(OD, \"metadata.csv\"))\n",
    "assign = pd.read_csv(os.path.join(OD, \"state_assignments.csv\"))\n",
    "with open(os.path.join(OD, \"cognitive_alphabet.json\")) as f:\n",
    "    alpha_map = json.load(f)\n",
    "\n",
    "df = pd.concat([meta, assign[\"state\"]], axis=1)\n",
    "K = int(df[\"state\"].max() + 1)\n",
    "\n",
    "# 1) Show the alphabet with top features\n",
    "print(\"\\n=== Cognitive Alphabet (K = {}) ===\".format(K))\n",
    "for s in range(K):\n",
    "    info = alpha_map[str(s)]\n",
    "    print(f\"State {s}: top features â†’ {', '.join(info['top_features'])}\")\n",
    "\n",
    "# 2) Global dwell-time distribution (how much time in each letter)\n",
    "counts = df[\"state\"].value_counts().sort_index()\n",
    "dwell = (counts / counts.sum()).rename(\"fraction\")\n",
    "print(\"\\n=== Global Dwell Fractions ===\")\n",
    "print(dwell.to_frame())\n",
    "\n",
    "# 3) Per-file composition\n",
    "print(\"\\n=== Per-file State Composition (fractions) ===\")\n",
    "comp = (df.groupby(\"file\")[\"state\"]\n",
    "        .value_counts(normalize=True)\n",
    "        .rename(\"fraction\")\n",
    "        .reset_index()\n",
    "        .pivot(index=\"file\", columns=\"state\", values=\"fraction\").fillna(0.0))\n",
    "print(comp.round(3))\n",
    "\n",
    "# 4) Quick timelines per file (simple strip plots)\n",
    "files = df[\"file\"].unique()\n",
    "for f in files:\n",
    "    sub = df[df[\"file\"] == f].sort_values(\"t_start_s\")\n",
    "    t = sub[\"t_start_s\"].to_numpy()\n",
    "    s = sub[\"state\"].to_numpy()\n",
    "    plt.figure(figsize=(10, 1.6))\n",
    "    plt.scatter(t, s, s=8)\n",
    "    plt.yticks(range(K), [f\"S{s}\" for s in range(K)])\n",
    "    plt.xlabel(\"time (s)\"); plt.title(f\"Timeline â†’ {f}\")\n",
    "    plt.tight_layout()\n",
    "    outp = os.path.join(OD, f\"timeline_{os.path.splitext(f)[0]}.png\")\n",
    "    plt.savefig(outp, dpi=140); plt.close()\n",
    "    print(f\"Saved {outp}\")\n",
    "\n",
    "# 5) (Optional) Transition matrix across all data\n",
    "Smax = K\n",
    "T = np.zeros((Smax, Smax), dtype=int)\n",
    "for f in files:\n",
    "    sub = df[df[\"file\"] == f].sort_values(\"t_start_s\")[\"state\"].to_numpy()\n",
    "    T += np.histogram2d(sub[:-1], sub[1:], bins=[np.arange(Smax+1), np.arange(Smax+1)])[0].astype(int)\n",
    "Tm = T / np.maximum(T.sum(axis=1, keepdims=True), 1)  # row-normalized\n",
    "print(\"\\n=== Transition Matrix (row-normalized) ===\")\n",
    "print(pd.DataFrame(Tm, index=[f\"S{i}\" for i in range(K)], columns=[f\"S{i}\" for i in range(K)]).round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5471c787-510d-47c7-8608-60c384284b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./cog_alphabet_physio\\alphabet_named_summary.csv\n",
      "Saved: ./cog_alphabet_physio\\state_assignments_named.csv\n",
      "\n",
      "State names â†’ {0: 'Beta/Theta-Mixed', 1: 'Beta/Theta-Mixed', 2: 'Alpha-Dominant'}\n"
     ]
    }
   ],
   "source": [
    "# === Name & Export Alphabet (based on current results) ===\n",
    "import os, json, numpy as np, pandas as pd\n",
    "\n",
    "OD = \"./cog_alphabet_physio\"\n",
    "feat = pd.read_csv(os.path.join(OD, \"features.csv\"))\n",
    "meta = pd.read_csv(os.path.join(OD, \"metadata.csv\"))\n",
    "assign = pd.read_csv(os.path.join(OD, \"state_assignments.csv\"))\n",
    "with open(os.path.join(OD, \"cognitive_alphabet.json\")) as f:\n",
    "    amap = json.load(f)\n",
    "\n",
    "# 1) Heuristic naming: alpha-dominant vs beta/theta mixed\n",
    "def name_state(info):\n",
    "    tops = info[\"top_features\"]\n",
    "    if \"alpha_rel_med\" in tops or \"alpha_rel_std\" in tops or \"alpha_rel_iqr\" in tops:\n",
    "        return \"Alpha-Dominant\"\n",
    "    if \"beta_rel_med\" in tops or \"beta_over_alpha\" in tops:\n",
    "        if \"theta_over_alpha\" in tops or \"theta_rel_med\" in tops:\n",
    "            return \"Beta/Theta-Mixed\"\n",
    "        return \"Beta-Leaning\"\n",
    "    if \"spec_entropy_med\" in tops or \"hjorth_complexity_med\" in tops:\n",
    "        return \"High-Entropy\"\n",
    "    return \"Mixed\"\n",
    "\n",
    "state_names = {int(s): name_state(amap[str(s)]) for s in amap.keys()}\n",
    "\n",
    "# 2) Per-state median table for quick inspection\n",
    "feat_only = feat.copy()\n",
    "S = assign[\"state\"].to_numpy()\n",
    "med = pd.DataFrame({k: feat_only[k] for k in feat_only.columns}).assign(state=S).groupby(\"state\").median(numeric_only=True)\n",
    "med[\"name\"] = med.index.map(state_names)\n",
    "\n",
    "# 3) Save a compact CSV summary\n",
    "summary = []\n",
    "for s, row in med.sort_index().iterrows():\n",
    "    summary.append({\n",
    "        \"state\": s,\n",
    "        \"name\": state_names[s],\n",
    "        \"top_features\": \", \".join(amap[str(s)][\"top_features\"][:6])\n",
    "    })\n",
    "summary_df = pd.DataFrame(summary).sort_values(\"state\")\n",
    "summary_path = os.path.join(OD, \"alphabet_named_summary.csv\")\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "\n",
    "# 4) Export assignments with names\n",
    "out = pd.concat([meta, assign], axis=1)\n",
    "out[\"state_name\"] = out[\"state\"].map(state_names)\n",
    "assign_named_path = os.path.join(OD, \"state_assignments_named.csv\")\n",
    "out.to_csv(assign_named_path, index=False)\n",
    "\n",
    "print(\"Saved:\", summary_path)\n",
    "print(\"Saved:\", assign_named_path)\n",
    "print(\"\\nState names â†’\", state_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f3ea4e8-6198-4e40-95c0-3c4aee19fb39",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4200637393.py, line 22)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfrom sklearn.metrics import silhouette_score as _ss if find_spec(\"sklearn\") else None\u001b[39m\n                                                        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# === Refit from Saved Features (no raw reprocessing) ===\n",
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from importlib.util import find_spec\n",
    "\n",
    "OD_in = \"./cog_alphabet_physio\"\n",
    "OD_out = \"./cog_alphabet_physio_refit\"\n",
    "os.makedirs(OD_out, exist_ok=True)\n",
    "\n",
    "feat = pd.read_csv(os.path.join(OD_in, \"features.csv\"))\n",
    "meta = pd.read_csv(os.path.join(OD_in, \"metadata.csv\"))\n",
    "\n",
    "# Pull helpers from your session (from the mega cell)\n",
    "# StandardScaler, PCA, KMeans, silhouette_score, _hmm_smooth should already exist.\n",
    "\n",
    "# 1) Scale + reduce\n",
    "scaler = StandardScaler(); Xs = scaler.fit_transform(feat.values)\n",
    "pca = PCA(n_components=min(20, Xs.shape[1]), random_state=42)\n",
    "Z = pca.fit_transform(Xs)\n",
    "\n",
    "# 2) Choose K by silhouette (tighter range to nudge separation)\n",
    "def choose_k(Z, rng=(4,6), rs=42):\n",
    "    from sklearn.metrics import silhouette_score as _ss if find_spec(\"sklearn\") else None\n",
    "    best = {\"k\":None, \"sil\":-1.0, \"model\":None, \"labels\":None}\n",
    "    for k in range(rng[0], rng[1]+1):\n",
    "        km = KMeans(n_clusters=k, n_init=\"auto\" if find_spec(\"sklearn\") else 1, random_state=rs)\n",
    "        labels = km.fit_predict(Z)\n",
    "        sil = silhouette_score(Z, labels) if k>1 else -1.0\n",
    "        if sil > best[\"sil\"]:\n",
    "            best = {\"k\":k, \"sil\":sil, \"model\":km, \"labels\":labels}\n",
    "    return best\n",
    "\n",
    "best = choose_k(Z, rng=(4,6), rs=42)\n",
    "labels = best[\"labels\"]; K = best[\"k\"]\n",
    "print(f\"Refit selected K={K} (silhouette={best['sil']:.3f})\")\n",
    "\n",
    "# 3) Optional HMM smoothing\n",
    "use_hmm = True\n",
    "if use_hmm:\n",
    "    try:\n",
    "        labels = _hmm_smooth(labels, Z, K, 42)\n",
    "        print(\"âœ“ HMM smoothing applied.\")\n",
    "    except Exception as e:\n",
    "        print(\"(HMM) skipped:\", e)\n",
    "\n",
    "# 4) Summaries\n",
    "def summarize(df_feat, labels):\n",
    "    F = df_feat.copy(); F[\"state\"] = labels\n",
    "    med = F.groupby(\"state\").median(numeric_only=True).sort_index()\n",
    "    z = (med - df_feat.median())/(df_feat.std()+1e-9)\n",
    "    summary = {}\n",
    "    for s in med.index:\n",
    "        top = z.loc[s].abs().sort_values(ascending=False).head(8).index.tolist()\n",
    "        summary[int(s)] = {\"name\": f\"State-{int(s)}\", \"top_features\": top}\n",
    "    return med, summary\n",
    "\n",
    "med_table, amap = summarize(feat, labels)\n",
    "with open(os.path.join(OD_out, \"cognitive_alphabet.json\"), \"w\") as f:\n",
    "    json.dump(amap, f, indent=2)\n",
    "\n",
    "# 5) Save assignments & simple plots\n",
    "assign = meta.copy(); assign[\"state\"] = labels\n",
    "assign.to_csv(os.path.join(OD_out, \"state_assignments.csv\"), index=False)\n",
    "feat.to_csv(os.path.join(OD_out, \"features.csv\"), index=False)\n",
    "meta.to_csv(os.path.join(OD_out, \"metadata.csv\"), index=False)\n",
    "\n",
    "# 2D PCA view\n",
    "p2 = PCA(n_components=2, random_state=42); E2 = p2.fit_transform(Z)\n",
    "plt.figure(figsize=(7.2,6.0)); plt.scatter(E2[:,0], E2[:,1], c=labels, s=8)\n",
    "plt.title(f\"Cognitive Alphabet (Refit K={K})\"); plt.xlabel(\"Dim 1\"); plt.ylabel(\"Dim 2\"); plt.tight_layout()\n",
    "plt.savefig(os.path.join(OD_out, \"embedding.png\"), dpi=160); plt.close()\n",
    "\n",
    "# Heatmap\n",
    "M = (med_table - feat.median())/(feat.std()+1e-9)\n",
    "plt.figure(figsize=(min(14, 2+0.5*M.shape[1]), 0.6+0.3*K+2))\n",
    "im = plt.imshow(M.values, aspect=\"auto\"); plt.colorbar(im, fraction=0.025, pad=0.03, label=\"z-score vs overall\")\n",
    "plt.yticks(range(M.shape[0]), [f\"S{int(s)}\" for s in med_table.index]); plt.xticks(range(M.shape[1]), M.columns, rotation=90)\n",
    "plt.title(\"State Feature Signatures (Refit)\"); plt.tight_layout()\n",
    "plt.savefig(os.path.join(OD_out, \"state_feature_signatures.png\"), dpi=160); plt.close()\n",
    "\n",
    "print(\"Refit outputs â†’\", OD_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aab6ccbd-2ae4-4359-b86b-b9d37a21644b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refit selected K=5 (silhouette=0.325)\n",
      "âœ“ HMM smoothing applied.\n",
      "Refit outputs â†’ ./cog_alphabet_physio_refit\n"
     ]
    }
   ],
   "source": [
    "# === Refit from Saved Features (no raw I/O) â€” fixed & runnable ===\n",
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from importlib.util import find_spec\n",
    "\n",
    "# ---- Guards: expect the mega-cell to have provided these already ----\n",
    "needed = [\"StandardScaler\",\"PCA\",\"KMeans\",\"silhouette_score\",\"_hmm_smooth\"]\n",
    "missing = [n for n in needed if n not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(\"Missing from session: \" + \", \".join(missing) +\n",
    "                       \". Run the mega cell first so these are defined.\")\n",
    "\n",
    "# ---- Paths ----\n",
    "OD_IN  = \"./cog_alphabet_physio\"        # existing outputs (features/metadata)\n",
    "OD_OUT = \"./cog_alphabet_physio_refit\"  # new folder for refit\n",
    "os.makedirs(OD_OUT, exist_ok=True)\n",
    "\n",
    "# ---- Load feature table (no raw re-read) ----\n",
    "feat = pd.read_csv(os.path.join(OD_IN, \"features.csv\"))\n",
    "meta = pd.read_csv(os.path.join(OD_IN, \"metadata.csv\"))\n",
    "\n",
    "# ---- Scale & reduce ----\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(feat.values)\n",
    "pca  = PCA(n_components=min(20, Xs.shape[1]), random_state=42)\n",
    "Z    = pca.fit_transform(Xs)\n",
    "\n",
    "# ---- Choose K via silhouette OR fix K explicitly ----\n",
    "K_FIXED = None      # set to an int (e.g., 5) to force K; keep None to auto-select\n",
    "K_RANGE = (4, 6)    # only used when K_FIXED is None\n",
    "\n",
    "def choose_k(Z, rng=(4,6), rs=42):\n",
    "    best = {\"k\":None, \"sil\":-1.0, \"model\":None, \"labels\":None}\n",
    "    for k in range(rng[0], rng[1]+1):\n",
    "        km = KMeans(n_clusters=k, n_init=\"auto\" if find_spec(\"sklearn\") else 1, random_state=rs)\n",
    "        labels = km.fit_predict(Z)\n",
    "        sil = silhouette_score(Z, labels) if k > 1 else -1.0\n",
    "        if sil > best[\"sil\"]:\n",
    "            best = {\"k\":k, \"sil\":sil, \"model\":km, \"labels\":labels}\n",
    "    return best\n",
    "\n",
    "if K_FIXED is None:\n",
    "    best = choose_k(Z, rng=K_RANGE, rs=42)\n",
    "else:\n",
    "    km = KMeans(n_clusters=K_FIXED, n_init=\"auto\" if find_spec(\"sklearn\") else 1, random_state=42)\n",
    "    labels = km.fit_predict(Z)\n",
    "    best = {\"k\": K_FIXED, \"sil\": silhouette_score(Z, labels) if K_FIXED>1 else -1.0, \"model\": km, \"labels\": labels}\n",
    "\n",
    "labels = best[\"labels\"]; K = best[\"k\"]\n",
    "print(f\"Refit selected K={K} (silhouette={best['sil']:.3f})\")\n",
    "\n",
    "# ---- Optional HMM smoothing (if available) ----\n",
    "USE_HMM = True\n",
    "if USE_HMM:\n",
    "    try:\n",
    "        labels = _hmm_smooth(labels, Z, K, 42)\n",
    "        print(\"âœ“ HMM smoothing applied.\")\n",
    "    except Exception as e:\n",
    "        print(\"(HMM) skipped:\", e)\n",
    "\n",
    "# ---- Summarize states by median feature z-scores ----\n",
    "def summarize(df_feat, labels):\n",
    "    F = df_feat.copy(); F[\"state\"] = labels\n",
    "    med = F.groupby(\"state\").median(numeric_only=True).sort_index()\n",
    "    z = (med - df_feat.median())/(df_feat.std() + 1e-9)\n",
    "    summary = {}\n",
    "    for s in med.index:\n",
    "        top = z.loc[s].abs().sort_values(ascending=False).head(8).index.tolist()\n",
    "        summary[int(s)] = {\"name\": f\"State-{int(s)}\", \"top_features\": top}\n",
    "    return med, summary\n",
    "\n",
    "med_table, amap = summarize(feat, labels)\n",
    "\n",
    "# ---- Write outputs ----\n",
    "assign = meta.copy(); assign[\"state\"] = labels\n",
    "feat.to_csv(os.path.join(OD_OUT, \"features.csv\"), index=False)\n",
    "meta.to_csv(os.path.join(OD_OUT, \"metadata.csv\"), index=False)\n",
    "assign.to_csv(os.path.join(OD_OUT, \"state_assignments.csv\"), index=False)\n",
    "with open(os.path.join(OD_OUT, \"cognitive_alphabet.json\"), \"w\") as f:\n",
    "    json.dump(amap, f, indent=2)\n",
    "\n",
    "# ---- Quick plots ----\n",
    "p2 = PCA(n_components=2, random_state=42); E2 = p2.fit_transform(Z)\n",
    "plt.figure(figsize=(7.2,6.0))\n",
    "plt.scatter(E2[:,0], E2[:,1], c=labels, s=8)\n",
    "plt.title(f\"Cognitive Alphabet (Refit K={K})\"); plt.xlabel(\"Dim 1\"); plt.ylabel(\"Dim 2\"); plt.tight_layout()\n",
    "plt.savefig(os.path.join(OD_OUT, \"embedding.png\"), dpi=160); plt.close()\n",
    "\n",
    "M = (med_table - feat.median())/(feat.std()+1e-9)\n",
    "plt.figure(figsize=(min(14, 2+0.5*M.shape[1]), 0.6+0.3*K+2))\n",
    "im = plt.imshow(M.values, aspect=\"auto\")\n",
    "plt.colorbar(im, fraction=0.025, pad=0.03, label=\"z-score vs overall\")\n",
    "plt.yticks(range(M.shape[0]), [f\"S{int(s)}\" for s in med_table.index])\n",
    "plt.xticks(range(M.shape[1]), M.columns, rotation=90)\n",
    "plt.title(\"State Feature Signatures (Refit)\"); plt.tight_layout()\n",
    "plt.savefig(os.path.join(OD_OUT, \"state_feature_signatures.png\"), dpi=160); plt.close()\n",
    "\n",
    "print(\"Refit outputs â†’\", OD_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49976977-1c4f-404d-bbba-9d2554149098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Alignment: Refit â†’ Original (by cosine similarity of feature medians) ===\n",
      "   refit_state  orig_state  cosine_sim\n",
      "4            4           0    0.354015\n",
      "3            3           0    0.052798\n",
      "0            0           1    0.230625\n",
      "1            1           1    0.230625\n",
      "2            2           2    0.998078\n",
      "\n",
      "Original names: {0: 'Beta/Theta-Mixed', 1: 'Beta/Theta-Mixed', 2: 'Alpha-Dominant'}\n",
      "Refit names:    {0: 'Beta-Leaning', 1: 'Beta-Leaning', 2: 'Alpha-Dominant', 3: 'High-Entropy', 4: 'High-Entropy'}\n",
      "\n",
      "=== Per-file composition (Original K=3, by fraction) ===\n",
      "state              0      1      2\n",
      "file                              \n",
      "S001R01.edf    0.496  0.504  0.000\n",
      "S001R02.edf    0.000  0.000  1.000\n",
      "S001R03.edf    0.494  0.490  0.016\n",
      "sim_alpha.csv  0.496  0.504  0.000\n",
      "sim_theta.csv  0.496  0.504  0.000\n",
      "\n",
      "=== Per-file composition (Refit K=5, by fraction) ===\n",
      "state              0      1     2      3      4\n",
      "file                                           \n",
      "S001R01.edf    0.000  0.000  0.00  0.277  0.723\n",
      "S001R02.edf    0.000  0.000  1.00  0.000  0.000\n",
      "S001R03.edf    0.000  0.000  0.02  0.543  0.437\n",
      "sim_alpha.csv  0.453  0.547  0.00  0.000  0.000\n",
      "sim_theta.csv  0.453  0.547  0.00  0.000  0.000\n",
      "\n",
      "=== S001R01 alpha-high vs original states (row = alpha group) ===\n",
      "orig_state      0      1\n",
      "alpha_high              \n",
      "0           0.492  0.508\n",
      "1           0.500  0.500\n",
      "\n",
      "=== S001R01 alpha-high vs refit states (row = alpha group) ===\n",
      "refit_state      3      4\n",
      "alpha_high               \n",
      "0            0.475  0.525\n",
      "1            0.083  0.917\n",
      "Saved ./cog_alphabet_physio_refit\\timeline_S001R01_refit.png\n",
      "\n",
      "Saved: ./cog_alphabet_physio_refit\\alignment_refit_to_original.csv\n",
      "Saved: ./cog_alphabet_physio_refit\\refit_state_names.csv\n"
     ]
    }
   ],
   "source": [
    "# === Compare Original (K=3) vs Refit (K=5) & Heuristic Labeling ===\n",
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "OD0 = \"./cog_alphabet_physio\"         # original run (K=3)\n",
    "OD1 = \"./cog_alphabet_physio_refit\"   # refit run (K=5)\n",
    "\n",
    "# --- Load original ---\n",
    "feat0 = pd.read_csv(os.path.join(OD0, \"features.csv\"))\n",
    "meta0 = pd.read_csv(os.path.join(OD0, \"metadata.csv\"))\n",
    "asgn0 = pd.read_csv(os.path.join(OD0, \"state_assignments.csv\"))\n",
    "with open(os.path.join(OD0, \"cognitive_alphabet.json\")) as f: amap0 = json.load(f)\n",
    "\n",
    "# --- Load refit ---\n",
    "feat1 = pd.read_csv(os.path.join(OD1, \"features.csv\"))\n",
    "meta1 = pd.read_csv(os.path.join(OD1, \"metadata.csv\"))\n",
    "asgn1 = pd.read_csv(os.path.join(OD1, \"state_assignments.csv\"))\n",
    "with open(os.path.join(OD1, \"cognitive_alphabet.json\")) as f: amap1 = json.load(f)\n",
    "\n",
    "# Sanity check: features must match\n",
    "assert list(feat0.columns) == list(feat1.columns), \"Feature columns differ between runs.\"\n",
    "\n",
    "# --- Build median feature signatures per state ---\n",
    "def median_table(feat_df, labels):\n",
    "    F = feat_df.copy()\n",
    "    F[\"state\"] = labels\n",
    "    return F.groupby(\"state\").median(numeric_only=True).sort_index()\n",
    "\n",
    "med0 = median_table(feat0, asgn0[\"state\"])\n",
    "med1 = median_table(feat1, asgn1[\"state\"])\n",
    "\n",
    "# --- Align refit states to original via cosine similarity on z-scored medians ---\n",
    "sc = StandardScaler()\n",
    "Z0 = sc.fit_transform(med0.values)          # shape (K0, D)\n",
    "Z1 = sc.transform(med1.values)              # use same scaler for feature comparability\n",
    "# cosine sim = normalized dot product\n",
    "def cosine(A,B): \n",
    "    A = A / (np.linalg.norm(A,axis=1,keepdims=True)+1e-9)\n",
    "    B = B / (np.linalg.norm(B,axis=1,keepdims=True)+1e-9)\n",
    "    return A @ B.T\n",
    "\n",
    "S = cosine(Z1, Z0)  # rows = refit states, cols = original states\n",
    "align = []\n",
    "for i in range(S.shape[0]):\n",
    "    j = int(np.argmax(S[i]))\n",
    "    align.append((i, j, float(S[i, j])))\n",
    "align_df = pd.DataFrame(align, columns=[\"refit_state\",\"orig_state\",\"cosine_sim\"]).sort_values([\"orig_state\",\"cosine_sim\"], ascending=[True,False])\n",
    "\n",
    "print(\"\\n=== Alignment: Refit â†’ Original (by cosine similarity of feature medians) ===\")\n",
    "print(align_df)\n",
    "\n",
    "# --- Heuristic naming based on top features (from the JSON summaries) ---\n",
    "def name_state_from_tops(tops):\n",
    "    if any(f.startswith(\"alpha_rel_\") for f in tops) or \"alpha_rel_med\" in tops:\n",
    "        return \"Alpha-Dominant\"\n",
    "    if \"beta_rel_med\" in tops or \"beta_over_alpha\" in tops:\n",
    "        if \"theta_rel_med\" in tops or \"theta_over_alpha\" in tops:\n",
    "            return \"Beta/Theta-Mixed\"\n",
    "        return \"Beta-Leaning\"\n",
    "    if \"spec_entropy_med\" in tops or \"hjorth_complexity_med\" in tops:\n",
    "        return \"High-Entropy\"\n",
    "    return \"Mixed\"\n",
    "\n",
    "names0 = {int(s): name_state_from_tops(amap0[str(s)][\"top_features\"]) for s in amap0.keys()}\n",
    "names1 = {int(s): name_state_from_tops(amap1[str(s)][\"top_features\"]) for s in amap1.keys()}\n",
    "\n",
    "print(\"\\nOriginal names:\", names0)\n",
    "print(\"Refit names:   \", names1)\n",
    "\n",
    "# --- Per-file compositions: original vs refit ---\n",
    "def composition(meta, labels):\n",
    "    df = meta.copy()\n",
    "    df[\"state\"] = labels\n",
    "    return (df.groupby(\"file\")[\"state\"]\n",
    "              .value_counts(normalize=True)\n",
    "              .rename(\"fraction\")\n",
    "              .reset_index()\n",
    "              .pivot(index=\"file\", columns=\"state\", values=\"fraction\")\n",
    "              .fillna(0.0))\n",
    "\n",
    "comp0 = composition(meta0, asgn0[\"state\"])\n",
    "comp1 = composition(meta1, asgn1[\"state\"])\n",
    "\n",
    "print(\"\\n=== Per-file composition (Original K={}, by fraction) ===\".format(med0.shape[0]))\n",
    "print(comp0.round(3))\n",
    "print(\"\\n=== Per-file composition (Refit K={}, by fraction) ===\".format(med1.shape[0]))\n",
    "print(comp1.round(3))\n",
    "\n",
    "# --- Probe S001R01 for EO/EC-like separation using alpha as proxy ---\n",
    "F0 = pd.concat([meta0, feat0], axis=1)\n",
    "S001 = F0[F0[\"file\"]==\"S001R01.edf\"].copy()\n",
    "if len(S001):\n",
    "    # Use a simple proxy: alpha-high epochs vs alpha-low (median split)\n",
    "    thr = S001[\"alpha_rel_med\"].median()\n",
    "    S001[\"alpha_high\"] = (S001[\"alpha_rel_med\"] >= thr).astype(int)\n",
    "    # Bring in both sets of state assignments aligned by epoch order\n",
    "    lab0 = asgn0.loc[S001.index, \"state\"].to_numpy()\n",
    "    lab1 = asgn1.loc[S001.index, \"state\"].to_numpy()\n",
    "    S001[\"orig_state\"] = lab0\n",
    "    S001[\"refit_state\"] = lab1\n",
    "\n",
    "    crosstab_orig = pd.crosstab(S001[\"alpha_high\"], S001[\"orig_state\"], normalize=\"index\").round(3)\n",
    "    crosstab_refit = pd.crosstab(S001[\"alpha_high\"], S001[\"refit_state\"], normalize=\"index\").round(3)\n",
    "\n",
    "    print(\"\\n=== S001R01 alpha-high vs original states (row = alpha group) ===\")\n",
    "    print(crosstab_orig)\n",
    "    print(\"\\n=== S001R01 alpha-high vs refit states (row = alpha group) ===\")\n",
    "    print(crosstab_refit)\n",
    "\n",
    "    # Save quick timeline plots for refit on S001R01\n",
    "    t = meta1.loc[S001.index, \"t_start_s\"].to_numpy()\n",
    "    rs = lab1\n",
    "    plt.figure(figsize=(10,1.6))\n",
    "    plt.scatter(t, rs, s=8, c=rs)\n",
    "    plt.yticks(sorted(S001[\"refit_state\"].unique()), [f\"S{s}\" for s in sorted(S001[\"refit_state\"].unique())])\n",
    "    plt.xlabel(\"time (s)\"); plt.title(\"S001R01 â€” Refit states timeline\")\n",
    "    plt.tight_layout()\n",
    "    outp = os.path.join(OD1, \"timeline_S001R01_refit.png\")\n",
    "    plt.savefig(outp, dpi=140); plt.close()\n",
    "    print(\"Saved\", outp)\n",
    "\n",
    "# --- Export alignment & names to CSV in refit folder ---\n",
    "align_out = align_df.copy()\n",
    "align_out[\"orig_name\"] = align_out[\"orig_state\"].map(names0)\n",
    "align_out[\"refit_name\"] = align_out[\"refit_state\"].map(names1)\n",
    "align_out.to_csv(os.path.join(OD1, \"alignment_refit_to_original.csv\"), index=False)\n",
    "\n",
    "pd.DataFrame({\"state\": list(names1.keys()),\n",
    "              \"name\": [names1[s] for s in names1.keys()]}\n",
    "            ).to_csv(os.path.join(OD1, \"refit_state_names.csv\"), index=False)\n",
    "\n",
    "print(\"\\nSaved:\", os.path.join(OD1, \"alignment_refit_to_original.csv\"))\n",
    "print(\"Saved:\", os.path.join(OD1, \"refit_state_names.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "576ee952-8073-48f8-a2dd-a3b8c8c9da23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== K=3 composition by file ===\n",
      "state_name     Alpha-Dominant  Beta/Theta-Mixed\n",
      "file                                           \n",
      "S001R01.edf             0.000             1.000\n",
      "S001R02.edf             1.000             0.000\n",
      "S001R03.edf             0.016             0.984\n",
      "sim_alpha.csv           0.000             1.000\n",
      "sim_theta.csv           0.000             1.000\n",
      "\n",
      "=== K=5 composition by file ===\n",
      "state_name     Alpha-Dominant  Beta-Leaning  High-Entropy\n",
      "file                                                     \n",
      "S001R01.edf              0.00           0.0          1.00\n",
      "S001R02.edf              1.00           0.0          0.00\n",
      "S001R03.edf              0.02           0.0          0.98\n",
      "sim_alpha.csv            0.00           1.0          0.00\n",
      "sim_theta.csv            0.00           1.0          0.00\n",
      "\n",
      "=== Majority mapping (state â†’ condition) & accuracy ===\n",
      "K=3: {'Alpha-Dominant': 'EC', 'Beta/Theta-Mixed': 'ME'}  | acc=0.746\n",
      "K=5: {'Alpha-Dominant': 'EC', 'High-Entropy': 'ME'}  | acc=0.744\n",
      "\n",
      "=== Alpha-Dominant fraction by condition (K=3) ===\n",
      "condition       EC   EO     ME  UNK\n",
      "file                               \n",
      "S001R01.edf    0.0  0.0  0.000  0.0\n",
      "S001R02.edf    1.0  0.0  0.000  0.0\n",
      "S001R03.edf    0.0  0.0  0.016  0.0\n",
      "sim_alpha.csv  0.0  0.0  0.000  0.0\n",
      "sim_theta.csv  0.0  0.0  0.000  0.0\n",
      "\n",
      "=== Alpha-Dominant fraction by condition (K=5) ===\n",
      "condition       EC   EO    ME  UNK\n",
      "file                              \n",
      "S001R01.edf    0.0  0.0  0.00  0.0\n",
      "S001R02.edf    1.0  0.0  0.00  0.0\n",
      "S001R03.edf    0.0  0.0  0.02  0.0\n",
      "sim_alpha.csv  0.0  0.0  0.00  0.0\n",
      "sim_theta.csv  0.0  0.0  0.00  0.0\n",
      "Saved ./cog_alphabet_eval\\alpha_fraction_bars_K3.png\n",
      "Saved ./cog_alphabet_eval\\alpha_fraction_bars_K5.png\n",
      "\n",
      "Artifacts written to: ./cog_alphabet_eval\n"
     ]
    }
   ],
   "source": [
    "# === Label & Score: map runs â†’ EO/EC/ME, score K=3 vs K=5, export tidy outputs ===\n",
    "import os, re, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# --- Folders from your previous runs ---\n",
    "OD3 = \"./cog_alphabet_physio\"         # original K=3 outputs\n",
    "OD5 = \"./cog_alphabet_physio_refit\"   # refit K=5 outputs\n",
    "OUT = \"./cog_alphabet_eval\"\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "# --- Load helpers ---\n",
    "def load_run(folder):\n",
    "    feat = pd.read_csv(os.path.join(folder, \"features.csv\"))\n",
    "    meta = pd.read_csv(os.path.join(folder, \"metadata.csv\"))\n",
    "    asgn = pd.read_csv(os.path.join(folder, \"state_assignments.csv\"))\n",
    "    with open(os.path.join(folder, \"cognitive_alphabet.json\")) as f:\n",
    "        amap = json.load(f)\n",
    "    return feat, meta, asgn, amap\n",
    "\n",
    "feat3, meta3, asgn3, amap3 = load_run(OD3)\n",
    "feat5, meta5, asgn5, amap5 = load_run(OD5)\n",
    "\n",
    "# --- Heuristic names from top features (same logic you used) ---\n",
    "def name_state_from_tops(tops):\n",
    "    if any(f.startswith(\"alpha_rel_\") for f in tops) or \"alpha_rel_med\" in tops:\n",
    "        return \"Alpha-Dominant\"\n",
    "    if \"beta_rel_med\" in tops or \"beta_over_alpha\" in tops:\n",
    "        if \"theta_rel_med\" in tops or \"theta_over_alpha\" in tops:\n",
    "            return \"Beta/Theta-Mixed\"\n",
    "        return \"Beta-Leaning\"\n",
    "    if \"spec_entropy_med\" in tops or \"hjorth_complexity_med\" in tops:\n",
    "        return \"High-Entropy\"\n",
    "    return \"Mixed\"\n",
    "\n",
    "names3 = {int(s): name_state_from_tops(amap3[str(s)][\"top_features\"]) for s in amap3.keys()}\n",
    "names5 = {int(s): name_state_from_tops(amap5[str(s)][\"top_features\"]) for s in amap5.keys()}\n",
    "\n",
    "# --- PhysioNet run â†’ condition mapping (per official protocol) ---\n",
    "# R01: Eyes Open, R02: Eyes Closed, R03: Task 1 (executed movement)\n",
    "# (More runs exist; we map only those present.)\n",
    "def run_to_cond(filename: str) -> str:\n",
    "    m = re.search(r\"R(\\d+)\", filename)\n",
    "    r = int(m.group(1)) if m else None\n",
    "    if r == 1:  return \"EO\"\n",
    "    if r == 2:  return \"EC\"\n",
    "    if r == 3:  return \"ME\"   # executed movement (Task 1)\n",
    "    return \"UNK\"\n",
    "\n",
    "def add_cond(meta: pd.DataFrame) -> pd.DataFrame:\n",
    "    out = meta.copy()\n",
    "    out[\"condition\"] = out[\"file\"].apply(run_to_cond)\n",
    "    return out\n",
    "\n",
    "M3 = add_cond(meta3)\n",
    "M5 = add_cond(meta5)\n",
    "\n",
    "# --- Join assignments ---\n",
    "D3 = pd.concat([M3.reset_index(drop=True), asgn3[\"state\"]], axis=1)\n",
    "D5 = pd.concat([M5.reset_index(drop=True), asgn5[\"state\"]], axis=1)\n",
    "D3[\"state_name\"] = D3[\"state\"].map(names3)\n",
    "D5[\"state_name\"] = D5[\"state\"].map(names5)\n",
    "\n",
    "# --- Condition Ã— State composition ---\n",
    "def comp_table(df):\n",
    "    return (df.groupby([\"condition\",\"file\"])[\"state_name\"]\n",
    "              .value_counts(normalize=True)\n",
    "              .rename(\"fraction\")\n",
    "              .reset_index())\n",
    "\n",
    "C3 = comp_table(D3)\n",
    "C5 = comp_table(D5)\n",
    "C3.to_csv(os.path.join(OUT, \"comp_K3.csv\"), index=False)\n",
    "C5.to_csv(os.path.join(OUT, \"comp_K5.csv\"), index=False)\n",
    "\n",
    "# --- Print compact summaries ---\n",
    "print(\"\\n=== K=3 composition by file ===\")\n",
    "print(C3.pivot_table(index=[\"file\"], columns=\"state_name\", values=\"fraction\", aggfunc=\"mean\").fillna(0).round(3))\n",
    "print(\"\\n=== K=5 composition by file ===\")\n",
    "print(C5.pivot_table(index=[\"file\"], columns=\"state_name\", values=\"fraction\", aggfunc=\"mean\").fillna(0).round(3))\n",
    "\n",
    "# --- Simple â€œmajority mappingâ€ scoring per condition ---\n",
    "def majority_map_and_score(df):\n",
    "    # majority condition for each state_name\n",
    "    tbl = (df.groupby([\"state_name\",\"condition\"]).size()\n",
    "             .rename(\"n\").reset_index())\n",
    "    maj = tbl.loc[tbl.groupby(\"state_name\")[\"n\"].idxmax()].set_index(\"state_name\")[\"condition\"].to_dict()\n",
    "    # predicted condition = majority condition of the epoch's state\n",
    "    pred = df[\"state_name\"].map(maj)\n",
    "    acc  = (pred == df[\"condition\"]).mean()\n",
    "    return maj, float(acc)\n",
    "\n",
    "maj3, acc3 = majority_map_and_score(D3[D3[\"condition\"].isin([\"EO\",\"EC\",\"ME\"])])\n",
    "maj5, acc5 = majority_map_and_score(D5[D5[\"condition\"].isin([\"EO\",\"EC\",\"ME\"])])\n",
    "\n",
    "pd.DataFrame({\"K\":[3,5],\n",
    "              \"accuracy\":[acc3, acc5],\n",
    "              \"mapping\":[maj3, maj5]}).to_csv(os.path.join(OUT, \"condition_accuracy.csv\"), index=False)\n",
    "\n",
    "print(\"\\n=== Majority mapping (state â†’ condition) & accuracy ===\")\n",
    "print(\"K=3:\", maj3, f\" | acc={acc3:.3f}\")\n",
    "print(\"K=5:\", maj5, f\" | acc={acc5:.3f}\")\n",
    "\n",
    "# --- EO vs EC separation using Alpha-Dominant fraction ---\n",
    "def alpha_fraction_by_cond(df):\n",
    "    pivot = (df.assign(is_alpha=(df[\"state_name\"]==\"Alpha-Dominant\").astype(int))\n",
    "               .groupby([\"condition\",\"file\"])[\"is_alpha\"].mean()\n",
    "               .rename(\"alpha_fraction\").reset_index())\n",
    "    return pivot\n",
    "\n",
    "AF3 = alpha_fraction_by_cond(D3)\n",
    "AF5 = alpha_fraction_by_cond(D5)\n",
    "AF3.to_csv(os.path.join(OUT, \"alpha_fraction_K3.csv\"), index=False)\n",
    "AF5.to_csv(os.path.join(OUT, \"alpha_fraction_K5.csv\"), index=False)\n",
    "\n",
    "print(\"\\n=== Alpha-Dominant fraction by condition (K=3) ===\")\n",
    "print(AF3.pivot(index=\"file\", columns=\"condition\", values=\"alpha_fraction\").round(3).fillna(0))\n",
    "print(\"\\n=== Alpha-Dominant fraction by condition (K=5) ===\")\n",
    "print(AF5.pivot(index=\"file\", columns=\"condition\", values=\"alpha_fraction\").round(3).fillna(0))\n",
    "\n",
    "# --- Quick bars for EO vs EC ---\n",
    "for K, AF in [(3,AF3),(5,AF5)]:\n",
    "    PL = AF[AF[\"condition\"].isin([\"EO\",\"EC\"])]\n",
    "    if not len(PL): \n",
    "        continue\n",
    "    plt.figure(figsize=(5,3))\n",
    "    for cond, color in [(\"EO\",None),(\"EC\",None)]:\n",
    "        vals = PL[PL[\"condition\"]==cond][\"alpha_fraction\"].to_numpy()\n",
    "        x    = np.arange(len(vals)) + (0 if cond==\"EO\" else 0.45)\n",
    "        w    = 0.4\n",
    "        plt.bar(x, vals, width=w, label=cond)\n",
    "    plt.xticks([])\n",
    "    plt.ylabel(\"Alpha-Dominant fraction\")\n",
    "    plt.title(f\"EO vs EC â€” Alpha fraction (K={K})\")\n",
    "    plt.legend()\n",
    "    fn = os.path.join(OUT, f\"alpha_fraction_bars_K{K}.png\")\n",
    "    plt.tight_layout(); plt.savefig(fn, dpi=140); plt.close()\n",
    "    print(\"Saved\", fn)\n",
    "\n",
    "print(\"\\nArtifacts written to:\", OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ea8dc58-e434-40a3-8193-9828fda88c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€¢ Found 5 file(s). Processing â€¦\n",
      "[1/5] S001R01.edf\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "lowpass frequency 80.0 must be less than Nyquist (80.0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m cfg.random_state = \u001b[32m42\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# 2) Re-run full pipeline with upgraded features\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;66;03m# 3) Quick side-by-side evaluation: old K=3 vs new betaSplit run\u001b[39;00m\n\u001b[32m     34\u001b[39m OD_OLD = \u001b[33m\"\u001b[39m\u001b[33m./cog_alphabet_physio\u001b[39m\u001b[33m\"\u001b[39m             \u001b[38;5;66;03m# your K=3\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36mrun_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     49\u001b[39m raw = load_raw_any(fp)\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m raw \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m raw = \u001b[43mbasic_clean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m epochs, starts, ends = make_epochs(raw)\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(epochs)==\u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 205\u001b[39m, in \u001b[36mbasic_clean\u001b[39m\u001b[34m(raw)\u001b[39m\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m: \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[38;5;66;03m# Bandpass + resample\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m \u001b[43mraw\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43ml_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mh_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(raw.info[\u001b[33m\"\u001b[39m\u001b[33msfreq\u001b[39m\u001b[33m\"\u001b[39m]-cfg.target_sfreq)>\u001b[32m1e-3\u001b[39m:\n\u001b[32m    207\u001b[39m     raw.resample(cfg.target_sfreq, npad=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m, verbose=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\cnt_genome\\.venv\\Lib\\site-packages\\mne\\io\\base.py:1173\u001b[39m, in \u001b[36mBaseRaw.filter\u001b[39m\u001b[34m(self, l_freq, h_freq, picks, filter_length, l_trans_bandwidth, h_trans_bandwidth, n_jobs, method, iir_params, phase, fir_window, fir_design, skip_by_annotation, pad, verbose)\u001b[39m\n\u001b[32m   1154\u001b[39m \u001b[38;5;129m@copy_doc\u001b[39m(FilterMixin.filter)\n\u001b[32m   1155\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfilter\u001b[39m(\n\u001b[32m   1156\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1171\u001b[39m     verbose=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1172\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1173\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m        \u001b[49m\u001b[43ml_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mh_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpicks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilter_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m        \u001b[49m\u001b[43ml_trans_bandwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m        \u001b[49m\u001b[43mh_trans_bandwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[43miir_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43miir_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[43mphase\u001b[49m\u001b[43m=\u001b[49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfir_window\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfir_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfir_design\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfir_design\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m        \u001b[49m\u001b[43mskip_by_annotation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_by_annotation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<decorator-gen-58>:10\u001b[39m, in \u001b[36mfilter\u001b[39m\u001b[34m(self, l_freq, h_freq, picks, filter_length, l_trans_bandwidth, h_trans_bandwidth, n_jobs, method, iir_params, phase, fir_window, fir_design, skip_by_annotation, pad, verbose)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\cnt_genome\\.venv\\Lib\\site-packages\\mne\\filter.py:2558\u001b[39m, in \u001b[36mFilterMixin.filter\u001b[39m\u001b[34m(self, l_freq, h_freq, picks, filter_length, l_trans_bandwidth, h_trans_bandwidth, n_jobs, method, iir_params, phase, fir_window, fir_design, skip_by_annotation, pad, verbose)\u001b[39m\n\u001b[32m   2554\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m si, (start, stop) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(onsets, ends)):\n\u001b[32m   2555\u001b[39m     \u001b[38;5;66;03m# Only output filter params once (for info level), and only warn\u001b[39;00m\n\u001b[32m   2556\u001b[39m     \u001b[38;5;66;03m# once about the length criterion (longest segment is too short)\u001b[39;00m\n\u001b[32m   2557\u001b[39m     use_verbose = verbose \u001b[38;5;28;01mif\u001b[39;00m si == max_idx \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2558\u001b[39m     \u001b[43mfilter_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2559\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m:\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2560\u001b[39m \u001b[43m        \u001b[49m\u001b[43ms_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2561\u001b[39m \u001b[43m        \u001b[49m\u001b[43ml_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2562\u001b[39m \u001b[43m        \u001b[49m\u001b[43mh_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2563\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpicks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2564\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilter_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m        \u001b[49m\u001b[43ml_trans_bandwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m        \u001b[49m\u001b[43mh_trans_bandwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m        \u001b[49m\u001b[43miir_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m        \u001b[49m\u001b[43mphase\u001b[49m\u001b[43m=\u001b[49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfir_window\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfir_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2573\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfir_design\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfir_design\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2575\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2576\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2577\u001b[39m \u001b[38;5;66;03m# update info if filter is applied to all data channels/vertices,\u001b[39;00m\n\u001b[32m   2578\u001b[39m \u001b[38;5;66;03m# and it's not a band-stop filter\u001b[39;00m\n\u001b[32m   2579\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, _BaseSourceEstimate):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<decorator-gen-53>:10\u001b[39m, in \u001b[36mfilter_data\u001b[39m\u001b[34m(data, sfreq, l_freq, h_freq, picks, filter_length, l_trans_bandwidth, h_trans_bandwidth, n_jobs, method, iir_params, copy, phase, fir_window, fir_design, pad, verbose)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\cnt_genome\\.venv\\Lib\\site-packages\\mne\\filter.py:1016\u001b[39m, in \u001b[36mfilter_data\u001b[39m\u001b[34m(data, sfreq, l_freq, h_freq, picks, filter_length, l_trans_bandwidth, h_trans_bandwidth, n_jobs, method, iir_params, copy, phase, fir_window, fir_design, pad, verbose)\u001b[39m\n\u001b[32m   1014\u001b[39m data = _check_filterable(data)\n\u001b[32m   1015\u001b[39m iir_params, method = _check_method(method, iir_params)\n\u001b[32m-> \u001b[39m\u001b[32m1016\u001b[39m filt = \u001b[43mcreate_filter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m    \u001b[49m\u001b[43msfreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m    \u001b[49m\u001b[43ml_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m    \u001b[49m\u001b[43mh_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilter_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m    \u001b[49m\u001b[43ml_trans_bandwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m    \u001b[49m\u001b[43mh_trans_bandwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m    \u001b[49m\u001b[43miir_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m    \u001b[49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfir_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfir_design\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mfir\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfft\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1031\u001b[39m     data = _overlap_add_filter(data, filt, \u001b[38;5;28;01mNone\u001b[39;00m, phase, picks, n_jobs, copy, pad)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<decorator-gen-54>:12\u001b[39m, in \u001b[36mcreate_filter\u001b[39m\u001b[34m(data, sfreq, l_freq, h_freq, filter_length, l_trans_bandwidth, h_trans_bandwidth, method, iir_params, phase, fir_window, fir_design, verbose)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\cnt_genome\\.venv\\Lib\\site-packages\\mne\\filter.py:1312\u001b[39m, in \u001b[36mcreate_filter\u001b[39m\u001b[34m(data, sfreq, l_freq, h_freq, filter_length, l_trans_bandwidth, h_trans_bandwidth, method, iir_params, phase, fir_window, fir_design, verbose)\u001b[39m\n\u001b[32m   1297\u001b[39m l_freq, h_freq = l_freq.item(), h_freq.item()\n\u001b[32m   1298\u001b[39m logger.info(\n\u001b[32m   1299\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSetting up band-pass filter from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml_freq\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m0.2g\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh_freq\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m0.2g\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Hz\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1300\u001b[39m )\n\u001b[32m   1301\u001b[39m (\n\u001b[32m   1302\u001b[39m     data,\n\u001b[32m   1303\u001b[39m     sfreq,\n\u001b[32m   1304\u001b[39m     f_p1,\n\u001b[32m   1305\u001b[39m     f_p2,\n\u001b[32m   1306\u001b[39m     f_s1,\n\u001b[32m   1307\u001b[39m     f_s2,\n\u001b[32m   1308\u001b[39m     filter_length,\n\u001b[32m   1309\u001b[39m     phase,\n\u001b[32m   1310\u001b[39m     fir_window,\n\u001b[32m   1311\u001b[39m     fir_design,\n\u001b[32m-> \u001b[39m\u001b[32m1312\u001b[39m ) = \u001b[43m_triage_filter_params\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1313\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1314\u001b[39m \u001b[43m    \u001b[49m\u001b[43msfreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1315\u001b[39m \u001b[43m    \u001b[49m\u001b[43ml_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1316\u001b[39m \u001b[43m    \u001b[49m\u001b[43mh_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1317\u001b[39m \u001b[43m    \u001b[49m\u001b[43ml_trans_bandwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1318\u001b[39m \u001b[43m    \u001b[49m\u001b[43mh_trans_bandwidth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1319\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilter_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1320\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mphase\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfir_window\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfir_design\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1324\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33miir\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1326\u001b[39m     out = construct_iir_filter(\n\u001b[32m   1327\u001b[39m         iir_params,\n\u001b[32m   1328\u001b[39m         [f_p1, f_p2],\n\u001b[32m   (...)\u001b[39m\u001b[32m   1332\u001b[39m         phase=phase,\n\u001b[32m   1333\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\cnt_genome\\.venv\\Lib\\site-packages\\mne\\filter.py:2188\u001b[39m, in \u001b[36m_triage_filter_params\u001b[39m\u001b[34m(x, sfreq, l_freq, h_freq, l_trans_bandwidth, h_trans_bandwidth, filter_length, method, phase, fir_window, fir_design, bands, reverse)\u001b[39m\n\u001b[32m   2186\u001b[39m     h_freq = cast(h_freq)\n\u001b[32m   2187\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m np.any(h_freq >= sfreq / \u001b[32m2.0\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m2188\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2189\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlowpass frequency \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh_freq\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be less than Nyquist (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msfreq\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m2.0\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2190\u001b[39m         )\n\u001b[32m   2192\u001b[39m dB_cutoff = \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# meaning, don't try to compute or report\u001b[39;00m\n\u001b[32m   2193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m bands == \u001b[33m\"\u001b[39m\u001b[33mscalar\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(h_freq) == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(l_freq) == \u001b[32m1\u001b[39m):\n",
      "\u001b[31mValueError\u001b[39m: lowpass frequency 80.0 must be less than Nyquist (80.0)"
     ]
    }
   ],
   "source": [
    "# === One-Cell Upgrade: split beta, extend gamma, re-run + evaluate side-by-side ===\n",
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "# 0) Guard that the mega cell exists\n",
    "needed = [\"cfg\",\"run_pipeline\",\"StandardScaler\",\"PCA\",\"KMeans\",\"silhouette_score\",\"_hmm_smooth\",\n",
    "          \"spectral_features\",\"basic_clean\",\"load_raw_any\",\"_find_files\",\"_download_all\",\"make_epochs\",\"pd\",\"np\",\"plt\"]\n",
    "missing = [n for n in needed if n not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(\"Missing from session: \" + \", \".join(missing) + \". Run the mega cell first.\")\n",
    "\n",
    "# 1) Configure richer bands & outputs (keep your Physio URLs or local files)\n",
    "cfg.data_dir   = \"./brainwaves\"\n",
    "cfg.output_dir = \"./cog_alphabet_physio_betaSplit\"\n",
    "cfg.dataset_urls = []  # already downloaded; leave empty to reuse local files\n",
    "cfg.l_freq, cfg.h_freq = 0.5, 80.0\n",
    "cfg.bands = {\n",
    "    \"delta\": (1.0, 4.0),\n",
    "    \"theta\": (4.0, 8.0),\n",
    "    \"alpha\": (8.0, 13.0),\n",
    "    \"beta_low\":  (13.0, 20.0),\n",
    "    \"beta_high\": (20.0, 35.0),\n",
    "    \"gamma\": (35.0, 80.0),\n",
    "}\n",
    "cfg.epoch_len_s = 2.0\n",
    "cfg.step_s      = 0.5\n",
    "cfg.k_range     = (4, 7)     # a tad finer, now that features are richer\n",
    "cfg.use_hmm     = True\n",
    "cfg.random_state = 42\n",
    "\n",
    "# 2) Re-run full pipeline with upgraded features\n",
    "run_pipeline()\n",
    "\n",
    "# 3) Quick side-by-side evaluation: old K=3 vs new betaSplit run\n",
    "OD_OLD = \"./cog_alphabet_physio\"             # your K=3\n",
    "OD_NEW = \"./cog_alphabet_physio_betaSplit\"   # this run\n",
    "OUT    = \"./cog_alphabet_eval_betaSplit\"\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "def load_run(folder):\n",
    "    f = pd.read_csv(os.path.join(folder,\"features.csv\"))\n",
    "    m = pd.read_csv(os.path.join(folder,\"metadata.csv\"))\n",
    "    a = pd.read_csv(os.path.join(folder,\"state_assignments.csv\"))\n",
    "    with open(os.path.join(folder,\"cognitive_alphabet.json\")) as fh:\n",
    "        amap = json.load(fh)\n",
    "    return f,m,a,amap\n",
    "\n",
    "f_old, m_old, a_old, A_old = load_run(OD_OLD)\n",
    "f_new, m_new, a_new, A_new = load_run(OD_NEW)\n",
    "\n",
    "# harmonize file order & lengths (we processed the same raws with same epoching)\n",
    "assert len(m_old)==len(m_new) and (m_old[\"file\"].values==m_new[\"file\"].values).all(), \"Metadata misalignment.\"\n",
    "\n",
    "# heuristic namer\n",
    "def name_from_tops(tops):\n",
    "    tops = list(tops)\n",
    "    if any(t.startswith(\"alpha_rel_\") for t in tops): return \"Alpha-Dominant\"\n",
    "    if \"beta_high_rel_med\" in tops or \"beta_low_rel_med\" in tops or \"beta_over_alpha\" in tops: \n",
    "        if \"theta_rel_med\" in tops or \"theta_over_alpha\" in tops: return \"Beta/Theta-Mixed\"\n",
    "        return \"Beta-Leaning\"\n",
    "    if \"spec_entropy_med\" in tops or \"hjorth_complexity_med\" in tops: return \"High-Entropy\"\n",
    "    return \"Mixed\"\n",
    "\n",
    "names_old = {int(s): name_from_tops(A_old[str(s)][\"top_features\"]) for s in A_old}\n",
    "names_new = {int(s): name_from_tops(A_new[str(s)][\"top_features\"]) for s in A_new}\n",
    "\n",
    "# attach a coarse condition from run-id (R01=EO, R02=EC, R03=ME)\n",
    "import re\n",
    "def cond_from_file(fn):\n",
    "    m = re.search(r\"R(\\d+)\", fn); r = int(m.group(1)) if m else None\n",
    "    return {1:\"EO\", 2:\"EC\", 3:\"ME\"}.get(r, \"UNK\")\n",
    "\n",
    "D_old = m_old.copy(); D_old[\"state\"]=a_old[\"state\"]; D_old[\"name\"]=D_old[\"state\"].map(names_old)\n",
    "D_old[\"cond\"]=D_old[\"file\"].apply(cond_from_file)\n",
    "D_new = m_new.copy(); D_new[\"state\"]=a_new[\"state\"]; D_new[\"name\"]=D_new[\"state\"].map(names_new)\n",
    "D_new[\"cond\"]=D_new[\"file\"].apply(cond_from_file)\n",
    "\n",
    "# compositions\n",
    "def comp(df):\n",
    "    return (df.groupby([\"file\",\"cond\"])[\"name\"]\n",
    "              .value_counts(normalize=True)\n",
    "              .rename(\"fraction\")\n",
    "              .reset_index()\n",
    "              .pivot_table(index=[\"file\",\"cond\"], columns=\"name\", values=\"fraction\", fill_value=0.0)\n",
    "              .sort_index())\n",
    "\n",
    "C_old = comp(D_old); C_new = comp(D_new)\n",
    "C_old.to_csv(os.path.join(OUT,\"composition_old.csv\"))\n",
    "C_new.to_csv(os.path.join(OUT,\"composition_new_betaSplit.csv\"))\n",
    "\n",
    "print(\"\\n=== OLD (Kâ‰ˆ3) composition ===\")\n",
    "print(C_old.round(3))\n",
    "print(\"\\n=== NEW (Î²-split, Î³â†‘) composition ===\")\n",
    "print(C_new.round(3))\n",
    "\n",
    "# alpha dominance by condition\n",
    "def alpha_frac(df):\n",
    "    x = (df.assign(alpha=(df[\"name\"]==\"Alpha-Dominant\").astype(int))\n",
    "           .groupby([\"file\",\"cond\"])[\"alpha\"].mean().rename(\"alpha_fraction\").reset_index())\n",
    "    return x.pivot(index=\"file\", columns=\"cond\", values=\"alpha_fraction\").fillna(0.0)\n",
    "\n",
    "AF_old = alpha_frac(D_old); AF_new = alpha_frac(D_new)\n",
    "AF_old.to_csv(os.path.join(OUT,\"alpha_fraction_old.csv\"))\n",
    "AF_new.to_csv(os.path.join(OUT,\"alpha_fraction_new_betaSplit.csv\"))\n",
    "\n",
    "print(\"\\nAlpha fraction (OLD):\\n\", AF_old.round(3))\n",
    "print(\"\\nAlpha fraction (NEW Î²-split):\\n\", AF_new.round(3))\n",
    "\n",
    "# quick bar plot: EO vs EC alpha fraction (new)\n",
    "PL = AF_new.reindex(columns=[\"EO\",\"EC\"]).dropna(how=\"all\", axis=1)\n",
    "if not PL.empty:\n",
    "    plt.figure(figsize=(5,3))\n",
    "    x = np.arange(len(PL.index))\n",
    "    if \"EO\" in PL.columns: plt.bar(x-0.18, PL[\"EO\"].values, width=0.36, label=\"EO\")\n",
    "    if \"EC\" in PL.columns: plt.bar(x+0.18, PL[\"EC\"].values, width=0.36, label=\"EC\")\n",
    "    plt.xticks(x, PL.index, rotation=30, ha=\"right\")\n",
    "    plt.ylabel(\"Alpha-Dominant fraction\")\n",
    "    plt.title(\"EO vs EC (Î²-split run)\")\n",
    "    plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT,\"alpha_bars_betaSplit.png\"), dpi=140); plt.close()\n",
    "\n",
    "# save the new state names table\n",
    "pd.DataFrame({\"state\": list(names_new.keys()),\n",
    "              \"name\": [names_new[k] for k in names_new]}).to_csv(os.path.join(OUT,\"state_names_betaSplit.csv\"), index=False)\n",
    "\n",
    "print(\"\\nArtifacts â†’\", OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2038011c-1959-4310-873d-2daefbaed2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€¢ Found 5 file(s). Processing â€¦\n",
      "[1/5] S001R01.edf\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'beta_rel_med'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     60\u001b[39m cfg.use_hmm     = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     61\u001b[39m cfg.random_state = \u001b[32m42\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# 3) Quick side-by-side summary vs your original (optional, fast)\u001b[39;00m\n\u001b[32m     66\u001b[39m OD_OLD = \u001b[33m\"\u001b[39m\u001b[33m./cog_alphabet_physio\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mrun_pipeline\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(epochs)==\u001b[32m0\u001b[39m:\n\u001b[32m     54\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m   (no epochs)\u001b[39m\u001b[33m\"\u001b[39m); \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m feat = \u001b[43mspectral_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbands\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m meta = pd.DataFrame({\n\u001b[32m     57\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfile\u001b[39m\u001b[33m\"\u001b[39m: os.path.basename(fp),\n\u001b[32m     58\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mepoch_idx\u001b[39m\u001b[33m\"\u001b[39m: np.arange(\u001b[38;5;28mlen\u001b[39m(epochs)),\n\u001b[32m   (...)\u001b[39m\u001b[32m     61\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mn_channels\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[38;5;28mlen\u001b[39m(epochs.ch_names)]*\u001b[38;5;28mlen\u001b[39m(epochs)\n\u001b[32m     62\u001b[39m })\n\u001b[32m     63\u001b[39m all_feat.append(feat); all_meta.append(meta)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 258\u001b[39m, in \u001b[36mspectral_features\u001b[39m\u001b[34m(epochs, bands)\u001b[39m\n\u001b[32m    255\u001b[39m     feats[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_rel_iqr\u001b[39m\u001b[33m\"\u001b[39m] = np.subtract(*np.percentile(bp, [\u001b[32m75\u001b[39m,\u001b[32m25\u001b[39m], axis=\u001b[32m1\u001b[39m))\n\u001b[32m    256\u001b[39m     feats[\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_rel_std\u001b[39m\u001b[33m\"\u001b[39m] = np.std(bp, axis=\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m alpha = feats[\u001b[33m\"\u001b[39m\u001b[33malpha_rel_med\u001b[39m\u001b[33m\"\u001b[39m]; theta = feats[\u001b[33m\"\u001b[39m\u001b[33mtheta_rel_med\u001b[39m\u001b[33m\"\u001b[39m]; beta = \u001b[43mfeats\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbeta_rel_med\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    259\u001b[39m feats[\u001b[33m\"\u001b[39m\u001b[33mtheta_over_alpha\u001b[39m\u001b[33m\"\u001b[39m] = theta/np.maximum(alpha,\u001b[32m1e-6\u001b[39m)\n\u001b[32m    260\u001b[39m feats[\u001b[33m\"\u001b[39m\u001b[33mbeta_over_alpha\u001b[39m\u001b[33m\"\u001b[39m]  = beta /np.maximum(alpha,\u001b[32m1e-6\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'beta_rel_med'"
     ]
    }
   ],
   "source": [
    "# === Hotfix: resample before filter + safe Nyquist clamps, then re-run ===\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, os, json, gc, re\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# 1) Patch basic_clean to RESAMPLE FIRST and clamp freqs safely\n",
    "def basic_clean(raw):\n",
    "    raw = raw.copy()\n",
    "\n",
    "    # --- Resample FIRST to target_sfreq (guarantees roomy Nyquist) ---\n",
    "    if abs(raw.info[\"sfreq\"] - cfg.target_sfreq) > 1e-6:\n",
    "        raw.resample(cfg.target_sfreq, npad=\"auto\", verbose=False)\n",
    "\n",
    "    sf = float(raw.info[\"sfreq\"])\n",
    "    ny = sf / 2.0\n",
    "    # safe low/high with margins\n",
    "    h_eff = min(cfg.h_freq, ny - 1.0)              # keep â‰¥1 Hz margin from Nyquist\n",
    "    l_eff = max(cfg.l_freq, 0.0)\n",
    "    if l_eff >= h_eff:\n",
    "        l_eff = max(0.1, h_eff - 0.5)              # ensure valid band\n",
    "\n",
    "    # --- Notch: drop any tones >= Nyquist-1 ---\n",
    "    try:\n",
    "        if cfg.notch:\n",
    "            notch_list = [f for f in cfg.notch if f < (ny - 1.0)]\n",
    "            if notch_list:\n",
    "                raw.notch_filter(notch_list, verbose=False)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # --- Bandpass ---\n",
    "    raw.filter(l_eff, h_eff, verbose=False)\n",
    "\n",
    "    # --- Reference & montage if EEG present ---\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "        try:\n",
    "            raw.set_montage(mne.channels.make_standard_montage(cfg.montage),\n",
    "                            match_case=False, match_alias=True, on_missing=\"warn\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return raw\n",
    "\n",
    "# 2) Keep your richer bands & rerun end-to-end\n",
    "cfg.data_dir   = \"./brainwaves\"\n",
    "cfg.output_dir = \"./cog_alphabet_physio_betaSplit\"\n",
    "cfg.dataset_urls = []  # reuse already-downloaded EDFs\n",
    "cfg.target_sfreq = 250.0          # ensures Nyquist=125 Hz > 80 Hz\n",
    "cfg.l_freq, cfg.h_freq = 0.5, 80.0\n",
    "cfg.bands = {\n",
    "    \"delta\": (1.0, 4.0),\n",
    "    \"theta\": (4.0, 8.0),\n",
    "    \"alpha\": (8.0, 13.0),\n",
    "    \"beta_low\":  (13.0, 20.0),\n",
    "    \"beta_high\": (20.0, 35.0),\n",
    "    \"gamma\":     (35.0, 80.0),\n",
    "}\n",
    "cfg.epoch_len_s = 2.0\n",
    "cfg.step_s      = 0.5\n",
    "cfg.k_range     = (4, 7)\n",
    "cfg.use_hmm     = True\n",
    "cfg.random_state = 42\n",
    "\n",
    "run_pipeline()\n",
    "\n",
    "# 3) Quick side-by-side summary vs your original (optional, fast)\n",
    "OD_OLD = \"./cog_alphabet_physio\"\n",
    "OD_NEW = \"./cog_alphabet_physio_betaSplit\"\n",
    "OUT    = \"./cog_alphabet_eval_betaSplit\"; os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "def load_run(folder):\n",
    "    f = pd.read_csv(os.path.join(folder,\"features.csv\"))\n",
    "    m = pd.read_csv(os.path.join(folder,\"metadata.csv\"))\n",
    "    a = pd.read_csv(os.path.join(folder,\"state_assignments.csv\"))\n",
    "    with open(os.path.join(folder,\"cognitive_alphabet.json\")) as fh: A = json.load(fh)\n",
    "    return f,m,a,A\n",
    "\n",
    "f_old, m_old, a_old, A_old = load_run(OD_OLD)\n",
    "f_new, m_new, a_new, A_new = load_run(OD_NEW)\n",
    "assert (m_old[\"file\"].values==m_new[\"file\"].values).all(), \"Metadata misalignment; re-run both in same session.\"\n",
    "\n",
    "def name_from_tops(tops):\n",
    "    if any(t.startswith(\"alpha_rel_\") for t in tops): return \"Alpha-Dominant\"\n",
    "    if \"beta_high_rel_med\" in tops or \"beta_low_rel_med\" in tops or \"beta_over_alpha\" in tops:\n",
    "        if \"theta_rel_med\" in tops or \"theta_over_alpha\" in tops: return \"Beta/Theta-Mixed\"\n",
    "        return \"Beta-Leaning\"\n",
    "    if \"spec_entropy_med\" in tops or \"hjorth_complexity_med\" in tops: return \"High-Entropy\"\n",
    "    return \"Mixed\"\n",
    "\n",
    "names_old = {int(s): name_from_tops(A_old[str(s)][\"top_features\"]) for s in A_old}\n",
    "names_new = {int(s): name_from_tops(A_new[str(s)][\"top_features\"]) for s in A_new}\n",
    "\n",
    "def cond_from_file(fn):\n",
    "    m = re.search(r\"R(\\d+)\", fn); r = int(m.group(1)) if m else None\n",
    "    return {1:\"EO\", 2:\"EC\", 3:\"ME\"}.get(r, \"UNK\")\n",
    "\n",
    "D_old = m_old.copy(); D_old[\"state\"]=a_old[\"state\"]; D_old[\"name\"]=D_old[\"state\"].map(names_old); D_old[\"cond\"]=D_old[\"file\"].apply(cond_from_file)\n",
    "D_new = m_new.copy(); D_new[\"state\"]=a_new[\"state\"]; D_new[\"name\"]=D_new[\"state\"].map(names_new); D_new[\"cond\"]=D_new[\"file\"].apply(cond_from_file)\n",
    "\n",
    "def comp(df):\n",
    "    return (df.groupby([\"file\",\"cond\"])[\"name\"]\n",
    "              .value_counts(normalize=True)\n",
    "              .rename(\"fraction\")\n",
    "              .reset_index()\n",
    "              .pivot_table(index=[\"file\",\"cond\"], columns=\"name\", values=\"fraction\", fill_value=0.0)\n",
    "              .sort_index())\n",
    "\n",
    "C_old = comp(D_old); C_new = comp(D_new)\n",
    "C_old.to_csv(os.path.join(OUT,\"composition_old.csv\"))\n",
    "C_new.to_csv(os.path.join(OUT,\"composition_new_betaSplit.csv\"))\n",
    "print(\"\\n=== OLD (Kâ‰ˆ3) composition ===\\n\", C_old.round(3))\n",
    "print(\"\\n=== NEW (Î²-split, Î³â†‘) composition ===\\n\", C_new.round(3))\n",
    "\n",
    "# EO vs EC alpha contrast (new)\n",
    "def alpha_frac(df):\n",
    "    x = (df.assign(alpha=(df[\"name\"]==\"Alpha-Dominant\").astype(int))\n",
    "           .groupby([\"file\",\"cond\"])[\"alpha\"].mean().rename(\"alpha_fraction\").reset_index())\n",
    "    return x.pivot(index=\"file\", columns=\"cond\", values=\"alpha_fraction\").fillna(0.0)\n",
    "\n",
    "AF_new = alpha_frac(D_new); AF_new.to_csv(os.path.join(OUT,\"alpha_fraction_new_betaSplit.csv\"))\n",
    "print(\"\\nAlpha fraction (NEW Î²-split):\\n\", AF_new.round(3))\n",
    "\n",
    "print(\"\\nArtifacts â†’\", OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15ca7678-586e-4d23-90e7-b003247674f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€¢ Found 5 file(s). Processing â€¦\n",
      "[1/5] S001R01.edf\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[2/5] S001R02.edf\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[3/5] S001R03.edf\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[4/5] sim_alpha.csv\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[5/5] sim_theta.csv\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "âœ“ Saved features/metadata.\n",
      "â˜… Selected K=4 (silhouette=0.295)\n",
      "âœ“ HMM smoothing applied.\n",
      "âœ“ Saved embedding.png\n",
      "âœ“ Saved state_feature_signatures.png\n",
      "â± Done in 3.7s. Outputs â†’ ./cog_alphabet_physio_betaSplit\n",
      "\n",
      "=== OLD (Kâ‰ˆ3) composition ===\n",
      " name                Alpha-Dominant  Beta/Theta-Mixed\n",
      "file          cond                                  \n",
      "S001R01.edf   EO             0.000             1.000\n",
      "S001R02.edf   EC             1.000             0.000\n",
      "S001R03.edf   ME             0.016             0.984\n",
      "sim_alpha.csv UNK            0.000             1.000\n",
      "sim_theta.csv UNK            0.000             1.000\n",
      "\n",
      "=== NEW (Î²-split, Î³â†‘) composition ===\n",
      " name                Alpha-Dominant  Beta-Leaning  Beta/Theta-Mixed  Mixed\n",
      "file          cond                                                       \n",
      "S001R01.edf   EO             0.000           0.0             0.218  0.782\n",
      "S001R02.edf   EC             0.983           0.0             0.000  0.017\n",
      "S001R03.edf   ME             0.012           0.0             0.385  0.603\n",
      "sim_alpha.csv UNK            0.000           1.0             0.000  0.000\n",
      "sim_theta.csv UNK            0.000           1.0             0.000  0.000\n",
      "\n",
      "Alpha fraction (NEW Î²-split):\n",
      " cond              EC   EO     ME  UNK\n",
      "file                                 \n",
      "S001R01.edf    0.000  0.0  0.000  0.0\n",
      "S001R02.edf    0.983  0.0  0.000  0.0\n",
      "S001R03.edf    0.000  0.0  0.012  0.0\n",
      "sim_alpha.csv  0.000  0.0  0.000  0.0\n",
      "sim_theta.csv  0.000  0.0  0.000  0.0\n",
      "\n",
      "Artifacts â†’ ./cog_alphabet_eval_betaSplit\n"
     ]
    }
   ],
   "source": [
    "# === Hotfix: dynamic bands (combine beta_*), then re-run pipeline & summarize ===\n",
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt, re\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# 1) Patch spectral_features to handle beta_low/beta_high (and any alpha_*, theta_*), while\n",
    "#    still exporting per-band features. It also builds composite alpha/theta/beta for ratios.\n",
    "from scipy.signal import welch\n",
    "\n",
    "def spectral_features(epochs, bands):\n",
    "    X = epochs.get_data()  # (n_ep, n_ch, n_t)\n",
    "    n_ep, n_ch, n_t = X.shape\n",
    "    sf = epochs.info[\"sfreq\"]\n",
    "    nperseg = min(int(sf*2), n_t); noverlap = nperseg // 2\n",
    "    freqs, psd = welch(X, fs=sf, nperseg=nperseg, noverlap=noverlap, axis=-1, average=\"median\")\n",
    "\n",
    "    # Helper: indices within global analysis passband\n",
    "    def _band_idx(freqs, lo, hi): \n",
    "        return np.where((freqs >= lo) & (freqs < hi))[0]\n",
    "\n",
    "    aidx = _band_idx(freqs, cfg.l_freq, cfg.h_freq)\n",
    "    tot = np.maximum(psd[:, :, aidx].sum(axis=-1), 1e-12)  # (n_ep, n_ch)\n",
    "\n",
    "    feats = {}\n",
    "    # Accumulators for composite ratios\n",
    "    accum = {\"alpha\": np.zeros((n_ep, n_ch)), \"theta\": np.zeros((n_ep, n_ch)), \"beta\": np.zeros((n_ep, n_ch))}\n",
    "\n",
    "    for name, (lo, hi) in bands.items():\n",
    "        idx = _band_idx(freqs, lo, hi)\n",
    "        if idx.size == 0:\n",
    "            # No overlap with spectrum, fill zeros for this band\n",
    "            bp = np.zeros((n_ep, n_ch))\n",
    "        else:\n",
    "            bp = psd[:, :, idx].sum(axis=-1) / tot  # relative bandpower per epochÃ—channel\n",
    "\n",
    "        # Export per-band summaries\n",
    "        feats[f\"{name}_rel_med\"] = np.median(bp, axis=1)\n",
    "        feats[f\"{name}_rel_iqr\"] = np.subtract(*np.percentile(bp, [75, 25], axis=1))\n",
    "        feats[f\"{name}_rel_std\"] = np.std(bp, axis=1)\n",
    "\n",
    "        # Route into composites for ratio features\n",
    "        head = name.split(\"_\", 1)[0].lower()  # e.g., 'beta' from 'beta_low'\n",
    "        if head in accum:\n",
    "            accum[head] = accum[head] + bp  # sum sub-bands\n",
    "\n",
    "    # Now compute composite alpha/theta/beta medians for ratios\n",
    "    alpha_med = np.median(accum[\"alpha\"], axis=1) if accum[\"alpha\"].any() else np.zeros(n_ep)\n",
    "    theta_med = np.median(accum[\"theta\"], axis=1) if accum[\"theta\"].any() else np.zeros(n_ep)\n",
    "    beta_med  = np.median(accum[\"beta\"],  axis=1) if accum[\"beta\"].any()  else np.zeros(n_ep)\n",
    "\n",
    "    # Expose composite bands explicitly (useful for downstream inspection)\n",
    "    feats[\"alpha_rel_med\"] = alpha_med\n",
    "    feats[\"theta_rel_med\"] = theta_med\n",
    "    feats[\"beta_rel_med\"]  = beta_med\n",
    "\n",
    "    # Ratios (safe divisions)\n",
    "    feats[\"theta_over_alpha\"] = theta_med / np.maximum(alpha_med, 1e-6)\n",
    "    feats[\"beta_over_alpha\"]  = beta_med  / np.maximum(alpha_med, 1e-6)\n",
    "    feats[\"(alpha+theta)_over_beta\"] = (alpha_med + theta_med) / np.maximum(beta_med, 1e-6)\n",
    "\n",
    "    # Spectral entropy & centroid within analysis band\n",
    "    p = psd[:, :, aidx]\n",
    "    p_norm = p / np.maximum(p.sum(axis=-1, keepdims=True), 1e-12)\n",
    "    H = -np.sum(p_norm * np.log2(p_norm + 1e-12), axis=-1)\n",
    "    Hn = H / np.log2(p.shape[-1])\n",
    "    feats[\"spec_entropy_med\"] = np.median(Hn, axis=1)\n",
    "    feats[\"spec_entropy_std\"] = np.std(Hn, axis=1)\n",
    "\n",
    "    f = freqs[aidx].reshape(1, 1, -1)\n",
    "    centroid = (p * f).sum(axis=-1) / np.maximum(p.sum(axis=-1), 1e-12)\n",
    "    feats[\"spec_centroid_med\"] = np.median(centroid, axis=1)\n",
    "    feats[\"spec_centroid_std\"] = np.std(centroid, axis=1)\n",
    "\n",
    "    # Hjorth parameters on time series\n",
    "    def hjorth_params(x):\n",
    "        d1 = np.diff(x, axis=-1)\n",
    "        var0 = np.var(x, axis=-1) + 1e-12\n",
    "        var1 = np.var(d1, axis=-1) + 1e-12\n",
    "        mob  = np.sqrt(var1 / var0)\n",
    "        d2 = np.diff(d1, axis=-1)\n",
    "        var2 = np.var(d2, axis=-1) + 1e-12\n",
    "        comp = np.sqrt((var2 / var1) / (var1 / var0))\n",
    "        return var0, mob, comp\n",
    "    act, mob, comp = hjorth_params(X)\n",
    "    feats[\"hjorth_activity_med\"]   = np.median(act, axis=1)\n",
    "    feats[\"hjorth_mobility_med\"]   = np.median(mob, axis=1)\n",
    "    feats[\"hjorth_complexity_med\"] = np.median(comp, axis=1)\n",
    "\n",
    "    return pd.DataFrame(feats)\n",
    "\n",
    "# 2) Re-run your Î²-split / Î³-extended config (resample-first basic_clean was already patched)\n",
    "cfg.data_dir   = \"./brainwaves\"\n",
    "cfg.output_dir = \"./cog_alphabet_physio_betaSplit\"\n",
    "cfg.dataset_urls = []  # reuse local EDFs\n",
    "cfg.target_sfreq = 250.0\n",
    "cfg.l_freq, cfg.h_freq = 0.5, 80.0\n",
    "cfg.bands = {\n",
    "    \"delta\": (1.0, 4.0),\n",
    "    \"theta\": (4.0, 8.0),\n",
    "    \"alpha\": (8.0, 13.0),\n",
    "    \"beta_low\":  (13.0, 20.0),\n",
    "    \"beta_high\": (20.0, 35.0),\n",
    "    \"gamma\":     (35.0, 80.0),\n",
    "}\n",
    "cfg.epoch_len_s = 2.0\n",
    "cfg.step_s      = 0.5\n",
    "cfg.k_range     = (4, 7)\n",
    "cfg.use_hmm     = True\n",
    "cfg.random_state = 42\n",
    "\n",
    "run_pipeline()\n",
    "\n",
    "# 3) If the original (Kâ‰ˆ3) run exists, print a quick side-by-side composition & alpha contrast\n",
    "OD_OLD = \"./cog_alphabet_physio\"\n",
    "OD_NEW = \"./cog_alphabet_physio_betaSplit\"\n",
    "OUT    = \"./cog_alphabet_eval_betaSplit\"; os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "def _exists_all(folder):\n",
    "    return all(os.path.exists(os.path.join(folder, f)) for f in [\"features.csv\",\"metadata.csv\",\"state_assignments.csv\",\"cognitive_alphabet.json\"])\n",
    "\n",
    "if _exists_all(OD_OLD) and _exists_all(OD_NEW):\n",
    "    def load_run(folder):\n",
    "        f = pd.read_csv(os.path.join(folder,\"features.csv\"))\n",
    "        m = pd.read_csv(os.path.join(folder,\"metadata.csv\"))\n",
    "        a = pd.read_csv(os.path.join(folder,\"state_assignments.csv\"))\n",
    "        with open(os.path.join(folder,\"cognitive_alphabet.json\")) as fh: A = json.load(fh)\n",
    "        return f,m,a,A\n",
    "    f_old, m_old, a_old, A_old = load_run(OD_OLD)\n",
    "    f_new, m_new, a_new, A_new = load_run(OD_NEW)\n",
    "\n",
    "    def name_from_tops(tops):\n",
    "        if any(t.startswith(\"alpha_rel_\") for t in tops): return \"Alpha-Dominant\"\n",
    "        if \"beta_high_rel_med\" in tops or \"beta_low_rel_med\" in tops or \"beta_over_alpha\" in tops:\n",
    "            if \"theta_rel_med\" in tops or \"theta_over_alpha\" in tops: return \"Beta/Theta-Mixed\"\n",
    "            return \"Beta-Leaning\"\n",
    "        if \"spec_entropy_med\" in tops or \"hjorth_complexity_med\" in tops: return \"High-Entropy\"\n",
    "        return \"Mixed\"\n",
    "    names_old = {int(s): name_from_tops(A_old[str(s)][\"top_features\"]) for s in A_old}\n",
    "    names_new = {int(s): name_from_tops(A_new[str(s)][\"top_features\"]) for s in A_new}\n",
    "\n",
    "    def cond_from_file(fn):\n",
    "        m = re.search(r\"R(\\d+)\", fn); r = int(m.group(1)) if m else None\n",
    "        return {1:\"EO\", 2:\"EC\", 3:\"ME\"}.get(r, \"UNK\")\n",
    "\n",
    "    D_old = m_old.copy(); D_old[\"state\"]=a_old[\"state\"]; D_old[\"name\"]=D_old[\"state\"].map(names_old); D_old[\"cond\"]=D_old[\"file\"].apply(cond_from_file)\n",
    "    D_new = m_new.copy(); D_new[\"state\"]=a_new[\"state\"]; D_new[\"name\"]=D_new[\"state\"].map(names_new); D_new[\"cond\"]=D_new[\"file\"].apply(cond_from_file)\n",
    "\n",
    "    def comp(df):\n",
    "        return (df.groupby([\"file\",\"cond\"])[\"name\"]\n",
    "                  .value_counts(normalize=True)\n",
    "                  .rename(\"fraction\")\n",
    "                  .reset_index()\n",
    "                  .pivot_table(index=[\"file\",\"cond\"], columns=\"name\", values=\"fraction\", fill_value=0.0)\n",
    "                  .sort_index())\n",
    "    C_old = comp(D_old); C_new = comp(D_new)\n",
    "    C_old.to_csv(os.path.join(OUT,\"composition_old.csv\"))\n",
    "    C_new.to_csv(os.path.join(OUT,\"composition_new_betaSplit.csv\"))\n",
    "\n",
    "    print(\"\\n=== OLD (Kâ‰ˆ3) composition ===\\n\", C_old.round(3))\n",
    "    print(\"\\n=== NEW (Î²-split, Î³â†‘) composition ===\\n\", C_new.round(3))\n",
    "\n",
    "    def alpha_frac(df):\n",
    "        x = (df.assign(alpha=(df[\"name\"]==\"Alpha-Dominant\").astype(int))\n",
    "               .groupby([\"file\",\"cond\"])[\"alpha\"].mean().rename(\"alpha_fraction\").reset_index())\n",
    "        return x.pivot(index=\"file\", columns=\"cond\", values=\"alpha_fraction\").fillna(0.0)\n",
    "    AF_new = alpha_frac(D_new); AF_new.to_csv(os.path.join(OUT,\"alpha_fraction_new_betaSplit.csv\"))\n",
    "    print(\"\\nAlpha fraction (NEW Î²-split):\\n\", AF_new.round(3))\n",
    "\n",
    "    print(\"\\nArtifacts â†’\", OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a0400bc-fa4a-4000-be30-66a168066130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EO vs EC (runs 01/02) ===\n",
      "col_0     EO     EC\n",
      "row_0              \n",
      "EO     1.000  0.000\n",
      "EC     0.017  0.983\n",
      "Accuracy: 0.992  | rule: Alphaâ†’EC, non-Alphaâ†’EO\n",
      "\n",
      "=== Task vs Rest (run 03 annotations) ===\n",
      "col_0   rest   task\n",
      "row_0              \n",
      "rest   0.043  0.957\n",
      "task   0.000  1.000\n",
      "Accuracy: 0.733  | rule: Alphaâ†’rest, non-Alphaâ†’task\n",
      "\n",
      "Artifacts â†’ ./cog_alphabet_eval_groundtruth\n"
     ]
    }
   ],
   "source": [
    "# === EDF Annotation Scorer: EO/EC (runs 01/02) + Task/Rest from annotations (run 03) ===\n",
    "import os, re, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import mne\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# ----- Inputs / folders -----\n",
    "OD = \"./cog_alphabet_physio_betaSplit\"  # evaluate this alphabet\n",
    "BASE = \"./brainwaves\"                   # where EDFs live\n",
    "OUT = \"./cog_alphabet_eval_groundtruth\"; os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "# ----- Load outputs to evaluate -----\n",
    "feat = pd.read_csv(os.path.join(OD, \"features.csv\"))\n",
    "meta = pd.read_csv(os.path.join(OD, \"metadata.csv\"))\n",
    "asgn = pd.read_csv(os.path.join(OD, \"state_assignments.csv\"))\n",
    "with open(os.path.join(OD, \"cognitive_alphabet.json\")) as f:\n",
    "    amap = json.load(f)\n",
    "\n",
    "# Heuristic names from top features (same as before)\n",
    "def name_from_tops(tops):\n",
    "    tops = list(tops)\n",
    "    if any(t.startswith(\"alpha_rel_\") for t in tops): return \"Alpha-Dominant\"\n",
    "    if \"beta_high_rel_med\" in tops or \"beta_low_rel_med\" in tops or \"beta_over_alpha\" in tops:\n",
    "        if \"theta_rel_med\" in tops or \"theta_over_alpha\" in tops: return \"Beta/Theta-Mixed\"\n",
    "        return \"Beta-Leaning\"\n",
    "    if \"spec_entropy_med\" in tops or \"hjorth_complexity_med\" in tops: return \"High-Entropy\"\n",
    "    return \"Mixed\"\n",
    "\n",
    "state_names = {int(s): name_from_tops(amap[str(s)][\"top_features\"]) for s in amap}\n",
    "DF = pd.concat([meta.reset_index(drop=True), asgn[\"state\"]], axis=1)\n",
    "DF[\"state_name\"] = DF[\"state\"].map(state_names)\n",
    "\n",
    "# ----- File â†’ coarse condition (protocol) -----\n",
    "# R01: EO, R02: EC, R03: executed movement (ME) â€“ weâ€™ll refine R03 via annotations\n",
    "def cond_from_file(fn):\n",
    "    m = re.search(r\"R(\\d+)\", fn); r = int(m.group(1)) if m else None\n",
    "    return {1:\"EO\", 2:\"EC\", 3:\"ME\"}.get(r, \"UNK\")\n",
    "\n",
    "DF[\"file_cond\"] = DF[\"file\"].apply(cond_from_file)\n",
    "\n",
    "# ----- Build fileâ†’path map -----\n",
    "edfs = {f.lower(): os.path.join(BASE, f) for f in os.listdir(BASE) if f.lower().endswith(\".edf\")}\n",
    "\n",
    "# ----- Extract per-epoch ground truth using EDF annotations for R03 -----\n",
    "# EEGMMI uses T0 (rest), T1/T2 (task) within task runs.\n",
    "def epoch_label_from_annots(file, t0, t1):\n",
    "    fl = file.lower()\n",
    "    p = edfs.get(fl)\n",
    "    if p is None:\n",
    "        return None  # no file found\n",
    "    raw = mne.io.read_raw_edf(p, preload=False, verbose=False)\n",
    "    anns = getattr(raw, \"annotations\", None)\n",
    "    if anns is None or len(anns) == 0:\n",
    "        return None\n",
    "    # Mark epoch as 'task' if any T1/T2 overlaps epoch; 'rest' if any T0 overlaps; else None\n",
    "    lbl = None\n",
    "    for desc, onset, dur in zip(anns.description, anns.onset, anns.duration):\n",
    "        if (onset < t1) and (onset + dur > t0):  # overlap\n",
    "            d = str(desc).upper()\n",
    "            if \"T1\" in d or \"T2\" in d:\n",
    "                return \"task\"\n",
    "            if \"T0\" in d:\n",
    "                lbl = lbl or \"rest\"\n",
    "    return lbl\n",
    "\n",
    "# Compute ground labels\n",
    "gt = []\n",
    "for idx, row in DF.iterrows():\n",
    "    f, t0, t1 = row[\"file\"], float(row[\"t_start_s\"]), float(row[\"t_end_s\"])\n",
    "    if row[\"file_cond\"] in (\"EO\",\"EC\"):\n",
    "        gt.append(row[\"file_cond\"])\n",
    "    elif row[\"file_cond\"] == \"ME\":  # refine with annotations if present\n",
    "        lbl = epoch_label_from_annots(f, t0, t1)\n",
    "        gt.append(lbl if lbl is not None else \"ME\")  # fallback if no annots\n",
    "    else:\n",
    "        gt.append(None)\n",
    "DF[\"gt_label\"] = gt\n",
    "\n",
    "# ----- Scoring 1: EO vs EC (use only R01/R02 epochs) -----\n",
    "EOEC = DF[DF[\"gt_label\"].isin([\"EO\",\"EC\"])].copy()\n",
    "# Simple predictor from letters: Alpha-Dominant â†’ EC, else â†’ EO\n",
    "EOEC[\"pred\"] = np.where(EOEC[\"state_name\"]==\"Alpha-Dominant\", \"EC\", \"EO\")\n",
    "\n",
    "def acc_table(true, pred, labels):\n",
    "    cm = pd.crosstab(pd.Categorical(true, categories=labels),\n",
    "                     pd.Categorical(pred, categories=labels),\n",
    "                     normalize=\"index\").fillna(0.0)\n",
    "    acc = (np.array(true) == np.array(pred)).mean()\n",
    "    return cm, float(acc)\n",
    "\n",
    "cm_eoec, acc_eoec = acc_table(EOEC[\"gt_label\"].to_numpy(), EOEC[\"pred\"].to_numpy(), [\"EO\",\"EC\"])\n",
    "cm_eoec.round(3).to_csv(os.path.join(OUT, \"cm_eoec.csv\"))\n",
    "\n",
    "# ----- Scoring 2: Task vs Rest in R03 via annotations -----\n",
    "TR = DF[DF[\"gt_label\"].isin([\"task\",\"rest\"])].copy()\n",
    "# Predictor: non-Alpha letters â†’ task, Alpha-Dominant â†’ rest\n",
    "TR[\"pred\"] = np.where(TR[\"state_name\"]==\"Alpha-Dominant\", \"rest\", \"task\")\n",
    "cm_tr, acc_tr = acc_table(TR[\"gt_label\"].to_numpy(), TR[\"pred\"].to_numpy(), [\"rest\",\"task\"])\n",
    "cm_tr.round(3).to_csv(os.path.join(OUT, \"cm_task_rest.csv\"))\n",
    "\n",
    "# ----- Save per-epoch table with predictions -----\n",
    "DF.to_csv(os.path.join(OUT, \"epochs_with_predictions.csv\"), index=False)\n",
    "\n",
    "print(\"\\n=== EO vs EC (runs 01/02) ===\")\n",
    "print(cm_eoec.round(3))\n",
    "print(f\"Accuracy: {acc_eoec:.3f}  | rule: Alphaâ†’EC, non-Alphaâ†’EO\")\n",
    "\n",
    "print(\"\\n=== Task vs Rest (run 03 annotations) ===\")\n",
    "if len(TR):\n",
    "    print(cm_tr.round(3))\n",
    "    print(f\"Accuracy: {acc_tr:.3f}  | rule: Alphaâ†’rest, non-Alphaâ†’task\")\n",
    "else:\n",
    "    print(\"No annotation-derived task/rest windows found â€” falling back wasnâ€™t needed for R01/R02.\")\n",
    "print(\"\\nArtifacts â†’\", OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "745aca16-c833-4850-ab4f-c74b6038f340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: ./brainwaves\\S001R03.edf\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Labeled epochs: 247  (task=125, rest=122)\n",
      "\n",
      "=== ROI-ERD (C3/Cz/C4) 5-fold CV ===\n",
      "Accuracy: 0.595\n",
      "Confusion matrix [[restâ†’rest, restâ†’task],[taskâ†’rest, taskâ†’task]]:\n",
      " [[61 61]\n",
      " [39 86]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        rest      0.610     0.500     0.550       122\n",
      "        task      0.585     0.688     0.632       125\n",
      "\n",
      "    accuracy                          0.595       247\n",
      "   macro avg      0.598     0.594     0.591       247\n",
      "weighted avg      0.597     0.595     0.591       247\n",
      "\n",
      "\n",
      "Saved per-epoch features + scores to: ./cog_alphabet_eval_groundtruth\n"
     ]
    }
   ],
   "source": [
    "# === ME (Run 03) ROI-ERD Booster: C3/Cz/C4 Î¼/Î² ERD + 5-fold CV ===\n",
    "# - Reads S001R03.edf (or any R03 in ./brainwaves)\n",
    "# - Epochs with same cadence (2.0s, 0.5s hop)\n",
    "# - Labels epochs from EDF annotations (â‰¥50% overlap: task if T1/T2, rest if T0)\n",
    "# - Computes ROI ERD features (C3/Cz/C4, mu=8-13Hz, beta=13-30Hz)\n",
    "# - Trains LogisticRegression with 5-fold stratified CV; prints accuracy & confusion\n",
    "# - Saves per-epoch feature table + results under ./cog_alphabet_eval_groundtruth\n",
    "\n",
    "import os, re, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import mne\n",
    "from scipy.signal import welch\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "BASE = \"./brainwaves\"\n",
    "OUT  = \"./cog_alphabet_eval_groundtruth\"\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "# --- find an R03 file we already downloaded ---\n",
    "edf_files = [f for f in os.listdir(BASE) if f.lower().endswith(\".edf\")]\n",
    "r03 = [f for f in edf_files if re.search(r\"R0*3\", f, re.I)]\n",
    "if not r03:\n",
    "    raise RuntimeError(\"No R03 EDF found in ./brainwaves (e.g., S001R03.edf).\")\n",
    "fname = r03[0]\n",
    "path  = os.path.join(BASE, fname)\n",
    "print(\"Using:\", path)\n",
    "\n",
    "# --- params mirroring your pipeline ---\n",
    "epoch_len_s = 2.0\n",
    "step_s      = 0.5\n",
    "l_freq, h_freq = 0.5, 45.0\n",
    "target_sfreq = 250.0\n",
    "\n",
    "# --- load + preprocess (resample first â†’ safe Nyquist) ---\n",
    "raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "if abs(raw.info[\"sfreq\"] - target_sfreq) > 1e-6:\n",
    "    raw.resample(target_sfreq, npad=\"auto\", verbose=False)\n",
    "sf = float(raw.info[\"sfreq\"])\n",
    "ny = sf/2\n",
    "raw.filter(l_freq, min(h_freq, ny-1.0), verbose=False)\n",
    "if \"eeg\" in set(raw.get_channel_types()):\n",
    "    raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "    try:\n",
    "        raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"),\n",
    "                        match_case=False, match_alias=True, on_missing=\"warn\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# --- pick ROI channels, robust to name variants ---\n",
    "chs = [ch for ch in raw.ch_names]\n",
    "def pick_like(names):\n",
    "    want = []\n",
    "    for target in [\"C3\", \"Cz\", \"C4\"]:\n",
    "        for c in chs:\n",
    "            if re.fullmatch(target, c, re.I) or re.search(rf\"^{target}\\b\", c, re.I):\n",
    "                want.append(c)\n",
    "    return sorted(set(want))\n",
    "roi = pick_like(chs)\n",
    "if not roi:\n",
    "    # fallback: try common aliases\n",
    "    roi = [c for c in chs if re.match(r\"C[3z4]\", c, re.I)]\n",
    "if len(roi) < 2:\n",
    "    print(\"ROI channels not found robustly; falling back to all EEG channels.\")\n",
    "    roi = chs\n",
    "\n",
    "raw.pick(roi)\n",
    "\n",
    "# --- epoching ---\n",
    "overlap = max(0.0, epoch_len_s - step_s)\n",
    "epochs = mne.make_fixed_length_epochs(raw, duration=epoch_len_s, overlap=overlap, preload=True, verbose=False)\n",
    "starts = epochs.events[:,0]/epochs.info[\"sfreq\"]\n",
    "ends   = starts + epoch_len_s\n",
    "\n",
    "# --- label epochs by annotations with â‰¥50% overlap rule ---\n",
    "anns = raw.annotations if hasattr(raw, \"annotations\") else None\n",
    "def label_epoch(t0, t1):\n",
    "    if anns is None or len(anns)==0: return None\n",
    "    wins = [(float(o), float(d), str(s).upper()) for o,d,s in zip(anns.onset, anns.duration, anns.description)]\n",
    "    ov_task = 0.0; ov_rest = 0.0\n",
    "    for o,d,s in wins:\n",
    "        left  = max(t0, o); right = min(t1, o+d)\n",
    "        if right <= left: continue\n",
    "        if \"T1\" in s or \"T2\" in s: ov_task += (right-left)\n",
    "        elif \"T0\" in s:            ov_rest += (right-left)\n",
    "    span = t1 - t0\n",
    "    if ov_task >= 0.5*span: return \"task\"\n",
    "    if ov_rest >= 0.5*span: return \"rest\"\n",
    "    return None\n",
    "\n",
    "y = np.array([label_epoch(t0, t1) for t0, t1 in zip(starts, ends)], dtype=object)\n",
    "keep = np.array([lab in (\"task\",\"rest\") for lab in y])\n",
    "epochs = epochs[keep]\n",
    "starts = starts[keep]; ends = ends[keep]; y = y[keep]\n",
    "print(f\"Labeled epochs: {len(y)}  (task={np.sum(y=='task')}, rest={np.sum(y=='rest')})\")\n",
    "\n",
    "# --- compute ROI ERD features (mu=8-13Hz, beta=13-30Hz) vs rest baseline ---\n",
    "X = epochs.get_data()  # (n_ep, n_ch, n_t)\n",
    "sf = epochs.info[\"sfreq\"]\n",
    "def bandpow(sig, lo, hi):\n",
    "    n_ep, n_ch, n_t = sig.shape\n",
    "    nper = min(int(sf*2), n_t); nov = nper//2\n",
    "    freqs, psd = welch(sig, fs=sf, nperseg=nper, noverlap=nov, axis=-1, average=\"median\")\n",
    "    idx = np.where((freqs>=lo)&(freqs<hi))[0]\n",
    "    return psd[:,:,idx].sum(axis=-1)  # abs power\n",
    "\n",
    "P_mu   = bandpow(X, 8.0, 13.0)\n",
    "P_beta = bandpow(X, 13.0, 30.0)\n",
    "\n",
    "# baseline = mean over rest epochs (per channel)\n",
    "rest_mask = (y == \"rest\")\n",
    "if rest_mask.any():\n",
    "    mu_base   = P_mu[rest_mask].mean(axis=0)   + 1e-12\n",
    "    beta_base = P_beta[rest_mask].mean(axis=0) + 1e-12\n",
    "else:\n",
    "    mu_base   = P_mu.mean(axis=0)   + 1e-12\n",
    "    beta_base = P_beta.mean(axis=0) + 1e-12\n",
    "\n",
    "# ERD = (P_task - P_base)/P_base  (negative â†’ desync)\n",
    "ERD_mu   = (P_mu   - mu_base)  / mu_base\n",
    "ERD_beta = (P_beta - beta_base)/ beta_base\n",
    "\n",
    "# Summarize across ROI channels: median & iqr\n",
    "def summarize(mat):\n",
    "    med = np.median(mat, axis=1)\n",
    "    iqr = np.subtract(*np.percentile(mat, [75,25], axis=1))\n",
    "    return med, iqr\n",
    "\n",
    "mu_med, mu_iqr     = summarize(ERD_mu)\n",
    "beta_med, beta_iqr = summarize(ERD_beta)\n",
    "\n",
    "# Feature table\n",
    "F = pd.DataFrame({\n",
    "    \"t_start_s\": starts, \"t_end_s\": ends,\n",
    "    \"mu_ERD_med\": mu_med, \"mu_ERD_iqr\": mu_iqr,\n",
    "    \"beta_ERD_med\": beta_med, \"beta_ERD_iqr\": beta_iqr,\n",
    "    \"n_roi\": len(roi),\n",
    "    \"label\": y\n",
    "})\n",
    "F[\"file\"] = fname\n",
    "F.to_csv(os.path.join(OUT, f\"ME_ROI_ERD_epochs_{os.path.splitext(fname)[0]}.csv\"), index=False)\n",
    "\n",
    "# --- simple supervised readout (5-fold CV) ---\n",
    "Xf = F[[\"mu_ERD_med\",\"mu_ERD_iqr\",\"beta_ERD_med\",\"beta_ERD_iqr\"]].to_numpy()\n",
    "yf = (F[\"label\"]==\"task\").astype(int).to_numpy()\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "yh = np.zeros_like(yf)\n",
    "for tr, te in skf.split(Xf, yf):\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    clf.fit(Xf[tr], yf[tr])\n",
    "    yh[te] = clf.predict(Xf[te])\n",
    "\n",
    "acc = (yh == yf).mean()\n",
    "cm  = confusion_matrix(yf, yh, labels=[0,1])\n",
    "rep = classification_report(yf, yh, target_names=[\"rest\",\"task\"], digits=3)\n",
    "print(\"\\n=== ROI-ERD (C3/Cz/C4) 5-fold CV ===\")\n",
    "print(\"Accuracy:\", round(float(acc), 3))\n",
    "print(\"Confusion matrix [[restâ†’rest, restâ†’task],[taskâ†’rest, taskâ†’task]]:\\n\", cm)\n",
    "print(\"\\nReport:\\n\", rep)\n",
    "\n",
    "with open(os.path.join(OUT, \"ME_ROI_ERD_scores.json\"), \"w\") as f:\n",
    "    json.dump({\"accuracy\": float(acc), \"confusion\": cm.tolist()}, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved per-epoch features + scores to:\", OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4df357a6-0e03-4df3-a951-26c610657a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: ./brainwaves\\S001R03.edf\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Epochs built: 120  (task=60, rest=60)  | ROI channels: 3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "filter_data() got an unexpected keyword argument 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 116\u001b[39m\n\u001b[32m    114\u001b[39m feats_tr, feats_te = [], []\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, lo, hi \u001b[38;5;129;01min\u001b[39;00m bands:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     Xtr = \u001b[43mbandpass_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_all\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m     Xte = bandpass_array(X_all[te], lo, hi, sf)\n\u001b[32m    118\u001b[39m     csp = CSP(n_components=\u001b[32m4\u001b[39m, reg=\u001b[38;5;28;01mNone\u001b[39;00m, log=\u001b[38;5;28;01mTrue\u001b[39;00m, norm_trace=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 106\u001b[39m, in \u001b[36mbandpass_array\u001b[39m\u001b[34m(X, lo, hi, sf)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbandpass_array\u001b[39m(X, lo, hi, sf):\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmne\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msfreq\u001b[49m\u001b[43m=\u001b[49m\u001b[43msf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_freq\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: filter_data() got an unexpected keyword argument 'axis'"
     ]
    }
   ],
   "source": [
    "# === FBCSP Booster for Task vs Rest on R03 (C3/Cz/C4 preferred; falls back to all EEG) ===\n",
    "# - Builds non-overlapping 1.0 s epochs inside annotation segments (T0=rest, T1/T2=task)\n",
    "# - Filter-bank: [(8,13),(13,20),(20,30)] Hz\n",
    "# - CSP (n_components=4) per band â†’ concatenate â†’ LDA\n",
    "# - GroupKFold by contiguous annotation segments to reduce temporal leakage\n",
    "# - Reports 5-fold CV accuracy + confusion, saves tidy CSV\n",
    "\n",
    "import os, re, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import mne\n",
    "from mne.decoding import CSP\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "BASE = \"./brainwaves\"\n",
    "OUT  = \"./cog_alphabet_eval_groundtruth\"\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "# ---- 1) locate an R03 file ----\n",
    "edf_files = [f for f in os.listdir(BASE) if f.lower().endswith(\".edf\")]\n",
    "r03 = [f for f in edf_files if re.search(r\"R0*3\", f, re.I)]\n",
    "if not r03:\n",
    "    raise RuntimeError(\"No R03 EDF found in ./brainwaves (e.g., S001R03.edf).\")\n",
    "fname = r03[0]; path = os.path.join(BASE, fname)\n",
    "print(\"Using:\", path)\n",
    "\n",
    "# ---- 2) load + preprocess (resample first â†’ safe Nyquist) ----\n",
    "target_sfreq = 250.0\n",
    "raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "if abs(raw.info[\"sfreq\"] - target_sfreq) > 1e-6:\n",
    "    raw.resample(target_sfreq, npad=\"auto\", verbose=False)\n",
    "\n",
    "sf = float(raw.info[\"sfreq\"]); ny = sf/2\n",
    "raw.filter(0.5, min(40.0, ny-1.0), verbose=False)\n",
    "if \"eeg\" in set(raw.get_channel_types()):\n",
    "    raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "    try:\n",
    "        raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"),\n",
    "                        match_case=False, match_alias=True, on_missing=\"warn\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ---- 3) pick ROI channels (C3, Cz, C4); fallback to all EEG if not found ----\n",
    "chs = raw.ch_names\n",
    "roi = []\n",
    "for target in [\"C3\", \"Cz\", \"C4\"]:\n",
    "    for c in chs:\n",
    "        if re.fullmatch(target, c, re.I) or re.search(rf\"^{target}\\b\", c, re.I):\n",
    "            roi.append(c)\n",
    "roi = sorted(set(roi))\n",
    "if len(roi) < 2:\n",
    "    print(\"ROI channels not robustly found â€” using all EEG channels.\")\n",
    "    roi = [c for c,typ in zip(raw.ch_names, raw.get_channel_types()) if typ==\"eeg\"]\n",
    "raw.pick(roi)\n",
    "\n",
    "# ---- 4) cut clean non-overlapping 1.0 s epochs fully inside T0 (rest) or T1/T2 (task) segments ----\n",
    "anns = raw.annotations\n",
    "if anns is None or len(anns)==0:\n",
    "    raise RuntimeError(\"No annotations found in R03; cannot build task/rest epochs.\")\n",
    "\n",
    "segments = []  # (start, stop, label, seg_id)\n",
    "seg_id = 0\n",
    "for o, d, s in zip(anns.onset, anns.duration, anns.description):\n",
    "    lab = None\n",
    "    su = str(s).upper()\n",
    "    if \"T0\" in su: lab = \"rest\"\n",
    "    elif \"T1\" in su or \"T2\" in su: lab = \"task\"\n",
    "    if lab is None: \n",
    "        continue\n",
    "    segments.append((float(o), float(o+d), lab, seg_id))\n",
    "    seg_id += 1\n",
    "\n",
    "epoch_len = 1.0  # seconds, non-overlapping for cleaner CV\n",
    "starts, labels, groups = [], [], []\n",
    "for (a, b, lab, gid) in segments:\n",
    "    t = a\n",
    "    while t + epoch_len <= b - 1e-6:\n",
    "        starts.append(t); labels.append(lab); groups.append(gid)\n",
    "        t += epoch_len  # non-overlap\n",
    "\n",
    "if not starts:\n",
    "    raise RuntimeError(\"No 1s epochs fit within annotated segments.\")\n",
    "\n",
    "events = np.column_stack([\n",
    "    (np.array(starts) * sf).astype(int),\n",
    "    np.zeros(len(starts), dtype=int),\n",
    "    np.array([1 if lab==\"task\" else 0 for lab in labels], dtype=int)\n",
    "])\n",
    "\n",
    "event_id = dict(rest=0, task=1)\n",
    "epochs = mne.Epochs(raw, events, event_id=event_id, tmin=0.0, tmax=epoch_len, baseline=None,\n",
    "                    preload=True, verbose=False)\n",
    "\n",
    "y = np.array(labels)  # 'task'/'rest'\n",
    "groups = np.array(groups)\n",
    "\n",
    "print(f\"Epochs built: {len(y)}  (task={np.sum(y=='task')}, rest={np.sum(y=='rest')})  | ROI channels: {len(roi)}\")\n",
    "\n",
    "# ---- 5) Filter-Bank CSP features ----\n",
    "bands = [(\"mu\", 8.0, 13.0), (\"beta_low\", 13.0, 20.0), (\"beta_high\", 20.0, 30.0)]\n",
    "X_all = epochs.get_data()  # (n_ep, n_ch, n_t)\n",
    "\n",
    "# Helper: band-pass the epochs array (vectorized over epochs/channels)\n",
    "def bandpass_array(X, lo, hi, sf):\n",
    "    return mne.filter.filter_data(X.copy(), sfreq=sf, l_freq=lo, h_freq=hi, axis=-1, verbose=False)\n",
    "\n",
    "# Build per-band CSP-transformed features inside CV (fit CSP on train only)\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "yh = np.empty(len(y), dtype=int)\n",
    "yt = (y == \"task\").astype(int)\n",
    "\n",
    "for tr, te in gkf.split(X_all, yt, groups):\n",
    "    feats_tr, feats_te = [], []\n",
    "    for _, lo, hi in bands:\n",
    "        Xtr = bandpass_array(X_all[tr], lo, hi, sf)\n",
    "        Xte = bandpass_array(X_all[te], lo, hi, sf)\n",
    "        csp = CSP(n_components=4, reg=None, log=True, norm_trace=False)\n",
    "        csp.fit(Xtr, yt[tr])\n",
    "        feats_tr.append(csp.transform(Xtr))\n",
    "        feats_te.append(csp.transform(Xte))\n",
    "    Xtr_fb = np.concatenate(feats_tr, axis=1)\n",
    "    Xte_fb = np.concatenate(feats_te, axis=1)\n",
    "    clf = LDA(solver=\"lsqr\", shrinkage=\"auto\")  # stable, well-behaved\n",
    "    clf.fit(Xtr_fb, yt[tr])\n",
    "    yh[te] = clf.predict(Xte_fb)\n",
    "\n",
    "acc = (yh == yt).mean()\n",
    "cm  = confusion_matrix(yt, yh, labels=[0,1])\n",
    "rep = classification_report(yt, yh, target_names=[\"rest\",\"task\"], digits=3)\n",
    "\n",
    "print(\"\\n=== FBCSP (Î¼/Î²) with GroupKFold(5) ===\")\n",
    "print(\"Accuracy:\", round(float(acc), 3))\n",
    "print(\"Confusion [[restâ†’rest, restâ†’task],[taskâ†’rest, taskâ†’task]]:\\n\", cm)\n",
    "print(\"\\nReport:\\n\", rep)\n",
    "\n",
    "# ---- 6) Save per-epoch table with predictions ----\n",
    "out = pd.DataFrame({\n",
    "    \"file\": fname,\n",
    "    \"t_start_s\": epochs.events[:,0]/sf,\n",
    "    \"t_end_s\": epochs.events[:,0]/sf + epoch_len,\n",
    "    \"label\": y,\n",
    "    \"y_true\": yt,\n",
    "    \"y_pred\": yh,\n",
    "    \"roi_channels\": [\",\".join(roi)]*len(y)\n",
    "})\n",
    "out.to_csv(os.path.join(OUT, f\"ME_FBCSP_epochs_{os.path.splitext(fname)[0]}.csv\"), index=False)\n",
    "with open(os.path.join(OUT, \"ME_FBCSP_scores.json\"), \"w\") as f:\n",
    "    json.dump({\"accuracy\": float(acc), \"confusion\": cm.tolist()}, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved FBCSP features/scores â†’\", OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "769eb3a7-fa8a-4f50-9016-b088b1b37725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.2e-07 (2.2e-16 eps * 3 dim * 1.1e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=1 covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7e-07 (2.2e-16 eps * 3 dim * 1e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=1 covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.1e-07 (2.2e-16 eps * 3 dim * 9.1e+08  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=1 covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.1e-07 (2.2e-16 eps * 3 dim * 1.1e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=1 covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7e-07 (2.2e-16 eps * 3 dim * 1e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=1 covariance using EMPIRICAL\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6e-07 (2.2e-16 eps * 3 dim * 9e+08  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using EMPIRICAL\n",
      "Done.\n",
      "Estimating class=1 covariance using EMPIRICAL\n",
      "Done.\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "The leading minor of order 3 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLinAlgError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m Xte = bandpass_array(X_all[te], lo, hi, sf)\n\u001b[32m     32\u001b[39m csp = CSP(n_components=\u001b[32m4\u001b[39m, reg=\u001b[38;5;28;01mNone\u001b[39;00m, log=\u001b[38;5;28;01mTrue\u001b[39;00m, norm_trace=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mcsp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myt\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m feats_tr.append(csp.transform(Xtr))\n\u001b[32m     35\u001b[39m feats_te.append(csp.transform(Xte))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\cnt_genome\\.venv\\Lib\\site-packages\\mne\\decoding\\csp.py:185\u001b[39m, in \u001b[36mCSP.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    182\u001b[39m _validate_type(\u001b[38;5;28mself\u001b[39m.rank, (\u001b[38;5;28mdict\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mstr\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mrank\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    184\u001b[39m covs, sample_weights = \u001b[38;5;28mself\u001b[39m._compute_covariance_matrices(X, y)\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m eigen_vectors, eigen_values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_decompose_covs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcovs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m ix = \u001b[38;5;28mself\u001b[39m._order_components(\n\u001b[32m    187\u001b[39m     covs, sample_weights, eigen_vectors, eigen_values, \u001b[38;5;28mself\u001b[39m.component_order\n\u001b[32m    188\u001b[39m )\n\u001b[32m    190\u001b[39m eigen_vectors = eigen_vectors[:, ix]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\cnt_genome\\.venv\\Lib\\site-packages\\mne\\decoding\\csp.py:648\u001b[39m, in \u001b[36mCSP._decompose_covs\u001b[39m\u001b[34m(self, covs, sample_weights)\u001b[39m\n\u001b[32m    646\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m covs[\u001b[32m0\u001b[39m].shape == (mask.sum(),) * \u001b[32m2\u001b[39m\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_classes == \u001b[32m2\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m     eigen_values, eigen_vectors = \u001b[43meigh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcovs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcovs\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    650\u001b[39m     \u001b[38;5;66;03m# The multiclass case is adapted from\u001b[39;00m\n\u001b[32m    651\u001b[39m     \u001b[38;5;66;03m# http://github.com/alexandrebarachant/pyRiemann\u001b[39;00m\n\u001b[32m    652\u001b[39m     eigen_vectors, D = _ajd_pham(covs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\cnt_genome\\.venv\\Lib\\site-packages\\scipy\\_lib\\_util.py:1233\u001b[39m, in \u001b[36m_apply_over_batch.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1231\u001b[39m \u001b[38;5;66;03m# Early exit if call is not batched\u001b[39;00m\n\u001b[32m   1232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(batch_shapes):\n\u001b[32m-> \u001b[39m\u001b[32m1233\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mother_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[38;5;66;03m# Determine broadcasted batch shape\u001b[39;00m\n\u001b[32m   1236\u001b[39m batch_shape = np.broadcast_shapes(*batch_shapes)  \u001b[38;5;66;03m# Gives OK error message\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\cnt_genome\\.venv\\Lib\\site-packages\\scipy\\linalg\\_decomp.py:594\u001b[39m, in \u001b[36meigh\u001b[39m\u001b[34m(a, b, lower, eigvals_only, overwrite_a, overwrite_b, type, check_finite, subset_by_index, subset_by_value, driver)\u001b[39m\n\u001b[32m    591\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mIllegal value in argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m-info\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of internal \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    592\u001b[39m                       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdrv.typecode\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39mpfx\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39mdriver\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m info > n:\n\u001b[32m--> \u001b[39m\u001b[32m594\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mThe leading minor of order \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minfo-n\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of B is not \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    595\u001b[39m                       \u001b[33m'\u001b[39m\u001b[33mpositive definite. The factorization of B \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    596\u001b[39m                       \u001b[33m'\u001b[39m\u001b[33mcould not be completed and no eigenvalues \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    597\u001b[39m                       \u001b[33m'\u001b[39m\u001b[33mor eigenvectors were computed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    598\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    599\u001b[39m     drv_err = {\u001b[33m'\u001b[39m\u001b[33mev\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mThe algorithm failed to converge; \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    600\u001b[39m                      \u001b[33m'\u001b[39m\u001b[33moff-diagonal elements of an intermediate \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    601\u001b[39m                      \u001b[33m'\u001b[39m\u001b[33mtridiagonal form did not converge to zero.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    606\u001b[39m                \u001b[33m'\u001b[39m\u001b[33mevr\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mInternal Error.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    607\u001b[39m                }\n",
      "\u001b[31mLinAlgError\u001b[39m: The leading minor of order 3 of B is not positive definite. The factorization of B could not be completed and no eigenvalues or eigenvectors were computed."
     ]
    }
   ],
   "source": [
    "# === Hotfix: filter_data without 'axis' + rerun FBCSP CV ===\n",
    "import numpy as np\n",
    "from mne.decoding import CSP\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Expect these from the previous cell: epochs, sf, y ('task'/'rest'), groups, bands\n",
    "# If they aren't present, re-run the previous FBCSP setup cell first.\n",
    "\n",
    "# 1) Rebuild X_all from current epochs to be safe\n",
    "X_all = epochs.get_data()   # shape: (n_ep, n_ch, n_t)\n",
    "\n",
    "# 2) Filter helper: reshape â†’ filter_data (2D) â†’ reshape back\n",
    "def bandpass_array(X, lo, hi, sf):\n",
    "    # X: (n_ep, n_ch, n_t)\n",
    "    n_ep, n_ch, n_t = X.shape\n",
    "    X2 = X.reshape(n_ep * n_ch, n_t)\n",
    "    Xf2 = mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
    "    return Xf2.reshape(n_ep, n_ch, n_t)\n",
    "\n",
    "# 3) FBCSP CV (Î¼/Î² sub-bands) with GroupKFold by annotation segments\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "yt = (y == \"task\").astype(int)\n",
    "yh = np.empty_like(yt)\n",
    "\n",
    "for tr, te in gkf.split(X_all, yt, groups):\n",
    "    feats_tr, feats_te = [], []\n",
    "    for _, lo, hi in bands:  # e.g., (\"mu\",8,13), (\"beta_low\",13,20), (\"beta_high\",20,30)\n",
    "        Xtr = bandpass_array(X_all[tr], lo, hi, sf)\n",
    "        Xte = bandpass_array(X_all[te], lo, hi, sf)\n",
    "        csp = CSP(n_components=4, reg=None, log=True, norm_trace=False)\n",
    "        csp.fit(Xtr, yt[tr])\n",
    "        feats_tr.append(csp.transform(Xtr))\n",
    "        feats_te.append(csp.transform(Xte))\n",
    "    Xtr_fb = np.concatenate(feats_tr, axis=1)\n",
    "    Xte_fb = np.concatenate(feats_te, axis=1)\n",
    "\n",
    "    clf = LDA(solver=\"lsqr\", shrinkage=\"auto\")\n",
    "    clf.fit(Xtr_fb, yt[tr])\n",
    "    yh[te] = clf.predict(Xte_fb)\n",
    "\n",
    "acc = (yh == yt).mean()\n",
    "cm  = confusion_matrix(yt, yh, labels=[0,1])\n",
    "rep = classification_report(yt, yh, target_names=[\"rest\",\"task\"], digits=3)\n",
    "\n",
    "print(\"\\n=== FBCSP (Î¼/Î²) with GroupKFold(5) â€” Hotfix ===\")\n",
    "print(\"Accuracy:\", round(float(acc), 3))\n",
    "print(\"Confusion [[restâ†’rest, restâ†’task],[taskâ†’rest, taskâ†’task]]:\\n\", cm)\n",
    "print(\"\\nReport:\\n\", rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "17d40771-d902-4472-ac67-24a41a6af184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "\n",
      "=== Robust FBCSP (Î¼/Î²) with GroupKFold(5) ===\n",
      "Band modes per fold: [[\"CSP{'reg': 'ledoit_wolf', 'n_components': 6, 'rank': 'info'}\", \"CSP{'reg': 'ledoit_wolf', 'n_components': 6, 'rank': 'info'}\", \"CSP{'reg': 'ledoit_wolf', 'n_components': 6, 'rank': 'info'}\"], [\"CSP{'reg': 'ledoit_wolf', 'n_components': 6, 'rank': 'info'}\", \"CSP{'reg': 'ledoit_wolf', 'n_components': 6, 'rank': 'info'}\", \"CSP{'reg': 'ledoit_wolf', 'n_components': 6, 'rank': 'info'}\"], [\"CSP{'reg': 'ledoit_wolf', 'n_components': 6, 'rank': 'info'}\", \"CSP{'reg': 'ledoit_wolf', 'n_components': 6, 'rank': 'info'}\", \"CSP{'reg': 'ledoit_wolf', 'n_components': 6, 'rank': 'info'}\"], [\"CSP{'reg': 'ledoit_wolf', 'n_components': 6, 'rank': 'info'}\", \"CSP{'reg': 'ledoit_wolf', 'n_components': 6, 'rank': 'info'}\", \"CSP{'reg': 'ledoit_wolf', 'n_components': 6, 'rank': 'info'}\"], [\"CSP{'reg': 'ledoit_wolf', 'n_components': 6, 'rank': 'info'}\", \"CSP{'reg': 'ledoit_wolf', 'n_components': 6, 'rank': 'info'}\", \"CSP{'reg': 'ledoit_wolf', 'n_components': 6, 'rank': 'info'}\"]]\n",
      "Accuracy: 0.65\n",
      "Confusion [[restâ†’rest, restâ†’task],[taskâ†’rest, taskâ†’task]]:\n",
      " [[40 20]\n",
      " [22 38]]\n",
      "\n",
      "Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "        rest      0.645     0.667     0.656        60\n",
      "        task      0.655     0.633     0.644        60\n",
      "\n",
      "    accuracy                          0.650       120\n",
      "   macro avg      0.650     0.650     0.650       120\n",
      "weighted avg      0.650     0.650     0.650       120\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === Robust FBCSP CV (regularized CSP + graceful fallbacks) ===\n",
    "import numpy as np\n",
    "import mne\n",
    "from mne.decoding import CSP\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Expect from your previous cell: epochs, sf, y ('task'/'rest'), groups, bands\n",
    "X_all = epochs.get_data()  # (n_ep, n_ch, n_t)\n",
    "yt = (y == \"task\").astype(int)\n",
    "\n",
    "def bandpass_array(X, lo, hi, sf):\n",
    "    \"\"\"Filter per-epoch by reshaping to 2D to avoid axis arg issues.\"\"\"\n",
    "    n_ep, n_ch, n_t = X.shape\n",
    "    X2 = X.reshape(n_ep * n_ch, n_t)\n",
    "    Xf2 = mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
    "    return Xf2.reshape(n_ep, n_ch, n_t)\n",
    "\n",
    "def fit_transform_csp(Xtr, Xte, ytr):\n",
    "    \"\"\"\n",
    "    Try a few robust CSP configs; if all fail, fall back to log-variance features.\n",
    "    Returns (F_tr, F_te).\n",
    "    \"\"\"\n",
    "    tried = [\n",
    "        dict(reg=\"ledoit_wolf\", n_components=6, rank=\"info\"),\n",
    "        dict(reg=\"ledoit_wolf\", n_components=4, rank=\"info\"),\n",
    "        dict(reg=0.2,          n_components=4, rank=\"info\"),\n",
    "        dict(reg=0.05,         n_components=4, rank=\"info\"),\n",
    "        dict(reg=\"ledoit_wolf\", n_components=2, rank=\"info\"),\n",
    "    ]\n",
    "    for cfg in tried:\n",
    "        try:\n",
    "            csp = CSP(log=True, norm_trace=False, **cfg)\n",
    "            csp.fit(Xtr, ytr)\n",
    "            return csp.transform(Xtr), csp.transform(Xte), f\"CSP{cfg}\"\n",
    "        except Exception:\n",
    "            continue\n",
    "    # Fallback: channel-wise log-variance (no CSP)\n",
    "    Ft = np.log(np.var(Xtr, axis=-1) + 1e-12)\n",
    "    Fe = np.log(np.var(Xte, axis=-1) + 1e-12)\n",
    "    return Ft, Fe, \"LOGVAR\"\n",
    "\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "yhat = np.empty_like(yt)\n",
    "used_modes = []\n",
    "\n",
    "for tr, te in gkf.split(X_all, yt, groups):\n",
    "    feats_tr, feats_te, modes = [], [], []\n",
    "    for _, lo, hi in bands:  # e.g., (\"mu\",8,13), (\"beta_low\",13,20), (\"beta_high\",20,30)\n",
    "        Xtr = bandpass_array(X_all[tr], lo, hi, sf)\n",
    "        Xte = bandpass_array(X_all[te], lo, hi, sf)\n",
    "        Ft, Fe, mode = fit_transform_csp(Xtr, Xte, yt[tr])\n",
    "        feats_tr.append(Ft); feats_te.append(Fe); modes.append(mode)\n",
    "    Xtr_fb = np.concatenate(feats_tr, axis=1)\n",
    "    Xte_fb = np.concatenate(feats_te, axis=1)\n",
    "    used_modes.append(modes)\n",
    "\n",
    "    clf = LDA(solver=\"lsqr\", shrinkage=\"auto\")\n",
    "    clf.fit(Xtr_fb, yt[tr])\n",
    "    yhat[te] = clf.predict(Xte_fb)\n",
    "\n",
    "acc = (yhat == yt).mean()\n",
    "cm  = confusion_matrix(yt, yhat, labels=[0,1])\n",
    "rep = classification_report(yt, yhat, target_names=[\"rest\",\"task\"], digits=3)\n",
    "\n",
    "print(\"\\n=== Robust FBCSP (Î¼/Î²) with GroupKFold(5) ===\")\n",
    "print(\"Band modes per fold:\", used_modes)\n",
    "print(\"Accuracy:\", round(float(acc), 3))\n",
    "print(\"Confusion [[restâ†’rest, restâ†’task],[taskâ†’rest, taskâ†’task]]:\\n\", cm)\n",
    "print(\"\\nReport:\\n\", rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d437ad27-c1a3-46d1-a5c5-78beedf469eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "â˜… Hybrid alphabet: K=4 (silhouette=0.322)\n",
      "Saved â†’ ./cog_alphabet_hybrid\n"
     ]
    }
   ],
   "source": [
    "# === Hybrid Alphabet Fusion: spectral + FBCSP task proba (R03) â†’ re-cluster ===\n",
    "import os, re, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "import mne\n",
    "from mne.decoding import CSP\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "OD_IN  = \"./cog_alphabet_physio_betaSplit\"    # your latest spectral run (Î² split, Î³â†‘)\n",
    "OD_OUT = \"./cog_alphabet_hybrid\"              # new hybrid alphabet\n",
    "BASE   = \"./brainwaves\"                       # EDFs\n",
    "os.makedirs(OD_OUT, exist_ok=True)\n",
    "\n",
    "# --- Load existing spectral features/meta/assignments ---\n",
    "feat = pd.read_csv(os.path.join(OD_IN, \"features.csv\"))\n",
    "meta = pd.read_csv(os.path.join(OD_IN, \"metadata.csv\"))\n",
    "asgn = pd.read_csv(os.path.join(OD_IN, \"state_assignments.csv\"))\n",
    "\n",
    "# --- Find an R03 EDF and load raw (resample first, safe Nyquist) ---\n",
    "edf_files = [f for f in os.listdir(BASE) if f.lower().endswith(\".edf\")]\n",
    "r03 = [f for f in edf_files if re.search(r\"R0*3\", f, re.I)]\n",
    "fb_proba = np.full(len(meta), np.nan, dtype=float)\n",
    "\n",
    "if r03:\n",
    "    fname = r03[0]; raw = mne.io.read_raw_edf(os.path.join(BASE, fname), preload=True, verbose=False)\n",
    "    target_sfreq = 250.0\n",
    "    if abs(raw.info[\"sfreq\"] - target_sfreq) > 1e-6:\n",
    "        raw.resample(target_sfreq, npad=\"auto\", verbose=False)\n",
    "    sf = float(raw.info[\"sfreq\"]); ny = sf/2\n",
    "    raw.filter(0.5, min(40.0, ny-1.0), verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "        try:\n",
    "            raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"),\n",
    "                            match_case=False, match_alias=True, on_missing=\"warn\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Pick ROI channels (C3/Cz/C4); fallback to all EEG if not found\n",
    "    chs = raw.ch_names\n",
    "    roi = []\n",
    "    for target in [\"C3\",\"Cz\",\"C4\"]:\n",
    "        for c in chs:\n",
    "            if re.fullmatch(target, c, re.I) or re.search(rf\"^{target}\\b\", c, re.I):\n",
    "                roi.append(c)\n",
    "    roi = sorted(set(roi)) or [c for c,t in zip(raw.ch_names, raw.get_channel_types()) if t==\"eeg\"]\n",
    "    raw.pick(roi)\n",
    "\n",
    "    # Extract per-epoch windows for rows that belong to this file\n",
    "    idx = np.where(meta[\"file\"] == fname)[0]\n",
    "    if len(idx):\n",
    "        # Label each epoch by annotations (â‰¥50% overlap rule) and group by segment id to avoid leakage\n",
    "        anns = raw.annotations\n",
    "        def label_epoch(t0, t1):\n",
    "            if anns is None or len(anns)==0: return None, None\n",
    "            best_id = None; lbl = None; best_ov = 0.0\n",
    "            for j, (o,d,s) in enumerate(zip(anns.onset, anns.duration, anns.description)):\n",
    "                su = str(s).upper()\n",
    "                if not ((\"T0\" in su) or (\"T1\" in su) or (\"T2\" in su)): continue\n",
    "                left, right = max(t0, float(o)), min(t1, float(o)+float(d))\n",
    "                if right <= left: continue\n",
    "                ov = right-left\n",
    "                if ov > best_ov:\n",
    "                    best_ov = ov; best_id = j\n",
    "                    if \"T1\" in su or \"T2\" in su: lbl = \"task\"\n",
    "                    elif \"T0\" in su: lbl = \"rest\"\n",
    "            # Require majority overlap\n",
    "            if best_ov >= 0.5*(t1-t0):\n",
    "                return lbl, best_id\n",
    "            return None, None\n",
    "\n",
    "        # Build epoch cubes aligned to the meta times\n",
    "        starts = meta.loc[idx, \"t_start_s\"].to_numpy()\n",
    "        ends   = meta.loc[idx, \"t_end_s\"].to_numpy()\n",
    "        labs, groups = [], []\n",
    "        # Pull samples\n",
    "        def pull_epoch(t0, t1):\n",
    "            s = int(np.round(t0*sf)); e = int(np.round(t1*sf))\n",
    "            return raw.get_data(start=s, stop=e)  # (n_ch, n_t)\n",
    "\n",
    "        X_list = []\n",
    "        for t0, t1 in zip(starts, ends):\n",
    "            X_list.append(pull_epoch(t0, t1))\n",
    "            L, G = label_epoch(t0, t1)\n",
    "            labs.append(L); groups.append(G if G is not None else -1)\n",
    "        X = np.stack(X_list, axis=0)  # (n_ep, n_ch, n_t)\n",
    "        y = np.array(labs, dtype=object)\n",
    "        grp = np.array(groups)\n",
    "\n",
    "        # Keep only labeled epochs (task/rest)\n",
    "        keep = np.isin(y, [\"task\",\"rest\"])\n",
    "        if keep.any():\n",
    "            Xl = X[keep]; yl = (y[keep]==\"task\").astype(int); gl = grp[keep]\n",
    "            # Filter-bank bands\n",
    "            bands = [(\"mu\",8.0,13.0), (\"beta_low\",13.0,20.0), (\"beta_high\",20.0,30.0)]\n",
    "\n",
    "            def bandpass_array(Xband, lo, hi):\n",
    "                n_ep, n_ch, n_t = Xband.shape\n",
    "                X2 = Xband.reshape(n_ep*n_ch, n_t)\n",
    "                Xf2 = mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
    "                return Xf2.reshape(n_ep, n_ch, n_t)\n",
    "\n",
    "            # GroupKFold by annotation segment id\n",
    "            gkf = GroupKFold(n_splits=min(5, max(2, len(np.unique(gl[gl>=0])))))\n",
    "            proba = np.zeros(len(yl), dtype=float)\n",
    "\n",
    "            for tr, te in gkf.split(Xl, yl, gl):\n",
    "                feats_tr, feats_te = [], []\n",
    "                for _, lo, hi in bands:\n",
    "                    Xtr = bandpass_array(Xl[tr], lo, hi); Xte = bandpass_array(Xl[te], lo, hi)\n",
    "                    # Regularized CSP for stability\n",
    "                    csp = CSP(n_components=6, reg=\"ledoit_wolf\", log=True, norm_trace=False, rank=\"info\")\n",
    "                    csp.fit(Xtr, yl[tr])\n",
    "                    feats_tr.append(csp.transform(Xtr))\n",
    "                    feats_te.append(csp.transform(Xte))\n",
    "                Xtr_fb = np.concatenate(feats_tr, axis=1)\n",
    "                Xte_fb = np.concatenate(feats_te, axis=1)\n",
    "                clf = LDA(solver=\"lsqr\", shrinkage=\"auto\")\n",
    "                clf.fit(Xtr_fb, yl[tr])\n",
    "                proba[te] = clf.predict_proba(Xte_fb)[:,1]  # P(task)\n",
    "\n",
    "            # write back to global index positions\n",
    "            fb_proba[idx[keep]] = proba\n",
    "\n",
    "# --- Append hybrid feature; fill NaN with 0.0 (non-R03 epochs) ---\n",
    "feat_h = feat.copy()\n",
    "feat_h[\"fbCSP_task_proba\"] = np.nan_to_num(fb_proba, nan=0.0)\n",
    "\n",
    "# --- Re-cluster on augmented features ---\n",
    "Xs = StandardScaler().fit_transform(feat_h.values)\n",
    "Z  = PCA(n_components=min(20, Xs.shape[1]), random_state=42).fit_transform(Xs)\n",
    "\n",
    "best = {\"k\":None, \"sil\":-1.0, \"model\":None, \"labels\":None}\n",
    "for k in range(4, 8):\n",
    "    km = KMeans(n_clusters=k, n_init=\"auto\", random_state=42)\n",
    "    lab = km.fit_predict(Z)\n",
    "    sil = silhouette_score(Z, lab) if k>1 else -1.0\n",
    "    if sil > best[\"sil\"]:\n",
    "        best = {\"k\":k, \"sil\":sil, \"model\":km, \"labels\":lab}\n",
    "\n",
    "labels = best[\"labels\"]; K = best[\"k\"]\n",
    "print(f\"â˜… Hybrid alphabet: K={K} (silhouette={best['sil']:.3f})\")\n",
    "\n",
    "# --- Save hybrid outputs (mirror run_pipelineâ€™s artifacts) ---\n",
    "feat_h.to_csv(os.path.join(OD_OUT, \"features.csv\"), index=False)\n",
    "meta.to_csv(os.path.join(OD_OUT, \"metadata.csv\"), index=False)\n",
    "pd.DataFrame({\"state\": labels}).to_csv(os.path.join(OD_OUT, \"state_assignments.csv\"), index=False)\n",
    "\n",
    "# Per-state feature medians + top features\n",
    "F = feat_h.copy(); F[\"state\"] = labels\n",
    "med = F.groupby(\"state\").median(numeric_only=True).sort_index()\n",
    "z = (med - feat_h.median())/(feat_h.std()+1e-9)\n",
    "alpha_map = {int(s): {\"name\": f\"State-{int(s)}\",\n",
    "                      \"top_features\": z.loc[s].abs().sort_values(ascending=False).head(8).index.tolist()}\n",
    "             for s in med.index}\n",
    "with open(os.path.join(OD_OUT,\"cognitive_alphabet.json\"), \"w\") as f:\n",
    "    json.dump(alpha_map, f, indent=2)\n",
    "\n",
    "# Quick plots\n",
    "p2 = PCA(n_components=2, random_state=42); E2 = p2.fit_transform(Z)\n",
    "plt.figure(figsize=(7.2,6.0)); plt.scatter(E2[:,0], E2[:,1], c=labels, s=8)\n",
    "plt.title(f\"Hybrid Alphabet (K={K})\"); plt.xlabel(\"Dim 1\"); plt.ylabel(\"Dim 2\"); plt.tight_layout()\n",
    "plt.savefig(os.path.join(OD_OUT,\"embedding.png\"), dpi=160); plt.close()\n",
    "\n",
    "M = (med - feat_h.median())/(feat_h.std()+1e-9)\n",
    "plt.figure(figsize=(min(14, 2+0.5*M.shape[1]), 0.6+0.3*K+2))\n",
    "im = plt.imshow(M.values, aspect=\"auto\"); plt.colorbar(im, fraction=0.025, pad=0.03, label=\"z-score vs overall\")\n",
    "plt.yticks(range(M.shape[0]), [f\"S{int(s)}\" for s in med.index]); plt.xticks(range(M.shape[1]), M.columns, rotation=90)\n",
    "plt.title(\"State Feature Signatures (Hybrid)\"); plt.tight_layout()\n",
    "plt.savefig(os.path.join(OD_OUT,\"state_feature_signatures.png\"), dpi=160); plt.close()\n",
    "\n",
    "print(\"Saved â†’\", OD_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3e6f1d1-2f79-47da-bbd4-cfd13c7755c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Hybrid State Top-Features ===\n",
      "S0: fbCSP_task_proba, beta_low_rel_med, theta_over_alpha, delta_rel_med, hjorth_complexity_med, beta_rel_med, spec_entropy_med, beta_high_rel_med âŸµ includes fbCSP_task_proba\n",
      "S1: hjorth_activity_med, gamma_rel_med, hjorth_mobility_med, spec_centroid_med, beta_high_rel_med, spec_entropy_std, spec_entropy_med, delta_rel_std\n",
      "S2: alpha_rel_med, alpha_rel_std, alpha_rel_iqr, (alpha+theta)_over_beta, theta_over_alpha, beta_over_alpha, beta_rel_med, gamma_rel_iqr\n",
      "S3: theta_rel_med, beta_low_rel_iqr, beta_low_rel_std, theta_rel_iqr, spec_centroid_std, theta_rel_std, beta_high_rel_std, beta_high_rel_iqr\n",
      "\n",
      "=== Hybrid per-file composition (fractions) ===\n",
      "state              0    1      2      3\n",
      "file                                   \n",
      "S001R01.edf    0.311  0.0  0.000  0.689\n",
      "S001R02.edf    0.000  0.0  0.874  0.126\n",
      "S001R03.edf    0.453  0.0  0.004  0.543\n",
      "sim_alpha.csv  0.000  1.0  0.000  0.000\n",
      "sim_theta.csv  0.000  1.0  0.000  0.000\n",
      "\n",
      "Scoring movement file: S001R03.edf\n",
      "\n",
      "AUC(fbCSP_task_proba â†’ task) on R03: 0.740\n",
      "\n",
      "R03 per-state task stats (sorted by mean proba):\n",
      "       mean_proba  task_frac    n\n",
      "state                            \n",
      "0           0.538      0.554  112\n",
      "3           0.463      0.470  134\n",
      "2           0.002      0.000    1\n",
      "\n",
      "R03 confusion by 0.5 threshold [[restâ†’rest, restâ†’task],[taskâ†’rest, taskâ†’task]]:\n",
      "[[83 39]\n",
      " [44 81]]\n",
      "\n",
      "Artifacts â†’ ./cog_alphabet_hybrid_eval\n"
     ]
    }
   ],
   "source": [
    "# === Hybrid Alphabet Evaluation: what changed, how strong is the task signal? ===\n",
    "import os, re, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "import mne\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "OD_H  = \"./cog_alphabet_hybrid\"          # new hybrid (with fbCSP_task_proba)\n",
    "OD_S  = \"./cog_alphabet_physio_betaSplit\" # spectral-only baseline you ran last\n",
    "BASE  = \"./brainwaves\"\n",
    "OUT   = \"./cog_alphabet_hybrid_eval\"; os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "# --- load hybrid ---\n",
    "feat_h = pd.read_csv(os.path.join(OD_H, \"features.csv\"))\n",
    "meta_h = pd.read_csv(os.path.join(OD_H, \"metadata.csv\"))\n",
    "asg_h  = pd.read_csv(os.path.join(OD_H, \"state_assignments.csv\"))\n",
    "with open(os.path.join(OD_H, \"cognitive_alphabet.json\")) as f:\n",
    "    amap_h = json.load(f)\n",
    "labels_h = asg_h[\"state\"].to_numpy()\n",
    "\n",
    "# --- quick summary: which features define each state? (does fbCSP_task_proba show up) ---\n",
    "print(\"\\n=== Hybrid State Top-Features ===\")\n",
    "for s in sorted(map(int, amap_h.keys())):\n",
    "    tops = amap_h[str(s)][\"top_features\"]\n",
    "    flag = \" âŸµ includes fbCSP_task_proba\" if \"fbCSP_task_proba\" in tops else \"\"\n",
    "    print(f\"S{s}: \" + \", \".join(tops) + flag)\n",
    "\n",
    "# --- per-file composition in hybrid ---\n",
    "DfH = meta_h.copy(); DfH[\"state\"] = labels_h\n",
    "comp = (DfH.groupby(\"file\")[\"state\"].value_counts(normalize=True)\n",
    "          .rename(\"fraction\").reset_index()\n",
    "          .pivot(index=\"file\", columns=\"state\", values=\"fraction\").fillna(0.0))\n",
    "comp.to_csv(os.path.join(OUT, \"hybrid_composition.csv\"))\n",
    "print(\"\\n=== Hybrid per-file composition (fractions) ===\")\n",
    "print(comp.round(3))\n",
    "\n",
    "# --- R03 scoring with ground truth from EDF annotations (task/rest) ---\n",
    "edf_files = {f.lower(): os.path.join(BASE, f) for f in os.listdir(BASE) if f.lower().endswith(\".edf\")}\n",
    "r03_name = None\n",
    "for fn in meta_h[\"file\"].unique():\n",
    "    m = re.search(r\"R0*3\", fn, re.I)\n",
    "    if m and fn.lower() in edf_files:\n",
    "        r03_name = fn\n",
    "        break\n",
    "\n",
    "if r03_name is None:\n",
    "    print(\"\\n(No R03 EDF found locally â€” skipping task/rest scoring.)\")\n",
    "else:\n",
    "    print(f\"\\nScoring movement file: {r03_name}\")\n",
    "    raw = mne.io.read_raw_edf(edf_files[r03_name.lower()], preload=False, verbose=False)\n",
    "    anns = getattr(raw, \"annotations\", None)\n",
    "\n",
    "    # rows for R03 only\n",
    "    idx = np.where(meta_h[\"file\"] == r03_name)[0]\n",
    "    sub_meta = meta_h.iloc[idx].reset_index(drop=True)\n",
    "    sub_feat = feat_h.iloc[idx].reset_index(drop=True)\n",
    "    sub_lab  = labels_h[idx]\n",
    "\n",
    "    # derive epoch labels by â‰¥50% overlap with T0/T1/T2\n",
    "    def label_epoch(t0, t1):\n",
    "        if anns is None or len(anns)==0: return None\n",
    "        ov_task = ov_rest = 0.0\n",
    "        for o,d,s in zip(anns.onset, anns.duration, anns.description):\n",
    "            su = str(s).upper()\n",
    "            if \"T0\" in su or \"T1\" in su or \"T2\" in su:\n",
    "                left, right = max(t0, float(o)), min(t1, float(o)+float(d))\n",
    "                if right > left:\n",
    "                    if \"T1\" in su or \"T2\" in su: ov_task += (right-left)\n",
    "                    elif \"T0\" in su:             ov_rest += (right-left)\n",
    "        span = t1 - t0\n",
    "        if ov_task >= 0.5*span: return \"task\"\n",
    "        if ov_rest >= 0.5*span: return \"rest\"\n",
    "        return None\n",
    "\n",
    "    gt = [label_epoch(t0, t1) for t0, t1 in zip(sub_meta[\"t_start_s\"], sub_meta[\"t_end_s\"])]\n",
    "    keep = [g in (\"task\",\"rest\") for g in gt]\n",
    "    if not any(keep):\n",
    "        print(\"No labeled epochs found in R03 annotations; skipping.\")\n",
    "    else:\n",
    "        GT = np.array([g for g,k in zip(gt, keep) if k])\n",
    "        ST = sub_lab[keep]\n",
    "        P  = sub_feat.loc[keep, \"fbCSP_task_proba\"].to_numpy()\n",
    "\n",
    "        # AUC for task proba\n",
    "        y_bin = (GT == \"task\").astype(int)\n",
    "        try:\n",
    "            auc = roc_auc_score(y_bin, P)\n",
    "        except Exception:\n",
    "            auc = float(\"nan\")\n",
    "        # map each state to mean proba & task fraction\n",
    "        df_state = pd.DataFrame({\"state\": ST, \"proba\": P, \"gt\": GT})\n",
    "        state_stats = (df_state.groupby(\"state\")\n",
    "                       .agg(mean_proba=(\"proba\",\"mean\"),\n",
    "                            task_frac=(\"gt\", lambda x: np.mean(x==\"task\")),\n",
    "                            n=(\"gt\",\"size\"))\n",
    "                       .sort_values(\"mean_proba\", ascending=False))\n",
    "        state_stats.to_csv(os.path.join(OUT, \"R03_state_task_stats.csv\"))\n",
    "\n",
    "        # simple thresholding by proba 0.5\n",
    "        pred_bin = (P >= 0.5).astype(int)\n",
    "        cm = confusion_matrix(y_bin, pred_bin, labels=[0,1])\n",
    "\n",
    "        print(f\"\\nAUC(fbCSP_task_proba â†’ task) on R03: {auc:.3f}\")\n",
    "        print(\"\\nR03 per-state task stats (sorted by mean proba):\")\n",
    "        print(state_stats.round(3))\n",
    "        print(\"\\nR03 confusion by 0.5 threshold [[restâ†’rest, restâ†’task],[taskâ†’rest, taskâ†’task]]:\")\n",
    "        print(cm)\n",
    "\n",
    "        # plots\n",
    "        plt.figure(figsize=(6,3))\n",
    "        order = state_stats.index.tolist()\n",
    "        plt.bar(range(len(order)), state_stats[\"mean_proba\"].values)\n",
    "        plt.xticks(range(len(order)), [f\"S{s}\" for s in order])\n",
    "        plt.ylabel(\"mean fbCSP_task_proba\"); plt.title(\"R03: task-proba by state\")\n",
    "        plt.tight_layout(); plt.savefig(os.path.join(OUT, \"R03_task_proba_by_state.png\"), dpi=140); plt.close()\n",
    "\n",
    "        plt.figure(figsize=(6,3))\n",
    "        plt.bar(range(len(order)), state_stats[\"task_frac\"].values)\n",
    "        plt.xticks(range(len(order)), [f\"S{s}\" for s in order])\n",
    "        plt.ylabel(\"fraction of epochs labeled task\"); plt.title(\"R03: task fraction by state\")\n",
    "        plt.tight_layout(); plt.savefig(os.path.join(OUT, \"R03_task_frac_by_state.png\"), dpi=140); plt.close()\n",
    "\n",
    "        # save per-epoch table\n",
    "        per_epoch = pd.DataFrame({\n",
    "            \"file\": r03_name,\n",
    "            \"t_start_s\": sub_meta.loc[keep, \"t_start_s\"].to_numpy(),\n",
    "            \"t_end_s\":   sub_meta.loc[keep, \"t_end_s\"].to_numpy(),\n",
    "            \"state\":     ST,\n",
    "            \"fbCSP_task_proba\": P,\n",
    "            \"gt\":        GT\n",
    "        })\n",
    "        per_epoch.to_csv(os.path.join(OUT, \"R03_epochs_hybrid.csv\"), index=False)\n",
    "\n",
    "print(\"\\nArtifacts â†’\", OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb4af411-cbdc-404c-8a6e-7d3347beea3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved report â†’ ./cog_alphabet_report\\CNT_CognitiveAlphabet_Validation_Report.png\n",
      "Saved report â†’ ./cog_alphabet_report\\CNT_CognitiveAlphabet_Validation_Report.pdf\n",
      "Saved metrics â†’ ./cog_alphabet_report\\metrics.json\n"
     ]
    }
   ],
   "source": [
    "# === Cognitive Alphabet â€” One-Page Validation Report (PNG + PDF) ===\n",
    "# Gathers your latest hybrid outputs + EO/EC + R03 task stats into a single page.\n",
    "# Saves to ./cog_alphabet_report/CNT_CognitiveAlphabet_Validation_Report.(png|pdf)\n",
    "\n",
    "import os, re, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# --------- Paths (edit if you moved things) ---------\n",
    "OD_H   = \"./cog_alphabet_hybrid\"             # hybrid run (has fbCSP_task_proba)\n",
    "OD_HE  = \"./cog_alphabet_hybrid_eval\"        # hybrid eval artifacts\n",
    "OD_GT  = \"./cog_alphabet_eval_groundtruth\"   # EO/EC & task/rest eval from earlier\n",
    "OUTDIR = \"./cog_alphabet_report\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# --------- Load Hybrid Artifacts ----------\n",
    "feat_h = pd.read_csv(os.path.join(OD_H, \"features.csv\"))\n",
    "meta_h = pd.read_csv(os.path.join(OD_H, \"metadata.csv\"))\n",
    "asg_h  = pd.read_csv(os.path.join(OD_H, \"state_assignments.csv\"))\n",
    "with open(os.path.join(OD_H, \"cognitive_alphabet.json\")) as f:\n",
    "    amap_h = json.load(f)\n",
    "labels_h = asg_h[\"state\"].to_numpy()\n",
    "\n",
    "# Read hybrid plots if present\n",
    "path_embed = os.path.join(OD_H, \"embedding.png\")\n",
    "path_heat  = os.path.join(OD_H, \"state_feature_signatures.png\")\n",
    "img_embed = Image.open(path_embed) if os.path.exists(path_embed) else None\n",
    "img_heat  = Image.open(path_heat)  if os.path.exists(path_heat)  else None\n",
    "\n",
    "# Read hybrid eval plots if present\n",
    "path_proba = os.path.join(OD_HE, \"R03_task_proba_by_state.png\")\n",
    "path_frac  = os.path.join(OD_HE, \"R03_task_frac_by_state.png\")\n",
    "img_proba  = Image.open(path_proba) if os.path.exists(path_proba) else None\n",
    "img_frac   = Image.open(path_frac)  if os.path.exists(path_frac)  else None\n",
    "\n",
    "# --------- EO/EC scoring (recompute from hybrid if needed) ----------\n",
    "# EO/EC rule: alpha-dominant letter â†’ EC, others â†’ EO\n",
    "def guess_alpha_state(feat_df, labels):\n",
    "    F = feat_df.copy(); F[\"state\"]=labels\n",
    "    med = F.groupby(\"state\")[\"alpha_rel_med\"].median(numeric_only=True)\n",
    "    return int(med.idxmax())\n",
    "\n",
    "alpha_state = guess_alpha_state(feat_h, labels_h)\n",
    "\n",
    "def coarse_cond_from_file(fn):\n",
    "    m = re.search(r\"R(\\d+)\", fn)\n",
    "    r = int(m.group(1)) if m else None\n",
    "    return {1:\"EO\", 2:\"EC\", 3:\"ME\"}.get(r, \"UNK\")\n",
    "\n",
    "meta_h[\"cond\"] = meta_h[\"file\"].apply(coarse_cond_from_file)\n",
    "\n",
    "# Try to read earlier EO/EC confusion; otherwise compute now\n",
    "cm_eoec_path = os.path.join(OD_GT, \"cm_eoec.csv\")\n",
    "if os.path.exists(cm_eoec_path):\n",
    "    cm_eoec = pd.read_csv(cm_eoec_path, index_col=0).values\n",
    "    eoec_acc = np.trace(cm_eoec)/cm_eoec.sum()\n",
    "else:\n",
    "    sub = meta_h[\"cond\"].isin([\"EO\",\"EC\"])\n",
    "    gt  = meta_h.loc[sub, \"cond\"].to_numpy()\n",
    "    pred= np.where(asg_h.loc[sub, \"state\"].to_numpy()==alpha_state, \"EC\", \"EO\")\n",
    "    cm_eoec = confusion_matrix(gt, pred, labels=[\"EO\",\"EC\"]).astype(float)\n",
    "    cm_eoec = (cm_eoec.T / cm_eoec.sum(1)).T  # row-normalized for display\n",
    "    eoec_acc = (gt==pred).mean()\n",
    "\n",
    "# --------- R03 task/rest scoring from hybrid eval (preferred) ----------\n",
    "r03_epochs_path = os.path.join(OD_HE, \"R03_epochs_hybrid.csv\")\n",
    "if os.path.exists(r03_epochs_path):\n",
    "    R = pd.read_csv(r03_epochs_path)\n",
    "    y_bin = (R[\"gt\"].to_numpy() == \"task\").astype(int)\n",
    "    p     = R[\"fbCSP_task_proba\"].to_numpy()\n",
    "    try:\n",
    "        auc_task = roc_auc_score(y_bin, p)\n",
    "    except Exception:\n",
    "        auc_task = float(\"nan\")\n",
    "    yhat = (p >= 0.5).astype(int)\n",
    "    cm_task = confusion_matrix(y_bin, yhat, labels=[0,1]).astype(int)\n",
    "else:\n",
    "    # fallback: approximate from groundtruth cm if present\n",
    "    cm_task_path = os.path.join(OD_GT, \"cm_task_rest.csv\")\n",
    "    if os.path.exists(cm_task_path):\n",
    "        cm_task = pd.read_csv(cm_task_path, index_col=0).values.astype(int)\n",
    "        auc_task = np.nan\n",
    "    else:\n",
    "        cm_task = np.array([[0,0],[0,0]], dtype=int); auc_task = np.nan\n",
    "\n",
    "# --------- Per-file composition table (hybrid) ----------\n",
    "DfH = meta_h.copy(); DfH[\"state\"] = labels_h\n",
    "comp = (DfH.groupby(\"file\")[\"state\"].value_counts(normalize=True)\n",
    "          .rename(\"fraction\")\n",
    "          .reset_index()\n",
    "          .pivot(index=\"file\", columns=\"state\", values=\"fraction\")\n",
    "          .fillna(0.0)\n",
    "          .sort_index())\n",
    "\n",
    "# Map state -> top features\n",
    "top_map = {int(s): amap_h[str(s)][\"top_features\"] for s in amap_h.keys()}\n",
    "\n",
    "# --------- Compose Report Page ----------\n",
    "W, H = 1600, 1200\n",
    "fig = plt.figure(figsize=(W/100, H/100), dpi=100)\n",
    "gs  = fig.add_gridspec(3, 4, height_ratios=[0.18, 0.42, 0.40], width_ratios=[0.38, 0.62, 0.0, 0.0])\n",
    "\n",
    "# Header / Metrics\n",
    "ax0 = fig.add_subplot(gs[0, :])\n",
    "ax0.axis(\"off\")\n",
    "title = \"Cognitive Alphabet â€” Validation Report (Hybrid Spectral + fbCSP)\"\n",
    "ax0.text(0.01, 0.72, title, fontsize=20, weight=\"bold\")\n",
    "ax0.text(0.01, 0.40, f\"Alphabet size (hybrid): {len(np.unique(labels_h))}    Alpha state: S{alpha_state}\", fontsize=12)\n",
    "ax0.text(0.01, 0.17, f\"EO vs EC accuracy: {eoec_acc:.3f}    R03 task AUC (fbCSP_task_proba): {auc_task if not np.isnan(auc_task) else 'n/a'}\", fontsize=12)\n",
    "\n",
    "# Left column: EO/EC & Task confusion tables + per-file composition\n",
    "from matplotlib.table import Table\n",
    "def add_table(ax, data, rowlab, collab, title):\n",
    "    ax.axis(\"off\"); ax.set_title(title, fontsize=12, pad=6)\n",
    "    tb = Table(ax, bbox=[0,0,1,1])\n",
    "    nrows, ncols = data.shape\n",
    "    # column headers\n",
    "    tb.add_cell(0, 0, 0.25, 0.12, text=\"\", facecolor=\"#f0f0f0\")\n",
    "    for j,c in enumerate(collab):\n",
    "        tb.add_cell(0, j+1, 0.25, 0.12, text=str(c), facecolor=\"#f0f0f0\")\n",
    "    # rows\n",
    "    for i,r in enumerate(rowlab):\n",
    "        tb.add_cell(i+1, 0, 0.25, 0.12, text=str(r), facecolor=\"#f0f0f0\")\n",
    "        for j in range(ncols):\n",
    "            tb.add_cell(i+1, j+1, 0.25, 0.12, text=(f\"{data[i,j]:.3f}\" if isinstance(data[i,j], float) else str(data[i,j])))\n",
    "    ax.add_table(tb)\n",
    "\n",
    "# EO/EC confusion (row-normalized)\n",
    "ax1 = fig.add_subplot(gs[1, 0])\n",
    "add_table(ax1, cm_eoec, [\"EO\",\"EC\"], [\"EO\",\"EC\"], \"EO vs EC (row-normalized)\")\n",
    "\n",
    "# Task confusion (counts)\n",
    "ax2 = fig.add_subplot(gs[2, 0])\n",
    "add_table(ax2, cm_task.astype(float), [\"rest\",\"task\"], [\"rest\",\"task\"], \"R03 Task vs Rest (counts @ 0.5)\")\n",
    "\n",
    "# Right column: images (embedding / signatures) and hybrid eval charts if available\n",
    "def paste_image(ax, pil_img, title):\n",
    "    ax.axis(\"off\"); \n",
    "    if pil_img is not None:\n",
    "        ax.imshow(pil_img); \n",
    "        ax.set_title(title, fontsize=12, pad=6)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, \"Image not found\", ha=\"center\", va=\"center\")\n",
    "\n",
    "ax3 = fig.add_subplot(gs[1, 1])\n",
    "paste_image(ax3, img_embed, \"Hybrid Embedding\")\n",
    "\n",
    "ax4 = fig.add_subplot(gs[2, 1])\n",
    "paste_image(ax4, img_heat, \"State Feature Signatures (Hybrid)\")\n",
    "\n",
    "# Underlay: a slim row of task-proba charts if present (overlayed at bottom of ax4)\n",
    "inset_y = 0.0\n",
    "if img_proba is not None or img_frac is not None:\n",
    "    box = ax4.get_position()\n",
    "    # reserve an inset strip below signatures\n",
    "    ax4.set_position([box.x0, box.y0+0.08, box.width, box.height-0.08])\n",
    "    inset = fig.add_axes([box.x0, box.y0, box.width/2-0.01, 0.08])\n",
    "    inset2= fig.add_axes([box.x0+box.width/2+0.01, box.y0, box.width/2-0.01, 0.08])\n",
    "    paste_image(inset,  img_proba, \"R03: mean task-proba by state\")\n",
    "    paste_image(inset2, img_frac,  \"R03: task fraction by state\")\n",
    "\n",
    "# Small text panel with state summaries\n",
    "state_lines = []\n",
    "for s in sorted(top_map.keys()):\n",
    "    tops = top_map[s][:6]\n",
    "    mark = \" â† task-aware\" if \"fbCSP_task_proba\" in top_map[s] else \"\"\n",
    "    state_lines.append(f\"S{s}: \" + \", \".join(tops) + mark)\n",
    "\n",
    "ax5 = fig.add_axes([0.01, 0.58, 0.36, 0.12])  # under header, left\n",
    "ax5.axis(\"off\")\n",
    "ax5.set_title(\"State Top Features\", fontsize=12, pad=4)\n",
    "ax5.text(0.0, 1.0, \"\\n\".join(state_lines), va=\"top\", fontsize=9, family=\"monospace\")\n",
    "\n",
    "# Per-file composition (hybrid) â€” show top 6 rows\n",
    "ax6 = fig.add_axes([0.01, 0.45, 0.36, 0.10])\n",
    "ax6.axis(\"off\")\n",
    "ax6.set_title(\"Per-file Composition (hybrid, fractions)\", fontsize=12, pad=4)\n",
    "comp_show = comp.copy()\n",
    "# convert to nice text\n",
    "lines = []\n",
    "for file, row in comp_show.head(6).iterrows():\n",
    "    parts = [f\"S{int(c)}={row[c]:.2f}\" for c in comp_show.columns]\n",
    "    lines.append(f\"{file}: \" + \", \".join(parts))\n",
    "ax6.text(0.0, 1.0, \"\\n\".join(lines), va=\"top\", fontsize=9, family=\"monospace\")\n",
    "\n",
    "# Footer\n",
    "axF = fig.add_axes([0.01, 0.01, 0.98, 0.06]); axF.axis(\"off\")\n",
    "axF.text(0.01, 0.65, \"Notes:\", fontsize=9, weight=\"bold\")\n",
    "axF.text(0.01, 0.35, \"â€¢ EC â†” Alpha-Dominant is near-perfect.  â€¢ Hybrid adds task-aware consonants via fbCSP_task_proba without harming EC/EO.  â€¢ Confusions are per-epoch.\",\n",
    "         fontsize=9)\n",
    "axF.text(0.01, 0.05, \"Generated by CNT Cognitive Alphabet Engine â€” spectral + motor-ROI FBCSP fusion.\",\n",
    "         fontsize=9, style=\"italic\")\n",
    "\n",
    "# Save\n",
    "png_path = os.path.join(OUTDIR, \"CNT_CognitiveAlphabet_Validation_Report.png\")\n",
    "pdf_path = os.path.join(OUTDIR, \"CNT_CognitiveAlphabet_Validation_Report.pdf\")\n",
    "fig.savefig(png_path, dpi=150, bbox_inches=\"tight\")\n",
    "fig.savefig(pdf_path, dpi=150, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "# Also dump a tiny metrics.json / .csv\n",
    "metrics = {\n",
    "    \"hybrid_K\": int(len(np.unique(labels_h))),\n",
    "    \"alpha_state\": int(alpha_state),\n",
    "    \"eoec_accuracy\": float(eoec_acc),\n",
    "    \"r03_task_auc\": (None if np.isnan(auc_task) else float(auc_task)),\n",
    "    \"r03_cm_rest_task\": cm_task.tolist(),\n",
    "}\n",
    "with open(os.path.join(OUTDIR, \"metrics.json\"), \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "pd.DataFrame([metrics]).to_csv(os.path.join(OUTDIR, \"metrics.csv\"), index=False)\n",
    "\n",
    "print(\"Saved report â†’\", png_path)\n",
    "print(\"Saved report â†’\", pdf_path)\n",
    "print(\"Saved metrics â†’\", os.path.join(OUTDIR, \"metrics.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ef8a2aa-b01a-4859-ba95-77da709a4707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved scaler/pca/kmeans for K=4 â†’ ./cog_alphabet_hybrid\n"
     ]
    }
   ],
   "source": [
    "# â€” Save hybrid models for reuse â€”\n",
    "import os, json, pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from joblib import dump\n",
    "\n",
    "OD_H = \"./cog_alphabet_hybrid\"\n",
    "feat_h = pd.read_csv(os.path.join(OD_H, \"features.csv\"))\n",
    "labels = pd.read_csv(os.path.join(OD_H, \"state_assignments.csv\"))[\"state\"].to_numpy()\n",
    "\n",
    "scaler = StandardScaler().fit(feat_h.values)\n",
    "Xs = scaler.transform(feat_h.values)\n",
    "pca = PCA(n_components=min(20, Xs.shape[1]), random_state=42).fit(Xs)\n",
    "Z = pca.transform(Xs)\n",
    "\n",
    "K = len(set(labels))\n",
    "km = KMeans(n_clusters=K, n_init=\"auto\", random_state=42).fit(Z)\n",
    "\n",
    "dump(scaler, os.path.join(OD_H, \"scaler_hybrid.joblib\"))\n",
    "dump(pca,    os.path.join(OD_H, \"pca_hybrid.joblib\"))\n",
    "dump(km,     os.path.join(OD_H, \"kmeans_hybrid.joblib\"))\n",
    "\n",
    "print(f\"Saved scaler/pca/kmeans for K={K} â†’\", OD_H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "938ed60b-23a4-46b0-8b39-c575026de557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Saved: ./cog_alphabet_decode\\decode_S001R01.csv\n",
      "Saved: ./cog_alphabet_decode\\decode_S001R01.png\n"
     ]
    }
   ],
   "source": [
    "# â€” Decode any EEG file into your hybrid letters â€”\n",
    "import os, re, json, numpy as np, pandas as pd, matplotlib.pyplot as plt, mne\n",
    "from joblib import load\n",
    "\n",
    "# Expect these helpers from the mega cell; if needed, re-run the mega cell once:\n",
    "#   load_raw_any, basic_clean, make_epochs, spectral_features\n",
    "# (We previously patched basic_clean & spectral_features for Nyquist and beta-split.)\n",
    "\n",
    "OD_H = \"./cog_alphabet_hybrid\"\n",
    "scaler = load(os.path.join(OD_H, \"scaler_hybrid.joblib\"))\n",
    "pca    = load(os.path.join(OD_H, \"pca_hybrid.joblib\"))\n",
    "km     = load(os.path.join(OD_H, \"kmeans_hybrid.joblib\"))\n",
    "\n",
    "INPATH = \"./brainwaves/S001R01.edf\"   # â† change to any new file\n",
    "OUTDIR = \"./cog_alphabet_decode\"; os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# 1) Load & preprocess using your pipeline settings\n",
    "raw = load_raw_any(INPATH)\n",
    "raw = basic_clean(raw)\n",
    "epochs, starts, ends = make_epochs(raw)\n",
    "if len(epochs) == 0:\n",
    "    raise RuntimeError(\"No epochs produced; adjust epoch_len_s/step_s in the mega cell config.\")\n",
    "\n",
    "# 2) Spectral features (same function you used)\n",
    "F_spec = spectral_features(epochs, cfg.bands)\n",
    "\n",
    "# 3) fbCSP_task_proba (optional; only if annotations present & overlap â‰¥50%)\n",
    "def fbCSP_task_proba_for_epochs(raw, starts, ends):\n",
    "    anns = getattr(raw, \"annotations\", None)\n",
    "    if anns is None or len(anns)==0: \n",
    "        return np.zeros(len(starts))\n",
    "    # label epochs by overlap with T1/T2 (task) vs T0 (rest)\n",
    "    labs, groups = [], []\n",
    "    for t0, t1 in zip(starts, ends):\n",
    "        ov_task = ov_rest = 0.0\n",
    "        for o,d,s in zip(anns.onset, anns.duration, anns.description):\n",
    "            su = str(s).upper()\n",
    "            if any(k in su for k in [\"T0\",\"T1\",\"T2\"]):\n",
    "                L, R = max(t0, float(o)), min(t1, float(o)+float(d))\n",
    "                if R > L:\n",
    "                    if \"T1\" in su or \"T2\" in su: ov_task += (R-L)\n",
    "                    elif \"T0\" in su:             ov_rest += (R-L)\n",
    "        if ov_task >= 0.5*(t1-t0): labs.append(1)\n",
    "        elif ov_rest >= 0.5*(t1-t0): labs.append(0)\n",
    "        else: labs.append(-1)\n",
    "        groups.append(-1)\n",
    "    # If we donâ€™t have labeled epochs, return zeros\n",
    "    if not any(np.array(labs) >= 0): \n",
    "        return np.zeros(len(starts))\n",
    "    # Build quick FBCSP â†’ LDA on labeled subset only, then predict proba for all\n",
    "    from mne.decoding import CSP\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "    X = epochs.get_data()  # (n_ep, n_ch, n_t)\n",
    "    sf = epochs.info[\"sfreq\"]\n",
    "    def bp(X, lo, hi):\n",
    "        n_ep, n_ch, n_t = X.shape\n",
    "        X2 = X.reshape(n_ep*n_ch, n_t)\n",
    "        Xf2 = mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
    "        return Xf2.reshape(n_ep, n_ch, n_t)\n",
    "    bands = [(8,13),(13,20),(20,30)]\n",
    "    Lmask = np.array(labs) >= 0\n",
    "    feats_tr = []; feats_all = []\n",
    "    for lo, hi in bands:\n",
    "        Xtr = bp(X[Lmask], lo, hi); Xall = bp(X, lo, hi)\n",
    "        csp = CSP(n_components=6, reg=\"ledoit_wolf\", log=True, norm_trace=False, rank=\"info\")\n",
    "        csp.fit(Xtr, np.array(labs)[Lmask])\n",
    "        feats_tr.append(csp.transform(Xtr))\n",
    "        feats_all.append(csp.transform(Xall))\n",
    "    Xtr_fb  = np.concatenate(feats_tr, axis=1)\n",
    "    Xall_fb = np.concatenate(feats_all, axis=1)\n",
    "    clf = LDA(solver=\"lsqr\", shrinkage=\"auto\").fit(Xtr_fb, np.array(labs)[Lmask])\n",
    "    proba = clf.predict_proba(Xall_fb)[:,1]\n",
    "    return proba\n",
    "\n",
    "try:\n",
    "    fbP = fbCSP_task_proba_for_epochs(raw, starts, ends)\n",
    "except Exception:\n",
    "    fbP = np.zeros(len(starts))\n",
    "\n",
    "# 4) Assemble hybrid feature matrix in the same order\n",
    "F_h = F_spec.copy()\n",
    "F_h[\"fbCSP_task_proba\"] = fbP\n",
    "\n",
    "# 5) Map â†’ letters with saved models\n",
    "Z = pca.transform(scaler.transform(F_h.values))\n",
    "letters = km.predict(Z)\n",
    "\n",
    "# 6) Save assignments & a tiny timeline plot\n",
    "out = pd.DataFrame({\n",
    "    \"file\": os.path.basename(INPATH),\n",
    "    \"t_start_s\": starts, \"t_end_s\": ends,\n",
    "    \"state\": letters\n",
    "})\n",
    "CSV = os.path.join(OUTDIR, f\"decode_{os.path.splitext(os.path.basename(INPATH))[0]}.csv\")\n",
    "out.to_csv(CSV, index=False)\n",
    "\n",
    "plt.figure(figsize=(10,1.8))\n",
    "plt.scatter(starts, letters, s=8, c=letters)\n",
    "plt.yticks(sorted(np.unique(letters)), [f\"S{s}\" for s in sorted(np.unique(letters))])\n",
    "plt.xlabel(\"time (s)\"); plt.title(f\"Letters â†’ {os.path.basename(INPATH)}\")\n",
    "plt.tight_layout()\n",
    "PNG = os.path.join(OUTDIR, f\"decode_{os.path.splitext(os.path.basename(INPATH))[0]}.png\")\n",
    "plt.savefig(PNG, dpi=140); plt.close()\n",
    "\n",
    "print(\"Saved:\", CSV)\n",
    "print(\"Saved:\", PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20c60154-a9e7-4e5b-a165-e9f57606c9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â€” decode_S001R01.csv â€”\n",
      "Top dwell (by name):\n",
      "state_name\n",
      "Mixed               0.689\n",
      "Beta/Theta-Mixed    0.311\n",
      "Name: fraction, dtype: float64\n",
      "Saved: ./cog_alphabet_decode_reports\\decode_S001R01_timeline_named.png\n",
      "Saved: ./cog_alphabet_decode_reports\\decode_S001R01_dwell_by_id.csv\n",
      "Saved: ./cog_alphabet_decode_reports\\decode_S001R01_dwell_by_name.csv\n",
      "Saved: ./cog_alphabet_decode_reports\\decode_S001R01_run_lengths.csv\n"
     ]
    }
   ],
   "source": [
    "# === Annotate & Summarize a Decoded File (hybrid alphabet) ===\n",
    "import os, json, re, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "OD_H   = \"./cog_alphabet_hybrid\"            # where cognitive_alphabet.json lives\n",
    "DECDIR = \"./cog_alphabet_decode\"            # where your decode_*.csv lives\n",
    "OUTDIR = \"./cog_alphabet_decode_reports\"; os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# pick one or many decoded CSVs (change the pattern to target others)\n",
    "targets = [os.path.join(DECDIR, f) for f in os.listdir(DECDIR) if f.endswith(\".csv\")]\n",
    "assert targets, \"No decoded CSVs found. Run the decode cell first.\"\n",
    "\n",
    "# load hybrid state feature map\n",
    "with open(os.path.join(OD_H, \"cognitive_alphabet.json\")) as f:\n",
    "    amap = json.load(f)\n",
    "\n",
    "def name_state_from_tops(tops):\n",
    "    tops = list(tops)\n",
    "    if any(t.startswith(\"alpha_rel_\") for t in tops) or \"alpha_rel_med\" in tops:\n",
    "        return \"Alpha-Dominant\"\n",
    "    if \"beta_high_rel_med\" in tops or \"beta_low_rel_med\" in tops or \"beta_rel_med\" in tops or \"beta_over_alpha\" in tops:\n",
    "        if \"theta_rel_med\" in tops or \"theta_over_alpha\" in tops:\n",
    "            return \"Beta/Theta-Mixed\"\n",
    "        return \"Beta-Leaning\"\n",
    "    if \"spec_entropy_med\" in tops or \"hjorth_complexity_med\" in tops:\n",
    "        return \"High-Entropy\"\n",
    "    return \"Mixed\"\n",
    "\n",
    "state_names = {int(s): name_state_from_tops(amap[str(s)][\"top_features\"]) for s in amap}\n",
    "\n",
    "def run_lengths(states):\n",
    "    \"\"\"Return list of (state, start_idx, end_idx, length) for consecutive runs.\"\"\"\n",
    "    runs = []\n",
    "    if len(states)==0: return runs\n",
    "    cur = states[0]; start = 0\n",
    "    for i in range(1, len(states)):\n",
    "        if states[i] != cur:\n",
    "            runs.append((int(cur), start, i-1, i-start))\n",
    "            cur = states[i]; start = i\n",
    "    runs.append((int(cur), start, len(states)-1, len(states)-start))\n",
    "    return runs\n",
    "\n",
    "for csv_path in targets:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    assert {\"file\",\"t_start_s\",\"t_end_s\",\"state\"}.issubset(df.columns), f\"Missing columns in {csv_path}\"\n",
    "    df[\"state_name\"] = df[\"state\"].map(state_names)\n",
    "    # dwell fractions\n",
    "    dwell = (df[\"state\"].value_counts(normalize=True).rename(\"fraction\").sort_index())\n",
    "    dwell_named = (df[\"state_name\"].value_counts(normalize=True).rename(\"fraction\"))\n",
    "    # run-length stats\n",
    "    rl = run_lengths(df[\"state\"].to_numpy())\n",
    "    RL = pd.DataFrame(rl, columns=[\"state\",\"start_idx\",\"end_idx\",\"length_epochs\"])\n",
    "    RL[\"state_name\"] = RL[\"state\"].map(state_names)\n",
    "    RL[\"dur_s\"] = RL[\"length_epochs\"] * (df[\"t_end_s\"].iloc[0] - df[\"t_start_s\"].iloc[0])  # epoch length\n",
    "    # save summaries\n",
    "    stem = os.path.splitext(os.path.basename(csv_path))[0]\n",
    "    dwell.to_csv(os.path.join(OUTDIR, f\"{stem}_dwell_by_id.csv\"))\n",
    "    dwell_named.to_csv(os.path.join(OUTDIR, f\"{stem}_dwell_by_name.csv\"))\n",
    "    RL.to_csv(os.path.join(OUTDIR, f\"{stem}_run_lengths.csv\"), index=False)\n",
    "\n",
    "    # clean timeline plot with names\n",
    "    plt.figure(figsize=(12,1.8))\n",
    "    plt.scatter(df[\"t_start_s\"], df[\"state\"], c=df[\"state\"], s=8, cmap=\"tab10\")\n",
    "    ticks = sorted(df[\"state\"].unique())\n",
    "    plt.yticks(ticks, [f\"S{s} ({state_names[int(s)]})\" for s in ticks])\n",
    "    plt.xlabel(\"time (s)\")\n",
    "    plt.title(f\"Letters â€” {df['file'].iloc[0]}\")\n",
    "    plt.tight_layout()\n",
    "    out_png = os.path.join(OUTDIR, f\"{stem}_timeline_named.png\")\n",
    "    plt.savefig(out_png, dpi=140); plt.close()\n",
    "\n",
    "    print(f\"\\nâ€” {os.path.basename(csv_path)} â€”\")\n",
    "    print(\"Top dwell (by name):\")\n",
    "    print(dwell_named.sort_values(ascending=False).round(3).head(6))\n",
    "    print(\"Saved:\", out_png)\n",
    "    print(\"Saved:\", os.path.join(OUTDIR, f\"{stem}_dwell_by_id.csv\"))\n",
    "    print(\"Saved:\", os.path.join(OUTDIR, f\"{stem}_dwell_by_name.csv\"))\n",
    "    print(\"Saved:\", os.path.join(OUTDIR, f\"{stem}_run_lengths.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540962a3-9c35-404f-87d3-dcb9edd8efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Batch & Bundle: decode all files â†’ atlas + overview + zip ===\n",
    "import os, re, json, numpy as np, pandas as pd, matplotlib.pyplot as plt, mne, shutil\n",
    "from joblib import load\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# ---- Paths / assets ----\n",
    "BASE   = \"./brainwaves\"\n",
    "OUT    = \"./cog_alphabet_decode\"\n",
    "REPRT  = \"./cog_alphabet_decode_reports\"\n",
    "ATLAS  = \"./cog_alphabet_atlas\"\n",
    "MODEL  = \"./cog_alphabet_hybrid\"\n",
    "os.makedirs(OUT, exist_ok=True)\n",
    "os.makedirs(REPRT, exist_ok=True)\n",
    "os.makedirs(ATLAS, exist_ok=True)\n",
    "\n",
    "# ---- Load alphabet + models ----\n",
    "with open(os.path.join(MODEL, \"cognitive_alphabet.json\")) as f:\n",
    "    AMAP = json.load(f)\n",
    "state_names = {\n",
    "    int(s): (\"Alpha-Dominant\" if any(t.startswith(\"alpha_rel_\") for t in AMAP[str(s)][\"top_features\"])\n",
    "             else \"Beta/Theta-Mixed\" if (\"theta_over_alpha\" in AMAP[str(s)][\"top_features\"] or \"theta_rel_med\" in AMAP[str(s)][\"top_features\"])\n",
    "             else \"Beta-Leaning\" if (\"beta_over_alpha\" in AMAP[str(s)][\"top_features\"] or \"beta_rel_med\" in AMAP[str(s)][\"top_features\"])\n",
    "             else \"High-Entropy\" if (\"spec_entropy_med\" in AMAP[str(s)][\"top_features\"] or \"hjorth_complexity_med\" in AMAP[str(s)][\"top_features\"])\n",
    "             else \"Mixed\")\n",
    "    for s in AMAP\n",
    "}\n",
    "\n",
    "scaler = load(os.path.join(MODEL, \"scaler_hybrid.joblib\"))\n",
    "pca    = load(os.path.join(MODEL, \"pca_hybrid.joblib\"))\n",
    "km     = load(os.path.join(MODEL, \"kmeans_hybrid.joblib\"))\n",
    "\n",
    "# ---- Helpers from the mega cell must exist in this session:\n",
    "# load_raw_any, basic_clean, make_epochs, spectral_features, cfg (for bands)\n",
    "needed = [\"load_raw_any\",\"basic_clean\",\"make_epochs\",\"spectral_features\",\"cfg\"]\n",
    "missing = [n for n in needed if n not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(\"Missing from session: \" + \", \".join(missing) + \". Run the mega cell first.\")\n",
    "\n",
    "# fbCSP task-proba (optional; uses annotations if present)\n",
    "def fbCSP_task_proba_for_epochs(raw, epochs, starts, ends):\n",
    "    try:\n",
    "        from mne.decoding import CSP\n",
    "        from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "    except Exception:\n",
    "        return np.zeros(len(starts))\n",
    "    anns = getattr(raw, \"annotations\", None)\n",
    "    if anns is None or len(anns)==0:\n",
    "        return np.zeros(len(starts))\n",
    "    # label epochs by â‰¥50% overlap with T1/T2 (task) vs T0 (rest)\n",
    "    lab = []\n",
    "    for t0, t1 in zip(starts, ends):\n",
    "        ov_task = ov_rest = 0.0\n",
    "        for o,d,s in zip(anns.onset, anns.duration, anns.description):\n",
    "            su = str(s).upper()\n",
    "            if any(k in su for k in [\"T0\",\"T1\",\"T2\"]):\n",
    "                L, R = max(t0, float(o)), min(t1, float(o)+float(d))\n",
    "                if R > L:\n",
    "                    if \"T1\" in su or \"T2\" in su: ov_task += (R-L)\n",
    "                    elif \"T0\" in su:             ov_rest += (R-L)\n",
    "        if ov_task >= 0.5*(t1-t0): lab.append(1)\n",
    "        elif ov_rest >= 0.5*(t1-t0): lab.append(0)\n",
    "        else: lab.append(-1)\n",
    "    lab = np.array(lab)\n",
    "    X = epochs.get_data()  # (n_ep, n_ch, n_t)\n",
    "    sf = epochs.info[\"sfreq\"]\n",
    "    def bp(X, lo, hi):\n",
    "        n_ep, n_ch, n_t = X.shape\n",
    "        X2 = X.reshape(n_ep*n_ch, n_t)\n",
    "        Xf2 = mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
    "        return Xf2.reshape(n_ep, n_ch, n_t)\n",
    "    bands = [(8,13),(13,20),(20,30)]\n",
    "    mask = lab >= 0\n",
    "    if not mask.any():\n",
    "        return np.zeros(len(starts))\n",
    "    feats_tr, feats_all = [], []\n",
    "    for lo, hi in bands:\n",
    "        Xtr  = bp(X[mask], lo, hi); Xall = bp(X, lo, hi)\n",
    "        from mne.decoding import CSP\n",
    "        csp = CSP(n_components=6, reg=\"ledoit_wolf\", log=True, norm_trace=False, rank=\"info\")\n",
    "        csp.fit(Xtr, lab[mask])\n",
    "        feats_tr.append(csp.transform(Xtr))\n",
    "        feats_all.append(csp.transform(Xall))\n",
    "    Xtr_fb  = np.concatenate(feats_tr, axis=1)\n",
    "    Xall_fb = np.concatenate(feats_all, axis=1)\n",
    "    clf = LDA(solver=\"lsqr\", shrinkage=\"auto\").fit(Xtr_fb, lab[mask])\n",
    "    proba = clf.predict_proba(Xall_fb)[:,1]\n",
    "    return proba\n",
    "\n",
    "def coarse_cond_from_file(fn):\n",
    "    m = re.search(r\"R(\\d+)\", fn); r = int(m.group(1)) if m else None\n",
    "    return {1:\"EO\", 2:\"EC\", 3:\"ME\"}.get(r, \"UNK\")\n",
    "\n",
    "# ---- Collect files and decode\n",
    "exts = (\".edf\",\".bdf\",\".fif\",\".vhdr\",\".eeg\",\".set\",\".fdt\",\".csv\",\".tsv\")\n",
    "files = sorted([f for f in os.listdir(BASE) if f.lower().endswith(exts)])\n",
    "if not files:\n",
    "    raise RuntimeError(\"No EEG files found in ./brainwaves\")\n",
    "\n",
    "rows = []\n",
    "for fn in files:\n",
    "    try:\n",
    "        raw = load_raw_any(os.path.join(BASE, fn))\n",
    "        raw = basic_clean(raw)\n",
    "        epochs, starts, ends = make_epochs(raw)\n",
    "        if len(epochs)==0:\n",
    "            print(f\"[skip] {fn}: no epochs\")\n",
    "            continue\n",
    "        F = spectral_features(epochs, cfg.bands)\n",
    "        # optional task proba\n",
    "        try:\n",
    "            fb = fbCSP_task_proba_for_epochs(raw, epochs, starts, ends)\n",
    "        except Exception:\n",
    "            fb = np.zeros(len(epochs))\n",
    "        Fh = F.copy(); Fh[\"fbCSP_task_proba\"] = fb\n",
    "\n",
    "        # map to letters\n",
    "        Z = pca.transform(scaler.transform(Fh.values))\n",
    "        letters = km.predict(Z)\n",
    "        out_csv = os.path.join(OUT, f\"decode_{os.path.splitext(fn)[0]}.csv\")\n",
    "        pd.DataFrame({\n",
    "            \"file\": fn,\n",
    "            \"t_start_s\": starts, \"t_end_s\": ends,\n",
    "            \"state\": letters\n",
    "        }).to_csv(out_csv, index=False)\n",
    "\n",
    "        # dwell & name\n",
    "        dwell = (pd.Series(letters).value_counts(normalize=True).sort_index())\n",
    "        name_frac = {}\n",
    "        for sid, frac in dwell.items():\n",
    "            nm = state_names.get(int(sid), f\"S{int(sid)}\")\n",
    "            name_frac[nm] = name_frac.get(nm, 0.0) + float(frac)\n",
    "        alpha_frac = name_frac.get(\"Alpha-Dominant\", 0.0)\n",
    "        mean_fb = float(np.mean(fb)) if len(fb) else 0.0\n",
    "\n",
    "        rows.append({\n",
    "            \"file\": fn,\n",
    "            \"cond\": coarse_cond_from_file(fn),\n",
    "            \"n_epochs\": int(len(epochs)),\n",
    "            \"alpha_frac\": round(alpha_frac, 3),\n",
    "            \"mean_fbCSP_task_proba\": round(mean_fb, 3),\n",
    "            **{f\"frac_{k.replace('/','_')}\": round(v,3) for k,v in name_frac.items()}\n",
    "        })\n",
    "\n",
    "        # quick timeline\n",
    "        plt.figure(figsize=(10,1.8))\n",
    "        plt.scatter(starts, letters, s=8, c=letters)\n",
    "        yt = sorted(np.unique(letters))\n",
    "        plt.yticks(yt, [f\"S{s}\" for s in yt])\n",
    "        plt.xlabel(\"time (s)\"); plt.title(f\"Letters â€” {fn}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(REPRT, f\"decode_{os.path.splitext(fn)[0]}_timeline.png\"), dpi=140)\n",
    "        plt.close()\n",
    "        print(f\"[ok] {fn} â†’ {out_csv}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[err] {fn}: {e}\")\n",
    "\n",
    "# ---- Master atlas CSV + overview heatmap\n",
    "ATLAS_CSV = os.path.join(ATLAS, \"cog_alphabet_atlas.csv\")\n",
    "atlas = pd.DataFrame(rows).fillna(0.0)\n",
    "cols_order = [\"file\",\"cond\",\"n_epochs\",\"alpha_frac\",\"mean_fbCSP_task_proba\"] + \\\n",
    "             sorted([c for c in atlas.columns if c.startswith(\"frac_\")])\n",
    "atlas = atlas[cols_order]\n",
    "atlas.to_csv(ATLAS_CSV, index=False)\n",
    "\n",
    "plt.figure(figsize=(10, max(3, 0.4*len(atlas))))\n",
    "heat = atlas.set_index(\"file\")[[c for c in atlas.columns if c.startswith(\"frac_\")]]\n",
    "plt.imshow(heat.values, aspect=\"auto\")\n",
    "plt.colorbar(label=\"fraction\")\n",
    "plt.yticks(range(len(heat.index)), heat.index)\n",
    "plt.xticks(range(len(heat.columns)), heat.columns, rotation=90)\n",
    "plt.title(\"Per-file letter fractions\")\n",
    "plt.tight_layout()\n",
    "OVPNG = os.path.join(ATLAS, \"overview_letter_fractions.png\")\n",
    "plt.savefig(OVPNG, dpi=150); plt.close()\n",
    "\n",
    "# ---- Bundle zip (optional)\n",
    "ZIP = shutil.make_archive(\"CNT_CognitiveAlphabet_Bundle\", \"zip\",\n",
    "                          root_dir=\".\", base_dir=\".\",\n",
    "                          logger=None)  # creates CNT_CognitiveAlphabet_Bundle.zip in cwd\n",
    "\n",
    "print(\"\\n=== Batch complete ===\")\n",
    "print(\"Atlas CSV â†’\", ATLAS_CSV)\n",
    "print(\"Overview PNG â†’\", OVPNG)\n",
    "print(\"Decoded files â†’\", OUT)\n",
    "print(\"Timelines â†’\", REPRT)\n",
    "print(\"Bundle â†’\", ZIP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f463e5d-16fc-4320-8ee3-b3e0075f4677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_14696\\427448052.py:58: RuntimeWarning: DigMontage is only a subset of info. There are 64 channel positions not present in the DigMontage. The channels missing from the montage are:\n",
      "\n",
      "['Fc5.', 'Fc3.', 'Fc1.', 'Fcz.', 'Fc2.', 'Fc4.', 'Fc6.', 'C5..', 'C3..', 'C1..', 'Cz..', 'C2..', 'C4..', 'C6..', 'Cp5.', 'Cp3.', 'Cp1.', 'Cpz.', 'Cp2.', 'Cp4.', 'Cp6.', 'Fp1.', 'Fpz.', 'Fp2.', 'Af7.', 'Af3.', 'Afz.', 'Af4.', 'Af8.', 'F7..', 'F5..', 'F3..', 'F1..', 'Fz..', 'F2..', 'F4..', 'F6..', 'F8..', 'Ft7.', 'Ft8.', 'T7..', 'T8..', 'T9..', 'T10.', 'Tp7.', 'Tp8.', 'P7..', 'P5..', 'P3..', 'P1..', 'Pz..', 'P2..', 'P4..', 'P6..', 'P8..', 'Po7.', 'Po3.', 'Poz.', 'Po4.', 'Po8.', 'O1..', 'Oz..', 'O2..', 'Iz..'].\n",
      "\n",
      "Consider using inst.rename_channels to match the montage nomenclature, or inst.set_channel_types if these are not EEG channels, or use the on_missing parameter if the channel positions are allowed to be unknown in your analyses.\n",
      "  raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"),\n",
      "WARNING:root:Did not find any electrode locations (in the info object), will attempt to use digitization points instead. However, if digitization points do not correspond to the EEG electrodes, this will lead to bad results. Please verify that the sensor locations in the plot are accurate.\n",
      "WARNING:root:Did not find any electrode locations (in the info object), will attempt to use digitization points instead. However, if digitization points do not correspond to the EEG electrodes, this will lead to bad results. Please verify that the sensor locations in the plot are accurate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ./cog_alphabet_letters\\S0_portrait.png\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unsupported file type (.csv). Consider using a dedicated reader function for more options.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 122\u001b[39m\n\u001b[32m    119\u001b[39m segs = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(meta.iloc[idx][\u001b[33m\"\u001b[39m\u001b[33mt_start_s\u001b[39m\u001b[33m\"\u001b[39m].to_numpy(),\n\u001b[32m    120\u001b[39m                 meta.iloc[idx][\u001b[33m\"\u001b[39m\u001b[33mt_end_s\u001b[39m\u001b[33m\"\u001b[39m].to_numpy()))\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# load raw & compute PSD/Topomaps\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m raw = \u001b[43mload_and_prep_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    123\u001b[39m out = psd_for_segments(raw, segs)\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# start figure\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mload_and_prep_raw\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_and_prep_raw\u001b[39m(path):\n\u001b[32m     48\u001b[39m     raw = mne.io.read_raw_edf(path, preload=\u001b[38;5;28;01mTrue\u001b[39;00m, verbose=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m path.lower().endswith(\u001b[33m\"\u001b[39m\u001b[33m.edf\u001b[39m\u001b[33m\"\u001b[39m) \\\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m           \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mmne\u001b[49m\u001b[43m.\u001b[49m\u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_raw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m     \u001b[38;5;66;03m# resample first â†’ safe Nyquist\u001b[39;00m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(raw.info[\u001b[33m\"\u001b[39m\u001b[33msfreq\u001b[39m\u001b[33m\"\u001b[39m] - TARGET_SF) > \u001b[32m1e-6\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\cnt_genome\\.venv\\Lib\\site-packages\\mne\\io\\_read_raw.py:176\u001b[39m, in \u001b[36mread_raw\u001b[39m\u001b[34m(fname, preload, verbose, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m readers = _get_readers()\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m readers:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[43m_read_unsupported\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m these_readers = \u001b[38;5;28mlist\u001b[39m(readers[ext].values())\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m reader \u001b[38;5;129;01min\u001b[39;00m these_readers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\cnt_genome\\.venv\\Lib\\site-packages\\mne\\io\\_read_raw.py:21\u001b[39m, in \u001b[36m_read_unsupported\u001b[39m\u001b[34m(fname, **kwargs)\u001b[39m\n\u001b[32m     19\u001b[39m     msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Try reading a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggest\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m file instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     20\u001b[39m msg += \u001b[33m\"\u001b[39m\u001b[33m Consider using a dedicated reader function for more options.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: Unsupported file type (.csv). Consider using a dedicated reader function for more options."
     ]
    }
   ],
   "source": [
    "# === Letter Portraits: What each letter looks like (spectrum + topomaps + feature signature) ===\n",
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import mne\n",
    "from scipy.signal import welch\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# ---- Paths ----\n",
    "OD = \"./cog_alphabet_hybrid\"   # hybrid run folder\n",
    "BASE = \"./brainwaves\"          # raw files\n",
    "OUT = \"./cog_alphabet_letters\"; os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "# ---- Load hybrid outputs ----\n",
    "feat = pd.read_csv(os.path.join(OD, \"features.csv\"))\n",
    "meta = pd.read_csv(os.path.join(OD, \"metadata.csv\"))\n",
    "asg  = pd.read_csv(os.path.join(OD, \"state_assignments.csv\"))[\"state\"].to_numpy()\n",
    "with open(os.path.join(OD, \"cognitive_alphabet.json\")) as f:\n",
    "    amap = json.load(f)\n",
    "\n",
    "# ---- Bands (match your upgraded config if present) ----\n",
    "BANDS = {\n",
    "    \"theta\": (4.0, 8.0),\n",
    "    \"alpha\": (8.0, 13.0),\n",
    "    \"beta\":  (13.0, 30.0),\n",
    "}\n",
    "# Target processing\n",
    "TARGET_SF = 250.0\n",
    "L_FREQ, H_FREQ = 0.5, 45.0  # for portrait PSD/topomap\n",
    "\n",
    "# ---- Helpers ----\n",
    "def z_signature(feat_df, labels, state):\n",
    "    F = feat_df.copy(); F[\"state\"] = labels\n",
    "    med = F.groupby(\"state\").median(numeric_only=True)\n",
    "    z = (med.loc[state] - feat_df.median()) / (feat_df.std() + 1e-9)\n",
    "    # keep only real-valued columns\n",
    "    z = z[~z.isna()]\n",
    "    # pick top 10 by magnitude, but prefer the JSON â€œtop_featuresâ€ ordering\n",
    "    tops = amap[str(int(state))][\"top_features\"]\n",
    "    tops = [t for t in tops if t in z.index][:8]\n",
    "    if len(tops) < 8:\n",
    "        extra = z.abs().sort_values(ascending=False).index.tolist()\n",
    "        for t in extra:\n",
    "            if t not in tops and len(tops) < 8:\n",
    "                tops.append(t)\n",
    "    return z[tops]\n",
    "\n",
    "def load_and_prep_raw(path):\n",
    "    raw = mne.io.read_raw_edf(path, preload=True, verbose=False) if path.lower().endswith(\".edf\") \\\n",
    "          else mne.io.read_raw(path, preload=True, verbose=False)\n",
    "    # resample first â†’ safe Nyquist\n",
    "    if abs(raw.info[\"sfreq\"] - TARGET_SF) > 1e-6:\n",
    "        raw.resample(TARGET_SF, npad=\"auto\", verbose=False)\n",
    "    ny = raw.info[\"sfreq\"] / 2.0\n",
    "    raw.filter(L_FREQ, min(H_FREQ, ny - 1.0), verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "        try:\n",
    "            raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"),\n",
    "                            match_case=False, match_alias=True, on_missing=\"warn\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return raw\n",
    "\n",
    "def state_repr_file(state):\n",
    "    # choose the file with most epochs in this state\n",
    "    idx = np.where(asg == state)[0]\n",
    "    if len(idx) == 0: return None\n",
    "    subset = meta.iloc[idx]\n",
    "    counts = subset[\"file\"].value_counts()\n",
    "    return counts.index[0]\n",
    "\n",
    "def psd_for_segments(raw, segments):\n",
    "    \"\"\"Return (freqs, mean_psd_over_channels, channel_psd_by_band) for segments list of (t0,t1).\"\"\"\n",
    "    sf = raw.info[\"sfreq\"]\n",
    "    ch_names = raw.ch_names\n",
    "    # accumulate PSDs\n",
    "    psd_list = []\n",
    "    for (t0, t1) in segments:\n",
    "        s = int(np.round(t0 * sf)); e = int(np.round(t1 * sf))\n",
    "        data = raw.get_data(start=s, stop=e)  # (n_ch, n_t)\n",
    "        nper = min(int(sf*2), data.shape[1]); nov = nper//2\n",
    "        freqs, psd = welch(data, fs=sf, nperseg=nper, noverlap=nov, axis=-1, average=\"median\")  # (n_ch, n_f)\n",
    "        psd_list.append(psd)\n",
    "    if not psd_list:\n",
    "        return None, None, None\n",
    "    P = np.stack(psd_list, axis=0)  # (n_seg, n_ch, n_f)\n",
    "    Pm = P.mean(axis=0)             # (n_ch, n_f)\n",
    "    # band powers per channel\n",
    "    band_ch = {}\n",
    "    for name, (lo, hi) in BANDS.items():\n",
    "        idx = np.where((freqs >= lo) & (freqs < hi))[0]\n",
    "        band_ch[name] = Pm[:, idx].sum(axis=1)  # (n_ch,)\n",
    "    # 1D prototype spectrum (mean across channels)\n",
    "    proto = Pm.mean(axis=0)\n",
    "    return freqs, proto, band_ch\n",
    "\n",
    "def plot_topomap(ax, raw, values, title):\n",
    "    ax.set_title(title, fontsize=10, pad=4)\n",
    "    try:\n",
    "        mne.viz.plot_topomap(values, raw.info, axes=ax, show=False)\n",
    "    except Exception:\n",
    "        ax.axis(\"off\"); ax.text(0.5, 0.5, \"topomap unavailable\", ha=\"center\", va=\"center\", fontsize=9)\n",
    "\n",
    "# ---- Build portraits per state ----\n",
    "states = sorted(np.unique(asg))\n",
    "sheet_paths = []\n",
    "\n",
    "for s in states:\n",
    "    # representative file & its segments for this state\n",
    "    rep = state_repr_file(s)\n",
    "    if rep is None: continue\n",
    "    fpath = os.path.join(BASE, rep)\n",
    "    if not os.path.exists(fpath):\n",
    "        # try local dir lookup\n",
    "        alt = os.path.join(BASE, os.path.basename(rep))\n",
    "        if os.path.exists(alt): fpath = alt\n",
    "    # collect segments (t0,t1) for this state in the representative file\n",
    "    idx = np.where((asg == s) & (meta[\"file\"] == rep))[0]\n",
    "    segs = list(zip(meta.iloc[idx][\"t_start_s\"].to_numpy(),\n",
    "                    meta.iloc[idx][\"t_end_s\"].to_numpy()))\n",
    "    # load raw & compute PSD/Topomaps\n",
    "    raw = load_and_prep_raw(fpath)\n",
    "    out = psd_for_segments(raw, segs)\n",
    "    # start figure\n",
    "    fig = plt.figure(figsize=(11, 7))\n",
    "    gs = fig.add_gridspec(2, 3, width_ratios=[1.2, 1, 1], height_ratios=[1, 1])\n",
    "    ax0 = fig.add_subplot(gs[0, 0])  # spectrum\n",
    "    ax1 = fig.add_subplot(gs[0, 1])  # alpha topomap\n",
    "    ax2 = fig.add_subplot(gs[0, 2])  # beta topomap\n",
    "    ax3 = fig.add_subplot(gs[1, :])  # feature bar\n",
    "\n",
    "    # Panel: spectrum\n",
    "    ax0.set_title(f\"State S{s} â€” Prototype Spectrum\", fontsize=12)\n",
    "    if out[0] is not None:\n",
    "        freqs, proto, band_ch = out\n",
    "        ax0.plot(freqs, proto)\n",
    "        ax0.set_xlim(0, 45)\n",
    "        ax0.set_xlabel(\"Hz\"); ax0.set_ylabel(\"Power\")\n",
    "        # guide bands\n",
    "        for name, (lo, hi) in BANDS.items():\n",
    "            ax0.axvspan(lo, hi, alpha=0.07)\n",
    "    else:\n",
    "        ax0.text(0.5, 0.5, \"no PSD (no segments?)\", ha=\"center\", va=\"center\")\n",
    "        band_ch = None\n",
    "\n",
    "    # Panels: topomaps (alpha & beta)\n",
    "    if band_ch is not None:\n",
    "        plot_topomap(ax1, raw, band_ch[\"alpha\"], \"Alpha (8â€“13 Hz)\")\n",
    "        plot_topomap(ax2, raw, band_ch[\"beta\"],  \"Beta (13â€“30 Hz)\")\n",
    "    else:\n",
    "        for ax in (ax1, ax2):\n",
    "            ax.axis(\"off\"); ax.text(0.5, 0.5, \"topomap unavailable\", ha=\"center\", va=\"center\", fontsize=9)\n",
    "\n",
    "    # Panel: feature z-signature bar\n",
    "    z = z_signature(feat, asg, s)\n",
    "    # If fbCSP_task_proba exists but NaN (e.g., not R03), replace with 0 for plotting\n",
    "    z = z.fillna(0.0)\n",
    "    ax3.barh(z.index, z.values)\n",
    "    ax3.set_title(\"Top Feature Signature (z vs. overall)\", fontsize=12)\n",
    "    ax3.invert_yaxis()\n",
    "    ax3.set_xlabel(\"z-score\")\n",
    "\n",
    "    fig.suptitle(f\"Letter Portrait â€” State S{s} ({rep})\", fontsize=14, weight=\"bold\")\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    out_png = os.path.join(OUT, f\"S{s}_portrait.png\")\n",
    "    fig.savefig(out_png, dpi=150)\n",
    "    plt.close(fig)\n",
    "    sheet_paths.append(out_png)\n",
    "    print(f\"Saved {out_png}\")\n",
    "\n",
    "# ---- Build a simple sheet (mosaic of portraits) ----\n",
    "if sheet_paths:\n",
    "    # make a grid (2 per row)\n",
    "    imgs = [Image.open(p) for p in sheet_paths]\n",
    "    W = max(i.width for i in imgs); H = max(i.height for i in imgs)\n",
    "    per_row = 2\n",
    "    rows = (len(imgs) + per_row - 1) // per_row\n",
    "    sheet = Image.new(\"RGB\", (per_row*W, rows*H), (255,255,255))\n",
    "    for k, im in enumerate(imgs):\n",
    "        r = k // per_row; c = k % per_row\n",
    "        sheet.paste(im, (c*W, r*H))\n",
    "    SHEET = os.path.join(OUT, \"CognitiveAlphabet_Letters.png\")\n",
    "    sheet.save(SHEET)\n",
    "    print(\"Saved sheet â†’\", SHEET)\n",
    "else:\n",
    "    print(\"No portraits generated (no states?).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82d58d2a-f1d0-432f-af33-7f2a162b17b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Saved ./cog_alphabet_letters\\S0_portrait.png\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Saved ./cog_alphabet_letters\\S1_portrait.png\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Saved ./cog_alphabet_letters\\S2_portrait.png\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Saved ./cog_alphabet_letters\\S3_portrait.png\n",
      "Saved sheet â†’ ./cog_alphabet_letters\\CognitiveAlphabet_Letters.png\n"
     ]
    }
   ],
   "source": [
    "# === Letter Portraits (robust): supports CSV, prefers real EEG for head maps, fixes montage names ===\n",
    "import os, re, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import mne\n",
    "from scipy.signal import welch\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "OD   = \"./cog_alphabet_hybrid\"     # hybrid run outputs\n",
    "BASE = \"./brainwaves\"              # where raw files live\n",
    "OUT  = \"./cog_alphabet_letters\"; os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "# Load alphabet artifacts\n",
    "feat = pd.read_csv(os.path.join(OD, \"features.csv\"))\n",
    "meta = pd.read_csv(os.path.join(OD, \"metadata.csv\"))\n",
    "asg  = pd.read_csv(os.path.join(OD, \"state_assignments.csv\"))[\"state\"].to_numpy()\n",
    "with open(os.path.join(OD, \"cognitive_alphabet.json\")) as f:\n",
    "    amap = json.load(f)\n",
    "\n",
    "# Portrait config\n",
    "BANDS = {\"theta\": (4.0, 8.0), \"alpha\": (8.0, 13.0), \"beta\": (13.0, 30.0)}\n",
    "TARGET_SF = 250.0\n",
    "L_FREQ, H_FREQ = 0.5, 45.0  # for portrait PSD/topomap (head maps work best â‰¤45 Hz)\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def z_signature(feat_df, labels, state):\n",
    "    F = feat_df.copy(); F[\"state\"] = labels\n",
    "    med = F.groupby(\"state\").median(numeric_only=True)\n",
    "    z = (med.loc[state] - feat_df.median()) / (feat_df.std() + 1e-9)\n",
    "    z = z[~z.isna()]\n",
    "    tops = [t for t in amap[str(int(state))][\"top_features\"] if t in z.index][:8]\n",
    "    if len(tops) < 8:\n",
    "        extra = z.abs().sort_values(ascending=False).index.tolist()\n",
    "        for t in extra:\n",
    "            if t not in tops and len(tops) < 8: tops.append(t)\n",
    "    return z[tops]\n",
    "\n",
    "def _load_csv_tsv(path, sf_guess=TARGET_SF):\n",
    "    df = pd.read_csv(path) if path.lower().endswith(\".csv\") else pd.read_csv(path, sep=\"\\t\")\n",
    "    if \"time\" in df.columns:\n",
    "        t = df[\"time\"].to_numpy(); dt = np.median(np.diff(t)); sf = 1.0/max(dt,1e-12)\n",
    "        ch_names = [c for c in df.columns if c!=\"time\"]; data = df[ch_names].to_numpy().T\n",
    "    else:\n",
    "        sf = sf_guess; ch_names = list(df.columns); data = df[ch_names].to_numpy().T\n",
    "    info = mne.create_info(ch_names=ch_names, sfreq=sf, ch_types=[\"eeg\"]*len(ch_names))\n",
    "    return mne.io.RawArray(data, info, verbose=False)\n",
    "\n",
    "def normalize_chan_names(raw):\n",
    "    \"\"\"Map FP1â†’Fp1, CZâ†’Cz, PZâ†’Pz, etc., and strip punctuation (e.g., 'Cz..'â†’'Cz').\"\"\"\n",
    "    fixes = {\n",
    "        \"FP1\":\"Fp1\",\"FP2\":\"Fp2\",\"FZ\":\"Fz\",\"CZ\":\"Cz\",\"PZ\":\"Pz\",\"OZ\":\"Oz\",\n",
    "        \"POZ\":\"POz\",\"CPZ\":\"CPz\",\"AFZ\":\"AFz\",\"TP7\":\"TP7\",\"TP8\":\"TP8\"\n",
    "    }\n",
    "    def fix(name):\n",
    "        n = re.sub(r'[^A-Za-z0-9]', '', name)  # strip dots etc.\n",
    "        u = n.upper()\n",
    "        if u in fixes: return fixes[u]\n",
    "        # heuristic: trailing Z lower, Fp special-case\n",
    "        if u.startswith(\"FP\"): return \"Fp\" + u[2:]\n",
    "        if u.endswith(\"Z\") and len(u)>1: return u[:-1] + \"z\"\n",
    "        return u  # MNE accepts many uppercase labels (e.g., FC5, CP3)\n",
    "    mne.rename_channels(raw.info, mapping={ch: fix(ch) for ch in raw.ch_names}, allow_duplicates=True)\n",
    "    return raw\n",
    "\n",
    "def load_and_prep_raw_any(path):\n",
    "    p = path.lower()\n",
    "    try:\n",
    "        if p.endswith((\".edf\",\".bdf\")):\n",
    "            raw = mne.io.read_raw_edf(path, preload=True, verbose=False)\n",
    "        elif p.endswith(\".fif\"):\n",
    "            raw = mne.io.read_raw_fif(path, preload=True, verbose=False)\n",
    "        elif p.endswith((\".vhdr\",\".eeg\")):\n",
    "            raw = mne.io.read_raw_brainvision(path, preload=True, verbose=False)\n",
    "        elif p.endswith((\".set\",\".fdt\")):\n",
    "            raw = mne.io.read_raw_eeglab(path, preload=True, verbose=False)\n",
    "        elif p.endswith((\".csv\",\".tsv\")):\n",
    "            raw = _load_csv_tsv(path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported file type\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Cannot load {os.path.basename(path)}: {e}\")\n",
    "\n",
    "    # Resample â†’ safe Nyquist; then band-limit\n",
    "    if abs(raw.info[\"sfreq\"] - TARGET_SF) > 1e-6:\n",
    "        raw.resample(TARGET_SF, npad=\"auto\", verbose=False)\n",
    "    ny = raw.info[\"sfreq\"]/2.0\n",
    "    raw.filter(L_FREQ, min(H_FREQ, ny-1.0), verbose=False)\n",
    "\n",
    "    # Reference + montage (best-effort)\n",
    "    try:\n",
    "        if \"eeg\" in set(raw.get_channel_types()):\n",
    "            raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "            raw = normalize_chan_names(raw)\n",
    "            montage = mne.channels.make_standard_montage(\"standard_1020\")\n",
    "            raw.set_montage(montage, match_case=False, match_alias=True, on_missing=\"ignore\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return raw\n",
    "\n",
    "def psd_for_segments(raw, segments):\n",
    "    sf = raw.info[\"sfreq\"]\n",
    "    psd_list = []\n",
    "    for (t0, t1) in segments:\n",
    "        s = int(round(t0*sf)); e = int(round(t1*sf))\n",
    "        if e <= s: continue\n",
    "        data = raw.get_data(start=s, stop=e)  # (n_ch, n_t)\n",
    "        nper = min(int(sf*2), data.shape[1]); nov = nper//2\n",
    "        freqs, psd = welch(data, fs=sf, nperseg=nper, noverlap=nov, axis=-1, average=\"median\")  # (n_ch, n_f)\n",
    "        psd_list.append(psd)\n",
    "    if not psd_list: return None, None, None\n",
    "    P = np.stack(psd_list, axis=0)        # (n_seg, n_ch, n_f)\n",
    "    Pm = P.mean(axis=0)                   # (n_ch, n_f)\n",
    "    band_ch = {}\n",
    "    for name,(lo,hi) in BANDS.items():\n",
    "        idx = np.where((freqs>=lo)&(freqs<hi))[0]\n",
    "        band_ch[name] = Pm[:, idx].sum(axis=1)\n",
    "    proto = Pm.mean(axis=0)               # average over channels\n",
    "    return freqs, proto, band_ch\n",
    "\n",
    "def plot_topomap(ax, raw, values, title):\n",
    "    ax.set_title(title, fontsize=10, pad=4)\n",
    "    try:\n",
    "        mne.viz.plot_topomap(values, raw.info, axes=ax, show=False)\n",
    "    except Exception:\n",
    "        ax.axis(\"off\"); ax.text(0.5, 0.5, \"topomap unavailable\", ha=\"center\", va=\"center\", fontsize=9)\n",
    "\n",
    "def representative_file_for_state(s):\n",
    "    \"\"\"Prefer true EEG files for topomaps; fall back to any file for spectrum.\"\"\"\n",
    "    idx = np.where(asg == s)[0]\n",
    "    if len(idx)==0: return None\n",
    "    sub = meta.iloc[idx]\n",
    "    # Prefer EDF/BV/FIF/SET over CSV/TSV\n",
    "    pri_ext = (\".edf\",\".bdf\",\".vhdr\",\".eeg\",\".fif\",\".set\",\".fdt\")\n",
    "    counts = sub[\"file\"].value_counts()\n",
    "    # choose best EEG-like\n",
    "    for fname in counts.index:\n",
    "        if fname.lower().endswith(pri_ext):\n",
    "            return fname\n",
    "    # otherwise return the most common (may be CSV)\n",
    "    return counts.index[0]\n",
    "\n",
    "# ---------- Generate portraits ----------\n",
    "states = sorted(np.unique(asg))\n",
    "sheet_paths = []\n",
    "\n",
    "for s in states:\n",
    "    rep = representative_file_for_state(s)\n",
    "    if rep is None: continue\n",
    "    fpath = os.path.join(BASE, rep)\n",
    "    if not os.path.exists(fpath):\n",
    "        # try flat name (if meta carried basename only)\n",
    "        alt = os.path.join(BASE, os.path.basename(rep))\n",
    "        if os.path.exists(alt): fpath = alt\n",
    "    idx = np.where((asg==s) & (meta[\"file\"]==rep))[0]\n",
    "    segs = list(zip(meta.iloc[idx][\"t_start_s\"].to_numpy(),\n",
    "                    meta.iloc[idx][\"t_end_s\"].to_numpy()))\n",
    "\n",
    "    # Build figure\n",
    "    fig = plt.figure(figsize=(11,7))\n",
    "    gs  = fig.add_gridspec(2,3, width_ratios=[1.2,1,1], height_ratios=[1,1])\n",
    "    ax0 = fig.add_subplot(gs[0,0])  # spectrum\n",
    "    ax1 = fig.add_subplot(gs[0,1])  # alpha map\n",
    "    ax2 = fig.add_subplot(gs[0,2])  # beta map\n",
    "    ax3 = fig.add_subplot(gs[1,:])  # feature bar\n",
    "\n",
    "    # Spectrum & topomaps (only if we can load the RAW; topomaps only for EEG formats)\n",
    "    topomaps_ok = rep.lower().endswith((\".edf\",\".bdf\",\".vhdr\",\".eeg\",\".fif\",\".set\",\".fdt\"))\n",
    "    try:\n",
    "        raw = load_and_prep_raw_any(fpath)\n",
    "        freqs, proto, band_ch = psd_for_segments(raw, segs)\n",
    "    except Exception as e:\n",
    "        freqs, proto, band_ch = None, None, None\n",
    "        topomaps_ok = False\n",
    "\n",
    "    # Spectrum\n",
    "    ax0.set_title(f\"State S{s} â€” Prototype Spectrum\", fontsize=12)\n",
    "    if freqs is not None:\n",
    "        ax0.plot(freqs, proto); ax0.set_xlim(0,45)\n",
    "        ax0.set_xlabel(\"Hz\"); ax0.set_ylabel(\"Power\")\n",
    "        for (lo,hi) in BANDS.values(): ax0.axvspan(lo, hi, alpha=0.07)\n",
    "    else:\n",
    "        ax0.text(0.5,0.5,\"no PSD available\", ha=\"center\", va=\"center\")\n",
    "\n",
    "    # Topomaps\n",
    "    if topomaps_ok and band_ch is not None:\n",
    "        plot_topomap(ax1, raw, band_ch[\"alpha\"], \"Alpha (8â€“13 Hz)\")\n",
    "        plot_topomap(ax2, raw, band_ch[\"beta\"],  \"Beta (13â€“30 Hz)\")\n",
    "    else:\n",
    "        for ax in (ax1,ax2):\n",
    "            ax.axis(\"off\"); ax.text(0.5,0.5,\"topomap unavailable\", ha=\"center\", va=\"center\", fontsize=9)\n",
    "\n",
    "    # Feature z-signature bar\n",
    "    z = z_signature(feat, asg, s).fillna(0.0)\n",
    "    ax3.barh(z.index, z.values)\n",
    "    ax3.set_title(\"Top Feature Signature (z vs. overall)\", fontsize=12)\n",
    "    ax3.invert_yaxis(); ax3.set_xlabel(\"z-score\")\n",
    "\n",
    "    fig.suptitle(f\"Letter Portrait â€” State S{s} ({rep})\", fontsize=14, weight=\"bold\")\n",
    "    fig.tight_layout(rect=[0,0,1,0.95])\n",
    "    out_png = os.path.join(OUT, f\"S{s}_portrait.png\")\n",
    "    fig.savefig(out_png, dpi=150); plt.close(fig)\n",
    "    sheet_paths.append(out_png)\n",
    "    print(f\"Saved {out_png}\")\n",
    "\n",
    "# Combine into a sheet\n",
    "if sheet_paths:\n",
    "    imgs = [Image.open(p) for p in sheet_paths]\n",
    "    W = max(i.width for i in imgs); H = max(i.height for i in imgs)\n",
    "    per_row = 2; rows = (len(imgs)+per_row-1)//per_row\n",
    "    sheet = Image.new(\"RGB\", (per_row*W, rows*H), (255,255,255))\n",
    "    for k, im in enumerate(imgs):\n",
    "        r, c = divmod(k, per_row)\n",
    "        sheet.paste(im, (c*W, r*H))\n",
    "    SHEET = os.path.join(OUT, \"CognitiveAlphabet_Letters.png\")\n",
    "    sheet.save(SHEET)\n",
    "    print(\"Saved sheet â†’\", SHEET)\n",
    "else:\n",
    "    print(\"No portraits generated (no states?).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b93412a7-c4c3-486f-820d-8a852cc46f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved ./cog_alphabet_letters/icons\\S0_glyph.svg\n",
      "Saved ./cog_alphabet_letters/icons\\S1_glyph.svg\n",
      "Saved ./cog_alphabet_letters/icons\\S2_glyph.svg\n",
      "Saved ./cog_alphabet_letters/icons\\S3_glyph.svg\n",
      "Saved sprite â†’ ./cog_alphabet_letters/icons\\CognitiveAlphabet_Glyphs.png\n",
      "Icons â†’ ./cog_alphabet_letters/icons\n"
     ]
    }
   ],
   "source": [
    "# === Cognitive Alphabet â†’ Glyph Foundry (SVG icons) ===\n",
    "# Builds a symbolic glyph for each state from your hybrid features.\n",
    "import os, json, math, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "OD   = \"./cog_alphabet_hybrid\"            # hybrid run folder (features + assignments + JSON)\n",
    "OUT  = \"./cog_alphabet_letters/icons\"; os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "# Load hybrid artifacts\n",
    "feat = pd.read_csv(os.path.join(OD, \"features.csv\"))\n",
    "meta = pd.read_csv(os.path.join(OD, \"metadata.csv\"))\n",
    "labels = pd.read_csv(os.path.join(OD, \"state_assignments.csv\"))[\"state\"].to_numpy()\n",
    "with open(os.path.join(OD, \"cognitive_alphabet.json\")) as f:\n",
    "    amap = json.load(f)\n",
    "\n",
    "# Build per-state medians\n",
    "F = feat.copy(); F[\"state\"] = labels\n",
    "med = F.groupby(\"state\").median(numeric_only=True).sort_index()\n",
    "\n",
    "# Helper to normalize columns safely to [0,1] then expand to [0.15, 1.0] for nicer shapes\n",
    "def _norm(x):\n",
    "    x = np.asarray(x, float)\n",
    "    lo, hi = np.nanmin(x), np.nanmax(x)\n",
    "    if not np.isfinite(lo) or hi - lo < 1e-12:\n",
    "        return np.full_like(x, 0.5)\n",
    "    z = (x - lo) / (hi - lo + 1e-12)\n",
    "    return 0.15 + 0.85*z\n",
    "\n",
    "# Composite Î² if split into beta_low/high\n",
    "beta_rel_med = np.zeros(len(med))\n",
    "for c in med.columns:\n",
    "    if c.startswith(\"beta_\") and c.endswith(\"_rel_med\"):\n",
    "        beta_rel_med += med[c].to_numpy()\n",
    "if \"beta_rel_med\" in med.columns:\n",
    "    beta_rel_med = beta_rel_med + med[\"beta_rel_med\"].to_numpy()\n",
    "\n",
    "# Extract core features (fallbacks to zeros if missing)\n",
    "delta = med[\"delta_rel_med\"].to_numpy() if \"delta_rel_med\" in med.columns else np.zeros(len(med))\n",
    "theta = med[\"theta_rel_med\"].to_numpy() if \"theta_rel_med\" in med.columns else np.zeros(len(med))\n",
    "alpha = med[\"alpha_rel_med\"].to_numpy() if \"alpha_rel_med\" in med.columns else np.zeros(len(med))\n",
    "gamma = med[\"gamma_rel_med\"].to_numpy() if \"gamma_rel_med\" in med.columns else np.zeros(len(med))\n",
    "entr  = med[\"spec_entropy_med\"].to_numpy() if \"spec_entropy_med\" in med.columns else np.zeros(len(med))\n",
    "comp  = med[\"hjorth_complexity_med\"].to_numpy() if \"hjorth_complexity_med\" in med.columns else np.zeros(len(med))\n",
    "taskp = med[\"fbCSP_task_proba\"].to_numpy() if \"fbCSP_task_proba\" in med.columns else np.zeros(len(med))\n",
    "\n",
    "# Normalize spokes + ring/center controls\n",
    "spokes = pd.DataFrame({\n",
    "    \"delta\":  _norm(delta),\n",
    "    \"theta\":  _norm(theta),\n",
    "    \"alpha\":  _norm(alpha),\n",
    "    \"beta\":   _norm(beta_rel_med),\n",
    "    \"gamma\":  _norm(gamma),\n",
    "    \"entropy\":_norm(entr),\n",
    "}, index=med.index)\n",
    "comp_n  = _norm(comp)         # ring thickness\n",
    "task_n  = _norm(taskp)        # center dot size\n",
    "\n",
    "angles = np.linspace(0, 2*np.pi, len(spokes.columns), endpoint=False)\n",
    "labels_names = list(spokes.columns)\n",
    "\n",
    "# Minimal style\n",
    "plt.rcParams.update({\"axes.facecolor\":\"none\", \"figure.facecolor\":\"white\"})\n",
    "\n",
    "paths = []\n",
    "for s in med.index:\n",
    "    vals = spokes.loc[s].values\n",
    "    A = np.concatenate([angles, [angles[0]]])\n",
    "    R = np.concatenate([vals,   [vals[0]]])\n",
    "\n",
    "    fig = plt.figure(figsize=(2.6, 2.6))\n",
    "    ax  = plt.subplot(111, polar=True)\n",
    "    ax.plot(A, R, linewidth=1.6)\n",
    "    ax.fill(A, R, alpha=0.25)\n",
    "    ax.set_ylim(0, 1.1)\n",
    "    ax.set_xticks(angles)\n",
    "    ax.set_xticklabels(labels_names, fontsize=7)\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "    # Ring thickness from complexity\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_linewidth(1.0 + 2.5 * comp_n[list(med.index).index(s)])\n",
    "\n",
    "    # Center dot from task probability\n",
    "    ax.scatter([0], [0], s=80 + 260 * task_n[list(med.index).index(s)])\n",
    "\n",
    "    ax.set_title(f\"S{s}\", va=\"bottom\")\n",
    "    svg = os.path.join(OUT, f\"S{s}_glyph.svg\")\n",
    "    png = os.path.join(OUT, f\"S{s}_glyph.png\")\n",
    "    fig.savefig(svg, bbox_inches=\"tight\", transparent=True)\n",
    "    fig.savefig(png, dpi=160, bbox_inches=\"tight\", transparent=True)\n",
    "    plt.close(fig)\n",
    "    paths.append(png)\n",
    "    print(f\"Saved {svg}\")\n",
    "\n",
    "# Make a small sprite sheet for quick viewing\n",
    "if paths:\n",
    "    import matplotlib.image as mpimg\n",
    "    imgs = [mpimg.imread(p) for p in paths]\n",
    "    h, w = imgs[0].shape[:2]\n",
    "    cols = 4\n",
    "    rows = math.ceil(len(imgs)/cols)\n",
    "    sheet = np.ones((rows*h, cols*w, imgs[0].shape[2]))\n",
    "    for i, im in enumerate(imgs):\n",
    "        r, c = divmod(i, cols)\n",
    "        sheet[r*h:(r+1)*h, c*w:(c+1)*w] = im\n",
    "    plt.imsave(os.path.join(OUT, \"CognitiveAlphabet_Glyphs.png\"), sheet)\n",
    "    print(\"Saved sprite â†’\", os.path.join(OUT, \"CognitiveAlphabet_Glyphs.png\"))\n",
    "\n",
    "print(\"Icons â†’\", OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d7b9e28-1317-499d-9b51-511c48693292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: ./cog_alphabet_letters/pack\\Glyph_Legend.png\n",
      "Saved: ./cog_alphabet_letters/pack\\glyph_mapping.json\n",
      "Exported multi-size icons â†’ ./cog_alphabet_letters/pack\n"
     ]
    }
   ],
   "source": [
    "# === Glyph Legend & Export Pack ===\n",
    "import os, json, pandas as pd\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "BASE = \"./cog_alphabet_letters/icons\"\n",
    "OUT  = \"./cog_alphabet_letters/pack\"; os.makedirs(OUT, exist_ok=True)\n",
    "\n",
    "# 1) Tiny legend image (reads the PNG glyphs you just made)\n",
    "pngs = [p for p in os.listdir(BASE) if p.endswith(\"_glyph.png\")]\n",
    "pngs = sorted(pngs, key=lambda x: int(x.split(\"_\")[0][1:]))  # S0_, S1_, ...\n",
    "rows = (len(pngs)+3)//4; cols = 4\n",
    "tile = 220; pad = 24\n",
    "W,H = cols*tile + (cols+1)*pad, rows*tile + (rows+1)*pad + 40\n",
    "legend = Image.new(\"RGBA\", (W,H), (255,255,255,255))\n",
    "draw = ImageDraw.Draw(legend)\n",
    "try:\n",
    "    fnt = ImageFont.truetype(\"arial.ttf\", 18)\n",
    "except:\n",
    "    fnt = ImageFont.load_default()\n",
    "\n",
    "draw.text((pad,10), \"Cognitive Alphabet â€” Glyph Legend\", fill=(0,0,0), font=fnt)\n",
    "\n",
    "for i,name in enumerate(pngs):\n",
    "    r,c = divmod(i, cols)\n",
    "    x = pad + c*(tile+pad); y = 40 + pad + r*(tile+pad)\n",
    "    im = Image.open(os.path.join(BASE,name)).convert(\"RGBA\")\n",
    "    im = im.resize((tile,tile), Image.LANCZOS)\n",
    "    legend.paste(im, (x,y), im)\n",
    "    draw.text((x, y+tile+4), name.replace(\"_glyph.png\",\"\"), fill=(0,0,0), font=fnt)\n",
    "\n",
    "LEGEND = os.path.join(OUT, \"Glyph_Legend.png\"); legend.save(LEGEND)\n",
    "\n",
    "# 2) Mapping JSON (state â†’ svg/png paths + quick meaning)\n",
    "meanings = {\n",
    "    \"S0\":\"Task-aware consonant (sensorimotor Î²; fbCSP-centered)\",\n",
    "    \"S1\":\"Synthetic/control (high entropy/activity)\",\n",
    "    \"S2\":\"Alpha-Dominant (eyes-closed calm)\",\n",
    "    \"S3\":\"Engaged Î¸â€“Î² mix (eyes-open/mixed)\"\n",
    "}\n",
    "pack = []\n",
    "for name in pngs:\n",
    "    sid = name.split(\"_\")[0]\n",
    "    pack.append({\n",
    "        \"state\": sid,\n",
    "        \"png\": f\"{BASE}/{sid}_glyph.png\",\n",
    "        \"svg\": f\"{BASE}/{sid}_glyph.svg\",\n",
    "        \"meaning\": meanings.get(sid, \"\")\n",
    "    })\n",
    "with open(os.path.join(OUT,\"glyph_mapping.json\"), \"w\") as f:\n",
    "    json.dump(pack, f, indent=2)\n",
    "\n",
    "# 3) App icons (multi-size) for each glyph\n",
    "SIZES = [32, 64, 128, 256]\n",
    "for name in pngs:\n",
    "    base = Image.open(os.path.join(BASE,name)).convert(\"RGBA\")\n",
    "    for s in SIZES:\n",
    "        outp = os.path.join(OUT, f\"{name.replace('_glyph.png','')}_{s}.png\")\n",
    "        base.resize((s,s), Image.LANCZOS).save(outp)\n",
    "\n",
    "print(\"Saved:\", LEGEND)\n",
    "print(\"Saved:\", os.path.join(OUT, \"glyph_mapping.json\"))\n",
    "print(\"Exported multi-size icons â†’\", OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "563e69e9-718a-458c-896b-c9b3f47126f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './cog_alphabet_hybrid\\\\features.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m os.makedirs(STAB_DIR, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# ---------- Load artifacts ----------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m feat = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBASE_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfeatures.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m meta = pd.read_csv(os.path.join(BASE_DIR, \u001b[33m\"\u001b[39m\u001b[33mmetadata.csv\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m     22\u001b[39m assign = pd.read_csv(os.path.join(BASE_DIR, \u001b[33m\"\u001b[39m\u001b[33mstate_assignments.csv\u001b[39m\u001b[33m\"\u001b[39m))[\u001b[33m\"\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m\"\u001b[39m].to_numpy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\CNT\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\CNT\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\CNT\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\CNT\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\CNT\\.venv\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './cog_alphabet_hybrid\\\\features.csv'"
     ]
    }
   ],
   "source": [
    "# === CNT Cognitive Alphabet â€” Stability Audit (one cell) ===\n",
    "# Produces: CSVs + \"CNT_CognitiveAlphabet_Stability_Addendum.(png|pdf)\"\n",
    "import os, json, math, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from joblib import load\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score as ARI, normalized_mutual_info_score as NMI, silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# ---------- Paths ----------\n",
    "BASE_DIR = \"./cog_alphabet_hybrid\"\n",
    "OUT_DIR  = \"./cog_alphabet_report\"\n",
    "STAB_DIR = os.path.join(OUT_DIR, \"stability\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(STAB_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- Load artifacts ----------\n",
    "feat = pd.read_csv(os.path.join(BASE_DIR, \"features.csv\"))\n",
    "meta = pd.read_csv(os.path.join(BASE_DIR, \"metadata.csv\"))\n",
    "assign = pd.read_csv(os.path.join(BASE_DIR, \"state_assignments.csv\"))[\"state\"].to_numpy()\n",
    "\n",
    "scaler = load(os.path.join(BASE_DIR, \"scaler_hybrid.joblib\"))\n",
    "pca    = load(os.path.join(BASE_DIR, \"pca_hybrid.joblib\"))\n",
    "km0    = load(os.path.join(BASE_DIR, \"kmeans_hybrid.joblib\"))\n",
    "\n",
    "X  = feat.values\n",
    "Z  = pca.transform(scaler.transform(X))\n",
    "L0 = assign.astype(int)\n",
    "K  = len(np.unique(L0))\n",
    "N  = len(L0)\n",
    "\n",
    "# helper: fit a fresh kmeans and return labels on provided indices\n",
    "def fit_kmeans(Z_fit, Z_pred, k, seed):\n",
    "    km = KMeans(n_clusters=k, n_init=\"auto\", random_state=seed)\n",
    "    km.fit(Z_fit)\n",
    "    return km, km.predict(Z_pred)\n",
    "\n",
    "# ---------- 1) Bootstrap stability ----------\n",
    "np.random.seed(42)\n",
    "B = 100        # number of bootstraps\n",
    "FRAC = 0.80    # subsample fraction\n",
    "\n",
    "boot_rows = []\n",
    "for b in range(B):\n",
    "    idx = np.random.choice(N, size=int(N*FRAC), replace=False)\n",
    "    km_b, Lb = fit_kmeans(Z[idx], Z[idx], K, seed=1000+b)\n",
    "    ari = ARI(L0[idx], Lb)\n",
    "    nmi = NMI(L0[idx], Lb)\n",
    "    try:\n",
    "        sil = silhouette_score(Z[idx], Lb) if K > 1 else np.nan\n",
    "    except Exception:\n",
    "        sil = np.nan\n",
    "    boot_rows.append({\"bootstrap\": b, \"size\": len(idx), \"ari\": ari, \"nmi\": nmi, \"silhouette\": sil})\n",
    "boot_df = pd.DataFrame(boot_rows)\n",
    "boot_df.to_csv(os.path.join(STAB_DIR, \"bootstrap_stability.csv\"), index=False)\n",
    "\n",
    "# ---------- 2) Multi-seed refits on full set ----------\n",
    "SEEDS = list(range(50, 70))\n",
    "ms_rows = []\n",
    "for s in SEEDS:\n",
    "    km_s, Ls = fit_kmeans(Z, Z, K, seed=s)\n",
    "    ari = ARI(L0, Ls); nmi = NMI(L0, Ls)\n",
    "    try:\n",
    "        sil = silhouette_score(Z, Ls) if K > 1 else np.nan\n",
    "    except Exception:\n",
    "        sil = np.nan\n",
    "    ms_rows.append({\"seed\": s, \"ari\": ari, \"nmi\": nmi, \"silhouette\": sil})\n",
    "ms_df = pd.DataFrame(ms_rows)\n",
    "ms_df.to_csv(os.path.join(STAB_DIR, \"multiseed_refit.csv\"), index=False)\n",
    "\n",
    "# ---------- 3) Leave-one-run-out (by file) ----------\n",
    "def is_real_run(fn):\n",
    "    fn = fn.lower()\n",
    "    return fn.endswith((\".edf\", \".vhdr\", \".eeg\", \".fif\", \".set\", \".fdt\"))\n",
    "\n",
    "files = [f for f in meta[\"file\"].unique().tolist() if is_real_run(f)]\n",
    "lor_rows = []\n",
    "\n",
    "def center_alignment_cosine(Ca, Cb):\n",
    "    # returns mean cosine similarity after Hungarian alignment\n",
    "    S = cosine_similarity(Ca, Cb)               # similarity matrix\n",
    "    cost = 1.0 - S                              # minimize (1 - cos)\n",
    "    r, c = linear_sum_assignment(cost)\n",
    "    sims = S[r, c]\n",
    "    return float(np.mean(sims)), sims.tolist(), r.tolist(), c.tolist()\n",
    "\n",
    "for f in files:\n",
    "    test_mask  = (meta[\"file\"] == f).to_numpy()\n",
    "    train_mask = ~test_mask\n",
    "    if train_mask.sum() < K or test_mask.sum() < K:\n",
    "        continue\n",
    "    # fit on train, predict on test\n",
    "    km_tr, Ltr_test = fit_kmeans(Z[train_mask], Z[test_mask], K, seed=2025)\n",
    "    ari = ARI(L0[test_mask], Ltr_test)\n",
    "    # center alignment vs baseline model\n",
    "    mean_sim, sims, rr, cc = center_alignment_cosine(km_tr.cluster_centers_, km0.cluster_centers_)\n",
    "    lor_rows.append({\n",
    "        \"file\": f, \"n_train\": int(train_mask.sum()), \"n_test\": int(test_mask.sum()),\n",
    "        \"ari\": float(ari), \"mean_center_cosine\": mean_sim,\n",
    "        \"center_cosines\": sims, \"row_idx\": rr, \"col_idx\": cc\n",
    "    })\n",
    "lor_df = pd.DataFrame(lor_rows).sort_values(\"file\")\n",
    "lor_df.to_csv(os.path.join(STAB_DIR, \"leave_one_run_out.csv\"), index=False)\n",
    "\n",
    "# ---------- Summaries ----------\n",
    "def pct(a, q): \n",
    "    return float(np.nanpercentile(a, q))\n",
    "boot_ari = boot_df[\"ari\"].to_numpy()\n",
    "boot_sil = boot_df[\"silhouette\"].to_numpy()\n",
    "ms_ari   = ms_df[\"ari\"].to_numpy()\n",
    "\n",
    "summary = {\n",
    "    \"K\": int(K),\n",
    "    \"N_epochs\": int(N),\n",
    "    \"bootstrap_ARI_mean\": float(np.nanmean(boot_ari)),\n",
    "    \"bootstrap_ARI_p10\":  pct(boot_ari, 10),\n",
    "    \"bootstrap_ARI_p50\":  pct(boot_ari, 50),\n",
    "    \"bootstrap_ARI_p90\":  pct(boot_ari, 90),\n",
    "    \"bootstrap_Sil_mean\": float(np.nanmean(boot_sil)),\n",
    "    \"multiseed_ARI_mean\": float(np.nanmean(ms_ari)),\n",
    "    \"multiseed_ARI_min\":  float(np.nanmin(ms_ari)),\n",
    "    \"multiseed_ARI_max\":  float(np.nanmax(ms_ari)),\n",
    "    \"LOR_rows\": int(len(lor_df))\n",
    "}\n",
    "with open(os.path.join(STAB_DIR, \"summary.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"=== Stability Summary ===\")\n",
    "for k,v in summary.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "# ---------- Addendum Figure ----------\n",
    "fig = plt.figure(figsize=(12, 8), dpi=130)\n",
    "gs  = fig.add_gridspec(2, 2, height_ratios=[1,1], width_ratios=[1,1], hspace=0.35, wspace=0.30)\n",
    "\n",
    "# (A) Bootstrap ARI distribution\n",
    "axA = fig.add_subplot(gs[0,0])\n",
    "axA.hist(boot_df[\"ari\"], bins=20, alpha=0.85)\n",
    "axA.set_title(\"Bootstrap Stability â€” Adjusted Rand Index (80% subsamples)\")\n",
    "axA.set_xlabel(\"ARI\"); axA.set_ylabel(\"Count\")\n",
    "m, p10, p50, p90 = summary[\"bootstrap_ARI_mean\"], summary[\"bootstrap_ARI_p10\"], summary[\"bootstrap_ARI_p50\"], summary[\"bootstrap_ARI_p90\"]\n",
    "axA.axvline(m, color=\"k\", linestyle=\"--\", label=f\"mean={m:.3f}\")\n",
    "axA.axvline(p50, color=\"gray\", linestyle=\":\", label=f\"median={p50:.3f}\")\n",
    "axA.legend()\n",
    "\n",
    "# (B) Bootstrap Silhouette\n",
    "axB = fig.add_subplot(gs[0,1])\n",
    "axB.hist(boot_df[\"silhouette\"].dropna(), bins=20, alpha=0.85, color=\"#8888ff\")\n",
    "axB.set_title(\"Bootstrap Silhouette (labels refit per sample)\")\n",
    "axB.set_xlabel(\"Silhouette\"); axB.set_ylabel(\"Count\")\n",
    "\n",
    "# (C) Multi-seed ARI vs baseline\n",
    "axC = fig.add_subplot(gs[1,0])\n",
    "axC.plot(ms_df[\"seed\"], ms_df[\"ari\"], \"o-\")\n",
    "axC.set_title(\"Multi-seed Refit â€” ARI vs Baseline Labels\")\n",
    "axC.set_xlabel(\"Random Seed\"); axC.set_ylabel(\"ARI\")\n",
    "axC.set_ylim(0,1)\n",
    "\n",
    "# (D) Leave-one-run-out â€” ARI & center alignment\n",
    "axD = fig.add_subplot(gs[1,1])\n",
    "if len(lor_df):\n",
    "    x = np.arange(len(lor_df))\n",
    "    axD.bar(x - 0.18, lor_df[\"ari\"].to_numpy(), width=0.36, label=\"ARI (held-out)\")\n",
    "    axD.bar(x + 0.18, lor_df[\"mean_center_cosine\"].to_numpy(), width=0.36, label=\"Center cosine\")\n",
    "    axD.set_xticks(x); axD.set_xticklabels([os.path.splitext(f)[0] for f in lor_df[\"file\"]], rotation=20)\n",
    "    axD.set_ylim(0,1)\n",
    "    axD.set_title(\"Leave-one-run-out (by file)\")\n",
    "    axD.legend()\n",
    "else:\n",
    "    axD.text(0.5, 0.5, \"No real runs to cross-validate.\", ha=\"center\", va=\"center\")\n",
    "    axD.axis(\"off\")\n",
    "\n",
    "# Header strip\n",
    "fig.suptitle(\"CNT Cognitive Alphabet â€” Stability Addendum\", fontsize=16, weight=\"bold\")\n",
    "txt = f\"K={K} | N={N} | Boot ARI mean={m:.3f} (p10={p10:.3f}, p50={p50:.3f}, p90={p90:.3f}) | Multi-seed ARI mean={summary['multiseed_ARI_mean']:.3f}\"\n",
    "fig.text(0.5, 0.94, txt, ha=\"center\", fontsize=10)\n",
    "\n",
    "ADD_PNG = os.path.join(OUT_DIR, \"CNT_CognitiveAlphabet_Stability_Addendum.png\")\n",
    "ADD_PDF = os.path.join(OUT_DIR, \"CNT_CognitiveAlphabet_Stability_Addendum.pdf\")\n",
    "fig.savefig(ADD_PNG, bbox_inches=\"tight\")\n",
    "fig.savefig(ADD_PDF, bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "print(\"\\nSaved addendum â†’\", ADD_PNG)\n",
    "print(\"Saved addendum â†’\", ADD_PDF)\n",
    "print(\"CSV outputs â†’\", STAB_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20065068-90db-4626-a527-96292971506a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Could not find a run folder with features.csv / metadata.csv / state_assignments.csv.\nTip: run the pipeline first, or ensure ./cog_alphabet_hybrid exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     35\u001b[39m BASE_DIR, how = find_run_dir()\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m BASE_DIR \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m     38\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not find a run folder with features.csv / metadata.csv / state_assignments.csv.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     39\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTip: run the pipeline first, or ensure ./cog_alphabet_hybrid exists.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     40\u001b[39m     )\n\u001b[32m     42\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[stability] Using run dir: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  (found via \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhow\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# ---------- 1) Load artifacts ----------\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Could not find a run folder with features.csv / metadata.csv / state_assignments.csv.\nTip: run the pipeline first, or ensure ./cog_alphabet_hybrid exists."
     ]
    }
   ],
   "source": [
    "# === CNT Cognitive Alphabet â€” Stability Audit (auto-locate & run) ===\n",
    "import os, json, glob, time, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score as ARI, normalized_mutual_info_score as NMI, silhouette_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# ---------- 0) Find a run folder with required files ----------\n",
    "def find_run_dir():\n",
    "    candidates = [\n",
    "        \"./cog_alphabet_hybrid\",\n",
    "        \"./cog_alphabet_physio_betaSplit\",\n",
    "        \"./cog_alphabet_physio\",\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if all(os.path.exists(os.path.join(c, f)) for f in [\"features.csv\",\"metadata.csv\",\"state_assignments.csv\"]):\n",
    "            return os.path.abspath(c), \"candidate\"\n",
    "    # recursive search (skip common noisy dirs)\n",
    "    hits = []\n",
    "    for path in glob.glob(\"./**/features.csv\", recursive=True):\n",
    "        d = os.path.dirname(path)\n",
    "        if any(x in d.lower() for x in [\".venv\",\"env\",\"site-packages\",\"AppData\".lower()]):\n",
    "            continue\n",
    "        if all(os.path.exists(os.path.join(d, f)) for f in [\"metadata.csv\",\"state_assignments.csv\"]):\n",
    "            mtime = os.path.getmtime(path)\n",
    "            hits.append((mtime, os.path.abspath(d)))\n",
    "    if hits:\n",
    "        hits.sort(reverse=True)\n",
    "        return hits[0][1], \"search\"\n",
    "    return None, None\n",
    "\n",
    "BASE_DIR, how = find_run_dir()\n",
    "if BASE_DIR is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find a run folder with features.csv / metadata.csv / state_assignments.csv.\\n\"\n",
    "        \"Tip: run the pipeline first, or ensure ./cog_alphabet_hybrid exists.\"\n",
    "    )\n",
    "\n",
    "print(f\"[stability] Using run dir: {BASE_DIR}  (found via {how})\")\n",
    "\n",
    "# ---------- 1) Load artifacts ----------\n",
    "feat = pd.read_csv(os.path.join(BASE_DIR, \"features.csv\"))\n",
    "meta = pd.read_csv(os.path.join(BASE_DIR, \"metadata.csv\"))\n",
    "assign = pd.read_csv(os.path.join(BASE_DIR, \"state_assignments.csv\"))[\"state\"].to_numpy()\n",
    "K = int(len(np.unique(assign))); N = len(assign)\n",
    "\n",
    "# Try to load saved models; if missing, fall back to fresh scaler/PCA fit\n",
    "HAS_MODELS = all(os.path.exists(os.path.join(BASE_DIR, nm)) for nm in [\n",
    "    \"scaler_hybrid.joblib\",\"pca_hybrid.joblib\",\"kmeans_hybrid.joblib\"\n",
    "])\n",
    "if HAS_MODELS:\n",
    "    from joblib import load\n",
    "    scaler = load(os.path.join(BASE_DIR, \"scaler_hybrid.joblib\"))\n",
    "    pca    = load(os.path.join(BASE_DIR, \"pca_hybrid.joblib\"))\n",
    "    km0    = load(os.path.join(BASE_DIR, \"kmeans_hybrid.joblib\"))\n",
    "    Z      = pca.transform(scaler.transform(feat.values))\n",
    "    print(\"[stability] Loaded saved scaler/pca/kmeans.\")\n",
    "else:\n",
    "    print(\"[stability] Saved models not found â€” fitting fresh scaler/PCA (center alignment will be skipped).\")\n",
    "    scaler = StandardScaler().fit(feat.values)\n",
    "    Z      = scaler.transform(feat.values)\n",
    "    pca    = PCA(n_components=min(20, Z.shape[1]), random_state=42).fit(Z)\n",
    "    Z      = pca.transform(Z)\n",
    "    km0    = None  # skip center-alignment if baseline centers unavailable\n",
    "\n",
    "L0 = assign.astype(int)\n",
    "\n",
    "def fit_kmeans(Z_fit, Z_pred, k, seed):\n",
    "    km = KMeans(n_clusters=k, n_init=\"auto\", random_state=seed)\n",
    "    km.fit(Z_fit)\n",
    "    return km, km.predict(Z_pred)\n",
    "\n",
    "# ---------- 2) Bootstrap stability (80% subsamples) ----------\n",
    "np.random.seed(42)\n",
    "B = 100\n",
    "FRAC = 0.80\n",
    "boot_rows = []\n",
    "for b in range(B):\n",
    "    idx = np.random.choice(N, size=int(N*FRAC), replace=False)\n",
    "    km_b, Lb = fit_kmeans(Z[idx], Z[idx], K, seed=1000+b)\n",
    "    ari = ARI(L0[idx], Lb)\n",
    "    nmi = NMI(L0[idx], Lb)\n",
    "    try:\n",
    "        sil = silhouette_score(Z[idx], Lb) if K>1 else np.nan\n",
    "    except Exception:\n",
    "        sil = np.nan\n",
    "    boot_rows.append({\"bootstrap\": b, \"size\": len(idx), \"ari\": ari, \"nmi\": nmi, \"silhouette\": sil})\n",
    "boot_df = pd.DataFrame(boot_rows)\n",
    "\n",
    "# ---------- 3) Multi-seed refits (full set) ----------\n",
    "SEEDS = list(range(50, 70))\n",
    "ms_rows = []\n",
    "for s in SEEDS:\n",
    "    km_s, Ls = fit_kmeans(Z, Z, K, seed=s)\n",
    "    ari = ARI(L0, Ls); nmi = NMI(L0, Ls)\n",
    "    try:\n",
    "        sil = silhouette_score(Z, Ls) if K>1 else np.nan\n",
    "    except Exception:\n",
    "        sil = np.nan\n",
    "    ms_rows.append({\"seed\": s, \"ari\": ari, \"nmi\": nmi, \"silhouette\": sil})\n",
    "ms_df = pd.DataFrame(ms_rows)\n",
    "\n",
    "# ---------- 4) Leave-one-run-out (by 'file') ----------\n",
    "def is_real_run(fn):\n",
    "    fn = (fn or \"\").lower()\n",
    "    return fn.endswith((\".edf\",\".vhdr\",\".eeg\",\".fif\",\".set\",\".fdt\"))\n",
    "\n",
    "files = [f for f in meta[\"file\"].unique().tolist() if is_real_run(f)]\n",
    "lor_rows = []\n",
    "\n",
    "def center_alignment_cosine(Ca, Cb):\n",
    "    S = cosine_similarity(Ca, Cb)\n",
    "    cost = 1.0 - S\n",
    "    r, c = linear_sum_assignment(cost)\n",
    "    return float(np.mean(S[r, c])), S[r, c].tolist(), r.tolist(), c.tolist()\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # after function to avoid NameError\n",
    "\n",
    "for f in files:\n",
    "    test_mask  = (meta[\"file\"] == f).to_numpy()\n",
    "    train_mask = ~test_mask\n",
    "    if train_mask.sum() < K or test_mask.sum() < K:\n",
    "        continue\n",
    "    km_tr, Ltr_test = fit_kmeans(Z[train_mask], Z[test_mask], K, seed=2025)\n",
    "    ari = ARI(L0[test_mask], Ltr_test)\n",
    "    if HAS_MODELS:\n",
    "        mean_sim, sims, rr, cc = center_alignment_cosine(km_tr.cluster_centers_, km0.cluster_centers_)\n",
    "    else:\n",
    "        mean_sim, sims, rr, cc = np.nan, [], [], []\n",
    "    lor_rows.append({\n",
    "        \"file\": f, \"n_train\": int(train_mask.sum()), \"n_test\": int(test_mask.sum()),\n",
    "        \"ari\": float(ari), \"mean_center_cosine\": mean_sim,\n",
    "        \"center_cosines\": sims, \"row_idx\": rr, \"col_idx\": cc\n",
    "    })\n",
    "lor_df = pd.DataFrame(lor_rows).sort_values(\"file\")\n",
    "\n",
    "# ---------- 5) Summaries ----------\n",
    "def pct(a, q): return float(np.nanpercentile(a, q))\n",
    "boot_ari = boot_df[\"ari\"].to_numpy()\n",
    "boot_sil = boot_df[\"silhouette\"].to_numpy()\n",
    "ms_ari   = ms_df[\"ari\"].to_numpy()\n",
    "summary = {\n",
    "    \"used_dir\": BASE_DIR,\n",
    "    \"K\": int(K),\n",
    "    \"N_epochs\": int(N),\n",
    "    \"bootstrap_ARI_mean\": float(np.nanmean(boot_ari)),\n",
    "    \"bootstrap_ARI_p10\":  pct(boot_ari, 10),\n",
    "    \"bootstrap_ARI_p50\":  pct(boot_ari, 50),\n",
    "    \"bootstrap_ARI_p90\":  pct(boot_ari, 90),\n",
    "    \"bootstrap_Sil_mean\": float(np.nanmean(boot_sil)),\n",
    "    \"multiseed_ARI_mean\": float(np.nanmean(ms_ari)),\n",
    "    \"multiseed_ARI_min\":  float(np.nanmin(ms_ari)),\n",
    "    \"multiseed_ARI_max\":  float(np.nanmax(ms_ari)),\n",
    "    \"LOR_rows\": int(len(lor_df))\n",
    "}\n",
    "print(\"=== Stability Summary ===\")\n",
    "for k,v in summary.items(): print(f\"{k}: {v}\")\n",
    "\n",
    "# ---------- 6) Save CSVs ----------\n",
    "OUT_DIR = \"./cog_alphabet_report\"\n",
    "STAB_DIR = os.path.join(OUT_DIR, \"stability\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(STAB_DIR, exist_ok=True)\n",
    "\n",
    "boot_df.to_csv(os.path.join(STAB_DIR, \"bootstrap_stability.csv\"), index=False)\n",
    "ms_df.to_csv(os.path.join(STAB_DIR, \"multiseed_refit.csv\"), index=False)\n",
    "lor_df.to_csv(os.path.join(STAB_DIR, \"leave_one_run_out.csv\"), index=False)\n",
    "with open(os.path.join(STAB_DIR, \"summary.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "# ---------- 7) Addendum figure ----------\n",
    "fig = plt.figure(figsize=(12, 8), dpi=130)\n",
    "gs  = fig.add_gridspec(2, 2, height_ratios=[1,1], width_ratios=[1,1], hspace=0.35, wspace=0.30)\n",
    "\n",
    "# (A) Bootstrap ARI\n",
    "axA = fig.add_subplot(gs[0,0])\n",
    "axA.hist(boot_df[\"ari\"], bins=20, alpha=0.85)\n",
    "axA.set_title(\"Bootstrap Stability â€” Adjusted Rand Index (80% subsamples)\")\n",
    "axA.set_xlabel(\"ARI\"); axA.set_ylabel(\"Count\")\n",
    "m, p10, p50, p90 = summary[\"bootstrap_ARI_mean\"], summary[\"bootstrap_ARI_p10\"], summary[\"bootstrap_ARI_p50\"], summary[\"bootstrap_ARI_p90\"]\n",
    "axA.axvline(m, color=\"k\", linestyle=\"--\", label=f\"mean={m:.3f}\")\n",
    "axA.axvline(p50, color=\"gray\", linestyle=\":\", label=f\"median={p50:.3f}\")\n",
    "axA.legend()\n",
    "\n",
    "# (B) Bootstrap Silhouette\n",
    "axB = fig.add_subplot(gs[0,1])\n",
    "axB.hist(boot_df[\"silhouette\"].dropna(), bins=20, alpha=0.85, color=\"#8888ff\")\n",
    "axB.set_title(\"Bootstrap Silhouette (labels refit per sample)\")\n",
    "axB.set_xlabel(\"Silhouette\"); axB.set_ylabel(\"Count\")\n",
    "\n",
    "# (C) Multi-seed ARI\n",
    "axC = fig.add_subplot(gs[1,0])\n",
    "axC.plot(ms_df[\"seed\"], ms_df[\"ari\"], \"o-\")\n",
    "axC.set_title(\"Multi-seed Refit â€” ARI vs Baseline Labels\")\n",
    "axC.set_xlabel(\"Random Seed\"); axC.set_ylabel(\"ARI\"); axC.set_ylim(0,1)\n",
    "\n",
    "# (D) Leave-one-run-out\n",
    "axD = fig.add_subplot(gs[1,1])\n",
    "if len(lor_df):\n",
    "    x = np.arange(len(lor_df))\n",
    "    axD.bar(x - 0.18, lor_df[\"ari\"].to_numpy(), width=0.36, label=\"ARI (held-out)\")\n",
    "    if HAS_MODELS:\n",
    "        axD.bar(x + 0.18, lor_df[\"mean_center_cosine\"].to_numpy(), width=0.36, label=\"Center cosine\")\n",
    "    axD.set_xticks(x); axD.set_xticklabels([os.path.splitext(f)[0] for f in lor_df[\"file\"]], rotation=20)\n",
    "    axD.set_ylim(0,1); axD.set_title(\"Leave-one-run-out (by file)\"); axD.legend()\n",
    "else:\n",
    "    axD.text(0.5, 0.5, \"No real runs to cross-validate.\", ha=\"center\", va=\"center\"); axD.axis(\"off\")\n",
    "\n",
    "fig.suptitle(\"CNT Cognitive Alphabet â€” Stability Addendum\", fontsize=16, weight=\"bold\")\n",
    "hdr = f\"K={K} | N={N} | Boot ARI mean={m:.3f} (p10={p10:.3f}, p50={p50:.3f}, p90={p90:.3f}) | Multi-seed ARI mean={summary['multiseed_ARI_mean']:.3f}\"\n",
    "fig.text(0.5, 0.94, hdr + f\" | dir: {os.path.basename(BASE_DIR)}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "ADD_PNG = os.path.join(OUT_DIR, \"CNT_CognitiveAlphabet_Stability_Addendum.png\")\n",
    "ADD_PDF = os.path.join(OUT_DIR, \"CNT_CognitiveAlphabet_Stability_Addendum.pdf\")\n",
    "fig.savefig(ADD_PNG, bbox_inches=\"tight\"); fig.savefig(ADD_PDF, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "print(\"\\nSaved addendum â†’\", ADD_PNG)\n",
    "print(\"Saved addendum â†’\", ADD_PDF)\n",
    "print(\"CSV outputs â†’\", STAB_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2075135-46b5-4189-aa2a-16e179a627ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Could not find a run folder with features.csv / metadata.csv / state_assignments.csv\nSearch paths included current dir+parents, E:\\CNT, C:\\Users\\caleb\\CNT_Lab, and HOME.\nTip: run your pipeline (hybrid or physio) first, or tell me where your run folder lives.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     62\u001b[39m BASE_DIR = _freshest_run_dir(run_dirs)\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m BASE_DIR \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[32m     65\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCould not find a run folder with features.csv / metadata.csv / state_assignments.csv\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     66\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mSearch paths included current dir+parents, E:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mCNT, C:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mcaleb\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mCNT_Lab, and HOME.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     67\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTip: run your pipeline (hybrid or physio) first, or tell me where your run folder lives.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     68\u001b[39m     )\n\u001b[32m     69\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[stability] Using run dir: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBASE_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# ---- Load artifacts\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Could not find a run folder with features.csv / metadata.csv / state_assignments.csv\nSearch paths included current dir+parents, E:\\CNT, C:\\Users\\caleb\\CNT_Lab, and HOME.\nTip: run your pipeline (hybrid or physio) first, or tell me where your run folder lives."
     ]
    }
   ],
   "source": [
    "# === CNT Cognitive Alphabet â€” Stability Audit (wide search, auto-run) ===\n",
    "import os, sys, glob, json, time, pathlib, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score as ARI, normalized_mutual_info_score as NMI, silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "def _valid_run_dir(d):\n",
    "    need = [\"features.csv\",\"metadata.csv\",\"state_assignments.csv\"]\n",
    "    return all(os.path.exists(os.path.join(d, f)) for f in need)\n",
    "\n",
    "def _freshest_run_dir(candidates):\n",
    "    hits = []\n",
    "    for d in candidates:\n",
    "        try:\n",
    "            m = os.path.getmtime(os.path.join(d, \"features.csv\"))\n",
    "            hits.append((m, os.path.abspath(d)))\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not hits: return None\n",
    "    hits.sort(reverse=True)\n",
    "    return hits[0][1]\n",
    "\n",
    "def _search_run_dirs():\n",
    "    roots = set()\n",
    "\n",
    "    # current dir + parents\n",
    "    p = pathlib.Path.cwd()\n",
    "    for q in [p] + list(p.parents):\n",
    "        roots.add(str(q))\n",
    "\n",
    "    # common roots we've used in this project\n",
    "    for guess in [r\"E:\\CNT\", r\"C:\\Users\\caleb\\CNT_Lab\", str(pathlib.Path.home())]:\n",
    "        if os.path.exists(guess):\n",
    "            roots.add(guess)\n",
    "\n",
    "    candidates = set()\n",
    "    for base in list(roots):\n",
    "        try:\n",
    "            # Fast checks for canonical folder names first\n",
    "            for name in [\"cog_alphabet_hybrid\", \"cog_alphabet_physio_betaSplit\", \"cog_alphabet_physio\"]:\n",
    "                d = os.path.join(base, name)\n",
    "                if _valid_run_dir(d):\n",
    "                    candidates.add(d)\n",
    "            # Wider search but pruned\n",
    "            for path in glob.iglob(os.path.join(base, \"**\", \"features.csv\"), recursive=True):\n",
    "                d = os.path.dirname(path)\n",
    "                low = d.lower()\n",
    "                if any(bad in low for bad in [\".venv\", \"site-packages\", \"appdata\", \"miniconda\", \"anaconda3\"]):\n",
    "                    continue\n",
    "                if _valid_run_dir(d):\n",
    "                    candidates.add(d)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return list(candidates)\n",
    "\n",
    "# ---- Locate run dir\n",
    "run_dirs = _search_run_dirs()\n",
    "BASE_DIR = _freshest_run_dir(run_dirs)\n",
    "if BASE_DIR is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find a run folder with features.csv / metadata.csv / state_assignments.csv\\n\"\n",
    "        \"Search paths included current dir+parents, E:\\\\CNT, C:\\\\Users\\\\caleb\\\\CNT_Lab, and HOME.\\n\"\n",
    "        \"Tip: run your pipeline (hybrid or physio) first, or tell me where your run folder lives.\"\n",
    "    )\n",
    "print(f\"[stability] Using run dir: {BASE_DIR}\")\n",
    "\n",
    "# ---- Load artifacts\n",
    "feat = pd.read_csv(os.path.join(BASE_DIR, \"features.csv\"))\n",
    "meta = pd.read_csv(os.path.join(BASE_DIR, \"metadata.csv\"))\n",
    "assign = pd.read_csv(os.path.join(BASE_DIR, \"state_assignments.csv\"))[\"state\"].to_numpy()\n",
    "K = int(len(np.unique(assign))); N = len(assign)\n",
    "\n",
    "# Try to load saved models; else fit fresh scaler/PCA (center-alignment skipped)\n",
    "HAS_MODELS = all(os.path.exists(os.path.join(BASE_DIR, nm)) for nm in\n",
    "                 [\"scaler_hybrid.joblib\",\"pca_hybrid.joblib\",\"kmeans_hybrid.joblib\"])\n",
    "if HAS_MODELS:\n",
    "    from joblib import load\n",
    "    scaler = load(os.path.join(BASE_DIR, \"scaler_hybrid.joblib\"))\n",
    "    pca    = load(os.path.join(BASE_DIR,  \"pca_hybrid.joblib\"))\n",
    "    km0    = load(os.path.join(BASE_DIR,  \"kmeans_hybrid.joblib\"))\n",
    "    Z      = pca.transform(scaler.transform(feat.values))\n",
    "    print(\"[stability] Loaded saved models.\")\n",
    "else:\n",
    "    print(\"[stability] Models not found here; fitting fresh scaler/PCA (center-alignment will be skipped).\")\n",
    "    scaler = StandardScaler().fit(feat.values)\n",
    "    Z      = scaler.transform(feat.values)\n",
    "    pca    = PCA(n_components=min(20, Z.shape[1]), random_state=42).fit(Z)\n",
    "    Z      = pca.transform(Z)\n",
    "    km0    = None\n",
    "\n",
    "L0 = assign.astype(int)\n",
    "\n",
    "def fit_kmeans(Z_fit, Z_pred, k, seed):\n",
    "    km = KMeans(n_clusters=k, n_init=\"auto\", random_state=seed)\n",
    "    km.fit(Z_fit)\n",
    "    return km, km.predict(Z_pred)\n",
    "\n",
    "# ---- 1) Bootstrap stability (80% subsamples)\n",
    "np.random.seed(42)\n",
    "B = 100\n",
    "FRAC = 0.80\n",
    "boot_rows = []\n",
    "for b in range(B):\n",
    "    idx = np.random.choice(N, size=int(N*FRAC), replace=False)\n",
    "    km_b, Lb = fit_kmeans(Z[idx], Z[idx], K, seed=1000+b)\n",
    "    ari = ARI(L0[idx], Lb)\n",
    "    nmi = NMI(L0[idx], Lb)\n",
    "    try:\n",
    "        sil = silhouette_score(Z[idx], Lb) if K>1 else np.nan\n",
    "    except Exception:\n",
    "        sil = np.nan\n",
    "    boot_rows.append({\"bootstrap\": b, \"size\": len(idx), \"ari\": ari, \"nmi\": nmi, \"silhouette\": sil})\n",
    "boot_df = pd.DataFrame(boot_rows)\n",
    "\n",
    "# ---- 2) Multi-seed refits (full set)\n",
    "SEEDS = list(range(50, 70))\n",
    "ms_rows = []\n",
    "for s in SEEDS:\n",
    "    km_s, Ls = fit_kmeans(Z, Z, K, seed=s)\n",
    "    ari = ARI(L0, Ls); nmi = NMI(L0, Ls)\n",
    "    try:\n",
    "        sil = silhouette_score(Z, Ls) if K>1 else np.nan\n",
    "    except Exception:\n",
    "        sil = np.nan\n",
    "    ms_rows.append({\"seed\": s, \"ari\": ari, \"nmi\": nmi, \"silhouette\": sil})\n",
    "ms_df = pd.DataFrame(ms_rows)\n",
    "\n",
    "# ---- 3) Leave-one-run-out by file (real recordings only)\n",
    "def _is_real(fn):\n",
    "    fn = (fn or \"\").lower()\n",
    "    return fn.endswith((\".edf\",\".vhdr\",\".eeg\",\".fif\",\".set\",\".fdt\"))\n",
    "\n",
    "files = [f for f in meta[\"file\"].unique().tolist() if _is_real(f)]\n",
    "lor_rows = []\n",
    "\n",
    "def center_alignment_cosine(Ca, Cb):\n",
    "    S = cosine_similarity(Ca, Cb)\n",
    "    r, c = linear_sum_assignment(1.0 - S)\n",
    "    return float(np.mean(S[r, c])), S[r, c].tolist(), r.tolist(), c.tolist()\n",
    "\n",
    "for f in files:\n",
    "    test = (meta[\"file\"] == f).to_numpy()\n",
    "    train = ~test\n",
    "    if train.sum() < K or test.sum() < K: \n",
    "        continue\n",
    "    km_tr, Ltr_test = fit_kmeans(Z[train], Z[test], K, seed=2025)\n",
    "    ari = ARI(L0[test], Ltr_test)\n",
    "    if km0 is not None:\n",
    "        mean_sim, sims, rr, cc = center_alignment_cosine(km_tr.cluster_centers_, km0.cluster_centers_)\n",
    "    else:\n",
    "        mean_sim, sims, rr, cc = np.nan, [], [], []\n",
    "    lor_rows.append({\n",
    "        \"file\": f, \"n_train\": int(train.sum()), \"n_test\": int(test.sum()),\n",
    "        \"ari\": float(ari), \"mean_center_cosine\": mean_sim,\n",
    "        \"center_cosines\": sims, \"row_idx\": rr, \"col_idx\": cc\n",
    "    })\n",
    "lor_df = pd.DataFrame(lor_rows).sort_values(\"file\")\n",
    "\n",
    "# ---- 4) Summaries\n",
    "def pct(a, q): return float(np.nanpercentile(a, q))\n",
    "boot_ari = boot_df[\"ari\"].to_numpy()\n",
    "boot_sil = boot_df[\"silhouette\"].to_numpy()\n",
    "ms_ari   = ms_df[\"ari\"].to_numpy()\n",
    "\n",
    "summary = {\n",
    "    \"used_dir\": BASE_DIR,\n",
    "    \"K\": int(K),\n",
    "    \"N_epochs\": int(N),\n",
    "    \"bootstrap_ARI_mean\": float(np.nanmean(boot_ari)),\n",
    "    \"bootstrap_ARI_p10\":  pct(boot_ari, 10),\n",
    "    \"bootstrap_ARI_p50\":  pct(boot_ari, 50),\n",
    "    \"bootstrap_ARI_p90\":  pct(boot_ari, 90),\n",
    "    \"bootstrap_Sil_mean\": float(np.nanmean(boot_sil)),\n",
    "    \"multiseed_ARI_mean\": float(np.nanmean(ms_ari)),\n",
    "    \"multiseed_ARI_min\":  float(np.nanmin(ms_ari)),\n",
    "    \"multiseed_ARI_max\":  float(np.nanmax(ms_ari)),\n",
    "    \"LOR_rows\": int(len(lor_df))\n",
    "}\n",
    "print(\"=== Stability Summary ===\")\n",
    "for k,v in summary.items(): print(f\"{k}: {v}\")\n",
    "\n",
    "# ---- 5) Save CSVs\n",
    "OUT_DIR = \"./cog_alphabet_report\"\n",
    "STAB_DIR = os.path.join(OUT_DIR, \"stability\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True); os.makedirs(STAB_DIR, exist_ok=True)\n",
    "boot_df.to_csv(os.path.join(STAB_DIR, \"bootstrap_stability.csv\"), index=False)\n",
    "ms_df.to_csv(os.path.join(STAB_DIR, \"multiseed_refit.csv\"), index=False)\n",
    "lor_df.to_csv(os.path.join(STAB_DIR, \"leave_one_run_out.csv\"), index=False)\n",
    "with open(os.path.join(STAB_DIR, \"summary.json\"), \"w\") as f: json.dump(summary, f, indent=2)\n",
    "\n",
    "# ---- 6) Addendum figure\n",
    "fig = plt.figure(figsize=(12, 8), dpi=130)\n",
    "gs  = fig.add_gridspec(2, 2, hspace=0.35, wspace=0.30)\n",
    "\n",
    "axA = fig.add_subplot(gs[0,0])\n",
    "axA.hist(boot_df[\"ari\"], bins=20, alpha=0.85)\n",
    "axA.set_title(\"Bootstrap Stability â€” Adjusted Rand (80% subsamples)\")\n",
    "axA.set_xlabel(\"ARI\"); axA.set_ylabel(\"Count\")\n",
    "m, p10, p50, p90 = summary[\"bootstrap_ARI_mean\"], summary[\"bootstrap_ARI_p10\"], summary[\"bootstrap_ARI_p50\"], summary[\"bootstrap_ARI_p90\"]\n",
    "axA.axvline(m,  color=\"k\",    linestyle=\"--\", label=f\"mean={m:.3f}\")\n",
    "axA.axvline(p50,color=\"gray\", linestyle=\":\",  label=f\"median={p50:.3f}\")\n",
    "axA.legend()\n",
    "\n",
    "axB = fig.add_subplot(gs[0,1])\n",
    "axB.hist(boot_df[\"silhouette\"].dropna(), bins=20, alpha=0.85, color=\"#8888ff\")\n",
    "axB.set_title(\"Bootstrap Silhouette (refit per sample)\")\n",
    "axB.set_xlabel(\"Silhouette\"); axB.set_ylabel(\"Count\")\n",
    "\n",
    "axC = fig.add_subplot(gs[1,0])\n",
    "axC.plot(ms_df[\"seed\"], ms_df[\"ari\"], \"o-\")\n",
    "axC.set_title(\"Multi-seed Refit â€” ARI vs Baseline\")\n",
    "axC.set_xlabel(\"Random Seed\"); axC.set_ylabel(\"ARI\"); axC.set_ylim(0,1)\n",
    "\n",
    "axD = fig.add_subplot(gs[1,1])\n",
    "if len(lor_df):\n",
    "    x = np.arange(len(lor_df))\n",
    "    axD.bar(x - 0.18, lor_df[\"ari\"].to_numpy(), width=0.36, label=\"ARI (held-out)\")\n",
    "    if \"mean_center_cosine\" in lor_df:\n",
    "        axD.bar(x + 0.18, lor_df[\"mean_center_cosine\"].to_numpy(), width=0.36, label=\"Center cosine\")\n",
    "    axD.set_xticks(x); axD.set_xticklabels([os.path.splitext(f)[0] for f in lor_df[\"file\"]], rotation=20)\n",
    "    axD.set_ylim(0,1); axD.set_title(\"Leave-one-run-out (by file)\"); axD.legend()\n",
    "else:\n",
    "    axD.text(0.5, 0.5, \"No real runs to cross-validate.\", ha=\"center\", va=\"center\"); axD.axis(\"off\")\n",
    "\n",
    "fig.suptitle(\"CNT Cognitive Alphabet â€” Stability Addendum\", fontsize=16, weight=\"bold\")\n",
    "hdr = f\"K={K} | N={N} | Boot ARI mean={m:.3f} (p10={p10:.3f}, p50={p50:.3f}, p90={p90:.3f}) | Multi-seed ARI mean={summary['multiseed_ARI_mean']:.3f}\"\n",
    "fig.text(0.5, 0.94, hdr + f\" | dir: {os.path.basename(BASE_DIR)}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "ADD_PNG = os.path.join(OUT_DIR, \"CNT_CognitiveAlphabet_Stability_Addendum.png\")\n",
    "ADD_PDF = os.path.join(OUT_DIR, \"CNT_CognitiveAlphabet_Stability_Addendum.pdf\")\n",
    "fig.savefig(ADD_PNG, bbox_inches=\"tight\"); fig.savefig(ADD_PDF, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "print(\"\\nSaved addendum â†’\", ADD_PNG)\n",
    "print(\"Saved addendum â†’\", ADD_PDF)\n",
    "print(\"CSV outputs â†’\", STAB_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8518b6e0-43d6-43ea-bfd6-8db56244987a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No run folders with ['features.csv', 'metadata.csv', 'state_assignments.csv'] under E:\\CNT\\artifacts.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     38\u001b[39m     runs = find_runs(ROOT)\n\u001b[32m     39\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m runs:\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo run folders with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mREQ\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m under \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mROOT\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m     RUN_DIR = runs[\u001b[32m0\u001b[39m]\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[stability] Using run dir:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRUN_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No run folders with ['features.csv', 'metadata.csv', 'state_assignments.csv'] under E:\\CNT\\artifacts."
     ]
    }
   ],
   "source": [
    "# === CNT Cognitive Alphabet â€” Stability Audit (root = E:\\CNT\\artifacts) ===\n",
    "import os, glob, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score as ARI, normalized_mutual_info_score as NMI, silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "ROOT = r\"E:\\CNT\\artifacts\"              # â† your root\n",
    "RUN_DIR_FORCE = None                     # set to a specific run folder if you want to force it\n",
    "\n",
    "REQ = [\"features.csv\",\"metadata.csv\",\"state_assignments.csv\"]\n",
    "OUT_DIR = os.path.join(ROOT, \"cog_alphabet_report\")\n",
    "STAB_DIR = os.path.join(OUT_DIR, \"stability\")\n",
    "os.makedirs(STAB_DIR, exist_ok=True)\n",
    "\n",
    "def has_trio(d): return all(os.path.exists(os.path.join(d,f)) for f in REQ)\n",
    "\n",
    "def find_runs(root):\n",
    "    cands = []\n",
    "    for path in glob.iglob(os.path.join(root, \"**\", \"features.csv\"), recursive=True):\n",
    "        d = os.path.dirname(path)\n",
    "        low = d.lower()\n",
    "        if any(bad in low for bad in [\".venv\",\"site-packages\",\"appdata\",\"miniconda\",\"anaconda3\"]):\n",
    "            continue\n",
    "        if has_trio(d):\n",
    "            cands.append(d)\n",
    "    # dedup & sort by recency\n",
    "    cands = sorted(set(map(os.path.abspath, cands)),\n",
    "                   key=lambda d: os.path.getmtime(os.path.join(d, \"features.csv\")), reverse=True)\n",
    "    return cands\n",
    "\n",
    "if RUN_DIR_FORCE is not None and has_trio(RUN_DIR_FORCE):\n",
    "    RUN_DIR = RUN_DIR_FORCE\n",
    "else:\n",
    "    runs = find_runs(ROOT)\n",
    "    if not runs:\n",
    "        raise FileNotFoundError(f\"No run folders with {REQ} under {ROOT}.\")\n",
    "    RUN_DIR = runs[0]\n",
    "\n",
    "print(f\"[stability] Using run dir:\\n  {RUN_DIR}\")\n",
    "\n",
    "# â€” Load artifacts\n",
    "feat = pd.read_csv(os.path.join(RUN_DIR, \"features.csv\"))\n",
    "meta = pd.read_csv(os.path.join(RUN_DIR, \"metadata.csv\"))\n",
    "L0   = pd.read_csv(os.path.join(RUN_DIR, \"state_assignments.csv\"))[\"state\"].to_numpy().astype(int)\n",
    "\n",
    "K = int(len(np.unique(L0))); N = len(L0)\n",
    "\n",
    "# â€” Try saved models; else fit fresh\n",
    "HAS_MODELS = all(os.path.exists(os.path.join(RUN_DIR, nm)) for nm in\n",
    "                 [\"scaler_hybrid.joblib\",\"pca_hybrid.joblib\",\"kmeans_hybrid.joblib\"])\n",
    "if HAS_MODELS:\n",
    "    from joblib import load\n",
    "    scaler = load(os.path.join(RUN_DIR, \"scaler_hybrid.joblib\"))\n",
    "    pca    = load(os.path.join(RUN_DIR, \"pca_hybrid.joblib\"))\n",
    "    km0    = load(os.path.join(RUN_DIR, \"kmeans_hybrid.joblib\"))\n",
    "    Z      = pca.transform(scaler.transform(feat.values))\n",
    "    print(\"[stability] Loaded saved scaler/pca/kmeans.\")\n",
    "else:\n",
    "    print(\"[stability] No saved models; fitting fresh scaler/PCA (center alignment skipped).\")\n",
    "    scaler = StandardScaler().fit(feat.values)\n",
    "    Z      = scaler.transform(feat.values)\n",
    "    pca    = PCA(n_components=min(20, Z.shape[1]), random_state=42).fit(Z)\n",
    "    Z      = pca.transform(Z)\n",
    "    km0    = None\n",
    "\n",
    "def fit_km(Z_fit, Z_pred, k, seed):\n",
    "    km = KMeans(n_clusters=k, n_init=\"auto\", random_state=seed)\n",
    "    km.fit(Z_fit)\n",
    "    return km, km.predict(Z_pred)\n",
    "\n",
    "# â€” 1) Bootstraps (80% subsamples)\n",
    "np.random.seed(42)\n",
    "B = 100; FRAC = 0.80\n",
    "boot = []\n",
    "for b in range(B):\n",
    "    idx = np.random.choice(N, size=int(N*FRAC), replace=False)\n",
    "    km_b, Lb = fit_km(Z[idx], Z[idx], K, 1000+b)\n",
    "    try: sil = silhouette_score(Z[idx], Lb) if K>1 else np.nan\n",
    "    except Exception: sil = np.nan\n",
    "    boot.append({\"bootstrap\": b, \"size\": len(idx),\n",
    "                 \"ari\": ARI(L0[idx], Lb), \"nmi\": NMI(L0[idx], Lb), \"silhouette\": sil})\n",
    "boot_df = pd.DataFrame(boot).to_csv(os.path.join(STAB_DIR, \"bootstrap_stability.csv\"), index=False)\n",
    "\n",
    "# â€” 2) Multi-seed refits (full set)\n",
    "seeds = list(range(50, 70))\n",
    "rows = []\n",
    "for s in seeds:\n",
    "    km_s, Ls = fit_km(Z, Z, K, s)\n",
    "    try: sil = silhouette_score(Z, Ls) if K>1 else np.nan\n",
    "    except Exception: sil = np.nan\n",
    "    rows.append({\"seed\": s, \"ari\": ARI(L0, Ls), \"nmi\": NMI(L0, Ls), \"silhouette\": sil})\n",
    "ms_df = pd.DataFrame(rows).to_csv(os.path.join(STAB_DIR, \"multiseed_refit.csv\"), index=False)\n",
    "\n",
    "# â€” 3) Leave-one-run-out (by file)\n",
    "def is_real(fn):\n",
    "    fn = (fn or \"\").lower()\n",
    "    return fn.endswith((\".edf\",\".vhdr\",\".eeg\",\".fif\",\".set\",\".fdt\"))\n",
    "files = [f for f in meta[\"file\"].unique().tolist() if is_real(f)]\n",
    "\n",
    "def center_cos(Ca, Cb):\n",
    "    S = cosine_similarity(Ca, Cb)\n",
    "    r, c = linear_sum_assignment(1.0 - S)\n",
    "    return float(np.mean(S[r,c])), S[r,c].tolist(), r.tolist(), c.tolist()\n",
    "\n",
    "lor = []\n",
    "for f in files:\n",
    "    te = (meta[\"file\"] == f).to_numpy(); tr = ~te\n",
    "    if tr.sum()<K or te.sum()<K: continue\n",
    "    km_tr, Lte = fit_km(Z[tr], Z[te], K, 2025)\n",
    "    if km0 is not None:\n",
    "        mean_sim, sims, rr, cc = center_cos(km_tr.cluster_centers_, km0.cluster_centers_)\n",
    "    else:\n",
    "        mean_sim, sims, rr, cc = np.nan, [], [], []\n",
    "    lor.append({\"file\": f, \"n_train\": int(tr.sum()), \"n_test\": int(te.sum()),\n",
    "                \"ari\": float(ARI(L0[te], Lte)), \"mean_center_cosine\": mean_sim,\n",
    "                \"center_cosines\": sims, \"row_idx\": rr, \"col_idx\": cc})\n",
    "lor_df = pd.DataFrame(lor).sort_values(\"file\")\n",
    "lor_df.to_csv(os.path.join(STAB_DIR, \"leave_one_run_out.csv\"), index=False)\n",
    "\n",
    "# â€” 4) Summaries + figure\n",
    "def pct(a,q): return float(np.nanpercentile(a,q))\n",
    "boot_df2 = pd.read_csv(os.path.join(STAB_DIR, \"bootstrap_stability.csv\"))\n",
    "ms_df2   = pd.read_csv(os.path.join(STAB_DIR, \"multiseed_refit.csv\"))\n",
    "summary = {\n",
    "    \"used_dir\": RUN_DIR,\n",
    "    \"K\": K,\n",
    "    \"N_epochs\": N,\n",
    "    \"bootstrap_ARI_mean\": float(np.nanmean(boot_df2[\"ari\"])),\n",
    "    \"bootstrap_ARI_p10\":  pct(boot_df2[\"ari\"],10),\n",
    "    \"bootstrap_ARI_p50\":  pct(boot_df2[\"ari\"],50),\n",
    "    \"bootstrap_ARI_p90\":  pct(boot_df2[\"ari\"],90),\n",
    "    \"bootstrap_Sil_mean\": float(np.nanmean(boot_df2[\"silhouette\"])),\n",
    "    \"multiseed_ARI_mean\": float(np.nanmean(ms_df2[\"ari\"])),\n",
    "    \"multiseed_ARI_min\":  float(np.nanmin(ms_df2[\"ari\"])),\n",
    "    \"multiseed_ARI_max\":  float(np.nanmax(ms_df2[\"ari\"])),\n",
    "    \"LOR_rows\": int(len(lor_df))\n",
    "}\n",
    "with open(os.path.join(STAB_DIR, \"summary.json\"), \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"=== Stability Summary ===\")\n",
    "for k,v in summary.items(): print(f\"{k}: {v}\")\n",
    "\n",
    "fig = plt.figure(figsize=(12,8), dpi=130)\n",
    "gs  = fig.add_gridspec(2,2, hspace=0.35, wspace=0.30)\n",
    "\n",
    "axA = fig.add_subplot(gs[0,0])\n",
    "axA.hist(boot_df2[\"ari\"], bins=20, alpha=0.85)\n",
    "axA.set_title(\"Bootstrap Stability â€” Adjusted Rand (80% subsamples)\")\n",
    "axA.set_xlabel(\"ARI\"); axA.set_ylabel(\"Count\")\n",
    "m, p10, p50, p90 = summary[\"bootstrap_ARI_mean\"], summary[\"bootstrap_ARI_p10\"], summary[\"bootstrap_ARI_p50\"], summary[\"bootstrap_ARI_p90\"]\n",
    "axA.axvline(m, color=\"k\", linestyle=\"--\", label=f\"mean={m:.3f}\")\n",
    "axA.axvline(p50, color=\"gray\", linestyle=\":\", label=f\"median={p50:.3f}\")\n",
    "axA.legend()\n",
    "\n",
    "axB = fig.add_subplot(gs[0,1])\n",
    "axB.hist(boot_df2[\"silhouette\"].dropna(), bins=20, alpha=0.85, color=\"#8888ff\")\n",
    "axB.set_title(\"Bootstrap Silhouette (refit per sample)\")\n",
    "axB.set_xlabel(\"Silhouette\"); axB.set_ylabel(\"Count\")\n",
    "\n",
    "axC = fig.add_subplot(gs[1,0])\n",
    "axC.plot(ms_df2[\"seed\"], ms_df2[\"ari\"], \"o-\")\n",
    "axC.set_title(\"Multi-seed Refit â€” ARI vs Baseline\")\n",
    "axC.set_xlabel(\"Random Seed\"); axC.set_ylabel(\"ARI\"); axC.set_ylim(0,1)\n",
    "\n",
    "axD = fig.add_subplot(gs[1,1])\n",
    "if len(lor_df):\n",
    "    x = np.arange(len(lor_df))\n",
    "    axD.bar(x - 0.18, lor_df[\"ari\"].to_numpy(), width=0.36, label=\"ARI (held-out)\")\n",
    "    if \"mean_center_cosine\" in lor_df:\n",
    "        axD.bar(x + 0.18, lor_df[\"mean_center_cosine\"].to_numpy(), width=0.36, label=\"Center cosine\")\n",
    "    axD.set_xticks(x); axD.set_xticklabels([os.path.splitext(f)[0] for f in lor_df[\"file\"]], rotation=20)\n",
    "    axD.set_ylim(0,1); axD.set_title(\"Leave-one-run-out (by file)\"); axD.legend()\n",
    "else:\n",
    "    axD.text(0.5,0.5,\"No real runs to cross-validate.\", ha=\"center\", va=\"center\"); axD.axis(\"off\")\n",
    "\n",
    "fig.suptitle(\"CNT Cognitive Alphabet â€” Stability Addendum\", fontsize=16, weight=\"bold\")\n",
    "hdr = f\"K={K} | N={N} | Boot ARI mean={m:.3f} (p10={p10:.3f}, p50={p50:.3f}, p90={p90:.3f}) | Multi-seed ARI mean={summary['multiseed_ARI_mean']:.3f}\"\n",
    "fig.text(0.5, 0.94, hdr + f\" | dir: {os.path.basename(RUN_DIR)}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "ADD_PNG = os.path.join(OUT_DIR, \"CNT_CognitiveAlphabet_Stability_Addendum.png\")\n",
    "ADD_PDF = os.path.join(OUT_DIR, \"CNT_CognitiveAlphabet_Stability_Addendum.pdf\")\n",
    "fig.savefig(ADD_PNG, bbox_inches=\"tight\"); fig.savefig(ADD_PDF, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "print(\"\\nSaved addendum â†’\", ADD_PNG)\n",
    "print(\"Saved addendum â†’\", ADD_PDF)\n",
    "print(\"CSV outputs â†’\", STAB_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "571ae99c-deb6-4716-bdc1-d330290824b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Missing files in ./cog_alphabet_hybrid. Point RUN_DIR_FORCE to the folder that has ['features.csv', 'metadata.csv', 'state_assignments.csv'].",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m RUN_DIR_FORCE = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m./cog_alphabet_hybrid\u001b[39m\u001b[33m\"\u001b[39m   \u001b[38;5;66;03m# â† change to your exact folder if different\u001b[39;00m\n\u001b[32m     14\u001b[39m REQ = [\u001b[33m\"\u001b[39m\u001b[33mfeatures.csv\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mmetadata.csv\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mstate_assignments.csv\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(os.path.exists(os.path.join(RUN_DIR_FORCE,f)) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m REQ), \\\n\u001b[32m     16\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing files in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRUN_DIR_FORCE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Point RUN_DIR_FORCE to the folder that has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mREQ\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Output locations\u001b[39;00m\n\u001b[32m     19\u001b[39m OUT_DIR = os.path.join(RUN_DIR_FORCE, \u001b[33m\"\u001b[39m\u001b[33m..\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcog_alphabet_report\u001b[39m\u001b[33m\"\u001b[39m); OUT_DIR = os.path.abspath(OUT_DIR)\n",
      "\u001b[31mAssertionError\u001b[39m: Missing files in ./cog_alphabet_hybrid. Point RUN_DIR_FORCE to the folder that has ['features.csv', 'metadata.csv', 'state_assignments.csv']."
     ]
    }
   ],
   "source": [
    "# === CNT Cognitive Alphabet â€” Stability Audit (force folder & run) ===\n",
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score as ARI, normalized_mutual_info_score as NMI, silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# 0) FORCE your run directory here (edit if needed)\n",
    "RUN_DIR_FORCE = r\"./cog_alphabet_hybrid\"   # â† change to your exact folder if different\n",
    "\n",
    "REQ = [\"features.csv\",\"metadata.csv\",\"state_assignments.csv\"]\n",
    "assert all(os.path.exists(os.path.join(RUN_DIR_FORCE,f)) for f in REQ), \\\n",
    "    f\"Missing files in {RUN_DIR_FORCE}. Point RUN_DIR_FORCE to the folder that has {REQ}.\"\n",
    "\n",
    "# Output locations\n",
    "OUT_DIR = os.path.join(RUN_DIR_FORCE, \"..\", \"cog_alphabet_report\"); OUT_DIR = os.path.abspath(OUT_DIR)\n",
    "STAB_DIR = os.path.join(OUT_DIR, \"stability\")\n",
    "os.makedirs(STAB_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Load artifacts\n",
    "feat = pd.read_csv(os.path.join(RUN_DIR_FORCE, \"features.csv\"))\n",
    "meta = pd.read_csv(os.path.join(RUN_DIR_FORCE, \"metadata.csv\"))\n",
    "L0   = pd.read_csv(os.path.join(RUN_DIR_FORCE, \"state_assignments.csv\"))[\"state\"].to_numpy().astype(int)\n",
    "K, N = int(len(np.unique(L0))), len(L0)\n",
    "\n",
    "# 2) Load saved models if present; otherwise fit fresh scaler/PCA (center-alignment skipped)\n",
    "HAS_MODELS = all(os.path.exists(os.path.join(RUN_DIR_FORCE, nm)) for nm in\n",
    "                 [\"scaler_hybrid.joblib\",\"pca_hybrid.joblib\",\"kmeans_hybrid.joblib\"])\n",
    "if HAS_MODELS:\n",
    "    from joblib import load\n",
    "    scaler = load(os.path.join(RUN_DIR_FORCE, \"scaler_hybrid.joblib\"))\n",
    "    pca    = load(os.path.join(RUN_DIR_FORCE, \"pca_hybrid.joblib\"))\n",
    "    km0    = load(os.path.join(RUN_DIR_FORCE, \"kmeans_hybrid.joblib\"))\n",
    "    Z      = pca.transform(scaler.transform(feat.values))\n",
    "else:\n",
    "    scaler = StandardScaler().fit(feat.values)\n",
    "    Z      = scaler.transform(feat.values)\n",
    "    pca    = PCA(n_components=min(20, Z.shape[1]), random_state=42).fit(Z)\n",
    "    Z      = pca.transform(Z)\n",
    "    km0    = None\n",
    "\n",
    "def fit_km(Z_fit, Z_pred, k, seed):\n",
    "    km = KMeans(n_clusters=k, n_init=\"auto\", random_state=seed)\n",
    "    km.fit(Z_fit)\n",
    "    return km, km.predict(Z_pred)\n",
    "\n",
    "# 3) Bootstraps (80%)\n",
    "np.random.seed(42)\n",
    "B, FRAC = 100, 0.80\n",
    "boot = []\n",
    "for b in range(B):\n",
    "    idx = np.random.choice(N, size=int(N*FRAC), replace=False)\n",
    "    km_b, Lb = fit_km(Z[idx], Z[idx], K, 1000+b)\n",
    "    try: sil = silhouette_score(Z[idx], Lb) if K>1 else np.nan\n",
    "    except Exception: sil = np.nan\n",
    "    boot.append({\"bootstrap\": b, \"size\": len(idx),\n",
    "                 \"ari\": ARI(L0[idx], Lb), \"nmi\": NMI(L0[idx], Lb), \"silhouette\": sil})\n",
    "boot_df = pd.DataFrame(boot); boot_df.to_csv(os.path.join(STAB_DIR, \"bootstrap_stability.csv\"), index=False)\n",
    "\n",
    "# 4) Multi-seed refits (full set)\n",
    "rows = []\n",
    "for s in range(50, 70):\n",
    "    km_s, Ls = fit_km(Z, Z, K, s)\n",
    "    try: sil = silhouette_score(Z, Ls) if K>1 else np.nan\n",
    "    except Exception: sil = np.nan\n",
    "    rows.append({\"seed\": s, \"ari\": ARI(L0, Ls), \"nmi\": NMI(L0, Ls), \"silhouette\": sil})\n",
    "ms_df = pd.DataFrame(rows); ms_df.to_csv(os.path.join(STAB_DIR, \"multiseed_refit.csv\"), index=False)\n",
    "\n",
    "# 5) Leave-one-run-out (by file)\n",
    "def is_real(fn): return (fn or \"\").lower().endswith((\".edf\",\".vhdr\",\".eeg\",\".fif\",\".set\",\".fdt\"))\n",
    "files = [f for f in meta[\"file\"].unique().tolist() if is_real(f)]\n",
    "def center_cos(Ca, Cb):\n",
    "    S = cosine_similarity(Ca, Cb); r, c = linear_sum_assignment(1.0 - S); return float(np.mean(S[r,c]))\n",
    "lor = []\n",
    "for f in files:\n",
    "    te = (meta[\"file\"] == f).to_numpy(); tr = ~te\n",
    "    if tr.sum()<K or te.sum()<K: continue\n",
    "    km_tr, Lte = fit_km(Z[tr], Z[te], K, 2025)\n",
    "    mean_cos = center_cos(km_tr.cluster_centers_, km0.cluster_centers_) if HAS_MODELS else np.nan\n",
    "    lor.append({\"file\": f, \"n_train\": int(tr.sum()), \"n_test\": int(te.sum()),\n",
    "                \"ari\": float(ARI(L0[te], Lte)), \"mean_center_cosine\": mean_cos})\n",
    "lor_df = pd.DataFrame(lor).sort_values(\"file\"); lor_df.to_csv(os.path.join(STAB_DIR, \"leave_one_run_out.csv\"), index=False)\n",
    "\n",
    "# 6) Summaries\n",
    "def pct(a,q): return float(np.nanpercentile(a,q))\n",
    "summary = {\n",
    "    \"used_dir\": RUN_DIR_FORCE,\n",
    "    \"K\": K, \"N_epochs\": N,\n",
    "    \"bootstrap_ARI_mean\": float(np.nanmean(boot_df[\"ari\"])),\n",
    "    \"bootstrap_ARI_p10\":  pct(boot_df[\"ari\"],10),\n",
    "    \"bootstrap_ARI_p50\":  pct(boot_df[\"ari\"],50),\n",
    "    \"bootstrap_ARI_p90\":  pct(boot_df[\"ari\"],90),\n",
    "    \"bootstrap_Sil_mean\": float(np.nanmean(boot_df[\"silhouette\"])),\n",
    "    \"multiseed_ARI_mean\": float(np.nanmean(ms_df[\"ari\"])),\n",
    "    \"multiseed_ARI_min\":  float(np.nanmin(ms_df[\"ari\"])),\n",
    "    \"multiseed_ARI_max\":  float(np.nanmax(ms_df[\"ari\"])),\n",
    "    \"LOR_rows\": int(len(lor_df))\n",
    "}\n",
    "with open(os.path.join(STAB_DIR, \"summary.json\"), \"w\") as f: json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"=== Stability Summary ===\")\n",
    "for k,v in summary.items(): print(f\"{k}: {v}\")\n",
    "\n",
    "# 7) Addendum figure\n",
    "fig = plt.figure(figsize=(12,8), dpi=130)\n",
    "gs  = fig.add_gridspec(2,2, hspace=0.35, wspace=0.30)\n",
    "\n",
    "axA = fig.add_subplot(gs[0,0])\n",
    "axA.hist(boot_df[\"ari\"], bins=20, alpha=0.85)\n",
    "axA.set_title(\"Bootstrap Stability â€” Adjusted Rand (80% subsamples)\")\n",
    "axA.set_xlabel(\"ARI\"); axA.set_ylabel(\"Count\")\n",
    "m, p10, p50, p90 = summary[\"bootstrap_ARI_mean\"], summary[\"bootstrap_ARI_p10\"], summary[\"bootstrap_ARI_p50\"], summary[\"bootstrap_ARI_p90\"]\n",
    "axA.axvline(m, color=\"k\", linestyle=\"--\", label=f\"mean={m:.3f}\"); axA.axvline(p50, color=\"gray\", linestyle=\":\", label=f\"median={p50:.3f}\")\n",
    "axA.legend()\n",
    "\n",
    "axB = fig.add_subplot(gs[0,1])\n",
    "axB.hist(boot_df[\"silhouette\"].dropna(), bins=20, alpha=0.85, color=\"#8888ff\")\n",
    "axB.set_title(\"Bootstrap Silhouette (refit per sample)\")\n",
    "axB.set_xlabel(\"Silhouette\"); axB.set_ylabel(\"Count\")\n",
    "\n",
    "axC = fig.add_subplot(gs[1,0])\n",
    "axC.plot(ms_df[\"seed\"], ms_df[\"ari\"], \"o-\")\n",
    "axC.set_title(\"Multi-seed Refit â€” ARI vs Baseline\"); axC.set_xlabel(\"Random Seed\"); axC.set_ylabel(\"ARI\"); axC.set_ylim(0,1)\n",
    "\n",
    "axD = fig.add_subplot(gs[1,1])\n",
    "if len(lor_df):\n",
    "    x = np.arange(len(lor_df))\n",
    "    axD.bar(x - 0.18, lor_df[\"ari\"].to_numpy(), width=0.36, label=\"ARI (held-out)\")\n",
    "    if \"mean_center_cosine\" in lor_df:\n",
    "        axD.bar(x + 0.18, lor_df[\"mean_center_cosine\"].to_numpy(), width=0.36, label=\"Center cosine\")\n",
    "    axD.set_xticks(x); axD.set_xticklabels([os.path.splitext(f)[0] for f in lor_df[\"file\"]], rotation=20)\n",
    "    axD.set_ylim(0,1); axD.set_title(\"Leave-one-run-out (by file)\"); axD.legend()\n",
    "else:\n",
    "    axD.text(0.5,0.5,\"No real runs to cross-validate.\", ha=\"center\", va=\"center\"); axD.axis(\"off\")\n",
    "\n",
    "fig.suptitle(\"CNT Cognitive Alphabet â€” Stability Addendum\", fontsize=16, weight=\"bold\")\n",
    "hdr = f\"K={K} | N={N} | Boot ARI mean={m:.3f} (p10={p10:.3f}, p50={p50:.3f}, p90={p90:.3f}) | Multi-seed ARI mean={summary['multiseed_ARI_mean']:.3f}\"\n",
    "fig.text(0.5, 0.94, hdr + f\" | dir: {os.path.basename(os.path.abspath(RUN_DIR_FORCE))}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "ADD_PNG = os.path.join(OUT_DIR, \"CNT_CognitiveAlphabet_Stability_Addendum.png\")\n",
    "ADD_PDF = os.path.join(OUT_DIR, \"CNT_CognitiveAlphabet_Stability_Addendum.pdf\")\n",
    "fig.savefig(ADD_PNG, bbox_inches=\"tight\"); fig.savefig(ADD_PDF, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "print(\"\\nSaved addendum â†’\", ADD_PNG)\n",
    "print(\"Saved addendum â†’\", ADD_PDF)\n",
    "print(\"CSV outputs â†’\", STAB_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44d85f93-08b3-42a3-bc1d-94f637621d62",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No data in ./brainwaves (edf/csv/tsv). Add files or point me to a run folder.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 214\u001b[39m\n\u001b[32m    212\u001b[39m     RUN_DIR, err = _build_run_from_brainwaves()\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(err)\n\u001b[32m    216\u001b[39m \u001b[38;5;66;03m# -----------------------------------\u001b[39;00m\n\u001b[32m    217\u001b[39m \u001b[38;5;66;03m# 2) Stability audit on RUN_DIR\u001b[39;00m\n\u001b[32m    218\u001b[39m \u001b[38;5;66;03m# -----------------------------------\u001b[39;00m\n\u001b[32m    219\u001b[39m OUT_DIR = Path(RUN_DIR).parent / \u001b[33m\"\u001b[39m\u001b[33mcog_alphabet_report\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No data in ./brainwaves (edf/csv/tsv). Add files or point me to a run folder."
     ]
    }
   ],
   "source": [
    "# === CNT Cognitive Alphabet â€” Find OR Build a Run, then Stability Audit (one cell) ===\n",
    "import os, re, glob, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score as ARI, normalized_mutual_info_score as NMI, silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Helpers to locate a run dir\n",
    "# -----------------------------\n",
    "REQ = [\"features.csv\", \"metadata.csv\", \"state_assignments.csv\"]\n",
    "\n",
    "def _has_trio(d): return all(Path(d, f).exists() for f in REQ)\n",
    "\n",
    "def _search_runs():\n",
    "    roots = [\n",
    "        Path.cwd(),\n",
    "        *(Path.cwd().parents),\n",
    "        Path(r\"E:\\CNT\"),\n",
    "        Path(r\"E:\\CNT\\artifacts\"),\n",
    "        Path(r\"C:\\Users\\caleb\\CNT_Lab\"),\n",
    "        Path.home()\n",
    "    ]\n",
    "    cand = []\n",
    "    # quick names first\n",
    "    names = [\"cog_alphabet_hybrid\", \"cog_alphabet_physio_betaSplit\", \"cog_alphabet_physio\", \"cog_alphabet_demo\"]\n",
    "    for r in roots:\n",
    "        for n in names:\n",
    "            d = r / n\n",
    "            if _has_trio(d): cand.append(d)\n",
    "    # broader scan but pruned\n",
    "    bad = (\"site-packages\",\"AppData\",\"Windows\",\"Program Files\",\"Microsoft\",\"Miniconda\",\"Anaconda3\",\".venv\",\".git\")\n",
    "    for r in roots:\n",
    "        if not r.exists(): continue\n",
    "        for p in r.rglob(\"features.csv\"):\n",
    "            d = p.parent\n",
    "            low = str(d).lower()\n",
    "            if any(b.lower() in low for b in bad): continue\n",
    "            if _has_trio(d): cand.append(d)\n",
    "    # pick freshest\n",
    "    cand = list(dict.fromkeys([d.resolve() for d in cand]))\n",
    "    if not cand: return None\n",
    "    cand.sort(key=lambda d: (d / \"features.csv\").stat().st_mtime, reverse=True)\n",
    "    return cand[0]\n",
    "\n",
    "RUN_DIR = _search_runs()\n",
    "\n",
    "# -----------------------------------\n",
    "# 1) If not found, build a fresh run\n",
    "# -----------------------------------\n",
    "def _build_run_from_brainwaves(out_dir=Path(\"./cog_alphabet_rebuilt\"), k_range=(3,6)):\n",
    "    \"\"\"Reconstruct minimal run folder from ./brainwaves EEG files.\"\"\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # Collect files\n",
    "    bw = Path(\"./brainwaves\")\n",
    "    files = []\n",
    "    for ext in (\"*.edf\",\"*.bdf\",\"*.fif\",\"*.vhdr\",\"*.eeg\",\"*.set\",\"*.fdt\",\"*.csv\",\"*.tsv\"):\n",
    "        files += list(bw.rglob(ext))\n",
    "    if not files:\n",
    "        return None, \"No data in ./brainwaves (edf/csv/tsv). Add files or point me to a run folder.\"\n",
    "\n",
    "    import mne\n",
    "    from scipy.signal import welch\n",
    "\n",
    "    # params (match your pipeline ethos; Î³ capped to 45 for portraits)\n",
    "    TARGET_SF = 250.0\n",
    "    L_FREQ, H_FREQ = 0.5, 45.0\n",
    "    EPOCH_LEN, STEP = 2.0, 0.5\n",
    "    BANDS = {\n",
    "        \"delta\": (1.0, 4.0),\n",
    "        \"theta\": (4.0, 8.0),\n",
    "        \"alpha\": (8.0, 13.0),\n",
    "        \"beta_low\":  (13.0, 20.0),\n",
    "        \"beta_high\": (20.0, 35.0),\n",
    "        \"gamma\":     (35.0, 45.0),\n",
    "    }\n",
    "\n",
    "    def _load_csv_tsv(path, sf_guess=TARGET_SF):\n",
    "        df = pd.read_csv(path) if path.suffix.lower()==\".csv\" else pd.read_csv(path, sep=\"\\t\")\n",
    "        if \"time\" in df.columns:\n",
    "            t = df[\"time\"].to_numpy(); dt = np.median(np.diff(t)); sf = 1.0/max(dt,1e-12)\n",
    "            ch = [c for c in df.columns if c!=\"time\"]; data = df[ch].to_numpy().T\n",
    "        else:\n",
    "            sf = sf_guess; ch = list(df.columns); data = df[ch].to_numpy().T\n",
    "        info = mne.create_info(ch_names=ch, sfreq=sf, ch_types=[\"eeg\"]*len(ch))\n",
    "        return mne.io.RawArray(data, info, verbose=False)\n",
    "\n",
    "    def load_raw_any(p):\n",
    "        pl = p.suffix.lower()\n",
    "        if pl in (\".edf\",\".bdf\"):\n",
    "            raw = mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "        elif pl==\".fif\":\n",
    "            raw = mne.io.read_raw_fif(str(p), preload=True, verbose=False)\n",
    "        elif pl in (\".vhdr\",\".eeg\"):\n",
    "            raw = mne.io.read_raw_brainvision(str(p), preload=True, verbose=False)\n",
    "        elif pl in (\".set\",\".fdt\"):\n",
    "            raw = mne.io.read_raw_eeglab(str(p), preload=True, verbose=False)\n",
    "        elif pl in (\".csv\",\".tsv\"):\n",
    "            raw = _load_csv_tsv(p)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported: {p}\")\n",
    "        # resample â†’ safe Nyquist; then filter\n",
    "        if abs(raw.info[\"sfreq\"] - TARGET_SF) > 1e-6:\n",
    "            raw.resample(TARGET_SF, npad=\"auto\", verbose=False)\n",
    "        ny = raw.info[\"sfreq\"]/2\n",
    "        raw.filter(L_FREQ, min(H_FREQ, ny-1.0), verbose=False)\n",
    "        if \"eeg\" in set(raw.get_channel_types()):\n",
    "            raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "        return raw\n",
    "\n",
    "    def make_epochs(raw):\n",
    "        ov = max(0.0, EPOCH_LEN - STEP)\n",
    "        return mne.make_fixed_length_epochs(raw, duration=EPOCH_LEN, overlap=ov, preload=True, verbose=False)\n",
    "\n",
    "    def hjorth_params(x):\n",
    "        d1 = np.diff(x, axis=-1)\n",
    "        var0 = np.var(x, axis=-1) + 1e-12\n",
    "        var1 = np.var(d1, axis=-1) + 1e-12\n",
    "        mob  = np.sqrt(var1/var0)\n",
    "        d2 = np.diff(d1, axis=-1)\n",
    "        var2 = np.var(d2, axis=-1) + 1e-12\n",
    "        comp = np.sqrt((var2/var1)/(var1/var0))\n",
    "        return var0, mob, comp\n",
    "\n",
    "    def spectral_features(epochs):\n",
    "        X = epochs.get_data()\n",
    "        n_ep, n_ch, n_t = X.shape\n",
    "        sf = epochs.info[\"sfreq\"]\n",
    "        nper = min(int(sf*2), n_t); nov = nper//2\n",
    "        freqs, psd = welch(X, fs=sf, nperseg=nper, noverlap=nov, axis=-1, average=\"median\")\n",
    "        def bidx(lo,hi): return np.where((freqs>=lo)&(freqs<hi))[0]\n",
    "        aidx = bidx(L_FREQ,H_FREQ); tot = np.maximum(psd[:,:,aidx].sum(-1), 1e-12)\n",
    "\n",
    "        feats = {}; accum={\"alpha\":np.zeros((n_ep,n_ch)),\"theta\":np.zeros((n_ep,n_ch)),\"beta\":np.zeros((n_ep,n_ch))}\n",
    "        for name,(lo,hi) in BANDS.items():\n",
    "            idx = bidx(lo,hi)\n",
    "            bp = psd[:,:,idx].sum(-1)/tot if idx.size else np.zeros((n_ep,n_ch))\n",
    "            feats[f\"{name}_rel_med\"] = np.median(bp, axis=1)\n",
    "            feats[f\"{name}_rel_iqr\"] = np.subtract(*np.percentile(bp,[75,25],axis=1))\n",
    "            feats[f\"{name}_rel_std\"] = np.std(bp, axis=1)\n",
    "            head = name.split(\"_\",1)[0]\n",
    "            if head in accum: accum[head]+=bp\n",
    "\n",
    "        alpha_med = np.median(accum[\"alpha\"],axis=1); theta_med = np.median(accum[\"theta\"],axis=1); beta_med = np.median(accum[\"beta\"],axis=1)\n",
    "        feats[\"alpha_rel_med\"]=alpha_med; feats[\"theta_rel_med\"]=theta_med; feats[\"beta_rel_med\"]=beta_med\n",
    "        feats[\"theta_over_alpha\"]= theta_med/np.maximum(alpha_med,1e-6)\n",
    "        feats[\"beta_over_alpha\"] = beta_med/np.maximum(alpha_med,1e-6)\n",
    "        feats[\"(alpha+theta)_over_beta\"] = (alpha_med+theta_med)/np.maximum(beta_med,1e-6)\n",
    "\n",
    "        p = psd[:,:,aidx]; p_norm = p/np.maximum(p.sum(-1,keepdims=True),1e-12)\n",
    "        H = -np.sum(p_norm*np.log2(p_norm+1e-12), axis=-1); Hn = H/np.log2(p.shape[-1])\n",
    "        feats[\"spec_entropy_med\"] = np.median(Hn, axis=1); feats[\"spec_entropy_std\"] = np.std(Hn, axis=1)\n",
    "        f = freqs[aidx].reshape(1,1,-1)\n",
    "        centroid = (p*f).sum(-1)/np.maximum(p.sum(-1),1e-12)\n",
    "        feats[\"spec_centroid_med\"]=np.median(centroid,axis=1); feats[\"spec_centroid_std\"]=np.std(centroid,axis=1)\n",
    "\n",
    "        act,mob,comp = hjorth_params(X)\n",
    "        feats[\"hjorth_activity_med\"]=np.median(act,axis=1)\n",
    "        feats[\"hjorth_mobility_med\"]=np.median(mob,axis=1)\n",
    "        feats[\"hjorth_complexity_med\"]=np.median(comp,axis=1)\n",
    "\n",
    "        # fbCSP_task_proba not available here â†’ zeros\n",
    "        feats[\"fbCSP_task_proba\"]=np.zeros(n_ep)\n",
    "        return pd.DataFrame(feats)\n",
    "\n",
    "    all_feat=[]; all_meta=[]\n",
    "    for fp in files:\n",
    "        try:\n",
    "            raw = load_raw_any(fp)\n",
    "            epochs = make_epochs(raw)\n",
    "            if len(epochs)==0: continue\n",
    "            feat = spectral_features(epochs)\n",
    "            starts = epochs.events[:,0]/epochs.info[\"sfreq\"]; ends = starts + EPOCH_LEN\n",
    "            meta = pd.DataFrame({\"file\": fp.name, \"epoch_idx\": np.arange(len(epochs)),\n",
    "                                 \"t_start_s\": starts, \"t_end_s\": ends,\n",
    "                                 \"sfreq\": epochs.info[\"sfreq\"], \"n_channels\": len(epochs.ch_names)})\n",
    "            all_feat.append(feat); all_meta.append(meta)\n",
    "        except Exception as e:\n",
    "            print(f\"[skip] {fp.name}: {e}\")\n",
    "\n",
    "    if not all_feat:\n",
    "        return None, \"Could not extract epochs from ./brainwaves.\"\n",
    "\n",
    "    F = pd.concat(all_feat, axis=0, ignore_index=True)\n",
    "    M = pd.concat(all_meta, axis=0, ignore_index=True)\n",
    "    # cluster\n",
    "    scaler = StandardScaler().fit(F.values); Xs = scaler.transform(F.values)\n",
    "    pca = PCA(n_components=min(20, Xs.shape[1]), random_state=42).fit(Xs); Z = pca.transform(Xs)\n",
    "    best={\"sil\":-1,\"k\":None,\"model\":None,\"labels\":None}\n",
    "    for k in range(k_range[0], k_range[1]+1):\n",
    "        km = KMeans(n_clusters=k, n_init=\"auto\", random_state=42)\n",
    "        lab = km.fit_predict(Z)\n",
    "        sil = silhouette_score(Z, lab) if k>1 else -1\n",
    "        if sil>best[\"sil\"]:\n",
    "            best.update({\"sil\":sil,\"k\":k,\"model\":km,\"labels\":lab})\n",
    "    # save trio\n",
    "    F.to_csv(out_dir/\"features.csv\", index=False)\n",
    "    M.to_csv(out_dir/\"metadata.csv\", index=False)\n",
    "    pd.DataFrame({\"state\": best[\"labels\"]}).to_csv(out_dir/\"state_assignments.csv\", index=False)\n",
    "    # also stash models if you want later reuse\n",
    "    try:\n",
    "        from joblib import dump\n",
    "        dump(scaler, out_dir/\"scaler_hybrid.joblib\"); dump(pca, out_dir/\"pca_hybrid.joblib\"); dump(best[\"model\"], out_dir/\"kmeans_hybrid.joblib\")\n",
    "    except Exception: pass\n",
    "    print(f\"[rebuilt] {out_dir} (K={best['k']}, silhouette={best['sil']:.3f})\")\n",
    "    return out_dir, None\n",
    "\n",
    "if RUN_DIR is None:\n",
    "    # attempt rebuild\n",
    "    RUN_DIR, err = _build_run_from_brainwaves()\n",
    "    if err:\n",
    "        raise FileNotFoundError(err)\n",
    "\n",
    "# -----------------------------------\n",
    "# 2) Stability audit on RUN_DIR\n",
    "# -----------------------------------\n",
    "OUT_DIR = Path(RUN_DIR).parent / \"cog_alphabet_report\"\n",
    "STAB_DIR = OUT_DIR / \"stability\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True); STAB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "feat = pd.read_csv(Path(RUN_DIR,\"features.csv\"))\n",
    "meta = pd.read_csv(Path(RUN_DIR,\"metadata.csv\"))\n",
    "L0   = pd.read_csv(Path(RUN_DIR,\"state_assignments.csv\"))[\"state\"].to_numpy().astype(int)\n",
    "K = len(np.unique(L0)); N = len(L0)\n",
    "\n",
    "# load models if present; else fit fresh (center alignment skipped)\n",
    "has_models = all(Path(RUN_DIR, nm).exists() for nm in [\"scaler_hybrid.joblib\",\"pca_hybrid.joblib\",\"kmeans_hybrid.joblib\"])\n",
    "if has_models:\n",
    "    from joblib import load\n",
    "    scaler = load(Path(RUN_DIR,\"scaler_hybrid.joblib\"))\n",
    "    pca    = load(Path(RUN_DIR,\"pca_hybrid.joblib\"))\n",
    "    km0    = load(Path(RUN_DIR,\"kmeans_hybrid.joblib\"))\n",
    "    Z      = pca.transform(scaler.transform(feat.values))\n",
    "else:\n",
    "    scaler = StandardScaler().fit(feat.values)\n",
    "    Z      = scaler.transform(feat.values)\n",
    "    pca    = PCA(n_components=min(20, Z.shape[1]), random_state=42).fit(Z)\n",
    "    Z      = pca.transform(Z)\n",
    "    km0    = None\n",
    "\n",
    "def fit_km(Z_fit, Z_pred, k, seed):\n",
    "    km = KMeans(n_clusters=k, n_init=\"auto\", random_state=seed)\n",
    "    km.fit(Z_fit); return km, km.predict(Z_pred)\n",
    "\n",
    "# Bootstraps\n",
    "np.random.seed(42)\n",
    "B, FRAC = 100, 0.80\n",
    "boot = []\n",
    "for b in range(B):\n",
    "    idx = np.random.choice(N, size=int(N*FRAC), replace=False)\n",
    "    km_b, Lb = fit_km(Z[idx], Z[idx], K, 1000+b)\n",
    "    try: sil = silhouette_score(Z[idx], Lb) if K>1 else np.nan\n",
    "    except Exception: sil = np.nan\n",
    "    boot.append({\"bootstrap\": b, \"size\": len(idx),\n",
    "                 \"ari\": ARI(L0[idx], Lb), \"nmi\": NMI(L0[idx], Lb), \"silhouette\": sil})\n",
    "boot_df = pd.DataFrame(boot); boot_df.to_csv(STAB_DIR/\"bootstrap_stability.csv\", index=False)\n",
    "\n",
    "# Multi-seed\n",
    "rows = []\n",
    "for s in range(50,70):\n",
    "    km_s, Ls = fit_km(Z, Z, K, s)\n",
    "    try: sil = silhouette_score(Z, Ls) if K>1 else np.nan\n",
    "    except Exception: sil = np.nan\n",
    "    rows.append({\"seed\": s, \"ari\": ARI(L0, Ls), \"nmi\": NMI(L0, Ls), \"silhouette\": sil})\n",
    "ms_df = pd.DataFrame(rows); ms_df.to_csv(STAB_DIR/\"multiseed_refit.csv\", index=False)\n",
    "\n",
    "# Leave-one-run-out by file\n",
    "def _is_real(fn): return (fn or \"\").lower().endswith((\".edf\",\".vhdr\",\".eeg\",\".fif\",\".set\",\".fdt\"))\n",
    "files = [f for f in meta[\"file\"].unique().tolist() if _is_real(f)]\n",
    "lor = []\n",
    "if files:\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    from scipy.optimize import linear_sum_assignment\n",
    "    def center_cos(Ca, Cb):\n",
    "        S = cosine_similarity(Ca, Cb); r, c = linear_sum_assignment(1.0 - S); return float(np.mean(S[r,c]))\n",
    "for f in files:\n",
    "    te = (meta[\"file\"] == f).to_numpy(); tr = ~te\n",
    "    if tr.sum()<K or te.sum()<K: continue\n",
    "    km_tr, Lte = fit_km(Z[tr], Z[te], K, 2025)\n",
    "    mean_cos = np.nan\n",
    "    if km0 is not None:\n",
    "        try: mean_cos = center_cos(km_tr.cluster_centers_, km0.cluster_centers_)\n",
    "        except Exception: pass\n",
    "    lor.append({\"file\": f, \"n_train\": int(tr.sum()), \"n_test\": int(te.sum()),\n",
    "                \"ari\": float(ARI(L0[te], Lte)), \"mean_center_cosine\": mean_cos})\n",
    "lor_df = pd.DataFrame(lor); lor_df.to_csv(STAB_DIR/\"leave_one_run_out.csv\", index=False)\n",
    "\n",
    "# Summary + addendum fig\n",
    "def pct(a,q): return float(np.nanpercentile(a,q))\n",
    "summary = {\n",
    "    \"used_dir\": str(RUN_DIR),\n",
    "    \"K\": int(K), \"N_epochs\": int(N),\n",
    "    \"bootstrap_ARI_mean\": float(np.nanmean(boot_df[\"ari\"])),\n",
    "    \"bootstrap_ARI_p10\":  pct(boot_df[\"ari\"],10),\n",
    "    \"bootstrap_ARI_p50\":  pct(boot_df[\"ari\"],50),\n",
    "    \"bootstrap_ARI_p90\":  pct(boot_df[\"ari\"],90),\n",
    "    \"bootstrap_Sil_mean\": float(np.nanmean(boot_df[\"silhouette\"])),\n",
    "    \"multiseed_ARI_mean\": float(np.nanmean(ms_df[\"ari\"])),\n",
    "    \"multiseed_ARI_min\":  float(np.nanmin(ms_df[\"ari\"])),\n",
    "    \"multiseed_ARI_max\":  float(np.nanmax(ms_df[\"ari\"])),\n",
    "    \"LOR_rows\": int(len(lor_df))\n",
    "}\n",
    "with open(STAB_DIR/\"summary.json\",\"w\") as f: json.dump(summary,f,indent=2)\n",
    "print(\"=== Stability Summary ===\"); [print(f\"{k}: {v}\") for k,v in summary.items()]\n",
    "\n",
    "fig = plt.figure(figsize=(12,8), dpi=130)\n",
    "gs  = fig.add_gridspec(2,2, hspace=0.35, wspace=0.30)\n",
    "\n",
    "axA = fig.add_subplot(gs[0,0]); axA.hist(boot_df[\"ari\"], bins=20, alpha=0.85)\n",
    "axA.set_title(\"Bootstrap Stability â€” Adjusted Rand (80% subsamples)\"); axA.set_xlabel(\"ARI\"); axA.set_ylabel(\"Count\")\n",
    "m,p10,p50,p90 = summary[\"bootstrap_ARI_mean\"],summary[\"bootstrap_ARI_p10\"],summary[\"bootstrap_ARI_p50\"],summary[\"bootstrap_ARI_p90\"]\n",
    "axA.axvline(m,color=\"k\",linestyle=\"--\",label=f\"mean={m:.3f}\"); axA.axvline(p50,color=\"gray\",linestyle=\":\",label=f\"median={p50:.3f}\"); axA.legend()\n",
    "\n",
    "axB = fig.add_subplot(gs[0,1]); axB.hist(boot_df[\"silhouette\"].dropna(), bins=20, alpha=0.85, color=\"#8888ff\")\n",
    "axB.set_title(\"Bootstrap Silhouette (refit per sample)\"); axB.set_xlabel(\"Silhouette\"); axB.set_ylabel(\"Count\")\n",
    "\n",
    "axC = fig.add_subplot(gs[1,0]); axC.plot(ms_df[\"seed\"], ms_df[\"ari\"], \"o-\")\n",
    "axC.set_title(\"Multi-seed Refit â€” ARI vs Baseline\"); axC.set_xlabel(\"Random Seed\"); axC.set_ylabel(\"ARI\"); axC.set_ylim(0,1)\n",
    "\n",
    "axD = fig.add_subplot(gs[1,1])\n",
    "if len(lor_df):\n",
    "    x = np.arange(len(lor_df)); axD.bar(x-0.18, lor_df[\"ari\"].to_numpy(), width=0.36, label=\"ARI (held-out)\")\n",
    "    if \"mean_center_cosine\" in lor_df: axD.bar(x+0.18, lor_df[\"mean_center_cosine\"].to_numpy(), width=0.36, label=\"Center cosine\")\n",
    "    axD.set_xticks(x); axD.set_xticklabels([Path(f).stem for f in lor_df[\"file\"]], rotation=20)\n",
    "    axD.set_ylim(0,1); axD.set_title(\"Leave-one-run-out (by file)\"); axD.legend()\n",
    "else:\n",
    "    axD.text(0.5,0.5,\"No real runs to cross-validate.\", ha=\"center\", va=\"center\"); axD.axis(\"off\")\n",
    "\n",
    "fig.suptitle(\"CNT Cognitive Alphabet â€” Stability Addendum\", fontsize=16, weight=\"bold\")\n",
    "hdr = f\"K={K} | N={N} | Boot ARI mean={m:.3f} (p10={p10:.3f}, p50={p50:.3f}, p90={p90:.3f}) | Multi-seed ARI mean={summary['multiseed_ARI_mean']:.3f}\"\n",
    "fig.text(0.5, 0.94, hdr + f\" | dir: {Path(RUN_DIR).name}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "ADD_PNG = OUT_DIR/\"CNT_CognitiveAlphabet_Stability_Addendum.png\"\n",
    "ADD_PDF = OUT_DIR/\"CNT_CognitiveAlphabet_Stability_Addendum.pdf\"\n",
    "fig.savefig(ADD_PNG, bbox_inches=\"tight\"); fig.savefig(ADD_PDF, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "print(\"\\nSaved addendum â†’\", ADD_PNG)\n",
    "print(\"Saved addendum â†’\", ADD_PDF)\n",
    "print(\"CSV outputs â†’\", STAB_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2ec56ef-e049-411f-a2f3-68a76237279f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Found run folders ===\n",
      "\n",
      "No existing run found. Use cell B to rebuild + audit.\n"
     ]
    }
   ],
   "source": [
    "# === Run locator: find folders that contain features.csv + metadata.csv + state_assignments.csv ===\n",
    "import os, glob, pathlib, pandas as pd\n",
    "\n",
    "REQ = [\"features.csv\",\"metadata.csv\",\"state_assignments.csv\"]\n",
    "def has_trio(d): return all(os.path.exists(os.path.join(d,f)) for f in REQ)\n",
    "\n",
    "roots = [\n",
    "    pathlib.Path.cwd(),\n",
    "    *(pathlib.Path.cwd().parents),\n",
    "    pathlib.Path(r\"E:\\CNT\"),\n",
    "    pathlib.Path(r\"E:\\CNT\\artifacts\"),\n",
    "    pathlib.Path(r\"C:\\Users\\caleb\\CNT_Lab\"),\n",
    "    pathlib.Path.home()\n",
    "]\n",
    "\n",
    "cands = []\n",
    "# quick names first\n",
    "names = [\"cog_alphabet_hybrid\",\"cog_alphabet_physio_betaSplit\",\"cog_alphabet_physio\",\"cog_alphabet_demo\"]\n",
    "for r in roots:\n",
    "    for n in names:\n",
    "        d = r / n\n",
    "        if has_trio(d): cands.append(d)\n",
    "\n",
    "# broad scan but skip noisy dirs\n",
    "bad = (\"site-packages\",\"AppData\",\"Windows\",\"Program Files\",\"Microsoft\",\"Miniconda\",\"Anaconda3\",\".venv\",\".git\")\n",
    "for r in roots:\n",
    "    if not r.exists(): continue\n",
    "    for p in r.rglob(\"features.csv\"):\n",
    "        d = p.parent\n",
    "        low = str(d).lower()\n",
    "        if any(bad_s in low for bad_s in bad): continue\n",
    "        if has_trio(d): cands.append(d)\n",
    "\n",
    "# dedup & sort by newest\n",
    "cands = sorted(set(map(lambda p: p.resolve(), cands)), key=lambda d: (d/\"features.csv\").stat().st_mtime, reverse=True)\n",
    "\n",
    "print(\"=== Found run folders ===\")\n",
    "for i, d in enumerate(cands, 1):\n",
    "    print(f\"[{i}] {d}\")\n",
    "if cands:\n",
    "    RUN_DIR_FORCE = str(cands[0])\n",
    "    print(\"\\nUsing freshest run:\", RUN_DIR_FORCE)\n",
    "else:\n",
    "    RUN_DIR_FORCE = None\n",
    "    print(\"\\nNo existing run found. Use cell B to rebuild + audit.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d15d90ed-aa0e-4c38-962f-f7adae1c283c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched: ['S001R01.edf', 'S001R02.edf', 'S001R03.edf']\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Saved run trio â†’ E:\\CNT\\artifacts\\cog_alphabet_rebuilt\n",
      "Rebuilt K=3 (sil=0.254)\n",
      "=== Stability Summary ===\n",
      "used_dir: E:\\CNT\\artifacts\\cog_alphabet_rebuilt\n",
      "K: 3\n",
      "N_epochs: 485\n",
      "bootstrap_ARI_mean: 0.9701030424155914\n",
      "bootstrap_ARI_p10: 0.9412802719646733\n",
      "bootstrap_ARI_p50: 0.974239763988985\n",
      "bootstrap_ARI_p90: 0.993018402682906\n",
      "bootstrap_Sil_mean: 0.2543321645051739\n",
      "multiseed_ARI_mean: 0.9906921096987917\n",
      "multiseed_ARI_min: 0.979315799330648\n",
      "multiseed_ARI_max: 1.0\n",
      "LOR_rows: 3\n",
      "\n",
      "Saved addendum â†’ E:\\CNT\\artifacts\\cog_alphabet_report\\CNT_CognitiveAlphabet_Stability_Addendum.png\n",
      "Saved addendum â†’ E:\\CNT\\artifacts\\cog_alphabet_report\\CNT_CognitiveAlphabet_Stability_Addendum.pdf\n",
      "CSV outputs â†’ E:\\CNT\\artifacts\\cog_alphabet_report\\stability\n"
     ]
    }
   ],
   "source": [
    "# === Rebuild a run from public EEG (R01â€“R03), then run the Stability Audit ===\n",
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt, requests\n",
    "from pathlib import Path\n",
    "import mne\n",
    "from scipy.signal import welch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score as ARI, normalized_mutual_info_score as NMI, silhouette_score\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# --- Where to store the rebuilt run (edit if you want) ---\n",
    "RUN_DIR = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_rebuilt\")\n",
    "DATA_DIR = RUN_DIR / \"brainwaves_rebuilt\"\n",
    "OUT_DIR  = RUN_DIR.parent / \"cog_alphabet_report\"\n",
    "STAB_DIR = OUT_DIR / \"stability\"\n",
    "for p in (RUN_DIR, DATA_DIR, OUT_DIR, STAB_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- 1) Fetch small EEG files (PhysioNet EEGMMI S001 R01â€“R03) ---\n",
    "URLS = [\n",
    "    \"https://physionet.org/files/eegmmidb/1.0.0/S001/S001R01.edf\",\n",
    "    \"https://physionet.org/files/eegmmidb/1.0.0/S001/S001R02.edf\",\n",
    "    \"https://physionet.org/files/eegmmidb/1.0.0/S001/S001R03.edf\",\n",
    "]\n",
    "def fetch(url, dest):\n",
    "    dest = Path(dest)\n",
    "    if dest.exists() and dest.stat().st_size>0: return dest\n",
    "    with requests.get(url, stream=True, timeout=60) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(dest, \"wb\") as f:\n",
    "            for ch in r.iter_content(8192):\n",
    "                if ch: f.write(ch)\n",
    "    return dest\n",
    "\n",
    "files = [fetch(u, DATA_DIR / u.split(\"/\")[-1]) for u in URLS]\n",
    "print(\"Fetched:\", [f.name for f in files])\n",
    "\n",
    "# --- 2) Rebuild a minimal run (spectral features + clustering) ---\n",
    "TARGET_SF = 250.0\n",
    "L_FREQ, H_FREQ = 0.5, 45.0\n",
    "EPOCH_LEN, STEP = 2.0, 0.5\n",
    "BANDS = {\n",
    "    \"delta\": (1.0, 4.0),\n",
    "    \"theta\": (4.0, 8.0),\n",
    "    \"alpha\": (8.0, 13.0),\n",
    "    \"beta_low\":  (13.0, 20.0),\n",
    "    \"beta_high\": (20.0, 35.0),\n",
    "    \"gamma\":     (35.0, 45.0),\n",
    "}\n",
    "\n",
    "def load_raw(path):\n",
    "    raw = mne.io.read_raw_edf(str(path), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"] - TARGET_SF) > 1e-6:\n",
    "        raw.resample(TARGET_SF, npad=\"auto\", verbose=False)\n",
    "    ny = raw.info[\"sfreq\"]/2\n",
    "    raw.filter(L_FREQ, min(H_FREQ, ny-1.0), verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "    return raw\n",
    "\n",
    "def make_epochs(raw):\n",
    "    ov = max(0.0, EPOCH_LEN - STEP)\n",
    "    return mne.make_fixed_length_epochs(raw, duration=EPOCH_LEN, overlap=ov, preload=True, verbose=False)\n",
    "\n",
    "def hjorth_params(x):\n",
    "    d1 = np.diff(x, axis=-1)\n",
    "    var0 = np.var(x, axis=-1) + 1e-12\n",
    "    var1 = np.var(d1, axis=-1) + 1e-12\n",
    "    mob  = np.sqrt(var1/var0)\n",
    "    d2 = np.diff(d1, axis=-1)\n",
    "    var2 = np.var(d2, axis=-1) + 1e-12\n",
    "    comp = np.sqrt((var2/var1)/(var1/var0))\n",
    "    return var0, mob, comp\n",
    "\n",
    "def spectral_features(epochs):\n",
    "    X = epochs.get_data()\n",
    "    n_ep, n_ch, n_t = X.shape\n",
    "    sf = epochs.info[\"sfreq\"]\n",
    "    nper = min(int(sf*2), n_t); nov = nper//2\n",
    "    freqs, psd = welch(X, fs=sf, nperseg=nper, noverlap=nov, axis=-1, average=\"median\")\n",
    "    def bidx(lo,hi): return np.where((freqs>=lo)&(freqs<hi))[0]\n",
    "    aidx = bidx(L_FREQ, H_FREQ); tot = np.maximum(psd[:,:,aidx].sum(-1), 1e-12)\n",
    "\n",
    "    feats = {}; accum={\"alpha\":np.zeros((n_ep,n_ch)),\"theta\":np.zeros((n_ep,n_ch)),\"beta\":np.zeros((n_ep,n_ch))}\n",
    "    for name,(lo,hi) in BANDS.items():\n",
    "        idx = bidx(lo,hi)\n",
    "        bp = psd[:,:,idx].sum(-1)/tot if idx.size else np.zeros((n_ep,n_ch))\n",
    "        feats[f\"{name}_rel_med\"] = np.median(bp, axis=1)\n",
    "        feats[f\"{name}_rel_iqr\"] = np.subtract(*np.percentile(bp,[75,25],axis=1))\n",
    "        feats[f\"{name}_rel_std\"] = np.std(bp, axis=1)\n",
    "        head = name.split(\"_\",1)[0]\n",
    "        if head in accum: accum[head]+=bp\n",
    "\n",
    "    alpha_med = np.median(accum[\"alpha\"],axis=1); theta_med = np.median(accum[\"theta\"],axis=1); beta_med = np.median(accum[\"beta\"],axis=1)\n",
    "    feats[\"alpha_rel_med\"]=alpha_med; feats[\"theta_rel_med\"]=theta_med; feats[\"beta_rel_med\"]=beta_med\n",
    "    feats[\"theta_over_alpha\"]= theta_med/np.maximum(alpha_med,1e-6)\n",
    "    feats[\"beta_over_alpha\"] = beta_med/np.maximum(alpha_med,1e-6)\n",
    "    feats[\"(alpha+theta)_over_beta\"] = (alpha_med+theta_med)/np.maximum(beta_med,1e-6)\n",
    "\n",
    "    p = psd[:,:,aidx]; p_norm = p/np.maximum(p.sum(-1,keepdims=True),1e-12)\n",
    "    H = -np.sum(p_norm*np.log2(p_norm+1e-12), axis=-1); Hn = H/np.log2(p.shape[-1])\n",
    "    feats[\"spec_entropy_med\"] = np.median(Hn, axis=1); feats[\"spec_entropy_std\"] = np.std(Hn, axis=1)\n",
    "    f = freqs[aidx].reshape(1,1,-1)\n",
    "    centroid = (p*f).sum(-1)/np.maximum(p.sum(-1),1e-12)\n",
    "    feats[\"spec_centroid_med\"]=np.median(centroid,axis=1); feats[\"spec_centroid_std\"]=np.std(centroid,axis=1)\n",
    "\n",
    "    act,mob,comp = hjorth_params(X)\n",
    "    feats[\"hjorth_activity_med\"]=np.median(act,axis=1)\n",
    "    feats[\"hjorth_mobility_med\"]=np.median(mob,axis=1)\n",
    "    feats[\"hjorth_complexity_med\"]=np.median(comp,axis=1)\n",
    "\n",
    "    feats[\"fbCSP_task_proba\"]=np.zeros(n_ep)  # not training hybrid here\n",
    "    return pd.DataFrame(feats)\n",
    "\n",
    "# extract features\n",
    "all_feat, all_meta = [], []\n",
    "for fp in files:\n",
    "    raw = load_raw(fp)\n",
    "    epochs = make_epochs(raw)\n",
    "    if len(epochs)==0: continue\n",
    "    feat = spectral_features(epochs)\n",
    "    starts = epochs.events[:,0]/epochs.info[\"sfreq\"]; ends = starts + EPOCH_LEN\n",
    "    meta = pd.DataFrame({\"file\": fp.name, \"epoch_idx\": np.arange(len(epochs)),\n",
    "                         \"t_start_s\": starts, \"t_end_s\": ends,\n",
    "                         \"sfreq\": epochs.info[\"sfreq\"], \"n_channels\": len(epochs.ch_names)})\n",
    "    all_feat.append(feat); all_meta.append(meta)\n",
    "\n",
    "F = pd.concat(all_feat, axis=0, ignore_index=True)\n",
    "M = pd.concat(all_meta, axis=0, ignore_index=True)\n",
    "F.to_csv(RUN_DIR/\"features.csv\", index=False)\n",
    "M.to_csv(RUN_DIR/\"metadata.csv\", index=False)\n",
    "print(f\"Saved run trio â†’ {RUN_DIR}\")\n",
    "\n",
    "# cluster (pick best K in 3..6)\n",
    "Xs = StandardScaler().fit_transform(F.values)\n",
    "Z  = PCA(n_components=min(20, Xs.shape[1]), random_state=42).fit_transform(Xs)\n",
    "best = {\"sil\":-1,\"k\":None,\"labels\":None}\n",
    "for k in range(3,7):\n",
    "    km = KMeans(n_clusters=k, n_init=\"auto\", random_state=42)\n",
    "    lab = km.fit_predict(Z)\n",
    "    sil = silhouette_score(Z, lab) if k>1 else -1\n",
    "    if sil>best[\"sil\"]: best = {\"sil\":sil,\"k\":k,\"labels\":lab}\n",
    "pd.DataFrame({\"state\": best[\"labels\"]}).to_csv(RUN_DIR/\"state_assignments.csv\", index=False)\n",
    "print(f\"Rebuilt K={best['k']} (sil={best['sil']:.3f})\")\n",
    "\n",
    "# --- 3) Stability audit on the rebuilt run ---\n",
    "L0 = best[\"labels\"]; K = len(np.unique(L0)); N = len(L0)\n",
    "\n",
    "np.random.seed(42)\n",
    "B, FRAC = 100, 0.80\n",
    "boot = []\n",
    "for b in range(B):\n",
    "    idx = np.random.choice(N, size=int(N*FRAC), replace=False)\n",
    "    km_b = KMeans(n_clusters=K, n_init=\"auto\", random_state=1000+b).fit(Z[idx])\n",
    "    Lb   = km_b.labels_\n",
    "    try: sil = silhouette_score(Z[idx], Lb) if K>1 else np.nan\n",
    "    except Exception: sil = np.nan\n",
    "    boot.append({\"bootstrap\": b, \"size\": len(idx),\n",
    "                 \"ari\": ARI(L0[idx], Lb), \"nmi\": NMI(L0[idx], Lb), \"silhouette\": sil})\n",
    "boot_df = pd.DataFrame(boot); boot_df.to_csv(STAB_DIR/\"bootstrap_stability.csv\", index=False)\n",
    "\n",
    "rows = []\n",
    "for s in range(50,70):\n",
    "    km_s = KMeans(n_clusters=K, n_init=\"auto\", random_state=s).fit(Z)\n",
    "    Ls   = km_s.labels_\n",
    "    try: sil = silhouette_score(Z, Ls) if K>1 else np.nan\n",
    "    except Exception: sil = np.nan\n",
    "    rows.append({\"seed\": s, \"ari\": ARI(L0, Ls), \"nmi\": NMI(L0, Ls), \"silhouette\": sil})\n",
    "ms_df = pd.DataFrame(rows); ms_df.to_csv(STAB_DIR/\"multiseed_refit.csv\", index=False)\n",
    "\n",
    "# leave-one-run-out by file (real EDFs only)\n",
    "def is_real(fn): return fn.lower().endswith((\".edf\",\".vhdr\",\".eeg\",\".fif\",\".set\",\".fdt\"))\n",
    "files_list = [f for f in M[\"file\"].unique().tolist() if is_real(f)]\n",
    "lor = []\n",
    "for f in files_list:\n",
    "    te = (M[\"file\"] == f).to_numpy(); tr = ~te\n",
    "    if tr.sum()<K or te.sum()<K: continue\n",
    "    km_tr = KMeans(n_clusters=K, n_init=\"auto\", random_state=2025).fit(Z[tr])\n",
    "    Lte   = km_tr.predict(Z[te])\n",
    "    lor.append({\"file\": f, \"n_train\": int(tr.sum()), \"n_test\": int(te.sum()),\n",
    "                \"ari\": float(ARI(L0[te], Lte))})\n",
    "lor_df = pd.DataFrame(lor); lor_df.to_csv(STAB_DIR/\"leave_one_run_out.csv\", index=False)\n",
    "\n",
    "# summarize & figure\n",
    "def pct(a,q): return float(np.nanpercentile(a,q))\n",
    "summary = {\n",
    "    \"used_dir\": str(RUN_DIR),\n",
    "    \"K\": int(K), \"N_epochs\": int(N),\n",
    "    \"bootstrap_ARI_mean\": float(np.nanmean(boot_df[\"ari\"])),\n",
    "    \"bootstrap_ARI_p10\":  pct(boot_df[\"ari\"],10),\n",
    "    \"bootstrap_ARI_p50\":  pct(boot_df[\"ari\"],50),\n",
    "    \"bootstrap_ARI_p90\":  pct(boot_df[\"ari\"],90),\n",
    "    \"bootstrap_Sil_mean\": float(np.nanmean(boot_df[\"silhouette\"])),\n",
    "    \"multiseed_ARI_mean\": float(np.nanmean(ms_df[\"ari\"])),\n",
    "    \"multiseed_ARI_min\":  float(np.nanmin(ms_df[\"ari\"])),\n",
    "    \"multiseed_ARI_max\":  float(np.nanmax(ms_df[\"ari\"])),\n",
    "    \"LOR_rows\": int(len(lor_df))\n",
    "}\n",
    "with open(STAB_DIR/\"summary.json\",\"w\") as f: json.dump(summary, f, indent=2)\n",
    "print(\"=== Stability Summary ===\"); [print(f\"{k}: {v}\") for k,v in summary.items()]\n",
    "\n",
    "fig = plt.figure(figsize=(12,8), dpi=130)\n",
    "gs  = fig.add_gridspec(2,2, hspace=0.35, wspace=0.30)\n",
    "\n",
    "axA = fig.add_subplot(gs[0,0]); axA.hist(boot_df[\"ari\"], bins=20, alpha=0.85)\n",
    "axA.set_title(\"Bootstrap Stability â€” Adjusted Rand (80% subsamples)\"); axA.set_xlabel(\"ARI\"); axA.set_ylabel(\"Count\")\n",
    "m,p10,p50,p90 = summary[\"bootstrap_ARI_mean\"],summary[\"bootstrap_ARI_p10\"],summary[\"bootstrap_ARI_p50\"],summary[\"bootstrap_ARI_p90\"]\n",
    "axA.axvline(m,color=\"k\",linestyle=\"--\",label=f\"mean={m:.3f}\"); axA.axvline(p50,color=\"gray\",linestyle=\":\",label=f\"median={p50:.3f}\"); axA.legend()\n",
    "\n",
    "axB = fig.add_subplot(gs[0,1]); axB.hist(boot_df[\"silhouette\"].dropna(), bins=20, alpha=0.85, color=\"#8888ff\")\n",
    "axB.set_title(\"Bootstrap Silhouette (refit per sample)\"); axB.set_xlabel(\"Silhouette\"); axB.set_ylabel(\"Count\")\n",
    "\n",
    "axC = fig.add_subplot(gs[1,0]); axC.plot(ms_df[\"seed\"], ms_df[\"ari\"], \"o-\")\n",
    "axC.set_title(\"Multi-seed Refit â€” ARI vs Baseline\"); axC.set_xlabel(\"Random Seed\"); axC.set_ylabel(\"ARI\"); axC.set_ylim(0,1)\n",
    "\n",
    "axD = fig.add_subplot(gs[1,1])\n",
    "if len(lor_df):\n",
    "    x = np.arange(len(lor_df))\n",
    "    axD.bar(x - 0.18, lor_df[\"ari\"].to_numpy(), width=0.36, label=\"ARI (held-out)\")\n",
    "    axD.set_xticks(x); axD.set_xticklabels([Path(f).stem for f in lor_df[\"file\"]], rotation=20)\n",
    "    axD.set_ylim(0,1); axD.set_title(\"Leave-one-run-out (by file)\"); axD.legend()\n",
    "else:\n",
    "    axD.text(0.5,0.5,\"No real runs to cross-validate.\", ha=\"center\", va=\"center\"); axD.axis(\"off\")\n",
    "\n",
    "fig.suptitle(\"CNT Cognitive Alphabet â€” Stability Addendum\", fontsize=16, weight=\"bold\")\n",
    "hdr = f\"K={K} | N={N} | Boot ARI mean={m:.3f} (p10={p10:.3f}, p50={p50:.3f}, p90={p90:.3f}) | Multi-seed ARI mean={summary['multiseed_ARI_mean']:.3f}\"\n",
    "fig.text(0.5, 0.94, hdr + f\" | dir: {RUN_DIR.name}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "ADD_PNG = OUT_DIR/\"CNT_CognitiveAlphabet_Stability_Addendum.png\"\n",
    "ADD_PDF = OUT_DIR/\"CNT_CognitiveAlphabet_Stability_Addendum.pdf\"\n",
    "fig.savefig(ADD_PNG, bbox_inches=\"tight\"); fig.savefig(ADD_PDF, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "print(\"\\nSaved addendum â†’\", ADD_PNG)\n",
    "print(\"Saved addendum â†’\", ADD_PDF)\n",
    "print(\"CSV outputs â†’\", STAB_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "679a5c8f-ac05-4468-89d7-6cb0067d368e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\3363335502.py:58: RuntimeWarning: DigMontage is only a subset of info. There are 64 channel positions not present in the DigMontage. The channels missing from the montage are:\n",
      "\n",
      "['Fc5.', 'Fc3.', 'Fc1.', 'Fcz.', 'Fc2.', 'Fc4.', 'Fc6.', 'C5..', 'C3..', 'C1..', 'Cz..', 'C2..', 'C4..', 'C6..', 'Cp5.', 'Cp3.', 'Cp1.', 'Cpz.', 'Cp2.', 'Cp4.', 'Cp6.', 'Fp1.', 'Fpz.', 'Fp2.', 'Af7.', 'Af3.', 'Afz.', 'Af4.', 'Af8.', 'F7..', 'F5..', 'F3..', 'F1..', 'Fz..', 'F2..', 'F4..', 'F6..', 'F8..', 'Ft7.', 'Ft8.', 'T7..', 'T8..', 'T9..', 'T10.', 'Tp7.', 'Tp8.', 'P7..', 'P5..', 'P3..', 'P1..', 'Pz..', 'P2..', 'P4..', 'P6..', 'P8..', 'Po7.', 'Po3.', 'Poz.', 'Po4.', 'Po8.', 'O1..', 'Oz..', 'O2..', 'Iz..'].\n",
      "\n",
      "Consider using inst.rename_channels to match the montage nomenclature, or inst.set_channel_types if these are not EEG channels, or use the on_missing parameter if the channel positions are allowed to be unknown in your analyses.\n",
      "  raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\3363335502.py:58: RuntimeWarning: DigMontage is only a subset of info. There are 64 channel positions not present in the DigMontage. The channels missing from the montage are:\n",
      "\n",
      "['Fc5.', 'Fc3.', 'Fc1.', 'Fcz.', 'Fc2.', 'Fc4.', 'Fc6.', 'C5..', 'C3..', 'C1..', 'Cz..', 'C2..', 'C4..', 'C6..', 'Cp5.', 'Cp3.', 'Cp1.', 'Cpz.', 'Cp2.', 'Cp4.', 'Cp6.', 'Fp1.', 'Fpz.', 'Fp2.', 'Af7.', 'Af3.', 'Afz.', 'Af4.', 'Af8.', 'F7..', 'F5..', 'F3..', 'F1..', 'Fz..', 'F2..', 'F4..', 'F6..', 'F8..', 'Ft7.', 'Ft8.', 'T7..', 'T8..', 'T9..', 'T10.', 'Tp7.', 'Tp8.', 'P7..', 'P5..', 'P3..', 'P1..', 'Pz..', 'P2..', 'P4..', 'P6..', 'P8..', 'Po7.', 'Po3.', 'Poz.', 'Po4.', 'Po8.', 'O1..', 'Oz..', 'O2..', 'Iz..'].\n",
      "\n",
      "Consider using inst.rename_channels to match the montage nomenclature, or inst.set_channel_types if these are not EEG channels, or use the on_missing parameter if the channel positions are allowed to be unknown in your analyses.\n",
      "  raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\3363335502.py:58: RuntimeWarning: DigMontage is only a subset of info. There are 64 channel positions not present in the DigMontage. The channels missing from the montage are:\n",
      "\n",
      "['Fc5.', 'Fc3.', 'Fc1.', 'Fcz.', 'Fc2.', 'Fc4.', 'Fc6.', 'C5..', 'C3..', 'C1..', 'Cz..', 'C2..', 'C4..', 'C6..', 'Cp5.', 'Cp3.', 'Cp1.', 'Cpz.', 'Cp2.', 'Cp4.', 'Cp6.', 'Fp1.', 'Fpz.', 'Fp2.', 'Af7.', 'Af3.', 'Afz.', 'Af4.', 'Af8.', 'F7..', 'F5..', 'F3..', 'F1..', 'Fz..', 'F2..', 'F4..', 'F6..', 'F8..', 'Ft7.', 'Ft8.', 'T7..', 'T8..', 'T9..', 'T10.', 'Tp7.', 'Tp8.', 'P7..', 'P5..', 'P3..', 'P1..', 'Pz..', 'P2..', 'P4..', 'P6..', 'P8..', 'Po7.', 'Po3.', 'Poz.', 'Po4.', 'Po8.', 'O1..', 'Oz..', 'O2..', 'Iz..'].\n",
      "\n",
      "Consider using inst.rename_channels to match the montage nomenclature, or inst.set_channel_types if these are not EEG channels, or use the on_missing parameter if the channel positions are allowed to be unknown in your analyses.\n",
      "  raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\3363335502.py:58: RuntimeWarning: DigMontage is only a subset of info. There are 64 channel positions not present in the DigMontage. The channels missing from the montage are:\n",
      "\n",
      "['Fc5.', 'Fc3.', 'Fc1.', 'Fcz.', 'Fc2.', 'Fc4.', 'Fc6.', 'C5..', 'C3..', 'C1..', 'Cz..', 'C2..', 'C4..', 'C6..', 'Cp5.', 'Cp3.', 'Cp1.', 'Cpz.', 'Cp2.', 'Cp4.', 'Cp6.', 'Fp1.', 'Fpz.', 'Fp2.', 'Af7.', 'Af3.', 'Afz.', 'Af4.', 'Af8.', 'F7..', 'F5..', 'F3..', 'F1..', 'Fz..', 'F2..', 'F4..', 'F6..', 'F8..', 'Ft7.', 'Ft8.', 'T7..', 'T8..', 'T9..', 'T10.', 'Tp7.', 'Tp8.', 'P7..', 'P5..', 'P3..', 'P1..', 'Pz..', 'P2..', 'P4..', 'P6..', 'P8..', 'Po7.', 'Po3.', 'Poz.', 'Po4.', 'Po8.', 'O1..', 'Oz..', 'O2..', 'Iz..'].\n",
      "\n",
      "Consider using inst.rename_channels to match the montage nomenclature, or inst.set_channel_types if these are not EEG channels, or use the on_missing parameter if the channel positions are allowed to be unknown in your analyses.\n",
      "  raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank='info'\n",
      "    data: rank 3 after 0 projectors applied to 3 channels\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "[hybrid] features/metadata â†’ E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\n",
      "[hybrid] K selected: 4 (sil=0.196)\n",
      "[hybrid] artifacts saved â†’ E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\n",
      "\n",
      "=== HYBRID Stability Summary ===\n",
      "used_dir: E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\n",
      "K: 4\n",
      "N_epochs: 485\n",
      "bootstrap_ARI_mean: 0.8029493373109611\n",
      "bootstrap_ARI_p10: 0.430928221991088\n",
      "bootstrap_ARI_p50: 0.9161151251100051\n",
      "bootstrap_ARI_p90: 0.9754133264005569\n",
      "bootstrap_Sil_mean: 0.19287847062457097\n",
      "multiseed_ARI_mean: 0.7842030901861337\n",
      "multiseed_ARI_min: 0.38284454173609966\n",
      "multiseed_ARI_max: 1.0\n",
      "LOR_rows: 3\n",
      "\n",
      "[v0]   K=3  silhouette=0.254\n",
      "[hybr] K=4  silhouette=0.196 (HMM=off)\n",
      "\n",
      "Saved HYBRID addendum â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\CNT_CognitiveAlphabet_Stability_Addendum_HYBRID.png\n",
      "Saved HYBRID addendum â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\CNT_CognitiveAlphabet_Stability_Addendum_HYBRID.pdf\n",
      "CSV outputs â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\stability\n",
      "\n",
      "Artifacts â†’ E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\n"
     ]
    }
   ],
   "source": [
    "# === CNT Cognitive Alphabet â€” HYBRID UPGRADE + STABILITY (one cell) ===\n",
    "# Converts your rebuilt run â†’ hybrid (Î²-split, Î³â†’80 Hz, fbCSP task-proba), reclusters (K=4..6),\n",
    "# saves artifacts, and runs a stability audit with a v0 vs hybrid summary.\n",
    "\n",
    "import os, re, json, math, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# ---------- Paths (edit if needed) ----------\n",
    "RUN_OLD = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_rebuilt\")          # rebuilt v0 you just made\n",
    "RAW_DIR = RUN_OLD / \"brainwaves_rebuilt\"                           # the 3 EDFs we fetched\n",
    "RUN_NEW = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")         # NEW hybrid run folder\n",
    "OUT_REP = RUN_NEW.parent / \"cog_alphabet_report_hybrid_v1\"\n",
    "STAB_DIR = OUT_REP / \"stability\"\n",
    "for p in [RUN_NEW, OUT_REP, STAB_DIR]: p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Imports (already installed in your env) ----------\n",
    "import mne\n",
    "from scipy.signal import welch\n",
    "from joblib import dump\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score as ARI, normalized_mutual_info_score as NMI\n",
    "try:\n",
    "    from hmmlearn.hmm import GaussianHMM\n",
    "    HAS_HMM = True\n",
    "except Exception:\n",
    "    HAS_HMM = False\n",
    "\n",
    "# ---------- Config (hybrid) ----------\n",
    "TARGET_SF = 250.0\n",
    "L_FREQ, H_FREQ = 0.5, 80.0\n",
    "EPOCH_LEN, STEP = 2.0, 0.5\n",
    "BANDS = {\n",
    "    \"delta\": (1.0, 4.0),\n",
    "    \"theta\": (4.0, 8.0),\n",
    "    \"alpha\": (8.0, 13.0),\n",
    "    \"beta_low\":  (13.0, 20.0),\n",
    "    \"beta_high\": (20.0, 35.0),\n",
    "    \"gamma\":     (35.0, 80.0),\n",
    "}\n",
    "K_RANGE = (4, 6)             # finer alphabet\n",
    "USE_HMM = True               # try HMM smoothing; auto-skips if hmmlearn not present\n",
    "RS = 42\n",
    "\n",
    "# ---------- IO helpers ----------\n",
    "def load_raw_edf(p: Path):\n",
    "    raw = mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "    # resample FIRST â†’ safe Nyquist\n",
    "    if abs(raw.info[\"sfreq\"] - TARGET_SF) > 1e-6:\n",
    "        raw.resample(TARGET_SF, npad=\"auto\", verbose=False)\n",
    "    ny = raw.info[\"sfreq\"]/2.0\n",
    "    raw.filter(L_FREQ, min(H_FREQ, ny-1.0), verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "        try:\n",
    "            raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"),\n",
    "                            match_case=False, match_alias=True, on_missing=\"warn\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return raw\n",
    "\n",
    "def make_epochs(raw):\n",
    "    ov = max(0.0, EPOCH_LEN - STEP)\n",
    "    return mne.make_fixed_length_epochs(raw, duration=EPOCH_LEN, overlap=ov, preload=True, verbose=False)\n",
    "\n",
    "# ---------- Feature extraction (hybrid bands + composites) ----------\n",
    "def hjorth_params(x):\n",
    "    d1 = np.diff(x, axis=-1)\n",
    "    var0 = np.var(x, axis=-1) + 1e-12\n",
    "    var1 = np.var(d1, axis=-1) + 1e-12\n",
    "    mob  = np.sqrt(var1/var0)\n",
    "    d2 = np.diff(d1, axis=-1)\n",
    "    var2 = np.var(d2, axis=-1) + 1e-12\n",
    "    comp = np.sqrt((var2/var1)/(var1/var0))\n",
    "    return var0, mob, comp\n",
    "\n",
    "def spectral_features(epochs):\n",
    "    X = epochs.get_data()  # (n_ep, n_ch, n_t)\n",
    "    n_ep, n_ch, n_t = X.shape\n",
    "    sf = epochs.info[\"sfreq\"]\n",
    "    nper = min(int(sf*2), n_t); nov = nper//2\n",
    "    freqs, psd = welch(X, fs=sf, nperseg=nper, noverlap=nov, axis=-1, average=\"median\")\n",
    "    def bidx(lo,hi): return np.where((freqs>=lo)&(freqs<hi))[0]\n",
    "    aidx = bidx(L_FREQ, H_FREQ)\n",
    "    tot  = np.maximum(psd[:,:,aidx].sum(-1), 1e-12)\n",
    "\n",
    "    feats = {}\n",
    "    # per-band\n",
    "    accum = {\"alpha\":np.zeros((n_ep,n_ch)), \"theta\":np.zeros((n_ep,n_ch)), \"beta\":np.zeros((n_ep,n_ch))}\n",
    "    for name,(lo,hi) in BANDS.items():\n",
    "        idx = bidx(lo,hi)\n",
    "        bp = psd[:,:,idx].sum(-1)/tot if idx.size else np.zeros((n_ep,n_ch))\n",
    "        feats[f\"{name}_rel_med\"] = np.median(bp, axis=1)\n",
    "        feats[f\"{name}_rel_iqr\"] = np.subtract(*np.percentile(bp,[75,25],axis=1))\n",
    "        feats[f\"{name}_rel_std\"] = np.std(bp, axis=1)\n",
    "        head = name.split(\"_\",1)[0]\n",
    "        if head in accum: accum[head] += bp\n",
    "\n",
    "    # composites for ratios\n",
    "    alpha_med = np.median(accum[\"alpha\"], axis=1)\n",
    "    theta_med = np.median(accum[\"theta\"], axis=1)\n",
    "    beta_med  = np.median(accum[\"beta\"],  axis=1)\n",
    "    feats[\"alpha_rel_med\"] = alpha_med\n",
    "    feats[\"theta_rel_med\"] = theta_med\n",
    "    feats[\"beta_rel_med\"]  = beta_med\n",
    "    feats[\"theta_over_alpha\"] = theta_med / np.maximum(alpha_med,1e-6)\n",
    "    feats[\"beta_over_alpha\"]  = beta_med  / np.maximum(alpha_med,1e-6)\n",
    "    feats[\"(alpha+theta)_over_beta\"] = (alpha_med + theta_med) / np.maximum(beta_med,1e-6)\n",
    "\n",
    "    # spectral entropy & centroid\n",
    "    p = psd[:,:,aidx]; p_norm = p/np.maximum(p.sum(-1,keepdims=True),1e-12)\n",
    "    H = -np.sum(p_norm*np.log2(p_norm+1e-12), axis=-1); Hn = H/np.log2(p.shape[-1])\n",
    "    feats[\"spec_entropy_med\"] = np.median(Hn, axis=1)\n",
    "    feats[\"spec_entropy_std\"] = np.std(Hn, axis=1)\n",
    "    f = freqs[aidx].reshape(1,1,-1)\n",
    "    centroid = (p*f).sum(-1)/np.maximum(p.sum(-1),1e-12)\n",
    "    feats[\"spec_centroid_med\"] = np.median(centroid, axis=1)\n",
    "    feats[\"spec_centroid_std\"] = np.std(centroid, axis=1)\n",
    "\n",
    "    # Hjorth time-domain\n",
    "    act, mob, comp = hjorth_params(X)\n",
    "    feats[\"hjorth_activity_med\"]   = np.median(act, axis=1)\n",
    "    feats[\"hjorth_mobility_med\"]   = np.median(mob, axis=1)\n",
    "    feats[\"hjorth_complexity_med\"] = np.median(comp, axis=1)\n",
    "\n",
    "    return pd.DataFrame(feats)\n",
    "\n",
    "# ---------- fbCSP task-proba on R03 (C3/Cz/C4 preferred; falls back to all EEG) ----------\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from mne.decoding import CSP\n",
    "\n",
    "def bandpass_array(X, lo, hi, sf):\n",
    "    n_ep, n_ch, n_t = X.shape\n",
    "    X2 = X.reshape(n_ep*n_ch, n_t)\n",
    "    Xf2 = mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
    "    return Xf2.reshape(n_ep, n_ch, n_t)\n",
    "\n",
    "def fbCSP_task_proba_for_file(edf_path: Path, starts, ends):\n",
    "    raw = load_raw_edf(edf_path)\n",
    "    sf  = raw.info[\"sfreq\"]\n",
    "    # pick ROI\n",
    "    chs = raw.ch_names\n",
    "    roi = []\n",
    "    for target in [\"C3\",\"Cz\",\"C4\"]:\n",
    "        for c in chs:\n",
    "            if re.fullmatch(target, c, re.I) or re.search(rf\"^{target}\\b\", c, re.I):\n",
    "                roi.append(c)\n",
    "    if len(roi)<2:\n",
    "        roi = [c for c,t in zip(raw.ch_names, raw.get_channel_types()) if t==\"eeg\"]\n",
    "    raw.pick(roi)\n",
    "    # epochs aligned to provided starts/ends\n",
    "    X_list = []\n",
    "    for t0,t1 in zip(starts, ends):\n",
    "        s = int(round(t0*sf)); e = int(round(t1*sf))\n",
    "        X_list.append(raw.get_data(start=s, stop=e))\n",
    "    X = np.stack(X_list, axis=0)  # (n_ep, n_ch, n_t)\n",
    "\n",
    "    # label via annotations with â‰¥50% overlap (task=T1/T2; rest=T0), build groups by segment id\n",
    "    anns = getattr(raw, \"annotations\", None)\n",
    "    y = []; groups = []\n",
    "    for i,(t0,t1) in enumerate(zip(starts, ends)):\n",
    "        if anns is None or len(anns)==0: y.append(-1); groups.append(-1); continue\n",
    "        ov_task = ov_rest = 0.0; best_id = -1; best_ov = 0.0; lab = -1\n",
    "        for j,(o,d,s) in enumerate(zip(anns.onset, anns.duration, anns.description)):\n",
    "            su = str(s).upper()\n",
    "            if \"T0\" in su or \"T1\" in su or \"T2\" in su:\n",
    "                L, R = max(t0,float(o)), min(t1,float(o)+float(d))\n",
    "                if R > L:\n",
    "                    ov = R-L\n",
    "                    if ov > best_ov:\n",
    "                        best_ov = ov; best_id = j\n",
    "                        if \"T1\" in su or \"T2\" in su: lab = 1\n",
    "                        elif \"T0\" in su: lab = 0\n",
    "        if best_ov >= 0.5*(t1-t0): y.append(lab); groups.append(best_id)\n",
    "        else: y.append(-1); groups.append(-1)\n",
    "    y = np.array(y); groups = np.array(groups)\n",
    "\n",
    "    # If no labels â†’ zeros\n",
    "    if not np.any(y>=0):\n",
    "        return np.zeros(len(starts))\n",
    "\n",
    "    # Filter-bank CSP with grouped CV (5x) â†’ out-of-fold P(task)\n",
    "    bands = [(8,13),(13,20),(20,30)]\n",
    "    mask = y>=0\n",
    "    yt = y[mask].astype(int)\n",
    "    Xt = X[mask]\n",
    "    gt = groups[mask]\n",
    "    gkf = GroupKFold(n_splits=min(5, max(2, len(np.unique(gt[gt>=0])))))  # robust folds\n",
    "\n",
    "    proba = np.zeros(len(yt), dtype=float)\n",
    "    for tr, te in gkf.split(Xt, yt, gt):\n",
    "        feats_tr, feats_te = [], []\n",
    "        for lo,hi in bands:\n",
    "            Xtr = bandpass_array(Xt[tr], lo, hi, sf)\n",
    "            Xte = bandpass_array(Xt[te], lo, hi, sf)\n",
    "            csp = CSP(n_components=6, reg=\"ledoit_wolf\", log=True, norm_trace=False, rank=\"info\")\n",
    "            csp.fit(Xtr, yt[tr])\n",
    "            feats_tr.append(csp.transform(Xtr))\n",
    "            feats_te.append(csp.transform(Xte))\n",
    "        Xtr_fb = np.concatenate(feats_tr, axis=1)\n",
    "        Xte_fb = np.concatenate(feats_te, axis=1)\n",
    "        clf = LDA(solver=\"lsqr\", shrinkage=\"auto\")\n",
    "        clf.fit(Xtr_fb, yt[tr])\n",
    "        proba[te] = clf.predict_proba(Xte_fb)[:,1]\n",
    "\n",
    "    # Return on full epoch list, zeros for unlabeled\n",
    "    full = np.zeros(len(starts), dtype=float)\n",
    "    full[np.where(mask)[0]] = proba\n",
    "    return full\n",
    "\n",
    "# ---------- Build NEW hybrid features/metadata from RAW_DIR ----------\n",
    "all_feat, all_meta = [], []\n",
    "edf_files = sorted([p for p in RAW_DIR.glob(\"*.edf\")])\n",
    "if not edf_files:\n",
    "    raise FileNotFoundError(f\"No EDF files found in {RAW_DIR}\")\n",
    "\n",
    "for p in edf_files:\n",
    "    raw = load_raw_edf(p)\n",
    "    epochs = make_epochs(raw)\n",
    "    if len(epochs)==0: continue\n",
    "    feat = spectral_features(epochs)\n",
    "    starts = epochs.events[:,0]/epochs.info[\"sfreq\"]; ends = starts + EPOCH_LEN\n",
    "\n",
    "    # fbCSP only for R03; zeros otherwise\n",
    "    if re.search(r\"R0*3\", p.name, re.I):\n",
    "        try:\n",
    "            fb = fbCSP_task_proba_for_file(p, starts, ends)\n",
    "        except Exception:\n",
    "            fb = np.zeros(len(starts))\n",
    "    else:\n",
    "        fb = np.zeros(len(starts))\n",
    "    feat[\"fbCSP_task_proba\"] = fb\n",
    "\n",
    "    meta = pd.DataFrame({\n",
    "        \"file\": p.name,\n",
    "        \"epoch_idx\": np.arange(len(epochs)),\n",
    "        \"t_start_s\": starts, \"t_end_s\": ends,\n",
    "        \"sfreq\": epochs.info[\"sfreq\"],\n",
    "        \"n_channels\": [len(epochs.ch_names)]*len(epochs)\n",
    "    })\n",
    "    all_feat.append(feat); all_meta.append(meta)\n",
    "\n",
    "Fh = pd.concat(all_feat, axis=0, ignore_index=True)\n",
    "Mh = pd.concat(all_meta, axis=0, ignore_index=True)\n",
    "Fh.to_csv(RUN_NEW/\"features.csv\", index=False)\n",
    "Mh.to_csv(RUN_NEW/\"metadata.csv\", index=False)\n",
    "print(f\"[hybrid] features/metadata â†’ {RUN_NEW}\")\n",
    "\n",
    "# ---------- Cluster (select K by silhouette), optional HMM smoothing ----------\n",
    "scaler = StandardScaler().fit(Fh.values)\n",
    "Xs = scaler.transform(Fh.values)\n",
    "pca  = PCA(n_components=min(20, Xs.shape[1]), random_state=RS).fit(Xs)\n",
    "Z    = pca.transform(Xs)\n",
    "\n",
    "best = {\"k\":None, \"sil\":-1.0, \"km\":None, \"lab\":None}\n",
    "for k in range(K_RANGE[0], K_RANGE[1]+1):\n",
    "    km = KMeans(n_clusters=k, n_init=\"auto\", random_state=RS)\n",
    "    lab = km.fit_predict(Z)\n",
    "    sil = silhouette_score(Z, lab) if k>1 else -1.0\n",
    "    if sil > best[\"sil\"]:\n",
    "        best = {\"k\":k, \"sil\":sil, \"km\":km, \"lab\":lab}\n",
    "\n",
    "labels = best[\"lab\"]\n",
    "K_new  = best[\"k\"]\n",
    "print(f\"[hybrid] K selected: {K_new} (sil={best['sil']:.3f})\")\n",
    "\n",
    "# Optional HMM smoothing on Z â†’ decode sequence & map to KMeans labels by max overlap\n",
    "if USE_HMM and HAS_HMM and K_new>1:\n",
    "    try:\n",
    "        hmm = GaussianHMM(n_components=K_new, covariance_type=\"diag\", random_state=RS, n_iter=200)\n",
    "        hmm.fit(Z)\n",
    "        z_lab = hmm.predict(Z)\n",
    "        # map HMM states to KMeans labels by majority vote\n",
    "        from scipy.stats import mode\n",
    "        mapping = {}\n",
    "        for s in range(K_new):\n",
    "            if np.sum(z_lab==s)==0: continue\n",
    "            km_label = mode(labels[z_lab==s], keepdims=True).mode[0]\n",
    "            mapping[s] = int(km_label)\n",
    "        labels_sm = np.array([mapping.get(s, s) for s in z_lab])\n",
    "        labels = labels_sm\n",
    "        print(\"[hybrid] HMM smoothing applied.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[hybrid] HMM failed: {e} (skipping)\")\n",
    "\n",
    "pd.DataFrame({\"state\": labels}).to_csv(RUN_NEW/\"state_assignments.csv\", index=False)\n",
    "dump(scaler, RUN_NEW/\"scaler_hybrid.joblib\")\n",
    "dump(pca,    RUN_NEW/\"pca_hybrid.joblib\")\n",
    "dump(best[\"km\"], RUN_NEW/\"kmeans_hybrid.joblib\")\n",
    "\n",
    "# ---------- Per-state signatures + JSON map ----------\n",
    "Fsig = Fh.copy(); Fsig[\"state\"] = labels\n",
    "med = Fsig.groupby(\"state\").median(numeric_only=True).sort_index()\n",
    "z = (med - Fh.median())/(Fh.std()+1e-9)\n",
    "alpha_map = {int(s): {\"name\": f\"State-{int(s)}\",\n",
    "                      \"top_features\": z.loc[s].abs().sort_values(ascending=False).head(8).index.tolist()}\n",
    "             for s in med.index}\n",
    "with open(RUN_NEW/\"cognitive_alphabet.json\", \"w\") as f:\n",
    "    json.dump(alpha_map, f, indent=2)\n",
    "\n",
    "# ---------- Quick plots (embedding + signature heatmap) ----------\n",
    "p2 = PCA(n_components=2, random_state=RS).fit(Z); E2 = p2.transform(Z)\n",
    "plt.figure(figsize=(7.2,6.0)); plt.scatter(E2[:,0], E2[:,1], c=labels, s=8)\n",
    "plt.title(f\"Hybrid Alphabet (K={K_new})\"); plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\"); plt.tight_layout()\n",
    "plt.savefig(RUN_NEW/\"embedding.png\", dpi=160); plt.close()\n",
    "\n",
    "Mheat = (med - Fh.median())/(Fh.std()+1e-9)\n",
    "plt.figure(figsize=(min(14, 2+0.5*Mheat.shape[1]), 0.6+0.3*K_new+2))\n",
    "im = plt.imshow(Mheat.values, aspect=\"auto\"); plt.colorbar(im, fraction=0.025, pad=0.03, label=\"z vs overall\")\n",
    "plt.yticks(range(Mheat.shape[0]), [f\"S{int(s)}\" for s in med.index]); plt.xticks(range(Mheat.shape[1]), Mheat.columns, rotation=90)\n",
    "plt.title(\"State Feature Signatures (Hybrid)\"); plt.tight_layout()\n",
    "plt.savefig(RUN_NEW/\"state_feature_signatures.png\", dpi=160); plt.close()\n",
    "print(f\"[hybrid] artifacts saved â†’ {RUN_NEW}\")\n",
    "\n",
    "# ---------- Stability audit on HYBRID ----------\n",
    "Znew = Z; Lnew = labels; N = len(Lnew); K = len(np.unique(Lnew))\n",
    "np.random.seed(42)\n",
    "\n",
    "# Bootstraps (80%)\n",
    "B, FRAC = 100, 0.80\n",
    "boot = []\n",
    "for b in range(B):\n",
    "    idx = np.random.choice(N, size=int(N*FRAC), replace=False)\n",
    "    km_b = KMeans(n_clusters=K, n_init=\"auto\", random_state=1000+b).fit(Znew[idx])\n",
    "    Lb   = km_b.labels_\n",
    "    try: sil = silhouette_score(Znew[idx], Lb) if K>1 else np.nan\n",
    "    except Exception: sil = np.nan\n",
    "    boot.append({\"bootstrap\": b, \"size\": len(idx),\n",
    "                 \"ari\": ARI(Lnew[idx], Lb), \"nmi\": NMI(Lnew[idx], Lb), \"silhouette\": sil})\n",
    "boot_df = pd.DataFrame(boot); boot_df.to_csv(STAB_DIR/\"bootstrap_stability_hybrid.csv\", index=False)\n",
    "\n",
    "# Multi-seed (full set)\n",
    "rows = []\n",
    "for s in range(50,70):\n",
    "    km_s = KMeans(n_clusters=K, n_init=\"auto\", random_state=s).fit(Znew)\n",
    "    Ls   = km_s.labels_\n",
    "    try: sil = silhouette_score(Znew, Ls) if K>1 else np.nan\n",
    "    except Exception: sil = np.nan\n",
    "    rows.append({\"seed\": s, \"ari\": ARI(Lnew, Ls), \"nmi\": NMI(Lnew, Ls), \"silhouette\": sil})\n",
    "ms_df = pd.DataFrame(rows); ms_df.to_csv(STAB_DIR/\"multiseed_refit_hybrid.csv\", index=False)\n",
    "\n",
    "# Leave-one-run-out (by file â†’ real EDFs)\n",
    "def is_real(fn): return (fn or \"\").lower().endswith((\".edf\",\".vhdr\",\".eeg\",\".fif\",\".set\",\".fdt\"))\n",
    "files = [f for f in Mh[\"file\"].unique().tolist() if is_real(f)]\n",
    "lor = []\n",
    "for f in files:\n",
    "    te = (Mh[\"file\"] == f).to_numpy(); tr = ~te\n",
    "    if tr.sum()<K or te.sum()<K: continue\n",
    "    km_tr = KMeans(n_clusters=K, n_init=\"auto\", random_state=2025).fit(Znew[tr])\n",
    "    Lte   = km_tr.predict(Znew[te])\n",
    "    lor.append({\"file\": f, \"n_train\": int(tr.sum()), \"n_test\": int(te.sum()),\n",
    "                \"ari\": float(ARI(Lnew[te], Lte))})\n",
    "lor_df = pd.DataFrame(lor); lor_df.to_csv(STAB_DIR/\"leave_one_run_out_hybrid.csv\", index=False)\n",
    "\n",
    "# Summaries\n",
    "def pct(a,q): return float(np.nanpercentile(a,q))\n",
    "boot_ari = boot_df[\"ari\"].to_numpy()\n",
    "boot_sil = boot_df[\"silhouette\"].to_numpy()\n",
    "ms_ari   = ms_df[\"ari\"].to_numpy()\n",
    "summary = {\n",
    "    \"used_dir\": str(RUN_NEW),\n",
    "    \"K\": int(K),\n",
    "    \"N_epochs\": int(N),\n",
    "    \"bootstrap_ARI_mean\": float(np.nanmean(boot_ari)),\n",
    "    \"bootstrap_ARI_p10\":  pct(boot_ari,10),\n",
    "    \"bootstrap_ARI_p50\":  pct(boot_ari,50),\n",
    "    \"bootstrap_ARI_p90\":  pct(boot_ari,90),\n",
    "    \"bootstrap_Sil_mean\": float(np.nanmean(boot_sil)),\n",
    "    \"multiseed_ARI_mean\": float(np.nanmean(ms_ari)),\n",
    "    \"multiseed_ARI_min\":  float(np.nanmin(ms_ari)),\n",
    "    \"multiseed_ARI_max\":  float(np.nanmax(ms_ari)),\n",
    "    \"LOR_rows\": int(len(lor_df))\n",
    "}\n",
    "with open(STAB_DIR/\"summary_hybrid.json\",\"w\") as f: json.dump(summary,f,indent=2)\n",
    "\n",
    "print(\"\\n=== HYBRID Stability Summary ===\")\n",
    "for k,v in summary.items(): print(f\"{k}: {v}\")\n",
    "\n",
    "# ---------- v0 vs HYBRID quick compare ----------\n",
    "# Load v0 (old) labels & compute silhouette on its Z (recompute from old features)\n",
    "Fold = pd.read_csv(RUN_OLD/\"features.csv\")\n",
    "Lold = pd.read_csv(RUN_OLD/\"state_assignments.csv\")[\"state\"].to_numpy()\n",
    "Xs0 = StandardScaler().fit_transform(Fold.values); Z0 = PCA(n_components=min(20, Xs0.shape[1]), random_state=RS).fit_transform(Xs0)\n",
    "sil_old = float(silhouette_score(Z0, Lold)) if len(np.unique(Lold))>1 else float('nan')\n",
    "print(f\"\\n[v0]   K={len(np.unique(Lold))}  silhouette={sil_old:.3f}\")\n",
    "print(f\"[hybr] K={K}  silhouette={best['sil']:.3f} (HMM={'on' if (USE_HMM and HAS_HMM) else 'off'})\")\n",
    "\n",
    "# ---------- Addendum figure (hybrid) ----------\n",
    "fig = plt.figure(figsize=(12,8), dpi=130)\n",
    "gs  = fig.add_gridspec(2,2, hspace=0.35, wspace=0.30)\n",
    "\n",
    "axA = fig.add_subplot(gs[0,0])\n",
    "axA.hist(boot_df[\"ari\"], bins=20, alpha=0.85)\n",
    "axA.set_title(\"Bootstrap Stability â€” Adjusted Rand (Hybrid, 80% subsamples)\")\n",
    "axA.set_xlabel(\"ARI\"); axA.set_ylabel(\"Count\")\n",
    "m, p10, p50, p90 = summary[\"bootstrap_ARI_mean\"], summary[\"bootstrap_ARI_p10\"], summary[\"bootstrap_ARI_p50\"], summary[\"bootstrap_ARI_p90\"]\n",
    "axA.axvline(m, color=\"k\", linestyle=\"--\", label=f\"mean={m:.3f}\")\n",
    "axA.axvline(p50, color=\"gray\", linestyle=\":\",  label=f\"median={p50:.3f}\")\n",
    "axA.legend()\n",
    "\n",
    "axB = fig.add_subplot(gs[0,1])\n",
    "axB.hist(boot_df[\"silhouette\"].dropna(), bins=20, alpha=0.85, color=\"#8888ff\")\n",
    "axB.set_title(\"Bootstrap Silhouette (Hybrid)\")\n",
    "axB.set_xlabel(\"Silhouette\"); axB.set_ylabel(\"Count\")\n",
    "\n",
    "axC = fig.add_subplot(gs[1,0])\n",
    "axC.plot(ms_df[\"seed\"], ms_df[\"ari\"], \"o-\")\n",
    "axC.set_title(\"Multi-seed Refit â€” ARI vs Baseline (Hybrid)\")\n",
    "axC.set_xlabel(\"Random Seed\"); axC.set_ylabel(\"ARI\"); axC.set_ylim(0,1)\n",
    "\n",
    "axD = fig.add_subplot(gs[1,1])\n",
    "if len(lor_df):\n",
    "    x = np.arange(len(lor_df))\n",
    "    axD.bar(x - 0.18, lor_df[\"ari\"].to_numpy(), width=0.36, label=\"ARI (held-out)\")\n",
    "    axD.set_xticks(x); axD.set_xticklabels([Path(f).stem for f in lor_df[\"file\"]], rotation=20)\n",
    "    axD.set_ylim(0,1); axD.set_title(\"Leave-one-run-out (Hybrid)\"); axD.legend()\n",
    "else:\n",
    "    axD.text(0.5,0.5,\"No real runs to cross-validate.\", ha=\"center\", va=\"center\"); axD.axis(\"off\")\n",
    "\n",
    "fig.suptitle(\"CNT Cognitive Alphabet â€” HYBRID Stability Addendum\", fontsize=16, weight=\"bold\")\n",
    "hdr = f\"K={K} | N={N} | Boot ARI mean={m:.3f} (p10={p10:.3f}, p50={p50:.3f}, p90={p90:.3f})\"\n",
    "fig.text(0.5, 0.94, hdr + f\" | dir: {RUN_NEW.name}\", ha=\"center\", fontsize=10)\n",
    "\n",
    "ADD_PNG = OUT_REP/\"CNT_CognitiveAlphabet_Stability_Addendum_HYBRID.png\"\n",
    "ADD_PDF = OUT_REP/\"CNT_CognitiveAlphabet_Stability_Addendum_HYBRID.pdf\"\n",
    "fig.savefig(ADD_PNG, bbox_inches=\"tight\"); fig.savefig(ADD_PDF, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "print(\"\\nSaved HYBRID addendum â†’\", ADD_PNG)\n",
    "print(\"Saved HYBRID addendum â†’\", ADD_PDF)\n",
    "print(\"CSV outputs â†’\", STAB_DIR)\n",
    "print(\"\\nArtifacts â†’\", RUN_NEW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e51a3c51-e454-4b03-b3c2-50820841fd1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hmmlearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load, dump\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhmmlearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhmm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GaussianHMM\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mode\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m silhouette_score, adjusted_rand_score \u001b[38;5;28;01mas\u001b[39;00m ARI\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'hmmlearn'"
     ]
    }
   ],
   "source": [
    "# === Sticky-HMM smoothing for the hybrid run (saves new labels + summary) ===\n",
    "import os, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from joblib import load, dump\n",
    "from hmmlearn.hmm import GaussianHMM\n",
    "from scipy.stats import mode\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score as ARI\n",
    "\n",
    "RUN = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")\n",
    "OUT = RUN.parent / \"cog_alphabet_report_hybrid_v1\" / \"stability\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load features, models, and current labels\n",
    "F = pd.read_csv(RUN/\"features.csv\")\n",
    "L = pd.read_csv(RUN/\"state_assignments.csv\")[\"state\"].to_numpy()\n",
    "scaler = load(RUN/\"scaler_hybrid.joblib\"); pca = load(RUN/\"pca_hybrid.joblib\")\n",
    "\n",
    "# Project to Z and fit sticky HMM\n",
    "Z = pca.transform(scaler.transform(F.values))\n",
    "K = int(len(np.unique(L)))\n",
    "hmm = GaussianHMM(n_components=K, covariance_type=\"diag\", n_iter=300, random_state=42)\n",
    "hmm.fit(Z)\n",
    "\n",
    "# Add stickiness: bias self-transitions, then re-decode\n",
    "A = hmm.transmat_\n",
    "sticky = 0.85\n",
    "A = (1-sticky)*A + sticky*np.eye(K)\n",
    "A = A / A.sum(axis=1, keepdims=True)\n",
    "hmm.transmat_ = A\n",
    "Z_labels = hmm.predict(Z)\n",
    "\n",
    "# Map HMM states â†’ original letters via majority vote\n",
    "map_to_km = {}\n",
    "for s in range(K):\n",
    "    if np.sum(Z_labels==s)==0: continue\n",
    "    map_to_km[s] = int(mode(L[Z_labels==s], keepdims=True).mode[0])\n",
    "L_hmm = np.array([map_to_km.get(s, s) for s in Z_labels], dtype=int)\n",
    "\n",
    "# Save & summarize\n",
    "pd.DataFrame({\"state\": L_hmm}).to_csv(RUN/\"state_assignments_hmm.csv\", index=False)\n",
    "sil = silhouette_score(Z, L_hmm) if K>1 else float(\"nan\")\n",
    "ari = ARI(L, L_hmm)\n",
    "with open(OUT/\"hmm_smoothing_summary.json\",\"w\") as f:\n",
    "    json.dump({\"K\":K, \"silhouette_after\": sil, \"ARI_vs_original\": float(ari)}, f, indent=2)\n",
    "print(f\"Saved smoothed labels â†’ {RUN/'state_assignments_hmm.csv'}\")\n",
    "print(f\"HMM sticky summary: K={K}  silhouette={sil:.3f}  ARI(originalâ†’HMM)={ari:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ca982bc-0d92-4027-a268-7eabf8dccaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\\state_assignments_sticky.csv\n",
      "K=4  stickiness=0.92  silhouette: 0.196 â†’ 0.116  ARI(origâ†’sticky)=0.402\n",
      "Summary â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\stability\\sticky_summary.json\n"
     ]
    }
   ],
   "source": [
    "# === Sticky Viterbi Smoothing (no hmmlearn) for the hybrid run ===\n",
    "# Uses KMeans centers in PCA space and a stay-probability to decode a coherent label path.\n",
    "# Saves: state_assignments_sticky.csv + sticky_summary.json\n",
    "\n",
    "import os, json, math, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from joblib import load\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score as ARI\n",
    "\n",
    "# --- paths (edit if your hybrid run lives elsewhere)\n",
    "RUN = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")\n",
    "OUT = RUN.parent / \"cog_alphabet_report_hybrid_v1\" / \"stability\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- load features, models, current labels\n",
    "F = pd.read_csv(RUN/\"features.csv\")\n",
    "L = pd.read_csv(RUN/\"state_assignments.csv\")[\"state\"].to_numpy()\n",
    "scaler = load(RUN/\"scaler_hybrid.joblib\")\n",
    "pca    = load(RUN/\"pca_hybrid.joblib\")\n",
    "km     = load(RUN/\"kmeans_hybrid.joblib\")\n",
    "\n",
    "Z = pca.transform(scaler.transform(F.values))   # (T, D)\n",
    "C = km.cluster_centers_                         # (K, D)\n",
    "K = C.shape[0]; T = Z.shape[0]\n",
    "\n",
    "# --- emissions: scaled squared distance to each center (converted to log-prob)\n",
    "# scale by per-dimension std in Z to balance axes\n",
    "sigma = Z.std(axis=0, ddof=0) + 1e-9\n",
    "Zd = (Z[:, None, :] - C[None, :, :]) / sigma[None, None, :]\n",
    "neg_half_sq = -0.5 * np.sum(Zd**2, axis=2)     # (T, K) ~ log-likelihood up to const\n",
    "E = neg_half_sq                                # emission log-probs\n",
    "\n",
    "# --- transitions: sticky Markov matrix (log), with strong self-bias\n",
    "stick = 0.92                                   # try 0.90â€“0.98; higher = more persistence\n",
    "off   = (1.0 - stick) / max(1, K-1)\n",
    "A = np.full((K, K), off, dtype=float)\n",
    "np.fill_diagonal(A, stick)\n",
    "logA = np.log(A + 1e-12)\n",
    "\n",
    "# --- initial: uniform (or bias to current first label if you prefer)\n",
    "logpi = np.full(K, -math.log(K), dtype=float)\n",
    "\n",
    "# --- Viterbi decode in log-space\n",
    "dp = np.empty((T, K), dtype=float)\n",
    "bp = np.empty((T, K), dtype=np.int32)\n",
    "\n",
    "dp[0] = logpi + E[0]\n",
    "bp[0] = -1\n",
    "for t in range(1, T):\n",
    "    # dp[t, j] = E[t, j] + max_i( dp[t-1, i] + logA[i, j] )\n",
    "    prev = dp[t-1][:, None] + logA\n",
    "    bp[t] = np.argmax(prev, axis=0)\n",
    "    dp[t] = E[t] + prev[bp[t], np.arange(K)]\n",
    "\n",
    "# backtrack\n",
    "y = np.empty(T, dtype=np.int32)\n",
    "y[-1] = int(np.argmax(dp[-1]))\n",
    "for t in range(T-2, -1, -1):\n",
    "    y[t] = bp[t+1, y[t+1]]\n",
    "\n",
    "# --- save & report\n",
    "pd.DataFrame({\"state\": y}).to_csv(RUN/\"state_assignments_sticky.csv\", index=False)\n",
    "sil_old = float(silhouette_score(Z, L)) if len(np.unique(L))>1 else float(\"nan\")\n",
    "sil_new = float(silhouette_score(Z, y)) if len(np.unique(y))>1 else float(\"nan\")\n",
    "ari_old_new = float(ARI(L, y))\n",
    "\n",
    "with open(OUT/\"sticky_summary.json\",\"w\") as f:\n",
    "    json.dump({\n",
    "        \"K\": int(K),\n",
    "        \"stickiness\": float(stick),\n",
    "        \"silhouette_before\": sil_old,\n",
    "        \"silhouette_after\":  sil_new,\n",
    "        \"ARI_original_vs_sticky\": ari_old_new\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", RUN/\"state_assignments_sticky.csv\")\n",
    "print(f\"K={K}  stickiness={stick:.2f}  silhouette: {sil_old:.3f} â†’ {sil_new:.3f}  ARI(origâ†’sticky)={ari_old_new:.3f}\")\n",
    "print(\"Summary â†’\", OUT/\"sticky_summary.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cd8cc28-ac1c-491e-b895-c53f581ee52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid:\n",
      " stick  silhouette  ari_vs_orig  trans_per_min  median_run_s  score\n",
      "  0.85       0.132        0.459          20.64          14.0  0.500\n",
      "  0.88       0.122        0.427          18.72          14.0  0.238\n",
      "  0.90       0.116        0.402          16.32          18.0  0.176\n",
      "  0.92       0.116        0.402          16.32          18.0  0.176\n",
      "  0.94       0.111        0.384          14.40          20.0  0.109\n",
      "  0.96       0.106        0.369          12.96          20.0 -0.006\n",
      "  0.98       0.101        0.351          11.04          26.0  0.000\n",
      "\n",
      "Chosen: {'chosen_stick': 0.85, 'silhouette': 0.132, 'ari_vs_original': 0.459, 'trans_per_min': 20.64, 'median_run_s': 14.0}\n",
      "Saved optimized labels â†’ E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\\state_assignments_sticky_opt.csv\n",
      "Details â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\stability\\sticky_tuner_summary.json\n"
     ]
    }
   ],
   "source": [
    "# === Sticky Tuner: sweep stickiness, pick best, save optimized labels ===\n",
    "import os, math, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from joblib import load\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score as ARI\n",
    "\n",
    "RUN = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")\n",
    "OUT = RUN.parent / \"cog_alphabet_report_hybrid_v1\" / \"stability\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load features/meta/models and current labels\n",
    "F  = pd.read_csv(RUN/\"features.csv\")\n",
    "M  = pd.read_csv(RUN/\"metadata.csv\").sort_values([\"file\",\"t_start_s\",\"t_end_s\"]).reset_index(drop=True)\n",
    "L0 = pd.read_csv(RUN/\"state_assignments.csv\")[\"state\"].to_numpy()\n",
    "scaler = load(RUN/\"scaler_hybrid.joblib\"); pca = load(RUN/\"pca_hybrid.joblib\"); km = load(RUN/\"kmeans_hybrid.joblib\")\n",
    "\n",
    "Z = pca.transform(scaler.transform(F.values))           # (T,D)\n",
    "C = km.cluster_centers_; K = C.shape[0]; T = Z.shape[0]\n",
    "sigma = Z.std(axis=0, ddof=0) + 1e-9\n",
    "E = -0.5 * np.sum(((Z[:,None,:]-C[None,:,:])/sigma[None,None,:])**2, axis=2)  # log-emissions (T,K)\n",
    "\n",
    "# epoch timing for temporal metrics\n",
    "epoch_sec = float((M[\"t_end_s\"] - M[\"t_start_s\"]).median()) if \"t_end_s\" in M and \"t_start_s\" in M else 2.0\n",
    "total_min = float((M[\"t_end_s\"].max() - M[\"t_start_s\"].min())/60) if \"t_end_s\" in M else (T*epoch_sec/60.0)\n",
    "\n",
    "def viterbi_sticky(E, stick):\n",
    "    off = (1.0 - stick) / max(1, K-1)\n",
    "    A = np.full((K,K), off); np.fill_diagonal(A, stick)\n",
    "    logA = np.log(A + 1e-12); logpi = np.full(K, -math.log(K))\n",
    "    T = E.shape[0]\n",
    "    dp = np.empty((T,K)); bp = np.empty((T,K), dtype=np.int32)\n",
    "    dp[0] = logpi + E[0]; bp[0] = -1\n",
    "    for t in range(1,T):\n",
    "        prev = dp[t-1][:,None] + logA\n",
    "        bp[t] = np.argmax(prev, axis=0)\n",
    "        dp[t] = E[t] + prev[bp[t], np.arange(K)]\n",
    "    y = np.empty(T, dtype=np.int32); y[-1] = int(np.argmax(dp[-1]))\n",
    "    for t in range(T-2,-1,-1): y[t] = bp[t+1, y[t+1]]\n",
    "    return y\n",
    "\n",
    "def transitions_per_min(y):\n",
    "    trans = int(np.sum(y[1:] != y[:-1]))\n",
    "    return trans / max(total_min, 1e-9)\n",
    "\n",
    "def median_run_seconds(y):\n",
    "    runs = []\n",
    "    cur, n = y[0], 1\n",
    "    for a,b in zip(y[:-1], y[1:]):\n",
    "        if b==a: n+=1\n",
    "        else: runs.append(n); cur=b; n=1\n",
    "    runs.append(n)\n",
    "    return float(np.median(runs)*epoch_sec)\n",
    "\n",
    "sticks = [0.85,0.88,0.90,0.92,0.94,0.96,0.98]\n",
    "rows = []\n",
    "for s in sticks:\n",
    "    y = viterbi_sticky(E, s)\n",
    "    try: sil = silhouette_score(Z, y) if K>1 else float(\"nan\")\n",
    "    except Exception: sil = float(\"nan\")\n",
    "    ari = ARI(L0, y)\n",
    "    tpm = transitions_per_min(y)\n",
    "    mrun = median_run_seconds(y)\n",
    "    rows.append({\"stick\":s, \"silhouette\":sil, \"ari_vs_orig\":ari, \"trans_per_min\":tpm, \"median_run_s\":mrun})\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "# Score: balance coherence and fidelity. Favor higher ARI & silhouette, penalize excessive switching.\n",
    "# Normalize metrics to [0,1]\n",
    "def norm(v): v=np.array(v, float); lo,hi=np.nanmin(v), np.nanmax(v); return (v-lo)/(hi-lo+1e-12)\n",
    "s_ari = norm(df[\"ari_vs_orig\"].values)\n",
    "s_sil = norm(df[\"silhouette\"].values)\n",
    "pen   = norm(-df[\"median_run_s\"].values) + norm(df[\"trans_per_min\"].values)  # higher = worse\n",
    "score = 0.5*s_ari + 0.5*s_sil - 0.25*pen\n",
    "df[\"score\"] = score\n",
    "\n",
    "best = df.iloc[int(np.nanargmax(df[\"score\"].values))]\n",
    "yopt = viterbi_sticky(E, float(best[\"stick\"]))\n",
    "pd.DataFrame({\"state\": yopt}).to_csv(RUN/\"state_assignments_sticky_opt.csv\", index=False)\n",
    "\n",
    "# Save summary + table\n",
    "summary = {\n",
    "    \"chosen_stick\": float(best[\"stick\"]),\n",
    "    \"silhouette\": float(best[\"silhouette\"]),\n",
    "    \"ari_vs_original\": float(best[\"ari_vs_orig\"]),\n",
    "    \"trans_per_min\": float(best[\"trans_per_min\"]),\n",
    "    \"median_run_s\": float(best[\"median_run_s\"])\n",
    "}\n",
    "with open(OUT/\"sticky_tuner_summary.json\",\"w\") as f: json.dump(summary, f, indent=2)\n",
    "df.to_csv(OUT/\"sticky_tuner_grid.csv\", index=False)\n",
    "\n",
    "print(\"Grid:\")\n",
    "print(df.round(3).to_string(index=False))\n",
    "print(\"\\nChosen:\", {k: round(v,3) for k,v in summary.items()})\n",
    "print(\"Saved optimized labels â†’\", RUN/\"state_assignments_sticky_opt.csv\")\n",
    "print(\"Details â†’\", OUT/\"sticky_tuner_summary.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbf19483-b42d-433e-930d-f5573789a717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final labels saved â†’ E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\\state_assignments_final.csv\n",
      "Transitions/min  : 20.64 â†’ 19.68\n",
      "Silhouette       : 0.132 â†’ 0.129\n",
      "ARI(origâ†’final)  : 0.454\n",
      "Saved addendum â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\CNT_CognitiveAlphabet_Stability_Addendum_HYBRID_final.png\n"
     ]
    }
   ],
   "source": [
    "# === Adopt sticky_opt â†’ enforce min run length â†’ quick re-audit & update addendum ===\n",
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import adjusted_rand_score as ARI, silhouette_score\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "RUN = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")\n",
    "REP = RUN.parent / \"cog_alphabet_report_hybrid_v1\"\n",
    "STB = REP / \"stability\"\n",
    "for p in (REP, STB): p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Load features + labels\n",
    "F = pd.read_csv(RUN/\"features.csv\")\n",
    "L_orig = pd.read_csv(RUN/\"state_assignments.csv\")[\"state\"].to_numpy()\n",
    "L_opt  = pd.read_csv(RUN/\"state_assignments_sticky_opt.csv\")[\"state\"].to_numpy()\n",
    "M = pd.read_csv(RUN/\"metadata.csv\").sort_values([\"file\",\"t_start_s\",\"t_end_s\"]).reset_index(drop=True)\n",
    "\n",
    "# 2) Enforce minimum run length (â‰¥3 epochs) with a simple run-length pass\n",
    "MIN_EPOCHS = 3\n",
    "y = L_opt.copy()\n",
    "start = 0\n",
    "for i in range(1, len(y)+1):\n",
    "    if i == len(y) or y[i] != y[start]:\n",
    "        run_len = i - start\n",
    "        if run_len < MIN_EPOCHS:\n",
    "            left = y[start-1] if start-1 >= 0 else None\n",
    "            right = y[i] if i < len(y) else None\n",
    "            fill = right if right is not None else left\n",
    "            if fill is None: fill = y[start]  # edge case\n",
    "            y[start:i] = fill\n",
    "        start = i\n",
    "\n",
    "pd.DataFrame({\"state\": y}).to_csv(RUN/\"state_assignments_final.csv\", index=False)\n",
    "\n",
    "# 3) Quick metrics (no re-projection needed for coherence checks)\n",
    "def transitions_per_min(labels, meta):\n",
    "    if \"t_start_s\" in meta and \"t_end_s\" in meta:\n",
    "        total_minutes = (meta[\"t_end_s\"].max() - meta[\"t_start_s\"].min()) / 60.0\n",
    "    else:\n",
    "        epoch_sec = float((meta[\"t_end_s\"] - meta[\"t_start_s\"]).median()) if \"t_end_s\" in meta else 2.0\n",
    "        total_minutes = len(labels)*epoch_sec/60.0\n",
    "    trans = int(np.sum(labels[1:] != labels[:-1]))\n",
    "    return trans / max(total_minutes, 1e-9)\n",
    "\n",
    "tpm_before = transitions_per_min(L_opt, M)\n",
    "tpm_after  = transitions_per_min(y, M)\n",
    "\n",
    "# (Optional) silhouette on PCA space if you want:\n",
    "try:\n",
    "    from joblib import load\n",
    "    scaler = load(RUN/\"scaler_hybrid.joblib\")\n",
    "    pca    = load(RUN/\"pca_hybrid.joblib\")\n",
    "    Z      = pca.transform(scaler.transform(F.values))\n",
    "    sil_before = silhouette_score(Z, L_opt) if len(np.unique(L_opt))>1 else float(\"nan\")\n",
    "    sil_after  = silhouette_score(Z, y)     if len(np.unique(y))>1 else float(\"nan\")\n",
    "except Exception:\n",
    "    sil_before = sil_after = float(\"nan\")\n",
    "\n",
    "ari_vs_orig = ARI(L_orig, y)\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    \"min_run_epochs\": int(MIN_EPOCHS),\n",
    "    \"trans_per_min_before\": float(tpm_before),\n",
    "    \"trans_per_min_after\":  float(tpm_after),\n",
    "    \"silhouette_before\":    float(sil_before),\n",
    "    \"silhouette_after\":     float(sil_after),\n",
    "    \"ARI_original_vs_final\": float(ari_vs_orig),\n",
    "}\n",
    "with open(STB/\"sticky_final_summary.json\",\"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"Final labels saved â†’\", RUN/\"state_assignments_final.csv\")\n",
    "print(\"Transitions/min  :\", round(tpm_before,2), \"â†’\", round(tpm_after,2))\n",
    "print(\"Silhouette       :\", (None if np.isnan(sil_before) else round(sil_before,3)), \"â†’\",\n",
    "      (None if np.isnan(sil_after) else round(sil_after,3)))\n",
    "print(\"ARI(origâ†’final)  :\", round(ari_vs_orig,3))\n",
    "\n",
    "# 4) Fast addendum refresh (reuse existing bootstrap file names w/ â€œfinalâ€ suffix to avoid overwrite)\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import normalized_mutual_info_score as NMI\n",
    "\n",
    "B, FRAC = 50, 0.80  # lighter pass for speed\n",
    "np.random.seed(42)\n",
    "N = len(y); K = len(np.unique(y))\n",
    "boot = []\n",
    "for b in range(B):\n",
    "    idx = np.random.choice(N, size=int(N*FRAC), replace=False)\n",
    "    km_b = KMeans(n_clusters=K, n_init=\"auto\", random_state=1000+b).fit(Z[idx])\n",
    "    Lb   = km_b.labels_\n",
    "    try: sil = silhouette_score(Z[idx], Lb) if K>1 else np.nan\n",
    "    except Exception: sil = np.nan\n",
    "    boot.append({\"bootstrap\": b, \"size\": len(idx),\n",
    "                 \"ari\": ARI(y[idx], Lb), \"nmi\": NMI(y[idx], Lb), \"silhouette\": sil})\n",
    "boot_df = pd.DataFrame(boot); boot_df.to_csv(STB/\"bootstrap_stability_final.csv\", index=False)\n",
    "\n",
    "rows = []\n",
    "for s in range(50,60):\n",
    "    km_s = KMeans(n_clusters=K, n_init=\"auto\", random_state=s).fit(Z)\n",
    "    Ls   = km_s.labels_\n",
    "    try: sil = silhouette_score(Z, Ls) if K>1 else np.nan\n",
    "    except Exception: sil = np.nan\n",
    "    rows.append({\"seed\": s, \"ari\": ARI(y, Ls), \"nmi\": NMI(y, Ls), \"silhouette\": sil})\n",
    "ms_df = pd.DataFrame(rows); ms_df.to_csv(STB/\"multiseed_refit_final.csv\", index=False)\n",
    "\n",
    "# Plot mini addendum\n",
    "fig = plt.figure(figsize=(10,6), dpi=130)\n",
    "gs  = fig.add_gridspec(2,2, hspace=0.35, wspace=0.30)\n",
    "\n",
    "axA = fig.add_subplot(gs[0,0]); axA.hist(boot_df[\"ari\"], bins=16, alpha=0.85)\n",
    "axA.set_title(\"Bootstrap ARI (final labels)\"); axA.set_xlabel(\"ARI\"); axA.set_ylabel(\"Count\")\n",
    "\n",
    "axB = fig.add_subplot(gs[0,1]); axB.hist(boot_df[\"silhouette\"].dropna(), bins=16, alpha=0.85, color=\"#8888ff\")\n",
    "axB.set_title(\"Bootstrap Silhouette (final)\"); axB.set_xlabel(\"Silhouette\"); axB.set_ylabel(\"Count\")\n",
    "\n",
    "axC = fig.add_subplot(gs[1,0]); axC.plot(ms_df[\"seed\"], ms_df[\"ari\"], \"o-\")\n",
    "axC.set_title(\"Multi-seed ARI (final)\"); axC.set_xlabel(\"Seed\"); axC.set_ylabel(\"ARI\"); axC.set_ylim(0,1)\n",
    "\n",
    "axD = fig.add_subplot(gs[1,1]); axD.axis(\"off\")\n",
    "axD.text(0.02, 0.9, \"Sticky + Min-run summary\", fontsize=11, weight=\"bold\")\n",
    "for i,(k,v) in enumerate(summary.items()):\n",
    "    axD.text(0.02, 0.74-0.12*i, f\"{k}: {round(v,3) if isinstance(v,(float,int)) else v}\", fontsize=10)\n",
    "\n",
    "fig.suptitle(\"CNT Cognitive Alphabet â€” Hybrid (Sticky + MinRun) Quick Addendum\", fontsize=14, weight=\"bold\")\n",
    "PTH = REP/\"CNT_CognitiveAlphabet_Stability_Addendum_HYBRID_final.png\"\n",
    "fig.savefig(PTH, bbox_inches=\"tight\"); plt.close(fig)\n",
    "print(\"Saved addendum â†’\", PTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e5541ca-e2a3-4d40-a780-eee55b8a5f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid:\n",
      " stick  ari_vs_anchor  silhouette  trans_per_min  median_run_s  score\n",
      "  0.84          0.463       0.132          20.64          14.0  0.500\n",
      "  0.86          0.463       0.132          20.64          14.0  0.500\n",
      "  0.88          0.429       0.122          18.72          14.0 -0.084\n",
      "  0.90          0.418       0.119          17.76          17.0  0.000\n",
      "\n",
      "Chosen stick: 0.84\n",
      "Saved final labels â†’ E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\\state_assignments_alpha_sticky_final.csv\n",
      "Final summary: {'K': 4, 'chosen_stick': 0.84, 'ari_anchorâ†’sticky': 0.463, 'sil_anchorâ†’sticky': 0.132, 'trans_per_min_sticky': 20.64, 'median_run_s_sticky': 14.0, 'final_ARI_orig': 0.454, 'final_ARI_anchor': 0.457, 'final_silhouette': 0.129, 'final_trans_per_min': 19.68}\n",
      "Addendum â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\CNT_CognitiveAlphabet_Stability_Addendum_HYBRID_anchor_sticky.png\n"
     ]
    }
   ],
   "source": [
    "# === Anchor+Sticky: lock Alpha, split the rest, then smooth (tuned) and min-run ===\n",
    "import os, re, json, math, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from joblib import load, dump\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score as ARI, normalized_mutual_info_score as NMI\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "RUN = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")\n",
    "REP = RUN.parent / \"cog_alphabet_report_hybrid_v1\"\n",
    "STB = REP / \"stability\"\n",
    "for p in (REP, STB): p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- load hybrid features/models/labels\n",
    "F   = pd.read_csv(RUN/\"features.csv\")\n",
    "M   = pd.read_csv(RUN/\"metadata.csv\").sort_values([\"file\",\"t_start_s\",\"t_end_s\"]).reset_index(drop=True)\n",
    "L0  = pd.read_csv(RUN/\"state_assignments.csv\")[\"state\"].to_numpy()\n",
    "sc  = load(RUN/\"scaler_hybrid.joblib\")\n",
    "pca = load(RUN/\"pca_hybrid.joblib\")\n",
    "km0 = load(RUN/\"kmeans_hybrid.joblib\")\n",
    "Z   = pca.transform(sc.transform(F.values))\n",
    "C0  = km0.cluster_centers_\n",
    "K    = C0.shape[0]\n",
    "epoch_sec = float((M[\"t_end_s\"] - M[\"t_start_s\"]).median()) if \"t_end_s\" in M else 2.0\n",
    "\n",
    "# --- 1) ANCHOR ALPHA\n",
    "alpha = F[\"alpha_rel_med\"].to_numpy()\n",
    "thr   = np.quantile(alpha, 0.80)               # top 20% alpha epochs\n",
    "A_idx = np.where(alpha >= thr)[0]\n",
    "R_idx = np.where(alpha  < thr)[0]\n",
    "\n",
    "def medoid(Zsub):\n",
    "    m = np.median(Zsub, axis=0)\n",
    "    d = np.linalg.norm(Zsub - m, axis=1)\n",
    "    return int(np.argmin(d))\n",
    "\n",
    "alpha_center = Z[A_idx][medoid(Z[A_idx])]\n",
    "\n",
    "# select remaining K-1 centers by greedy farthest-point init on remainder\n",
    "centers = [alpha_center.copy()]\n",
    "Zr = Z[R_idx]\n",
    "chosen = []\n",
    "for _ in range(K-1):\n",
    "    # distance to nearest existing chosen center\n",
    "    dmin = np.full(len(Zr), np.inf)\n",
    "    for c in centers:\n",
    "        dmin = np.minimum(dmin, np.linalg.norm(Zr - c, axis=1))\n",
    "    j = int(np.argmax(dmin))\n",
    "    centers.append(Zr[j].copy())\n",
    "    chosen.append(j)\n",
    "centers = np.vstack(centers)\n",
    "\n",
    "# run anchored KMeans\n",
    "kmA = KMeans(n_clusters=K, init=centers, n_init=1, random_state=42).fit(Z)\n",
    "La  = kmA.labels_\n",
    "\n",
    "# --- 2) STICKY (tuned on anchored labels): sweep stickiness and pick balance\n",
    "sigma = Z.std(axis=0, ddof=0) + 1e-9\n",
    "E = -0.5 * np.sum(((Z[:,None,:]-kmA.cluster_centers_[None,:,:])/sigma[None,None,:])**2, axis=2)  # (T,K)\n",
    "\n",
    "def viterbi_sticky(E, stick):\n",
    "    off = (1.0 - stick)/max(1,K-1)\n",
    "    A = np.full((K,K), off); np.fill_diagonal(A, stick)\n",
    "    logA = np.log(A + 1e-12); logpi = np.full(K, -math.log(K))\n",
    "    T = E.shape[0]\n",
    "    dp = np.empty((T,K)); bp = np.empty((T,K), dtype=np.int32)\n",
    "    dp[0] = logpi + E[0]; bp[0] = -1\n",
    "    for t in range(1,T):\n",
    "        prev = dp[t-1][:,None] + logA\n",
    "        bp[t] = np.argmax(prev, axis=0)\n",
    "        dp[t] = E[t] + prev[bp[t], np.arange(K)]\n",
    "    y = np.empty(T, dtype=np.int32); y[-1] = int(np.argmax(dp[-1]))\n",
    "    for t in range(T-2, -1, -1): y[t] = bp[t+1, y[t+1]]\n",
    "    return y\n",
    "\n",
    "def trans_per_min(y):\n",
    "    total_min = (M[\"t_end_s\"].max() - M[\"t_start_s\"].min())/60.0 if \"t_end_s\" in M else (len(y)*epoch_sec/60.0)\n",
    "    return int(np.sum(y[1:]!=y[:-1])) / max(total_min, 1e-9)\n",
    "\n",
    "def med_run_s(y):\n",
    "    runs=[]; n=1\n",
    "    for a,b in zip(y[:-1], y[1:]):\n",
    "        if b==a: n+=1\n",
    "        else: runs.append(n); n=1\n",
    "    runs.append(n)\n",
    "    return float(np.median(runs)*epoch_sec)\n",
    "\n",
    "sticks = [0.84,0.86,0.88,0.90]\n",
    "grid = []\n",
    "for s in sticks:\n",
    "    y = viterbi_sticky(E, s)\n",
    "    try: sil = silhouette_score(Z, y) if K>1 else float(\"nan\")\n",
    "    except Exception: sil = float(\"nan\")\n",
    "    grid.append({\n",
    "        \"stick\": s,\n",
    "        \"ari_vs_anchor\": ARI(La, y),\n",
    "        \"silhouette\": sil,\n",
    "        \"trans_per_min\": trans_per_min(y),\n",
    "        \"median_run_s\":  med_run_s(y)\n",
    "    })\n",
    "df = pd.DataFrame(grid)\n",
    "\n",
    "# simple score: keep fidelity & separation; discourage switchiness\n",
    "def norm(v): v=np.array(v,float); lo,hi=np.nanmin(v),np.nanmax(v); return (v-lo)/(hi-lo+1e-12)\n",
    "score = 0.5*norm(df[\"ari_vs_anchor\"]) + 0.5*norm(df[\"silhouette\"]) \\\n",
    "        - 0.25*(norm(df[\"trans_per_min\"]) + norm(-df[\"median_run_s\"]))\n",
    "df[\"score\"] = score\n",
    "best = df.iloc[int(np.nanargmax(df[\"score\"].values))]\n",
    "Ls   = viterbi_sticky(E, float(best[\"stick\"]))\n",
    "\n",
    "# --- 3) MIN-RUN filter (â‰¥3 epochs = 6 s)\n",
    "MIN_E = 3\n",
    "y = Ls.copy()\n",
    "start=0\n",
    "for i in range(1, len(y)+1):\n",
    "    if i==len(y) or y[i]!=y[start]:\n",
    "        run_len=i-start\n",
    "        if run_len<MIN_E:\n",
    "            left = y[start-1] if start-1>=0 else None\n",
    "            right= y[i] if i<len(y) else None\n",
    "            fill = right if right is not None else left\n",
    "            if fill is None: fill = y[start]\n",
    "            y[start:i]=fill\n",
    "        start=i\n",
    "\n",
    "# save final labels\n",
    "pd.DataFrame({\"state\": y}).to_csv(RUN/\"state_assignments_alpha_sticky_final.csv\", index=False)\n",
    "\n",
    "# --- 4) Quick metrics & mini addendum\n",
    "ari_vs_orig   = ARI(L0, y)\n",
    "ari_vs_anchor = ARI(La, y)\n",
    "sil_final     = silhouette_score(Z, y) if len(np.unique(y))>1 else float(\"nan\")\n",
    "tpm_final     = trans_per_min(y)\n",
    "summary = {\n",
    "    \"K\": int(K),\n",
    "    \"chosen_stick\": float(best[\"stick\"]),\n",
    "    \"ari_anchorâ†’sticky\": float(best[\"ari_vs_anchor\"]),\n",
    "    \"sil_anchorâ†’sticky\": float(best[\"silhouette\"]),\n",
    "    \"trans_per_min_sticky\": float(df.loc[df['stick']==best['stick'],\"trans_per_min\"].values[0]),\n",
    "    \"median_run_s_sticky\":  float(df.loc[df['stick']==best['stick'],\"median_run_s\"].values[0]),\n",
    "    \"final_ARI_orig\": float(ari_vs_orig),\n",
    "    \"final_ARI_anchor\": float(ari_vs_anchor),\n",
    "    \"final_silhouette\": float(sil_final),\n",
    "    \"final_trans_per_min\": float(tpm_final),\n",
    "}\n",
    "with open(STB/\"anchor_sticky_summary.json\",\"w\") as f: json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"Grid:\")\n",
    "print(df.round(3).to_string(index=False))\n",
    "print(\"\\nChosen stick:\", round(best['stick'],3))\n",
    "print(\"Saved final labels â†’\", RUN/\"state_assignments_alpha_sticky_final.csv\")\n",
    "print(\"Final summary:\", {k: (round(v,3) if isinstance(v,(float,int)) else v) for k,v in summary.items()})\n",
    "\n",
    "# small addendum figure\n",
    "fig = plt.figure(figsize=(9,5), dpi=130)\n",
    "gs  = fig.add_gridspec(2,2, hspace=0.35, wspace=0.30)\n",
    "axA = fig.add_subplot(gs[0,0]); axA.plot(df[\"stick\"], df[\"ari_vs_anchor\"], \"o-\"); axA.set_title(\"ARI vs Anchor\"); axA.set_xlabel(\"stick\"); axA.set_ylim(0,1)\n",
    "axB = fig.add_subplot(gs[0,1]); axB.plot(df[\"stick\"], df[\"silhouette\"], \"o-\");   axB.set_title(\"Silhouette\");  axB.set_xlabel(\"stick\")\n",
    "axC = fig.add_subplot(gs[1,0]); axC.plot(df[\"stick\"], df[\"trans_per_min\"], \"o-\");axC.set_title(\"Transitions/min\"); axC.set_xlabel(\"stick\")\n",
    "axD = fig.add_subplot(gs[1,1]); axD.axis(\"off\")\n",
    "axD.text(0.0, 0.95, \"Anchor+Sticky Final:\", fontsize=11, weight=\"bold\")\n",
    "y0=0.78\n",
    "for k in [\"final_ARI_orig\",\"final_silhouette\",\"final_trans_per_min\",\"chosen_stick\"]:\n",
    "    axD.text(0.0, y0, f\"{k}: {round(summary[k],3)}\"); y0 -= 0.14\n",
    "PTH = REP/\"CNT_CognitiveAlphabet_Stability_Addendum_HYBRID_anchor_sticky.png\"\n",
    "fig.savefig(PTH, bbox_inches=\"tight\"); plt.close(fig)\n",
    "print(\"Addendum â†’\", PTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e94153dd-c38a-4fd3-a774-7c7127d99cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promoted: E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\\state_assignments.csv\n",
      "FINAL summary: {'K': 4, 'N_epochs': 485, 'bootstrap_ARI_mean': 0.415, 'bootstrap_ARI_p10': 0.328, 'bootstrap_ARI_p50': 0.436, 'bootstrap_ARI_p90': 0.478, 'bootstrap_Sil_mean': 0.193, 'multiseed_ARI_mean': 0.408, 'multiseed_ARI_min': 0.298, 'multiseed_ARI_max': 0.454, 'LOR_rows': 3}\n",
      "Saved addendum â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\CNT_CognitiveAlphabet_Stability_Addendum_HYBRID_FINAL.png\n"
     ]
    }
   ],
   "source": [
    "# === Promote alpha+sticky labels â†’ full stability audit & addendum ===\n",
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score as ARI, normalized_mutual_info_score as NMI, silhouette_score\n",
    "from joblib import load\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "RUN = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")\n",
    "REP = RUN.parent / \"cog_alphabet_report_hybrid_v1\"\n",
    "STB = REP / \"stability\"\n",
    "REP.mkdir(parents=True, exist_ok=True); STB.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Promote final labels\n",
    "L_final = pd.read_csv(RUN/\"state_assignments_alpha_sticky_final.csv\")\n",
    "L_final.to_csv(RUN/\"state_assignments.csv\", index=False)\n",
    "print(\"Promoted:\", RUN/\"state_assignments.csv\")\n",
    "\n",
    "# 2) Load data & models\n",
    "F = pd.read_csv(RUN/\"features.csv\")\n",
    "M = pd.read_csv(RUN/\"metadata.csv\")\n",
    "L = L_final[\"state\"].to_numpy()\n",
    "scaler = load(RUN/\"scaler_hybrid.joblib\"); pca = load(RUN/\"pca_hybrid.joblib\")\n",
    "Z = pca.transform(scaler.transform(F.values))\n",
    "K = len(np.unique(L)); N = len(L)\n",
    "\n",
    "# 3) Full stability audit (100 bootstraps; seeds 50â€“70)\n",
    "np.random.seed(42)\n",
    "B, FRAC = 100, 0.80\n",
    "boot = []\n",
    "for b in range(B):\n",
    "    idx = np.random.choice(N, size=int(N*FRAC), replace=False)\n",
    "    km_b = KMeans(n_clusters=K, n_init=\"auto\", random_state=1000+b).fit(Z[idx])\n",
    "    Lb   = km_b.labels_\n",
    "    try: sil = silhouette_score(Z[idx], Lb) if K>1 else np.nan\n",
    "    except Exception: sil = np.nan\n",
    "    boot.append({\"bootstrap\": b, \"size\": len(idx),\n",
    "                 \"ari\": ARI(L[idx], Lb), \"nmi\": NMI(L[idx], Lb), \"silhouette\": sil})\n",
    "boot_df = pd.DataFrame(boot); boot_df.to_csv(STB/\"bootstrap_stability_FINAL.csv\", index=False)\n",
    "\n",
    "rows = []\n",
    "for s in range(50, 71):\n",
    "    km_s = KMeans(n_clusters=K, n_init=\"auto\", random_state=s).fit(Z)\n",
    "    Ls   = km_s.labels_\n",
    "    try: sil = silhouette_score(Z, Ls) if K>1 else np.nan\n",
    "    except Exception: sil = np.nan\n",
    "    rows.append({\"seed\": s, \"ari\": ARI(L, Ls), \"nmi\": NMI(L, Ls), \"silhouette\": sil})\n",
    "ms_df = pd.DataFrame(rows); ms_df.to_csv(STB/\"multiseed_refit_FINAL.csv\", index=False)\n",
    "\n",
    "# LOR by file (real recordings)\n",
    "def is_real(fn): return (fn or \"\").lower().endswith((\".edf\",\".vhdr\",\".eeg\",\".fif\",\".set\",\".fdt\"))\n",
    "files = [f for f in M[\"file\"].unique().tolist() if is_real(f)]\n",
    "lor = []\n",
    "for f in files:\n",
    "    te = (M[\"file\"] == f).to_numpy(); tr = ~te\n",
    "    if tr.sum()<K or te.sum()<K: continue\n",
    "    km_tr = KMeans(n_clusters=K, n_init=\"auto\", random_state=2025).fit(Z[tr])\n",
    "    Lte   = km_tr.predict(Z[te])\n",
    "    lor.append({\"file\": f, \"n_train\": int(tr.sum()), \"n_test\": int(te.sum()),\n",
    "                \"ari\": float(ARI(L[te], Lte))})\n",
    "lor_df = pd.DataFrame(lor).sort_values(\"file\"); lor_df.to_csv(STB/\"leave_one_run_out_FINAL.csv\", index=False)\n",
    "\n",
    "# Summary + addendum\n",
    "def pct(a,q): return float(np.nanpercentile(a,q))\n",
    "summary = {\n",
    "  \"K\": int(K), \"N_epochs\": int(N),\n",
    "  \"bootstrap_ARI_mean\": float(np.nanmean(boot_df[\"ari\"])),\n",
    "  \"bootstrap_ARI_p10\": pct(boot_df[\"ari\"],10),\n",
    "  \"bootstrap_ARI_p50\": pct(boot_df[\"ari\"],50),\n",
    "  \"bootstrap_ARI_p90\": pct(boot_df[\"ari\"],90),\n",
    "  \"bootstrap_Sil_mean\": float(np.nanmean(boot_df[\"silhouette\"])),\n",
    "  \"multiseed_ARI_mean\": float(np.nanmean(ms_df[\"ari\"])),\n",
    "  \"multiseed_ARI_min\":  float(np.nanmin(ms_df[\"ari\"])),\n",
    "  \"multiseed_ARI_max\":  float(np.nanmax(ms_df[\"ari\"])),\n",
    "  \"LOR_rows\": int(len(lor_df))\n",
    "}\n",
    "with open(STB/\"summary_FINAL.json\",\"w\") as f: json.dump(summary, f, indent=2)\n",
    "print(\"FINAL summary:\", {k: (round(v,3) if isinstance(v,(float,int)) else v) for k,v in summary.items()})\n",
    "\n",
    "fig = plt.figure(figsize=(12,8), dpi=130); gs = fig.add_gridspec(2,2, hspace=0.35, wspace=0.30)\n",
    "axA=fig.add_subplot(gs[0,0]); axA.hist(boot_df[\"ari\"], bins=20, alpha=0.85); axA.set_title(\"Bootstrap ARI (FINAL)\")\n",
    "axB=fig.add_subplot(gs[0,1]); axB.hist(boot_df[\"silhouette\"].dropna(), bins=20, alpha=0.85, color=\"#88f\"); axB.set_title(\"Bootstrap Silhouette (FINAL)\")\n",
    "axC=fig.add_subplot(gs[1,0]); axC.plot(ms_df[\"seed\"], ms_df[\"ari\"], \"o-\"); axC.set_ylim(0,1); axC.set_title(\"Multi-seed ARI (FINAL)\")\n",
    "axD=fig.add_subplot(gs[1,1]); axD.axis(\"off\"); t=0.95\n",
    "axD.text(0.0,t,\"Headline:\", fontsize=11, weight=\"bold\"); t-=0.12\n",
    "for k in [\"K\",\"N_epochs\",\"bootstrap_ARI_mean\",\"bootstrap_ARI_p50\",\"bootstrap_ARI_p90\",\"multiseed_ARI_mean\"]:\n",
    "    axD.text(0.0,t,f\"{k}: {round(summary[k],3) if isinstance(summary[k],(float,int)) else summary[k]}\"); t-=0.1\n",
    "fig.suptitle(\"CNT Cognitive Alphabet â€” HYBRID (FINAL) Stability Addendum\", fontsize=16, weight=\"bold\")\n",
    "PTH = REP/\"CNT_CognitiveAlphabet_Stability_Addendum_HYBRID_FINAL.png\"\n",
    "fig.savefig(PTH, bbox_inches=\"tight\"); plt.close(fig)\n",
    "print(\"Saved addendum â†’\", PTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa009031-ed3c-47bb-ab1b-bb06e6cb25ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   min_epochs  trans_per_min  ARI_vs_original\n",
      "0           3          19.68         1.000000\n",
      "1           4          15.36         0.923069\n",
      "2           5          12.48         0.832286\n",
      "Saved coherent labels â†’ E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\\state_assignments_coherent.csv\n"
     ]
    }
   ],
   "source": [
    "# === Min-run sweep (3/4/5 epochs) â†’ pick best trade-off and save ===\n",
    "import numpy as np, pandas as pd, json\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import adjusted_rand_score as ARI\n",
    "RUN = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")\n",
    "M   = pd.read_csv(RUN/\"metadata.csv\").sort_values([\"file\",\"t_start_s\",\"t_end_s\"]).reset_index(drop=True)\n",
    "epoch_sec = float((M[\"t_end_s\"] - M[\"t_start_s\"]).median()) if \"t_end_s\" in M else 2.0\n",
    "L_base = pd.read_csv(RUN/\"state_assignments_alpha_sticky_final.csv\")[\"state\"].to_numpy()\n",
    "L_orig = pd.read_csv(RUN/\"state_assignments.csv\")[\"state\"].to_numpy()\n",
    "\n",
    "def trans_per_min(y):\n",
    "    total_min = (M[\"t_end_s\"].max() - M[\"t_start_s\"].min())/60.0 if \"t_end_s\" in M else (len(y)*epoch_sec/60.0)\n",
    "    return int(np.sum(y[1:]!=y[:-1])) / max(total_min, 1e-9)\n",
    "\n",
    "def enforce_min(y, m):\n",
    "    y = y.copy(); s=0\n",
    "    for i in range(1,len(y)+1):\n",
    "        if i==len(y) or y[i]!=y[s]:\n",
    "            run_len = i - s\n",
    "            if run_len < m:\n",
    "                left = y[s-1] if s-1>=0 else None\n",
    "                right= y[i] if i<len(y) else None\n",
    "                fill = right if right is not None else left\n",
    "                if fill is None: fill=y[s]\n",
    "                y[s:i] = fill\n",
    "            s = i\n",
    "    return y\n",
    "\n",
    "rows=[]\n",
    "for m in [3,4,5]:\n",
    "    y = enforce_min(L_base, m)\n",
    "    rows.append({\"min_epochs\": m,\n",
    "                 \"trans_per_min\": trans_per_min(y),\n",
    "                 \"ARI_vs_original\": ARI(L_orig, y)})\n",
    "df = pd.DataFrame(rows).sort_values(\"min_epochs\")\n",
    "print(df)\n",
    "\n",
    "best = df.iloc[df[\"ARI_vs_original\"].idxmax()]  # favor fidelity among tested mins\n",
    "ybest = enforce_min(L_base, int(best[\"min_epochs\"]))\n",
    "pd.DataFrame({\"state\": ybest}).to_csv(RUN/\"state_assignments_coherent.csv\", index=False)\n",
    "print(\"Saved coherent labels â†’\", RUN/\"state_assignments_coherent.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "197a211a-42a5-4dda-ad23-feaee82f7277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Promoted: E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\\state_assignments.csv\n",
      "\n",
      "=== COHERENT summary ===\n",
      "{'K': 4, 'N_epochs': 485, 'bootstrap_ARI_mean': 0.415, 'bootstrap_ARI_p10': 0.328, 'bootstrap_ARI_p50': 0.436, 'bootstrap_ARI_p90': 0.478, 'bootstrap_Sil_mean': 0.193, 'multiseed_ARI_mean': 0.408, 'multiseed_ARI_min': 0.298, 'multiseed_ARI_max': 0.454, 'LOR_rows': 3}\n",
      "\n",
      "=== Î” vs HYBRID_FINAL (COHERENT - FINAL) ===\n",
      "bootstrap_ARI_mean : 0.0\n",
      "bootstrap_ARI_p50 : 0.0\n",
      "bootstrap_ARI_p90 : 0.0\n",
      "multiseed_ARI_mean : 0.0\n",
      "Saved addendum â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\CNT_CognitiveAlphabet_Stability_Addendum_HYBRID_COHERENT.png\n"
     ]
    }
   ],
   "source": [
    "# === Promote \"coherent\" labels â†’ full stability audit + addendum (COHERENT) ===\n",
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from joblib import load\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score as ARI, normalized_mutual_info_score as NMI, silhouette_score\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "RUN = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")\n",
    "REP = RUN.parent / \"cog_alphabet_report_hybrid_v1\"\n",
    "STB = REP / \"stability\"\n",
    "REP.mkdir(parents=True, exist_ok=True); STB.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Promote coherent labels\n",
    "L_coh = pd.read_csv(RUN/\"state_assignments_coherent.csv\")\n",
    "L_coh.to_csv(RUN/\"state_assignments.csv\", index=False)\n",
    "print(\"Promoted:\", RUN/\"state_assignments.csv\")\n",
    "\n",
    "# 2) Load data & project\n",
    "F = pd.read_csv(RUN/\"features.csv\")\n",
    "M = pd.read_csv(RUN/\"metadata.csv\")\n",
    "L = L_coh[\"state\"].to_numpy()\n",
    "scaler = load(RUN/\"scaler_hybrid.joblib\"); pca = load(RUN/\"pca_hybrid.joblib\")\n",
    "Z = pca.transform(scaler.transform(F.values))\n",
    "K = len(np.unique(L)); N = len(L)\n",
    "\n",
    "# 3) Full stability (100 bootstraps; seeds 50â€“70)\n",
    "np.random.seed(42)\n",
    "B, FRAC = 100, 0.80\n",
    "boot = []\n",
    "for b in range(B):\n",
    "    idx = np.random.choice(N, size=int(N*FRAC), replace=False)\n",
    "    km_b = KMeans(n_clusters=K, n_init=\"auto\", random_state=1000+b).fit(Z[idx])\n",
    "    Lb   = km_b.labels_\n",
    "    try: sil = silhouette_score(Z[idx], Lb) if K>1 else np.nan\n",
    "    except Exception: sil = np.nan\n",
    "    boot.append({\"bootstrap\": b, \"size\": len(idx),\n",
    "                 \"ari\": ARI(L[idx], Lb), \"nmi\": NMI(L[idx], Lb), \"silhouette\": sil})\n",
    "boot_df = pd.DataFrame(boot); boot_df.to_csv(STB/\"bootstrap_stability_COHERENT.csv\", index=False)\n",
    "\n",
    "ms = []\n",
    "for s in range(50, 71):\n",
    "    km_s = KMeans(n_clusters=K, n_init=\"auto\", random_state=s).fit(Z)\n",
    "    Ls   = km_s.labels_\n",
    "    try: sil = silhouette_score(Z, Ls) if K>1 else np.nan\n",
    "    except Exception: sil = np.nan\n",
    "    ms.append({\"seed\": s, \"ari\": ARI(L, Ls), \"nmi\": NMI(L, Ls), \"silhouette\": sil})\n",
    "ms_df = pd.DataFrame(ms); ms_df.to_csv(STB/\"multiseed_refit_COHERENT.csv\", index=False)\n",
    "\n",
    "# LOR by file (real recordings)\n",
    "def is_real(fn): return (fn or \"\").lower().endswith((\".edf\",\".vhdr\",\".eeg\",\".fif\",\".set\",\".fdt\"))\n",
    "files = [f for f in M[\"file\"].unique().tolist() if is_real(f)]\n",
    "lor = []\n",
    "for f in files:\n",
    "    te = (M[\"file\"] == f).to_numpy(); tr = ~te\n",
    "    if tr.sum()<K or te.sum()<K: continue\n",
    "    km_tr = KMeans(n_clusters=K, n_init=\"auto\", random_state=2025).fit(Z[tr])\n",
    "    Lte   = km_tr.predict(Z[te])\n",
    "    lor.append({\"file\": f, \"n_train\": int(tr.sum()), \"n_test\": int(te.sum()),\n",
    "                \"ari\": float(ARI(L[te], Lte))})\n",
    "lor_df = pd.DataFrame(lor).sort_values(\"file\"); lor_df.to_csv(STB/\"leave_one_run_out_COHERENT.csv\", index=False)\n",
    "\n",
    "# 4) Summaries + side-by-side with your HYBRID_FINAL\n",
    "def pct(a,q): return float(np.nanpercentile(a,q))\n",
    "sum_coh = {\n",
    "  \"K\": int(K), \"N_epochs\": int(N),\n",
    "  \"bootstrap_ARI_mean\": float(np.nanmean(boot_df[\"ari\"])),\n",
    "  \"bootstrap_ARI_p10\":  pct(boot_df[\"ari\"],10),\n",
    "  \"bootstrap_ARI_p50\":  pct(boot_df[\"ari\"],50),\n",
    "  \"bootstrap_ARI_p90\":  pct(boot_df[\"ari\"],90),\n",
    "  \"bootstrap_Sil_mean\": float(np.nanmean(boot_df[\"silhouette\"])),\n",
    "  \"multiseed_ARI_mean\": float(np.nanmean(ms_df[\"ari\"])),\n",
    "  \"multiseed_ARI_min\":  float(np.nanmin(ms_df[\"ari\"])),\n",
    "  \"multiseed_ARI_max\":  float(np.nanmax(ms_df[\"ari\"])),\n",
    "  \"LOR_rows\": int(len(lor_df))\n",
    "}\n",
    "with open(STB/\"summary_COHERENT.json\",\"w\") as f: json.dump(sum_coh, f, indent=2)\n",
    "\n",
    "# Load prior FINAL to print a quick compare if present\n",
    "final_path = STB/\"summary_FINAL.json\"\n",
    "prev = json.load(open(final_path)) if final_path.exists() else None\n",
    "\n",
    "print(\"\\n=== COHERENT summary ===\")\n",
    "print({k: (round(v,3) if isinstance(v,(float,int)) else v) for k,v in sum_coh.items()})\n",
    "if prev:\n",
    "    print(\"\\n=== Î” vs HYBRID_FINAL (COHERENT - FINAL) ===\")\n",
    "    for k in [\"bootstrap_ARI_mean\",\"bootstrap_ARI_p50\",\"bootstrap_ARI_p90\",\"multiseed_ARI_mean\"]:\n",
    "        print(k, \":\", round(sum_coh[k] - prev[k], 3))\n",
    "\n",
    "# 5) Render a COHERENT addendum page\n",
    "fig = plt.figure(figsize=(12,8), dpi=130); gs = fig.add_gridspec(2,2, hspace=0.35, wspace=0.30)\n",
    "axA=fig.add_subplot(gs[0,0]); axA.hist(boot_df[\"ari\"], bins=20, alpha=0.85); axA.set_title(\"Bootstrap ARI (COHERENT)\")\n",
    "axB=fig.add_subplot(gs[0,1]); axB.hist(boot_df[\"silhouette\"].dropna(), bins=20, alpha=0.85, color=\"#88f\"); axB.set_title(\"Bootstrap Silhouette (COHERENT)\")\n",
    "axC=fig.add_subplot(gs[1,0]); axC.plot(ms_df[\"seed\"], ms_df[\"ari\"], \"o-\"); axC.set_ylim(0,1); axC.set_title(\"Multi-seed ARI (COHERENT)\")\n",
    "axD=fig.add_subplot(gs[1,1]); axD.axis(\"off\"); t=0.95\n",
    "axD.text(0.0,t,\"Headline:\", fontsize=11, weight=\"bold\"); t-=0.12\n",
    "for k in [\"K\",\"N_epochs\",\"bootstrap_ARI_mean\",\"bootstrap_ARI_p50\",\"bootstrap_ARI_p90\",\"multiseed_ARI_mean\"]:\n",
    "    axD.text(0.0,t,f\"{k}: {round(sum_coh[k],3) if isinstance(sum_coh[k],(float,int)) else sum_coh[k]}\"); t-=0.1\n",
    "fig.suptitle(\"CNT Cognitive Alphabet â€” HYBRID (COHERENT) Stability Addendum\", fontsize=16, weight=\"bold\")\n",
    "PTH = REP/\"CNT_CognitiveAlphabet_Stability_Addendum_HYBRID_COHERENT.png\"\n",
    "fig.savefig(PTH, bbox_inches=\"tight\"); plt.close(fig)\n",
    "print(\"Saved addendum â†’\", PTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "821d8538-a7fb-4bda-a1be-930d133570e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ablation summary (worse at top) ===\n",
      "     ablation  ARI_vs_canonical  silhouette  max_abs_dwell_shift\n",
      "   drop_alpha             0.256       0.153                0.126\n",
      "    drop_beta             0.352       0.209                0.328\n",
      "   drop_theta             0.371       0.171                0.157\n",
      " drop_entropy             0.376       0.162                0.219\n",
      "drop_centroid             0.429       0.168                0.115\n",
      "  drop_ratios             0.446       0.195                0.105\n",
      "   drop_fbCSP             0.450       0.200                0.095\n",
      "  drop_hjorth             0.466       0.181                0.019\n",
      "\n",
      "=== Transitions ===\n",
      "Entropy rate (emp): 0.488  | null mean: 1.809  (p10=1.783, p90=1.824)\n",
      "Stationary Ï€: [0.231 0.277 0.083 0.409]\n",
      "\n",
      "=== Silhouette null ===\n",
      "sil_real=0.129  | null mean=-0.024  | null p90=-0.015\n",
      "Saved summary â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\analysis\\hypothesis_harness_summary.png\n"
     ]
    }
   ],
   "source": [
    "# === CNT v0.2 Hypothesis Harness: Ablations + Transitions + Null Silhouette ===\n",
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from joblib import load\n",
    "from sklearn.metrics import adjusted_rand_score as ARI, silhouette_score\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "RUN = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")           # promoted v0.2\n",
    "REP = RUN.parent / \"cog_alphabet_report_hybrid_v1\"\n",
    "OUT = REP / \"analysis\"; OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Load canonical artifacts\n",
    "F   = pd.read_csv(RUN/\"features.csv\")\n",
    "M   = pd.read_csv(RUN/\"metadata.csv\").sort_values([\"file\",\"t_start_s\",\"t_end_s\"]).reset_index(drop=True)\n",
    "L0  = pd.read_csv(RUN/\"state_assignments.csv\")[\"state\"].to_numpy()\n",
    "sc  = load(RUN/\"scaler_hybrid.joblib\")\n",
    "pca = load(RUN/\"pca_hybrid.joblib\")\n",
    "km  = load(RUN/\"kmeans_hybrid.joblib\")\n",
    "\n",
    "# Project canonical Z and compute canonical silhouette\n",
    "Z0 = pca.transform(sc.transform(F.values))\n",
    "sil_real = float(silhouette_score(Z0, L0)) if len(np.unique(L0))>1 else float(\"nan\")\n",
    "\n",
    "# Helper: reassign letters after feature perturbation (no retrain)\n",
    "def project_and_assign(F_pert):\n",
    "    Z = pca.transform(sc.transform(F_pert.values))\n",
    "    return km.predict(Z), Z\n",
    "\n",
    "# 1) Feature ablation suite\n",
    "ABLATIONS = {\n",
    "    \"drop_alpha\":      [c for c in F.columns if c.startswith(\"alpha_\")],\n",
    "    \"drop_theta\":      [c for c in F.columns if c.startswith(\"theta_\")],\n",
    "    \"drop_beta\":       [c for c in F.columns if c.startswith(\"beta_\")],\n",
    "    \"drop_entropy\":    [c for c in F.columns if c.startswith(\"spec_entropy_\")],\n",
    "    \"drop_centroid\":   [c for c in F.columns if c.startswith(\"spec_centroid_\")],\n",
    "    \"drop_hjorth\":     [c for c in F.columns if c.startswith(\"hjorth_\")],\n",
    "    \"drop_fbCSP\":      [c for c in F.columns if c==\"fbCSP_task_proba\"],\n",
    "    # composites (test ratios by zeroing their columns only)\n",
    "    \"drop_ratios\":     [c for c in F.columns if c in [\"theta_over_alpha\",\"beta_over_alpha\",\"(alpha+theta)_over_beta\"]],\n",
    "}\n",
    "\n",
    "abl_rows = []\n",
    "for name, cols in ABLATIONS.items():\n",
    "    Fp = F.copy()\n",
    "    for c in cols:\n",
    "        if c in Fp.columns:\n",
    "            Fp[c] = 0.0\n",
    "    Lp, Zp = project_and_assign(Fp)\n",
    "    ari = float(ARI(L0, Lp))\n",
    "    try:\n",
    "        sil = float(silhouette_score(Zp, Lp)) if len(np.unique(Lp))>1 else float(\"nan\")\n",
    "    except Exception:\n",
    "        sil = float(\"nan\")\n",
    "    # dwell shifts by state name\n",
    "    base = pd.Series(L0).value_counts(normalize=True).sort_index()\n",
    "    pert = pd.Series(Lp).value_counts(normalize=True).sort_index()\n",
    "    # align indices\n",
    "    idx = sorted(set(base.index).union(set(pert.index)))\n",
    "    base = base.reindex(idx, fill_value=0.0); pert = pert.reindex(idx, fill_value=0.0)\n",
    "    max_abs_shift = float(np.max(np.abs(pert.values - base.values)))\n",
    "    abl_rows.append({\"ablation\": name, \"ARI_vs_canonical\": ari, \"silhouette\": sil, \"max_abs_dwell_shift\": max_abs_shift})\n",
    "\n",
    "abl_df = pd.DataFrame(abl_rows).sort_values(\"ARI_vs_canonical\", ascending=True)\n",
    "abl_df.to_csv(OUT/\"ablation_summary.csv\", index=False)\n",
    "\n",
    "print(\"\\n=== Ablation summary (worse at top) ===\")\n",
    "print(abl_df.round(3).to_string(index=False))\n",
    "\n",
    "# 2) Transition grammar\n",
    "def transition_matrix(labels, K=None):\n",
    "    L = np.asarray(labels, int)\n",
    "    if K is None: K = int(np.max(L))+1\n",
    "    T = np.zeros((K,K), dtype=float)\n",
    "    for a,b in zip(L[:-1], L[1:]):\n",
    "        T[a,b] += 1\n",
    "    Tsum = T.sum(axis=1, keepdims=True); Tsum[Tsum==0]=1.0\n",
    "    return T / Tsum\n",
    "\n",
    "K = int(np.max(L0))+1\n",
    "T_emp = transition_matrix(L0, K)\n",
    "# stationary distribution (left eigenvector of Táµ€)\n",
    "w, V = np.linalg.eig(T_emp.T)\n",
    "pi = np.real(V[:, np.argmax(np.real(w))]); pi = np.maximum(pi, 0); pi = pi / np.sum(pi)\n",
    "\n",
    "# entropy rate H = -Î£_i Ï€_i Î£_j T[i,j] log2 T[i,j]\n",
    "def entropy_rate(T, pi):\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        Hrows = -np.nansum(T * np.log2(np.where(T>0, T, 1)), axis=1)\n",
    "    return float(np.sum(pi * Hrows))\n",
    "\n",
    "H_emp = entropy_rate(T_emp, pi)\n",
    "\n",
    "# shuffle baseline: break order while preserving dwell counts (random permutation)\n",
    "rng = np.random.default_rng(42)\n",
    "def shuffle_preserve_counts(L):\n",
    "    return rng.permutation(L)\n",
    "trials = 200\n",
    "H_null = []\n",
    "for _ in range(trials):\n",
    "    Ls = shuffle_preserve_counts(L0)\n",
    "    Tn  = transition_matrix(Ls, K)\n",
    "    # stationary of null\n",
    "    wn, Vn = np.linalg.eig(Tn.T); pin = np.real(Vn[:, np.argmax(np.real(wn))]); pin = np.maximum(pin,0); pin = pin/np.sum(pin) if pin.sum()>0 else np.full(K,1.0/K)\n",
    "    H_null.append(entropy_rate(Tn, pin))\n",
    "H_null = np.array(H_null)\n",
    "\n",
    "# Save transitions report\n",
    "trans_report = {\n",
    "    \"K\": K,\n",
    "    \"emp_entropy_rate\": H_emp,\n",
    "    \"null_entropy_rate_mean\": float(np.mean(H_null)),\n",
    "    \"null_entropy_rate_p10\":  float(np.percentile(H_null, 10)),\n",
    "    \"null_entropy_rate_p90\":  float(np.percentile(H_null, 90)),\n",
    "    \"stationary_pi\": pi.tolist()\n",
    "}\n",
    "with open(OUT/\"transitions_report.json\",\"w\") as f:\n",
    "    json.dump(trans_report, f, indent=2)\n",
    "\n",
    "print(\"\\n=== Transitions ===\")\n",
    "print(f\"Entropy rate (emp): {H_emp:.3f}  | null mean: {np.mean(H_null):.3f}  (p10={np.percentile(H_null,10):.3f}, p90={np.percentile(H_null,90):.3f})\")\n",
    "print(\"Stationary Ï€:\", np.round(pi,3))\n",
    "\n",
    "# 3) Null silhouette (row-permutation baseline)\n",
    "N = len(L0)\n",
    "perm_sils = []\n",
    "for b in range(200):\n",
    "    idx = rng.permutation(N)\n",
    "    try:\n",
    "        perm_sils.append(float(silhouette_score(Z0[idx], L0)))  # keep labels, scramble geometry\n",
    "    except Exception:\n",
    "        perm_sils.append(np.nan)\n",
    "perm_sils = np.array(perm_sils)\n",
    "null_summary = {\n",
    "    \"sil_real\": sil_real,\n",
    "    \"sil_null_mean\": float(np.nanmean(perm_sils)),\n",
    "    \"sil_null_p90\":  float(np.nanpercentile(perm_sils, 90))\n",
    "}\n",
    "with open(OUT/\"silhouette_null.json\",\"w\") as f:\n",
    "    json.dump(null_summary, f, indent=2)\n",
    "\n",
    "print(\"\\n=== Silhouette null ===\")\n",
    "print(f\"sil_real={sil_real:.3f}  | null mean={np.nanmean(perm_sils):.3f}  | null p90={np.nanpercentile(perm_sils,90):.3f}\")\n",
    "\n",
    "# Small figure summarizing ablations + nulls\n",
    "fig = plt.figure(figsize=(11,6), dpi=130); gs = fig.add_gridspec(2,2, hspace=0.35, wspace=0.30)\n",
    "ax1 = fig.add_subplot(gs[0,0]); ax1.barh(abl_df[\"ablation\"], abl_df[\"ARI_vs_canonical\"]); ax1.invert_yaxis()\n",
    "ax1.set_title(\"Ablation: ARI vs canonical\"); ax1.set_xlabel(\"ARI\"); ax1.set_xlim(0,1)\n",
    "ax2 = fig.add_subplot(gs[0,1]); ax2.barh(abl_df[\"ablation\"], abl_df[\"max_abs_dwell_shift\"]); ax2.invert_yaxis()\n",
    "ax2.set_title(\"Ablation: max |Î” dwell|\"); ax2.set_xlabel(\"fraction\")\n",
    "ax3 = fig.add_subplot(gs[1,0]); ax3.hist(perm_sils[~np.isnan(perm_sils)], bins=20, alpha=0.85); \n",
    "ax3.axvline(sil_real, color=\"k\", linestyle=\"--\", label=f\"sil_real={sil_real:.3f}\"); ax3.legend(); ax3.set_title(\"Silhouette null\")\n",
    "ax4 = fig.add_subplot(gs[1,1]); \n",
    "ax4.axis(\"off\"); ax4.text(0,0.9,\"Transitions:\", fontsize=11, weight=\"bold\")\n",
    "ax4.text(0,0.75, f\"Entropy rate (emp) = {H_emp:.3f}\")\n",
    "ax4.text(0,0.62, f\"Null mean = {np.mean(H_null):.3f}  (p10={np.percentile(H_null,10):.3f}, p90={np.percentile(H_null,90):.3f})\")\n",
    "FIG = OUT/\"hypothesis_harness_summary.png\"\n",
    "fig.savefig(FIG, bbox_inches=\"tight\"); plt.close(fig)\n",
    "print(\"Saved summary â†’\", FIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99a1405e-7373-42cd-a1a7-0128fc6dc031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "\n",
      "EO/EC acc: 0.771\n",
      "pred     EC     EO\n",
      "gt                \n",
      "EC    0.542  0.458\n",
      "EO    0.000  1.000\n",
      "\n",
      "Task/Rest acc (R03 rule alphaâ†’rest): 0.504\n",
      "pred   rest   task\n",
      "gt                \n",
      "rest  0.008  0.992\n",
      "task  0.016  0.984\n",
      "\n",
      "Saved outputs â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\generalization\n"
     ]
    }
   ],
   "source": [
    "# === CNT v0.2 â€” Cross-Subject Generalization: S002/S003 (R01â€“R03) ===\n",
    "import os, json, numpy as np, pandas as pd, requests, mne\n",
    "from pathlib import Path\n",
    "from joblib import load\n",
    "from scipy.signal import welch\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "\n",
    "# Paths: use your promoted v0.2 bundle\n",
    "RUN = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")\n",
    "REP = RUN.parent / \"cog_alphabet_report_hybrid_v1\" / \"generalization\"; REP.mkdir(parents=True, exist_ok=True)\n",
    "DATA = RUN.parent / \"generalization_data\"; DATA.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load models + feature schema\n",
    "scaler = load(RUN/\"scaler_hybrid.joblib\"); pca = load(RUN/\"pca_hybrid.joblib\"); km = load(RUN/\"kmeans_hybrid.joblib\")\n",
    "F_train = pd.read_csv(RUN/\"features.csv\")\n",
    "TRAIN_COLS = list(F_train.columns)\n",
    "\n",
    "# Identify alpha state by median alpha_rel_med in the training run\n",
    "def alpha_state_from_training():\n",
    "    L = pd.read_csv(RUN/\"state_assignments.csv\")[\"state\"].to_numpy()\n",
    "    F = F_train.copy(); F[\"state\"]=L\n",
    "    med = F.groupby(\"state\")[\"alpha_rel_med\"].median()\n",
    "    return int(med.idxmax())\n",
    "ALPHA_ID = alpha_state_from_training()\n",
    "\n",
    "# Download helper\n",
    "def fetch(url, dest):\n",
    "    if dest.exists() and dest.stat().st_size>0: return dest\n",
    "    with requests.get(url, stream=True, timeout=60) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(dest, \"wb\") as f:\n",
    "            for ch in r.iter_content(8192):\n",
    "                if ch: f.write(ch)\n",
    "    return dest\n",
    "\n",
    "# Target subjects/runs (small set)\n",
    "URLS = []\n",
    "for subj in [\"S002\",\"S003\"]:\n",
    "    for r in [1,2,3]:\n",
    "        URLS.append(f\"https://physionet.org/files/eegmmidb/1.0.0/{subj}/{subj}R{r:02d}.edf\")\n",
    "\n",
    "files=[]\n",
    "for u in URLS:\n",
    "    p = DATA / u.split(\"/\")[-1]\n",
    "    try: files.append(fetch(u, p))\n",
    "    except Exception as e: print(\"skip:\", u, e)\n",
    "\n",
    "# Processing params (match v0.2)\n",
    "TARGET_SF=250.0; L_FREQ,H_FREQ=0.5,80.0; EPOCH_LEN,STEP=2.0,0.5\n",
    "BANDS = {\"delta\":(1,4),\"theta\":(4,8),\"alpha\":(8,13),\"beta_low\":(13,20),\"beta_high\":(20,35),\"gamma\":(35,80)}\n",
    "\n",
    "def load_raw_edf(p):\n",
    "    raw = mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"]-TARGET_SF)>1e-6: raw.resample(TARGET_SF, npad=\"auto\", verbose=False)\n",
    "    ny = raw.info[\"sfreq\"]/2; raw.filter(L_FREQ, min(H_FREQ, ny-1), verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()): raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "    return raw\n",
    "\n",
    "def make_epochs(raw):\n",
    "    ov = max(0.0, EPOCH_LEN-STEP)\n",
    "    return mne.make_fixed_length_epochs(raw, duration=EPOCH_LEN, overlap=ov, preload=True, verbose=False)\n",
    "\n",
    "def spectral_features(epochs):\n",
    "    X = epochs.get_data(); sf = epochs.info[\"sfreq\"]\n",
    "    n_ep, n_ch, n_t = X.shape; nper=min(int(sf*2), n_t); nov=nper//2\n",
    "    freqs, psd = welch(X, fs=sf, nperseg=nper, noverlap=nov, axis=-1, average=\"median\")\n",
    "    def idx(lo,hi): return np.where((freqs>=lo)&(freqs<hi))[0]\n",
    "    aidx=idx(L_FREQ,H_FREQ); tot=np.maximum(psd[:,:,aidx].sum(-1),1e-12)\n",
    "    feats={}; acc={\"alpha\":np.zeros((n_ep,n_ch)),\"theta\":np.zeros((n_ep,n_ch)),\"beta\":np.zeros((n_ep,n_ch))}\n",
    "    for name,(lo,hi) in BANDS.items():\n",
    "        b=idx(lo,hi); bp=psd[:,:,b].sum(-1)/tot if b.size else np.zeros((n_ep,n_ch))\n",
    "        feats[f\"{name}_rel_med\"]=np.median(bp,axis=1); feats[f\"{name}_rel_iqr\"]=np.subtract(*np.percentile(bp,[75,25],axis=1)); feats[f\"{name}_rel_std\"]=np.std(bp,axis=1)\n",
    "        head=name.split(\"_\",1)[0]\n",
    "        if head in acc: acc[head]+=bp\n",
    "    alpha=np.median(acc[\"alpha\"],axis=1); theta=np.median(acc[\"theta\"],axis=1); beta=np.median(acc[\"beta\"],axis=1)\n",
    "    feats[\"alpha_rel_med\"]=alpha; feats[\"theta_rel_med\"]=theta; feats[\"beta_rel_med\"]=beta\n",
    "    feats[\"theta_over_alpha\"]=theta/np.maximum(alpha,1e-6); feats[\"beta_over_alpha\"]=beta/np.maximum(alpha,1e-6)\n",
    "    feats[\"(alpha+theta)_over_beta\"]=(alpha+theta)/np.maximum(beta,1e-6)\n",
    "    p=psd[:,:,aidx]; p_n=p/np.maximum(p.sum(-1,keepdims=True),1e-12)\n",
    "    H=-(p_n*np.log2(p_n+1e-12)).sum(-1); Hn=H/np.log2(p.shape[-1])\n",
    "    feats[\"spec_entropy_med\"]=np.median(Hn,axis=1); feats[\"spec_entropy_std\"]=np.std(Hn,axis=1)\n",
    "    f=freqs[aidx].reshape(1,1,-1); cen=(p*f).sum(-1)/np.maximum(p.sum(-1),1e-12)\n",
    "    feats[\"spec_centroid_med\"]=np.median(cen,axis=1); feats[\"spec_centroid_std\"]=np.std(cen,axis=1)\n",
    "    # Hjorth\n",
    "    d1=np.diff(X,axis=-1); var0=np.var(X,axis=-1)+1e-12; var1=np.var(d1,axis=-1)+1e-12\n",
    "    feats[\"hjorth_activity_med\"]=np.median(var0,axis=1); feats[\"hjorth_mobility_med\"]=np.median(np.sqrt(var1/var0),axis=1)\n",
    "    d2=np.diff(d1,axis=-1); var2=np.var(d2,axis=-1)+1e-12\n",
    "    feats[\"hjorth_complexity_med\"]=np.median(np.sqrt((var2/var1)/(var1/var0)),axis=1)\n",
    "    # fbCSP task proba not computed for new subjects here â†’ zeros\n",
    "    feats[\"fbCSP_task_proba\"]=np.zeros(n_ep)\n",
    "    return pd.DataFrame(feats)\n",
    "\n",
    "def run_file(p):\n",
    "    raw = load_raw_edf(p)\n",
    "    epochs = make_epochs(raw)\n",
    "    if len(epochs)==0: return None\n",
    "    F = spectral_features(epochs)\n",
    "    F = F.reindex(columns=TRAIN_COLS, fill_value=0.0)\n",
    "    Z = pca.transform(scaler.transform(F.values))\n",
    "    lab = km.predict(Z)\n",
    "    starts = epochs.events[:,0]/epochs.info[\"sfreq\"]; ends = starts+EPOCH_LEN\n",
    "    df = pd.DataFrame({\"file\": p.name, \"t_start_s\": starts, \"t_end_s\": ends, \"state\": lab})\n",
    "    return df, F, raw\n",
    "\n",
    "# EO/EC map via file name; R02=EC, R01=EO\n",
    "def coarse_cond(fn):\n",
    "    import re\n",
    "    m = re.search(r\"R(\\d+)\", fn); r = int(m.group(1)) if m else None\n",
    "    return {1:\"EO\", 2:\"EC\", 3:\"ME\"}.get(r, \"UNK\")\n",
    "\n",
    "# Execute\n",
    "rows=[]; preds=[]; eoec_rows=[]; task_rows=[]\n",
    "for p in files:\n",
    "    out = run_file(p)\n",
    "    if out is None: \n",
    "        print(\"no epochs:\", p.name); continue\n",
    "    df, Fm, raw = out\n",
    "    rows.append(df)\n",
    "    # EO/EC rule: alpha-state â†’ EC else EO\n",
    "    cond = coarse_cond(p.name)\n",
    "    if cond in (\"EO\",\"EC\"):\n",
    "        is_alpha = (df[\"state\"].to_numpy()==ALPHA_ID).astype(int)\n",
    "        pred = np.where(is_alpha==1, \"EC\", \"EO\")\n",
    "        eoec_rows.append(pd.DataFrame({\"file\": p.name, \"gt\": cond, \"pred\": pred}))\n",
    "    # Task/rest via annotations (R03 only)\n",
    "    if cond==\"ME\" and getattr(raw, \"annotations\", None) is not None and len(raw.annotations)>0:\n",
    "        gt=[]\n",
    "        for t0,t1 in zip(df[\"t_start_s\"], df[\"t_end_s\"]):\n",
    "            ov_task=ov_rest=0.0\n",
    "            for o,d,s in zip(raw.annotations.onset, raw.annotations.duration, raw.annotations.description):\n",
    "                su=str(s).upper()\n",
    "                if \"T0\" in su or \"T1\" in su or \"T2\" in su:\n",
    "                    L,R=max(t0,float(o)),min(t1,float(o)+float(d))\n",
    "                    if R>L:\n",
    "                        if \"T1\" in su or \"T2\" in su: ov_task+=R-L\n",
    "                        elif \"T0\" in su:            ov_rest+=R-L\n",
    "            if ov_task>=0.5*(t1-t0): gt.append(\"task\")\n",
    "            elif ov_rest>=0.5*(t1-t0): gt.append(\"rest\")\n",
    "            else: gt.append(None)\n",
    "        g = np.array([x if x is not None else \"UNK\" for x in gt])\n",
    "        # Simple: non-alpha â†’ task; alpha â†’ rest\n",
    "        pred_task = np.where(df[\"state\"].to_numpy()==ALPHA_ID, \"rest\", \"task\")\n",
    "        task_rows.append(pd.DataFrame({\"file\": p.name, \"gt\": g, \"pred\": pred_task}))\n",
    "\n",
    "ALL = pd.concat(rows, ignore_index=True)\n",
    "ALL.to_csv(REP/\"generalization_assignments.csv\", index=False)\n",
    "\n",
    "# EO/EC accuracy\n",
    "if eoec_rows:\n",
    "    E = pd.concat(eoec_rows, ignore_index=True)\n",
    "    acc = (E[\"gt\"]==E[\"pred\"]).mean()\n",
    "    cm  = pd.crosstab(E[\"gt\"], E[\"pred\"], normalize=\"index\").round(3)\n",
    "    E.to_csv(REP/\"eoec_preds.csv\", index=False); cm.to_csv(REP/\"eoec_cm.csv\")\n",
    "    print(\"\\nEO/EC acc:\", round(float(acc),3)); print(cm)\n",
    "\n",
    "# Task/rest (R03)\n",
    "if task_rows:\n",
    "    T = pd.concat(task_rows, ignore_index=True)\n",
    "    T = T[T[\"gt\"].isin([\"task\",\"rest\"])]\n",
    "    if len(T):\n",
    "        acc_t = (T[\"gt\"]==T[\"pred\"]).mean()\n",
    "        cm_t  = pd.crosstab(T[\"gt\"], T[\"pred\"], normalize=\"index\").round(3)\n",
    "        T.to_csv(REP/\"task_preds.csv\", index=False); cm_t.to_csv(REP/\"task_cm.csv\")\n",
    "        print(\"\\nTask/Rest acc (R03 rule alphaâ†’rest):\", round(float(acc_t),3)); print(cm_t)\n",
    "\n",
    "# Per-file mix\n",
    "mix = ALL.groupby(\"file\")[\"state\"].value_counts(normalize=True).rename(\"fraction\").reset_index()\n",
    "mix_pivot = mix.pivot(index=\"file\", columns=\"state\", values=\"fraction\").fillna(0.0)\n",
    "mix_pivot.to_csv(REP/\"generalization_letter_mix.csv\")\n",
    "\n",
    "print(\"\\nSaved outputs â†’\", REP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4297fe13-038d-4697-a24c-8c9587b49843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "S002 EO/EC (IAF): 0.811\n",
      "pred     EC     EO\n",
      "gt                \n",
      "EC    0.689  0.311\n",
      "EO    0.067  0.933\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "S003 EO/EC (IAF): 0.861\n",
      "pred     EC     EO\n",
      "gt                \n",
      "EC    0.723  0.277\n",
      "EO    0.000  1.000\n",
      "\n",
      "Saved â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\generalization\n"
     ]
    }
   ],
   "source": [
    "# === EO/EC: per-subject IAF + occipital alpha index ===\n",
    "import re, numpy as np, pandas as pd, mne, json\n",
    "from scipy.signal import welch\n",
    "from pathlib import Path\n",
    "\n",
    "RUN = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")\n",
    "GEN = RUN.parent / \"generalization_data\"   # where S002/S003 were fetched\n",
    "OUT = RUN.parent / \"cog_alphabet_report_hybrid_v1\" / \"generalization\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_raw(p):\n",
    "    raw = mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "    sf = raw.info[\"sfreq\"]\n",
    "    if abs(sf-250)>1e-6: raw.resample(250.0, npad=\"auto\", verbose=False)\n",
    "    sf = raw.info[\"sfreq\"]; ny = sf/2\n",
    "    raw.filter(0.5, min(45.0, ny-1.0), verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "    return raw\n",
    "\n",
    "def occipital_picks(raw):\n",
    "    names = [n.upper() for n in raw.ch_names]\n",
    "    idx = [i for i,n in enumerate(names) if re.match(r'(^O[12Z]$)|(^PO[Z12]$)', n)]\n",
    "    return idx if idx else mne.pick_types(raw.info, eeg=True)\n",
    "\n",
    "def psd(raw, t0=0, t1=None):\n",
    "    sf = raw.info[\"sfreq\"]; s = int(t0*sf); e = None if t1 is None else int(t1*sf)\n",
    "    X = raw.get_data(start=s, stop=e)  # (n_ch, n_t)\n",
    "    nper = min(int(sf*2), X.shape[1]); nov = nper//2\n",
    "    f, P = welch(X, fs=sf, nperseg=nper, noverlap=nov, axis=-1, average=\"median\") # (n_ch, n_f)\n",
    "    return f, P\n",
    "\n",
    "def iaf_from_ec(raw):\n",
    "    f, P = psd(raw)\n",
    "    idx = (f>=7) & (f<=14)\n",
    "    Pocc = P[occipital_picks(raw)][:, idx].mean(0)\n",
    "    iaf = f[idx][np.argmax(Pocc)]\n",
    "    return float(np.clip(iaf, 8.0, 12.0))\n",
    "\n",
    "def eoec_for_subject(subj):\n",
    "    r01 = next((p for p in GEN.glob(f\"{subj}R01.edf\")), None)\n",
    "    r02 = next((p for p in GEN.glob(f\"{subj}R02.edf\")), None)\n",
    "    if not (r01 and r02): return None\n",
    "    raw_ec = load_raw(r02); raw_eo = load_raw(r01)\n",
    "    iaf = iaf_from_ec(raw_ec)\n",
    "    band = (max(6.0, iaf-2.0), min(14.0, iaf+2.0))\n",
    "\n",
    "    def alpha_index(raw, band):\n",
    "        f, P = psd(raw); idx = (f>=band[0]) & (f<band[1])\n",
    "        a_pow = P[:, idx].sum(-1); tot = P.sum(-1) + 1e-12\n",
    "        oi = occipital_picks(raw)\n",
    "        return float((a_pow[oi].sum() / tot[oi].sum()) if len(oi) else (a_pow.sum()/tot.sum()))\n",
    "\n",
    "    # Epoch both runs and classify epoch-wise by occipital alpha index threshold\n",
    "    def classify_run(raw, cond):\n",
    "        sf = raw.info[\"sfreq\"]; win=2.0; hop=0.5\n",
    "        ts = np.arange(0, raw.n_times/sf - win + 1e-9, hop)\n",
    "        preds=[]; gts=[]\n",
    "        # threshold is mid of EC vs EO global indices\n",
    "        ec_idx = alpha_index(raw_ec, band); eo_idx = alpha_index(raw_eo, band)\n",
    "        thr = 0.5*(ec_idx + eo_idx)\n",
    "        for t in ts:\n",
    "            f, P = psd(raw, t, t+win)\n",
    "            idx = (f>=band[0]) & (f<band[1]); a = P[:, idx].sum(-1); tot = P.sum(-1)+1e-12\n",
    "            oi = occipital_picks(raw)\n",
    "            ai = (a[oi].sum()/tot[oi].sum()) if len(oi) else (a.sum()/tot.sum())\n",
    "            preds.append(\"EC\" if ai>=thr else \"EO\"); gts.append(cond)\n",
    "        return pd.DataFrame({\"gt\":gts, \"pred\":preds})\n",
    "\n",
    "    df = pd.concat([classify_run(raw_ec,\"EC\"), classify_run(raw_eo,\"EO\")], ignore_index=True)\n",
    "    acc = (df[\"gt\"]==df[\"pred\"]).mean()\n",
    "    cm = pd.crosstab(df[\"gt\"], df[\"pred\"], normalize=\"index\").round(3)\n",
    "    return acc, cm, iaf, band\n",
    "\n",
    "rows=[]\n",
    "for subj in [\"S002\",\"S003\"]:\n",
    "    try:\n",
    "        out = eoec_for_subject(subj)\n",
    "        if out:\n",
    "            acc, cm, iaf, band = out\n",
    "            rows.append({\"subject\": subj, \"IAF\": iaf, \"alpha_band\": band, \"acc\": float(acc)})\n",
    "            cm.to_csv(OUT/f\"{subj}_eoec_cm_iaf.csv\")\n",
    "            print(subj, \"EO/EC (IAF):\", round(acc,3)); print(cm)\n",
    "    except Exception as e:\n",
    "        print(\"skip\", subj, e)\n",
    "\n",
    "pd.DataFrame(rows).to_csv(OUT/\"eoec_iaf_summary.csv\", index=False)\n",
    "print(\"\\nSaved â†’\", OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcfaa0ea-e5a1-4f90-ba1f-8045f2b1216e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.4e-06 (2.2e-16 eps * 3 dim * 2.2e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.3e-06 (2.2e-16 eps * 3 dim * 1.9e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.4e-06 (2.2e-16 eps * 3 dim * 2.1e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.4e-06 (2.2e-16 eps * 3 dim * 2.1e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.2e-06 (2.2e-16 eps * 3 dim * 1.9e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (413) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (413) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (255) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (255) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (413) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (413) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (255) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (255) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.4e-06 (2.2e-16 eps * 3 dim * 2.1e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.4e-06 (2.2e-16 eps * 3 dim * 2.1e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.3e-06 (2.2e-16 eps * 3 dim * 1.9e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.5e-06 (2.2e-16 eps * 3 dim * 2.2e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.4e-06 (2.2e-16 eps * 3 dim * 2.2e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (413) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (413) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (255) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (255) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (413) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (413) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (255) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (255) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Using tolerance 1.3e-06 (2.2e-16 eps * 3 dim * 1.9e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.4e-06 (2.2e-16 eps * 3 dim * 2.2e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.4e-06 (2.2e-16 eps * 3 dim * 2.1e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.3e-06 (2.2e-16 eps * 3 dim * 1.9e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.5e-06 (2.2e-16 eps * 3 dim * 2.2e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "\n",
      "S002 R03 FBCSP â€” AUC=0.641  CM=[[36, 24], [20, 40]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        rest      0.643     0.600     0.621        60\n",
      "        task      0.625     0.667     0.645        60\n",
      "\n",
      "    accuracy                          0.633       120\n",
      "   macro avg      0.634     0.633     0.633       120\n",
      "weighted avg      0.634     0.633     0.633       120\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (413) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (413) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (255) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (255) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.5e-06 (2.2e-16 eps * 3 dim * 3.8e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2e-06 (2.2e-16 eps * 3 dim * 3e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.9e-06 (2.2e-16 eps * 3 dim * 2.9e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.6e-06 (2.2e-16 eps * 3 dim * 3.9e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2e-06 (2.2e-16 eps * 3 dim * 3.1e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.9e-06 (2.2e-16 eps * 3 dim * 2.9e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (413) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (413) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (255) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (255) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (413) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (413) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (255) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (255) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (413) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.5e-06 (2.2e-16 eps * 3 dim * 3.7e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2e-06 (2.2e-16 eps * 3 dim * 3e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.9e-06 (2.2e-16 eps * 3 dim * 2.8e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.6e-06 (2.2e-16 eps * 3 dim * 3.8e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2e-06 (2.2e-16 eps * 3 dim * 3e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.9e-06 (2.2e-16 eps * 3 dim * 2.9e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (413) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (255) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (255) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (413) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (413) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (255) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (255) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (413) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (413) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.5e-06 (2.2e-16 eps * 3 dim * 3.8e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2e-06 (2.2e-16 eps * 3 dim * 3e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 1.9e-06 (2.2e-16 eps * 3 dim * 2.9e+09  max singular value)\n",
      "    Estimated rank (data): 3\n",
      "    data: rank 3 computed from 3 data channels with 0 projectors\n",
      "Reducing data rank from 3 -> 3\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "\n",
      "S003 R03 FBCSP â€” AUC=0.501  CM=[[27, 33], [27, 33]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        rest      0.500     0.450     0.474        60\n",
      "        task      0.500     0.550     0.524        60\n",
      "\n",
      "    accuracy                          0.500       120\n",
      "   macro avg      0.500     0.500     0.499       120\n",
      "weighted avg      0.500     0.500     0.499       120\n",
      "\n",
      "\n",
      "Saved â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\generalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (255) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\2171112935.py:46: RuntimeWarning: filter_length (255) is longer than the signal (250), distortion is likely. Reduce filter length or filter a longer signal.\n",
      "  Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n"
     ]
    }
   ],
   "source": [
    "# === R03 task/rest on S002/S003: Î¼/Î² FBCSP (grouped CV) ===\n",
    "import re, numpy as np, pandas as pd, mne\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from mne.decoding import CSP\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "DATA = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\").parent / \"generalization_data\"\n",
    "OUT  = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\").parent / \"cog_alphabet_report_hybrid_v1\" / \"generalization\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def fbCSP_for_R03(p):\n",
    "    raw = mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"]-250)>1e-6: raw.resample(250.0, npad=\"auto\", verbose=False)\n",
    "    raw.filter(0.5, 40.0, verbose=False)\n",
    "    # pick C3/Cz/C4 or fall back to EEG\n",
    "    chs = raw.ch_names; roi=[]\n",
    "    for target in [\"C3\",\"Cz\",\"C4\"]:\n",
    "        for c in chs:\n",
    "            if re.fullmatch(target, c, re.I) or re.search(rf\"^{target}\\b\", c, re.I):\n",
    "                roi.append(c)\n",
    "    if len(roi)<2: roi=[c for c,t in zip(raw.ch_names, raw.get_channel_types()) if t==\"eeg\"]\n",
    "    raw.pick(roi)\n",
    "    anns = raw.annotations\n",
    "    if anns is None or len(anns)==0: return None\n",
    "    # build 1.0s non-overlapping epochs inside T0/T1/T2 segments\n",
    "    segs=[]; gid=0\n",
    "    for o,d,s in zip(anns.onset, anns.duration, anns.description):\n",
    "        su=str(s).upper()\n",
    "        if \"T0\" in su or \"T1\" in su or \"T2\" in su:\n",
    "            lab = 0 if \"T0\" in su else 1\n",
    "            segs.append((float(o), float(o+d), lab, gid)); gid+=1\n",
    "    X=[]; y=[]; g=[]\n",
    "    for (a,b,lab,segid) in segs:\n",
    "        t=a\n",
    "        while t+1.0 <= b-1e-6:\n",
    "            s=int(t*raw.info[\"sfreq\"]); e=s+int(1.0*raw.info[\"sfreq\"])\n",
    "            X.append(raw.get_data(start=s, stop=e)); y.append(lab); g.append(segid); t+=1.0\n",
    "    if not X: return None\n",
    "    X=np.stack(X,0); y=np.array(y,int); g=np.array(g,int)\n",
    "\n",
    "    # FBCSP (Î¼ 8â€“13, Î²l 13â€“20, Î²h 20â€“30), GroupKFold by segment id\n",
    "    def bp(X, lo, hi, sf):\n",
    "        X2=X.reshape(X.shape[0]*X.shape[1], X.shape[2])\n",
    "        Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, verbose=False)\n",
    "        return Xf.reshape(X.shape)\n",
    "    sf=raw.info[\"sfreq\"]; bands=[(8,13),(13,20),(20,30)]\n",
    "    gkf=GroupKFold(n_splits=min(5, max(2, len(np.unique(g)))))\n",
    "    yh=np.zeros_like(y); proba=np.zeros_like(y, dtype=float)\n",
    "    for tr,te in gkf.split(X,y,g):\n",
    "        feats_tr, feats_te = [], []\n",
    "        for lo,hi in bands:\n",
    "            Xtr=bp(X[tr],lo,hi,sf); Xte=bp(X[te],lo,hi,sf)\n",
    "            csp=CSP(n_components=6, reg=\"ledoit_wolf\", log=True, norm_trace=False)\n",
    "            csp.fit(Xtr, y[tr]); feats_tr.append(csp.transform(Xtr)); feats_te.append(csp.transform(Xte))\n",
    "        Xtr_fb=np.concatenate(feats_tr,1); Xte_fb=np.concatenate(feats_te,1)\n",
    "        clf=LDA(solver=\"lsqr\", shrinkage=\"auto\").fit(Xtr_fb, y[tr])\n",
    "        yh[te]=clf.predict(Xte_fb); proba[te]=clf.predict_proba(Xte_fb)[:,1]\n",
    "    auc=roc_auc_score(y, proba); cm=confusion_matrix(y, yh, labels=[0,1])\n",
    "    rep=classification_report(y, yh, target_names=[\"rest\",\"task\"], digits=3)\n",
    "    return auc, cm, rep\n",
    "\n",
    "rows=[]\n",
    "for subj in [\"S002\",\"S003\"]:\n",
    "    p = next((q for q in DATA.glob(f\"{subj}R03.edf\")), None)\n",
    "    if not p: continue\n",
    "    out = fbCSP_for_R03(p)\n",
    "    if out is None:\n",
    "        print(\"No annotations or epochs:\", subj); continue\n",
    "    auc, cm, rep = out\n",
    "    rows.append({\"subject\":subj, \"AUC_task\":float(auc)})\n",
    "    print(f\"\\n{subj} R03 FBCSP â€” AUC={auc:.3f}  CM={cm.tolist()}\\n{rep}\")\n",
    "\n",
    "pd.DataFrame(rows).to_csv(OUT/\"task_fbcsp_summary.csv\", index=False)\n",
    "print(\"\\nSaved â†’\", OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e763109f-c5f7-4708-8878-b852218ea543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9e-05 (2.2e-16 eps * 64 dim * 6.3e+09  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.9e-05 (2.2e-16 eps * 64 dim * 4.9e+09  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.5e-05 (2.2e-16 eps * 64 dim * 5.3e+09  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.9e-05 (2.2e-16 eps * 64 dim * 6.3e+09  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.7e-05 (2.2e-16 eps * 64 dim * 4.7e+09  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.5e-05 (2.2e-16 eps * 64 dim * 5.3e+09  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9e-05 (2.2e-16 eps * 64 dim * 6.3e+09  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.9e-05 (2.2e-16 eps * 64 dim * 4.9e+09  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.7e-05 (2.2e-16 eps * 64 dim * 5.4e+09  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9.4e-05 (2.2e-16 eps * 64 dim * 6.6e+09  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.1e-05 (2.2e-16 eps * 64 dim * 5e+09  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.7e-05 (2.2e-16 eps * 64 dim * 5.4e+09  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9.3e-05 (2.2e-16 eps * 64 dim * 6.5e+09  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.9e-05 (2.2e-16 eps * 64 dim * 4.9e+09  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.7e-05 (2.2e-16 eps * 64 dim * 5.4e+09  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "\n",
      "S002 R03 FBCSP v2 â€” AUC=0.818  CM=[[22, 8], [5, 25]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        rest      0.815     0.733     0.772        30\n",
      "        task      0.758     0.833     0.794        30\n",
      "\n",
      "    accuracy                          0.783        60\n",
      "   macro avg      0.786     0.783     0.783        60\n",
      "weighted avg      0.786     0.783     0.783        60\n",
      "\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00018 (2.2e-16 eps * 64 dim * 1.2e+10  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00015 (2.2e-16 eps * 64 dim * 1e+10  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00013 (2.2e-16 eps * 64 dim * 9e+09  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00018 (2.2e-16 eps * 64 dim * 1.2e+10  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00015 (2.2e-16 eps * 64 dim * 1.1e+10  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00013 (2.2e-16 eps * 64 dim * 9e+09  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00017 (2.2e-16 eps * 64 dim * 1.2e+10  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00014 (2.2e-16 eps * 64 dim * 1e+10  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00012 (2.2e-16 eps * 64 dim * 8.7e+09  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00018 (2.2e-16 eps * 64 dim * 1.3e+10  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00015 (2.2e-16 eps * 64 dim * 1e+10  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00013 (2.2e-16 eps * 64 dim * 9.1e+09  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00017 (2.2e-16 eps * 64 dim * 1.2e+10  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00015 (2.2e-16 eps * 64 dim * 1e+10  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00013 (2.2e-16 eps * 64 dim * 9.2e+09  max singular value)\n",
      "    Estimated rank (data): 64\n",
      "    data: rank 64 computed from 64 data channels with 0 projectors\n",
      "Reducing data rank from 64 -> 64\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "\n",
      "S003 R03 FBCSP v2 â€” AUC=0.931  CM=[[24, 6], [2, 28]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        rest      0.923     0.800     0.857        30\n",
      "        task      0.824     0.933     0.875        30\n",
      "\n",
      "    accuracy                          0.867        60\n",
      "   macro avg      0.873     0.867     0.866        60\n",
      "weighted avg      0.873     0.867     0.866        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === R03 task/rest â€” FBCSP v2: 2.0s, IIR Butter, wider ROI ===\n",
    "import re, numpy as np, pandas as pd, mne\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from mne.decoding import CSP\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "DATA = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\").parent / \"generalization_data\"\n",
    "OUT  = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\").parent / \"cog_alphabet_report_hybrid_v1\" / \"generalization\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def butter_bandpass_array(X, lo, hi, sf, order=4):\n",
    "    # shape (n_ep, n_ch, n_t)\n",
    "    X2 = X.reshape(X.shape[0]*X.shape[1], X.shape[2])\n",
    "    Xf = mne.filter.filter_data(\n",
    "        X2, sfreq=sf, l_freq=lo, h_freq=hi,\n",
    "        method=\"iir\", iir_params=dict(order=order, ftype=\"butter\"),\n",
    "        verbose=False\n",
    "    )\n",
    "    return Xf.reshape(X.shape)\n",
    "\n",
    "def fbcsp_v2_for_R03(p):\n",
    "    raw = mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"]-250)>1e-6: raw.resample(250.0, npad=\"auto\", verbose=False)\n",
    "    raw.filter(0.5, 40.0, verbose=False)  # wide prefilter\n",
    "\n",
    "    # Expanded motor ROI\n",
    "    want = {\"C3\",\"Cz\",\"C4\",\"FC3\",\"FCz\",\"FC4\",\"CP3\",\"CPz\",\"CP4\"}\n",
    "    picks = [i for i,ch in enumerate(raw.ch_names) if ch.upper().strip() in want]\n",
    "    if len(picks) < 3:\n",
    "        picks = mne.pick_types(raw.info, eeg=True)  # fallback\n",
    "    raw.pick(picks)\n",
    "\n",
    "    anns = raw.annotations\n",
    "    if anns is None or len(anns)==0: return None\n",
    "\n",
    "    # 2.0s non-overlapping epochs fully inside T0/T1/T2\n",
    "    sf = raw.info[\"sfreq\"]\n",
    "    segs=[]; gid=0\n",
    "    for o,d,s in zip(anns.onset, anns.duration, anns.description):\n",
    "        su = str(s).upper()\n",
    "        if \"T0\" in su or \"T1\" in su or \"T2\" in su:\n",
    "            lab = 0 if \"T0\" in su else 1\n",
    "            segs.append((float(o), float(o+d), lab, gid)); gid+=1\n",
    "    X=[]; y=[]; g=[]\n",
    "    for (a,b,lab,segid) in segs:\n",
    "        t=a\n",
    "        while t+2.0 <= b-1e-6:\n",
    "            s=int(t*sf); e=s+int(2.0*sf)\n",
    "            X.append(raw.get_data(start=s, stop=e)); y.append(lab); g.append(segid); t+=2.0\n",
    "    if not X: return None\n",
    "    X=np.stack(X,0); y=np.array(y,int); g=np.array(g,int)\n",
    "\n",
    "    # Filter-bank (Î¼, Î²l, Î²h) with IIR Butter\n",
    "    bands=[(8,13),(13,20),(20,30)]\n",
    "    gkf=GroupKFold(n_splits=min(5, max(2, len(np.unique(g)))))\n",
    "    yh=np.zeros_like(y); proba=np.zeros_like(y, float)\n",
    "\n",
    "    for tr,te in gkf.split(X,y,g):\n",
    "        feats_tr, feats_te = [], []\n",
    "        for lo,hi in bands:\n",
    "            Xtr = butter_bandpass_array(X[tr], lo, hi, sf)\n",
    "            Xte = butter_bandpass_array(X[te], lo, hi, sf)\n",
    "            csp = CSP(n_components=6, reg=\"ledoit_wolf\", log=True, norm_trace=False)\n",
    "            csp.fit(Xtr, y[tr])\n",
    "            feats_tr.append(csp.transform(Xtr))\n",
    "            feats_te.append(csp.transform(Xte))\n",
    "        Xtr_fb = np.concatenate(feats_tr,1); Xte_fb = np.concatenate(feats_te,1)\n",
    "        clf = LDA(solver=\"lsqr\", shrinkage=\"auto\").fit(Xtr_fb, y[tr])\n",
    "        yh[te] = clf.predict(Xte_fb); proba[te] = clf.predict_proba(Xte_fb)[:,1]\n",
    "\n",
    "    auc = roc_auc_score(y, proba)\n",
    "    cm  = confusion_matrix(y, yh, labels=[0,1])\n",
    "    rep = classification_report(y, yh, target_names=[\"rest\",\"task\"], digits=3)\n",
    "    return auc, cm, rep\n",
    "\n",
    "for subj in [\"S002\",\"S003\"]:\n",
    "    p = next((q for q in DATA.glob(f\"{subj}R03.edf\")), None)\n",
    "    if not p: continue\n",
    "    out = fbcsp_v2_for_R03(p)\n",
    "    if out is None:\n",
    "        print(\"No annotations or epochs:\", subj); continue\n",
    "    auc, cm, rep = out\n",
    "    print(f\"\\n{subj} R03 FBCSP v2 â€” AUC={auc:.3f}  CM={cm.tolist()}\\n{rep}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b87708e6-affb-41ee-b606-9316a94f19d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Ensemble: CSP + band log-variance (Î¼, Î²l, Î²h) ===\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Assume you defined butter_bandpass_array, built X,y,g as in v2 above\n",
    "\n",
    "def logvar_features(X):\n",
    "    # X: (n_ep, n_ch, n_t) â†’ log-var per channel\n",
    "    v = np.log(np.var(X, axis=-1) + 1e-12)\n",
    "    return v.reshape(v.shape[0], -1)\n",
    "\n",
    "def ensemble_probs(X, y, g, sf):\n",
    "    # build bands\n",
    "    bands=[(8,13),(13,20),(20,30)]\n",
    "    from mne.decoding import CSP\n",
    "    from sklearn.model_selection import GroupKFold\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "    gkf=GroupKFold(n_splits=min(5, max(2, len(np.unique(g)))))\n",
    "    proba=np.zeros(len(y), float)\n",
    "    for tr,te in gkf.split(X,y,g):\n",
    "        # CSP head\n",
    "        feats_tr, feats_te = [], []\n",
    "        for lo,hi in bands:\n",
    "            Xtr = butter_bandpass_array(X[tr], lo, hi, sf)\n",
    "            Xte = butter_bandpass_array(X[te], lo, hi, sf)\n",
    "            csp = CSP(n_components=6, reg=\"ledoit_wolf\", log=True, norm_trace=False)\n",
    "            csp.fit(Xtr, y[tr])\n",
    "            feats_tr.append(csp.transform(Xtr))\n",
    "            feats_te.append(csp.transform(Xte))\n",
    "        Xtr_fb = np.concatenate(feats_tr,1); Xte_fb = np.concatenate(feats_te,1)\n",
    "        lda = LDA(solver=\"lsqr\", shrinkage=\"auto\").fit(Xtr_fb, y[tr])\n",
    "        p1  = lda.predict_proba(Xte_fb)[:,1]\n",
    "\n",
    "        # log-var head\n",
    "        feats2_tr, feats2_te = [], []\n",
    "        for lo,hi in bands:\n",
    "            feats2_tr.append(logvar_features(butter_bandpass_array(X[tr], lo, hi, sf)))\n",
    "            feats2_te.append(logvar_features(butter_bandpass_array(X[te], lo, hi, sf)))\n",
    "        Xtr_lv = np.concatenate(feats2_tr,1); Xte_lv = np.concatenate(feats2_te,1)\n",
    "        lr  = LogisticRegression(max_iter=200, C=1.0).fit(Xtr_lv, y[tr])\n",
    "        p2  = lr.predict_proba(Xte_lv)[:,1]\n",
    "\n",
    "        proba[te] = 0.5*p1 + 0.5*p2\n",
    "    return proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6571181a-b88e-49de-9e1e-5bb60ec49b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00015 (2.2e-16 eps * 64 dim * 1e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.8e-05 (2.2e-16 eps * 64 dim * 6.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00016 (2.2e-16 eps * 64 dim * 1.1e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.9e-05 (2.2e-16 eps * 64 dim * 6.3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00016 (2.2e-16 eps * 64 dim * 1.1e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.9e-05 (2.2e-16 eps * 64 dim * 6.3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00016 (2.2e-16 eps * 64 dim * 1.1e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.8e-05 (2.2e-16 eps * 64 dim * 6.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00016 (2.2e-16 eps * 64 dim * 1.1e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.9e-05 (2.2e-16 eps * 64 dim * 6.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00011 (2.2e-16 eps * 64 dim * 7.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "\n",
      "S002 per-file letter mix:\n",
      "state            0      1      2      3\n",
      "file                                   \n",
      "S002R01.edf  0.000  0.782  0.067  0.151\n",
      "S002R02.edf  0.269  0.731  0.000  0.000\n",
      "S002R03.edf  0.004  0.951  0.000  0.045\n",
      "\n",
      "Task-proba by state (R03):\n",
      "   state   mean    n subject\n",
      "1      1  0.512  231    S002\n",
      "2      3  0.426   11    S002\n",
      "0      0  0.273    1    S002\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00021 (2.2e-16 eps * 64 dim * 1.5e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8e-05 (2.2e-16 eps * 64 dim * 5.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00021 (2.2e-16 eps * 64 dim * 1.5e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9.9e-05 (2.2e-16 eps * 64 dim * 7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.9e-05 (2.2e-16 eps * 64 dim * 5.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00021 (2.2e-16 eps * 64 dim * 1.5e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9.9e-05 (2.2e-16 eps * 64 dim * 7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.1e-05 (2.2e-16 eps * 64 dim * 5.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00021 (2.2e-16 eps * 64 dim * 1.5e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9.6e-05 (2.2e-16 eps * 64 dim * 6.8e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8e-05 (2.2e-16 eps * 64 dim * 5.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0002 (2.2e-16 eps * 64 dim * 1.4e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9.8e-05 (2.2e-16 eps * 64 dim * 6.9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.9e-05 (2.2e-16 eps * 64 dim * 5.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "\n",
      "S003 per-file letter mix:\n",
      "state            0      1      2      3\n",
      "file                                   \n",
      "S003R01.edf  0.000  0.193  0.487  0.319\n",
      "S003R02.edf  0.815  0.017  0.008  0.160\n",
      "S003R03.edf  0.012  0.243  0.296  0.449\n",
      "\n",
      "Task-proba by state (R03):\n",
      "   state   mean    n subject\n",
      "0      0  0.633    3    S003\n",
      "3      3  0.543  111    S003\n",
      "2      2  0.481   73    S003\n",
      "1      1  0.445   60    S003\n",
      "\n",
      "Wrote integrated outputs to: E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\generalization\n"
     ]
    }
   ],
   "source": [
    "# === CNT v0.2 â€” Integrate S002/S003 into the alphabet (add fbCSP_task_proba, assign letters) ===\n",
    "import re, json, numpy as np, pandas as pd, mne\n",
    "from pathlib import Path\n",
    "from joblib import load\n",
    "from scipy.signal import welch\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from mne.decoding import CSP\n",
    "\n",
    "# Paths\n",
    "RUN   = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")                 # promoted v0.2\n",
    "GEN   = RUN.parent / \"generalization_data\"                               # where S002/S003 *R01â€“R03* were fetched\n",
    "OUT   = RUN.parent / \"cog_alphabet_report_hybrid_v1\" / \"generalization\"  # write here\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Models + training schema\n",
    "scaler = load(RUN/\"scaler_hybrid.joblib\")\n",
    "pca    = load(RUN/\"pca_hybrid.joblib\")\n",
    "km     = load(RUN/\"kmeans_hybrid.joblib\")\n",
    "TRAIN_COLS = list(pd.read_csv(RUN/\"features.csv\", nrows=1).columns)\n",
    "\n",
    "# v0.2 processing params\n",
    "TARGET_SF=250.0; L_FREQ,H_FREQ=0.5,80.0; EPOCH_LEN,STEP=2.0,0.5\n",
    "BANDS = {\"delta\":(1,4),\"theta\":(4,8),\"alpha\":(8,13),\"beta_low\":(13,20),\"beta_high\":(20,35),\"gamma\":(35,80)}\n",
    "\n",
    "def load_raw_edf(p):\n",
    "    raw = mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"]-TARGET_SF)>1e-6: raw.resample(TARGET_SF, npad=\"auto\", verbose=False)\n",
    "    ny = raw.info[\"sfreq\"]/2.; raw.filter(L_FREQ, min(H_FREQ, ny-1.), verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()): raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "    return raw\n",
    "\n",
    "def make_epochs(raw):\n",
    "    ov = max(0.0, EPOCH_LEN-STEP)\n",
    "    return mne.make_fixed_length_epochs(raw, duration=EPOCH_LEN, overlap=ov, preload=True, verbose=False)\n",
    "\n",
    "def spectral_features(epochs):\n",
    "    X = epochs.get_data(); sf = epochs.info[\"sfreq\"]\n",
    "    n_ep,n_ch,n_t = X.shape; nper=min(int(sf*2), n_t); nov=nper//2\n",
    "    freqs, psd = welch(X, fs=sf, nperseg=nper, noverlap=nov, axis=-1, average=\"median\")\n",
    "    def idx(lo,hi): return np.where((freqs>=lo)&(freqs<hi))[0]\n",
    "    aidx=idx(L_FREQ,H_FREQ); tot=np.maximum(psd[:,:,aidx].sum(-1),1e-12)\n",
    "    feats={}; acc={\"alpha\":np.zeros((n_ep,n_ch)),\"theta\":np.zeros((n_ep,n_ch)),\"beta\":np.zeros((n_ep,n_ch))}\n",
    "    for name,(lo,hi) in BANDS.items():\n",
    "        b=idx(lo,hi); bp=psd[:,:,b].sum(-1)/tot if b.size else np.zeros((n_ep,n_ch))\n",
    "        feats[f\"{name}_rel_med\"]=np.median(bp,axis=1); feats[f\"{name}_rel_iqr\"]=np.subtract(*np.percentile(bp,[75,25],axis=1)); feats[f\"{name}_rel_std\"]=np.std(bp,axis=1)\n",
    "        head=name.split(\"_\",1)[0]; \n",
    "        if head in acc: acc[head]+=bp\n",
    "    alpha=np.median(acc[\"alpha\"],axis=1); theta=np.median(acc[\"theta\"],axis=1); beta=np.median(acc[\"beta\"],axis=1)\n",
    "    feats[\"alpha_rel_med\"]=alpha; feats[\"theta_rel_med\"]=theta; feats[\"beta_rel_med\"]=beta\n",
    "    feats[\"theta_over_alpha\"]=theta/np.maximum(alpha,1e-6); feats[\"beta_over_alpha\"]=beta/np.maximum(alpha,1e-6)\n",
    "    feats[\"(alpha+theta)_over_beta\"]=(alpha+theta)/np.maximum(beta,1e-6)\n",
    "    p=psd[:,:,aidx]; p_n=p/np.maximum(p.sum(-1,keepdims=True),1e-12)\n",
    "    H=-(p_n*np.log2(p_n+1e-12)).sum(-1); Hn=H/np.log2(p.shape[-1])\n",
    "    feats[\"spec_entropy_med\"]=np.median(Hn,axis=1); feats[\"spec_entropy_std\"]=np.std(Hn,axis=1)\n",
    "    f=freqs[aidx].reshape(1,1,-1); cen=(p*f).sum(-1)/np.maximum(p.sum(-1),1e-12)\n",
    "    feats[\"spec_centroid_med\"]=np.median(cen,axis=1); feats[\"spec_centroid_std\"]=np.std(cen,axis=1)\n",
    "    # Hjorth\n",
    "    d1=np.diff(X,axis=-1); var0=np.var(X,axis=-1)+1e-12; var1=np.var(d1,axis=-1)+1e-12\n",
    "    feats[\"hjorth_activity_med\"]=np.median(var0,axis=1); feats[\"hjorth_mobility_med\"]=np.median(np.sqrt(var1/var0),axis=1)\n",
    "    d2=np.diff(d1,axis=-1); var2=np.var(d2,axis=-1)+1e-12\n",
    "    feats[\"hjorth_complexity_med\"]=np.median(np.sqrt((var2/var1)/(var1/var0)),axis=1)\n",
    "    return pd.DataFrame(feats)\n",
    "\n",
    "def butter_bandpass_array(X, lo, hi, sf, order=4):\n",
    "    X2 = X.reshape(X.shape[0]*X.shape[1], X.shape[2])\n",
    "    Xf = mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi,\n",
    "                                method=\"iir\", iir_params=dict(order=order, ftype=\"butter\"),\n",
    "                                verbose=False)\n",
    "    return Xf.reshape(X.shape)\n",
    "\n",
    "def fbCSP_task_proba_for_R03(raw, starts, ends):\n",
    "    # Expanded motor ROI\n",
    "    want = {\"C3\",\"Cz\",\"C4\",\"FC3\",\"FCz\",\"FC4\",\"CP3\",\"CPz\",\"CP4\"}\n",
    "    picks = [i for i,ch in enumerate(raw.ch_names) if ch.upper().strip() in want]\n",
    "    if len(picks) < 3: picks = mne.pick_types(raw.info, eeg=True)\n",
    "    raw = raw.copy().pick(picks)\n",
    "    sf  = raw.info[\"sfreq\"]\n",
    "\n",
    "    # Build epoch tensor aligned to starts/ends\n",
    "    X=[]\n",
    "    for t0,t1 in zip(starts, ends):\n",
    "        s=int(round(t0*sf)); e=int(round(t1*sf))\n",
    "        X.append(raw.get_data(start=s, stop=e))\n",
    "    X=np.stack(X,0)\n",
    "\n",
    "    # Labels from annotations (â‰¥50% overlap)\n",
    "    y=[]; g=[]; anns = raw.annotations\n",
    "    for i,(t0,t1) in enumerate(zip(starts, ends)):\n",
    "        lab=-1; best=-1.0; gid=-1\n",
    "        if anns is not None and len(anns):\n",
    "            for j,(o,d,s) in enumerate(zip(anns.onset, anns.duration, anns.description)):\n",
    "                su=str(s).upper()\n",
    "                if \"T0\" in su or \"T1\" in su or \"T2\" in su:\n",
    "                    L,R=max(t0,float(o)),min(t1,float(o)+float(d))\n",
    "                    if R>L and (R-L)>best:\n",
    "                        best=R-L; gid=j; lab=0 if \"T0\" in su else 1\n",
    "        if best>=0.5*(t1-t0): y.append(lab); g.append(gid)\n",
    "        else: y.append(-1); g.append(-1)\n",
    "    y=np.array(y,int); g=np.array(g,int)\n",
    "    if not np.any(y>=0): return np.zeros(len(y))\n",
    "\n",
    "    # FBCSP v2 (Î¼, Î²l, Î²h) with GroupKFold by segment\n",
    "    bands=[(8,13),(13,20),(20,30)]\n",
    "    mask=(y>=0); yt=y[mask]; gt=g[mask]; Xt=X[mask]\n",
    "    gkf=GroupKFold(n_splits=min(5, max(2,len(np.unique(gt)))))\n",
    "    proba = np.zeros(len(yt), float)\n",
    "    for tr,te in gkf.split(Xt, yt, gt):\n",
    "        feats_tr, feats_te = [], []\n",
    "        for lo,hi in bands:\n",
    "            Xtr = butter_bandpass_array(Xt[tr], lo, hi, sf)\n",
    "            Xte = butter_bandpass_array(Xt[te], lo, hi, sf)\n",
    "            csp = CSP(n_components=6, reg=\"ledoit_wolf\", log=True, norm_trace=False)\n",
    "            csp.fit(Xtr, yt[tr])\n",
    "            feats_tr.append(csp.transform(Xtr))\n",
    "            feats_te.append(csp.transform(Xte))\n",
    "        Xtr_fb = np.concatenate(feats_tr,1); Xte_fb = np.concatenate(feats_te,1)\n",
    "        clf = LDA(solver=\"lsqr\", shrinkage=\"auto\").fit(Xtr_fb, yt[tr])\n",
    "        proba[te] = clf.predict_proba(Xte_fb)[:,1]\n",
    "    # Place back into full set\n",
    "    full = np.zeros(len(y), float); full[np.where(mask)[0]] = proba\n",
    "    return full\n",
    "\n",
    "def integrate_subject(subj):\n",
    "    out_rows=[]; state_summ=[]\n",
    "    for r in [1,2,3]:\n",
    "        p = GEN / f\"{subj}R{r:02d}.edf\"\n",
    "        if not p.exists(): continue\n",
    "        raw = load_raw_edf(p)\n",
    "        epochs = make_epochs(raw)\n",
    "        if len(epochs)==0: continue\n",
    "        Fsp = spectral_features(epochs)\n",
    "        starts = epochs.events[:,0]/epochs.info[\"sfreq\"]; ends = starts+EPOCH_LEN\n",
    "        if r==3:\n",
    "            try: fb = fbCSP_task_proba_for_R03(raw, starts, ends)\n",
    "            except Exception: fb = np.zeros(len(starts))\n",
    "        else:\n",
    "            fb = np.zeros(len(starts))\n",
    "        Fsp[\"fbCSP_task_proba\"] = fb\n",
    "        Fsp = Fsp.reindex(columns=TRAIN_COLS, fill_value=0.0)\n",
    "\n",
    "        Z = pca.transform(scaler.transform(Fsp.values))\n",
    "        lab = km.predict(Z)\n",
    "        df = pd.DataFrame({\"subject\":subj,\"file\":p.name,\"t_start_s\":starts,\"t_end_s\":ends,\"state\":lab,\"fb_task\":fb})\n",
    "        out_rows.append(df)\n",
    "\n",
    "        # per-state task-proba (for R03 only)\n",
    "        if r==3:\n",
    "            st = pd.DataFrame({\"state\":lab,\"proba\":fb}).groupby(\"state\").agg(mean=(\"proba\",\"mean\"), n=(\"proba\",\"size\"))\n",
    "            st[\"subject\"] = subj; state_summ.append(st.reset_index())\n",
    "\n",
    "    if not out_rows: return None, None\n",
    "    ALL = pd.concat(out_rows, ignore_index=True)\n",
    "    ALL.to_csv(OUT/f\"{subj}_assignments_with_fbCSP.csv\", index=False)\n",
    "    SUM = None\n",
    "    if state_summ:\n",
    "        SUM = pd.concat(state_summ, ignore_index=True).sort_values([\"subject\",\"mean\"], ascending=[True,False])\n",
    "        SUM.to_csv(OUT/f\"{subj}_state_taskprob_summary.csv\", index=False)\n",
    "    return ALL, SUM\n",
    "\n",
    "# Run integration\n",
    "for subj in [\"S002\",\"S003\"]:\n",
    "    ALL,SUM = integrate_subject(subj)\n",
    "    if ALL is None:\n",
    "        print(\"No epochs:\", subj); continue\n",
    "    print(f\"\\n{subj} per-file letter mix:\")\n",
    "    mix = (ALL.groupby(\"file\")[\"state\"].value_counts(normalize=True)\n",
    "            .rename(\"fraction\").reset_index()\n",
    "            .pivot(index=\"file\", columns=\"state\", values=\"fraction\").fillna(0.0))\n",
    "    print(mix.round(3))\n",
    "    if SUM is not None:\n",
    "        print(\"\\nTask-proba by state (R03):\")\n",
    "        print(SUM.round(3))\n",
    "print(\"\\nWrote integrated outputs to:\", OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "209c6fd8-0649-438c-8878-aa653ccab8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Wrote calibration â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\generalization\\subject_calibration.json\n",
      "{\n",
      "  \"S002\": {\n",
      "    \"IAF_hz\": 11.5,\n",
      "    \"alpha_band\": [\n",
      "      9.5,\n",
      "      13.5\n",
      "    ],\n",
      "    \"alpha_threshold\": 0.25391908287818066,\n",
      "    \"fbCSP_threshold\": 0.5,\n",
      "    \"task_letters\": [\n",
      "      1\n",
      "    ]\n",
      "  },\n",
      "  \"S003\": {\n",
      "    \"IAF_hz\": 10.5,\n",
      "    \"alpha_band\": [\n",
      "      8.5,\n",
      "      12.5\n",
      "    ],\n",
      "    \"alpha_threshold\": 0.18461962425884562,\n",
      "    \"fbCSP_threshold\": 0.5,\n",
      "    \"task_letters\": [\n",
      "      3\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# === CNT v0.2 â€” Subject Calibration + Upgraded Decoder (IAF EO/EC + fbCSP task) ===\n",
    "import os, re, json, numpy as np, pandas as pd, mne\n",
    "from pathlib import Path\n",
    "from joblib import load\n",
    "from scipy.signal import welch\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from mne.decoding import CSP\n",
    "\n",
    "# --- Paths (edit if needed) ---\n",
    "RUN   = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")                         # promoted v0.2\n",
    "GEN   = RUN.parent / \"generalization_data\"                                       # where S002/S003 R01â€“R03 live\n",
    "OUTR  = RUN.parent / \"cog_alphabet_report_hybrid_v1\" / \"generalization\"          # outputs here\n",
    "OUTR.mkdir(parents=True, exist_ok=True)\n",
    "CALIB = OUTR / \"subject_calibration.json\"\n",
    "\n",
    "# --- Load models / feature schema ---\n",
    "scaler = load(RUN/\"scaler_hybrid.joblib\")\n",
    "pca    = load(RUN/\"pca_hybrid.joblib\")\n",
    "km     = load(RUN/\"kmeans_hybrid.joblib\")\n",
    "TRAIN_COLS = list(pd.read_csv(RUN/\"features.csv\", nrows=1).columns)\n",
    "\n",
    "# --- Shared params (match v0.2) ---\n",
    "TARGET_SF=250.0\n",
    "L_FREQ,H_FREQ = 0.5, 80.0\n",
    "EPOCH_LEN, STEP = 2.0, 0.5\n",
    "BANDS = {\"delta\":(1,4),\"theta\":(4,8),\"alpha\":(8,13),\"beta_low\":(13,20),\"beta_high\":(20,35),\"gamma\":(35,80)}\n",
    "\n",
    "# =========================\n",
    "# Util: loaders + features\n",
    "# =========================\n",
    "def load_raw_edf(p):\n",
    "    raw = mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"]-TARGET_SF)>1e-6:\n",
    "        raw.resample(TARGET_SF, npad=\"auto\", verbose=False)\n",
    "    ny = raw.info[\"sfreq\"]/2.0\n",
    "    raw.filter(L_FREQ, min(H_FREQ, ny-1.0), verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "    return raw\n",
    "\n",
    "def make_epochs(raw):\n",
    "    ov = max(0.0, EPOCH_LEN-STEP)\n",
    "    return mne.make_fixed_length_epochs(raw, duration=EPOCH_LEN, overlap=ov, preload=True, verbose=False)\n",
    "\n",
    "def spectral_features(epochs):\n",
    "    X = epochs.get_data(); sf = epochs.info[\"sfreq\"]\n",
    "    n_ep,n_ch,n_t = X.shape; nper=min(int(sf*2), n_t); nov=nper//2\n",
    "    freqs, psd = welch(X, fs=sf, nperseg=nper, noverlap=nov, axis=-1, average=\"median\")\n",
    "    def idx(lo,hi): return np.where((freqs>=lo)&(freqs<hi))[0]\n",
    "    aidx=idx(L_FREQ,H_FREQ); tot=np.maximum(psd[:,:,aidx].sum(-1),1e-12)\n",
    "    feats={}; acc={\"alpha\":np.zeros((n_ep,n_ch)),\"theta\":np.zeros((n_ep,n_ch)),\"beta\":np.zeros((n_ep,n_ch))}\n",
    "    for name,(lo,hi) in BANDS.items():\n",
    "        b=idx(lo,hi); bp=psd[:,:,b].sum(-1)/tot if b.size else np.zeros((n_ep,n_ch))\n",
    "        feats[f\"{name}_rel_med\"]=np.median(bp,axis=1); feats[f\"{name}_rel_iqr\"]=np.subtract(*np.percentile(bp,[75,25],axis=1)); feats[f\"{name}_rel_std\"]=np.std(bp,axis=1)\n",
    "        head=name.split(\"_\",1)[0]\n",
    "        if head in acc: acc[head]+=bp\n",
    "    alpha=np.median(acc[\"alpha\"],axis=1); theta=np.median(acc[\"theta\"],axis=1); beta=np.median(acc[\"beta\"],axis=1)\n",
    "    feats[\"alpha_rel_med\"]=alpha; feats[\"theta_rel_med\"]=theta; feats[\"beta_rel_med\"]=beta\n",
    "    feats[\"theta_over_alpha\"]=theta/np.maximum(alpha,1e-6); feats[\"beta_over_alpha\"]=beta/np.maximum(alpha,1e-6)\n",
    "    feats[\"(alpha+theta)_over_beta\"]=(alpha+theta)/np.maximum(beta,1e-6)\n",
    "    p=psd[:,:,aidx]; p_n=p/np.maximum(p.sum(-1,keepdims=True),1e-12)\n",
    "    H=-(p_n*np.log2(p_n+1e-12)).sum(-1); Hn=H/np.log2(p.shape[-1])\n",
    "    feats[\"spec_entropy_med\"]=np.median(Hn,axis=1); feats[\"spec_entropy_std\"]=np.std(Hn,axis=1)\n",
    "    f=freqs[aidx].reshape(1,1,-1); cen=(p*f).sum(-1)/np.maximum(p.sum(-1),1e-12)\n",
    "    feats[\"spec_centroid_med\"]=np.median(cen,axis=1); feats[\"spec_centroid_std\"]=np.std(cen,axis=1)\n",
    "    # Hjorth\n",
    "    d1=np.diff(X,axis=-1); var0=np.var(X,axis=-1)+1e-12; var1=np.var(d1,axis=-1)+1e-12\n",
    "    feats[\"hjorth_activity_med\"]=np.median(var0,axis=1); feats[\"hjorth_mobility_med\"]=np.median(np.sqrt(var1/var0),axis=1)\n",
    "    d2=np.diff(d1,axis=-1); var2=np.var(d2,axis=-1)+1e-12\n",
    "    feats[\"hjorth_complexity_med\"]=np.median(np.sqrt((var2/var1)/(var1/var0)),axis=1)\n",
    "    return pd.DataFrame(feats)\n",
    "\n",
    "# =========================\n",
    "# EO/EC calibration helpers\n",
    "# =========================\n",
    "def occipital_picks(raw):\n",
    "    names = [n.upper().strip() for n in raw.ch_names]\n",
    "    wanted = (\"O1\",\"O2\",\"OZ\",\"POZ\",\"PO3\",\"PO4\")\n",
    "    idx = [i for i,n in enumerate(names) if n in wanted]\n",
    "    return idx if idx else mne.pick_types(raw.info, eeg=True)\n",
    "\n",
    "def iaf_and_threshold(subj):\n",
    "    \"\"\"Compute IAF from R02 (EC) and alpha-index threshold from R01/R02.\"\"\"\n",
    "    r01 = GEN / f\"{subj}R01.edf\"\n",
    "    r02 = GEN / f\"{subj}R02.edf\"\n",
    "    if not (r01.exists() and r02.exists()):\n",
    "        return None\n",
    "    raw_ec = load_raw_edf(r02); raw_eo = load_raw_edf(r01)\n",
    "    # IAF from EC, 7â€“14 Hz\n",
    "    sf = raw_ec.info[\"sfreq\"]\n",
    "    f, P = welch(raw_ec.get_data(), fs=sf, nperseg=min(int(sf*2), raw_ec.n_times), noverlap=int(min(int(sf*2), raw_ec.n_times)/2), axis=-1, average=\"median\")\n",
    "    band = (f>=7) & (f<=14)\n",
    "    Pocc = P[occipital_picks(raw_ec)][:, band].mean(0)\n",
    "    iaf = float(np.clip(f[band][np.argmax(Pocc)], 8.0, 12.0))\n",
    "    alpha_band = (max(6.0, iaf-2.0), min(14.0, iaf+2.0))\n",
    "\n",
    "    def alpha_index(raw, band):\n",
    "        sf = raw.info[\"sfreq\"]\n",
    "        f, P = welch(raw.get_data(), fs=sf, nperseg=min(int(sf*2), raw.n_times), noverlap=int(min(int(sf*2), raw.n_times)/2), axis=-1, average=\"median\")\n",
    "        idx = (f>=band[0]) & (f<band[1])\n",
    "        a_pow = P[:, idx].sum(-1); tot = P.sum(-1) + 1e-12\n",
    "        oi = occipital_picks(raw)\n",
    "        return float((a_pow[oi].sum() / tot[oi].sum()) if len(oi) else (a_pow.sum()/tot.sum()))\n",
    "    ec_idx = alpha_index(raw_ec, alpha_band)\n",
    "    eo_idx = alpha_index(raw_eo, alpha_band)\n",
    "    thr = float(0.5*(ec_idx + eo_idx))\n",
    "    return {\"IAF_hz\": iaf, \"alpha_band\": alpha_band, \"alpha_threshold\": thr}\n",
    "\n",
    "# =========================\n",
    "# fbCSP (task) helper (v2)\n",
    "# =========================\n",
    "def butter_bandpass_array(X, lo, hi, sf, order=4):\n",
    "    X2 = X.reshape(X.shape[0]*X.shape[1], X.shape[2])\n",
    "    Xf = mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi,\n",
    "                                method=\"iir\", iir_params=dict(order=order, ftype=\"butter\"), verbose=False)\n",
    "    return Xf.reshape(X.shape)\n",
    "\n",
    "def fbCSP_task_proba_for_R03(raw, starts, ends):\n",
    "    want = {\"C3\",\"Cz\",\"C4\",\"FC3\",\"FCz\",\"FC4\",\"CP3\",\"CPz\",\"CP4\"}\n",
    "    picks = [i for i,ch in enumerate(raw.ch_names) if ch.upper().strip() in want]\n",
    "    if len(picks) < 3: picks = mne.pick_types(raw.info, eeg=True)\n",
    "    sf  = raw.info[\"sfreq\"]; _raw = raw.copy().pick(picks)\n",
    "\n",
    "    # build epoch tensor\n",
    "    X=[]\n",
    "    for t0,t1 in zip(starts, ends):\n",
    "        s=int(round(t0*sf)); e=int(round(t1*sf))\n",
    "        X.append(_raw.get_data(start=s, stop=e))\n",
    "    X=np.stack(X,0)\n",
    "\n",
    "    # labels from annotations (>=50% overlap)\n",
    "    y=[]; g=[]; anns = _raw.annotations\n",
    "    for t0,t1 in zip(starts, ends):\n",
    "        lab=-1; best=-1.0; gid=-1\n",
    "        if anns is not None and len(anns):\n",
    "            for j,(o,d,s) in enumerate(zip(anns.onset, anns.duration, anns.description)):\n",
    "                su=str(s).upper()\n",
    "                if \"T0\" in su or \"T1\" in su or \"T2\" in su:\n",
    "                    L,R=max(t0,float(o)),min(t1,float(o)+float(d))\n",
    "                    if R>L and (R-L)>best:\n",
    "                        best=R-L; gid=j; lab=0 if \"T0\" in su else 1\n",
    "        if best>=0.5*(t1-t0): y.append(lab); g.append(gid)\n",
    "        else: y.append(-1); g.append(-1)\n",
    "    y=np.array(y,int); g=np.array(g,int)\n",
    "    if not np.any(y>=0): return np.zeros(len(y))\n",
    "\n",
    "    bands=[(8,13),(13,20),(20,30)]\n",
    "    mask=(y>=0); yt=y[mask]; gt=g[mask]; Xt=X[mask]\n",
    "    gkf=GroupKFold(n_splits=min(5, max(2, len(np.unique(gt)))))\n",
    "    proba=np.zeros(len(yt), float)\n",
    "\n",
    "    for tr,te in gkf.split(Xt, yt, gt):\n",
    "        feats_tr, feats_te = [], []\n",
    "        for lo,hi in bands:\n",
    "            Xtr = butter_bandpass_array(Xt[tr], lo, hi, sf)\n",
    "            Xte = butter_bandpass_array(Xt[te], lo, hi, sf)\n",
    "            csp = CSP(n_components=6, reg=\"ledoit_wolf\", log=True, norm_trace=False)\n",
    "            csp.fit(Xtr, yt[tr])\n",
    "            feats_tr.append(csp.transform(Xtr))\n",
    "            feats_te.append(csp.transform(Xte))\n",
    "        Xtr_fb = np.concatenate(feats_tr,1); Xte_fb = np.concatenate(feats_te,1)\n",
    "        clf = LDA(solver=\"lsqr\", shrinkage=\"auto\").fit(Xtr_fb, yt[tr])\n",
    "        proba[te] = clf.predict_proba(Xte_fb)[:,1]\n",
    "\n",
    "    full=np.zeros(len(y), float); full[np.where(mask)[0]] = proba\n",
    "    return full\n",
    "\n",
    "# =========================\n",
    "# Build/Update calibration\n",
    "# =========================\n",
    "def derive_task_letters_from_summary(subj, min_n=30, min_mean=0.53):\n",
    "    summ_path = OUTR/f\"{subj}_state_taskprob_summary.csv\"\n",
    "    if not summ_path.exists(): return []\n",
    "    df = pd.read_csv(summ_path)\n",
    "    df = df.sort_values(\"mean\", ascending=False)\n",
    "    picks = df[(df[\"n\"]>=min_n) & (df[\"mean\"]>=min_mean)][\"state\"].astype(int).tolist()\n",
    "    if not picks and len(df):\n",
    "        # fallback: take top 1 with n>=20\n",
    "        dff = df[df[\"n\"]>=20]\n",
    "        if len(dff):\n",
    "            picks = [int(dff.iloc[0][\"state\"])]\n",
    "    return picks\n",
    "\n",
    "calib = {}\n",
    "for subj in [\"S002\",\"S003\"]:\n",
    "    iaf_block = iaf_and_threshold(subj)\n",
    "    if iaf_block:\n",
    "        calib[subj] = {**iaf_block,\n",
    "                       \"fbCSP_threshold\": 0.50,\n",
    "                       \"task_letters\": derive_task_letters_from_summary(subj)}\n",
    "with open(CALIB, \"w\") as f:\n",
    "    json.dump(calib, f, indent=2)\n",
    "print(\"Wrote calibration â†’\", CALIB)\n",
    "print(json.dumps(calib, indent=2))\n",
    "\n",
    "# =========================\n",
    "# Upgraded decoder\n",
    "# =========================\n",
    "def decode_with_calibration(infile, subject_id=None, out_csv=None):\n",
    "    \"\"\"\n",
    "    Assign letters with v0.2 models; if subject_id is known:\n",
    "      - R01/R02: add EO/EC per epoch via IAF alpha-index threshold\n",
    "      - R03: compute fbCSP_task_proba and add task_pred via (state in task_letters) OR (proba >= fbCSP_threshold)\n",
    "    \"\"\"\n",
    "    infile = Path(infile)\n",
    "    raw = load_raw_edf(infile)\n",
    "    epochs = make_epochs(raw)\n",
    "    if len(epochs)==0:\n",
    "        raise RuntimeError(\"No epochs extracted.\")\n",
    "    # spectral + letters\n",
    "    Fsp = spectral_features(epochs)\n",
    "    starts = epochs.events[:,0]/epochs.info[\"sfreq\"]; ends = starts + EPOCH_LEN\n",
    "    # placeholder fb_task\n",
    "    fb = np.zeros(len(starts))\n",
    "    # per-subject augmentations\n",
    "    eoec_pred = np.array([\"UNK\"]*len(starts), dtype=object)\n",
    "    task_pred = np.array([\"UNK\"]*len(starts), dtype=object)\n",
    "\n",
    "    calib_local = {}\n",
    "    if subject_id and CALIB.exists():\n",
    "        calib_all = json.load(open(CALIB))\n",
    "        calib_local = calib_all.get(subject_id, {})\n",
    "\n",
    "    # EO/EC via IAF alpha-index if calibration present and run is R01 or R02\n",
    "    m = re.search(r\"R(\\d+)\", infile.name)\n",
    "    run_num = int(m.group(1)) if m else None\n",
    "    if subject_id and calib_local and run_num in (1,2):\n",
    "        band = calib_local.get(\"alpha_band\", [8.0, 13.0])\n",
    "        thr  = calib_local.get(\"alpha_threshold\", None)\n",
    "        if thr is not None:\n",
    "            # per-epoch occipital alpha index\n",
    "            sf = raw.info[\"sfreq\"]\n",
    "            for i,(t0,t1) in enumerate(zip(starts, ends)):\n",
    "                s=int(round(t0*sf)); e=int(round(t1*sf))\n",
    "                seg = raw.get_data(start=s, stop=e)\n",
    "                f,P = welch(seg, fs=sf, nperseg=min(int(sf*2), e-s), noverlap=int(min(int(sf*2), e-s)/2),\n",
    "                            axis=-1, average=\"median\")\n",
    "                idx = (f>=band[0]) & (f<band[1])\n",
    "                a = P[:, idx].sum(-1); tot = P.sum(-1)+1e-12\n",
    "                oi = occipital_picks(raw)\n",
    "                ai = (a[oi].sum()/tot[oi].sum()) if len(oi) else (a.sum()/tot.sum())\n",
    "                eoec_pred[i] = \"EC\" if ai>=thr else \"EO\"\n",
    "\n",
    "    # fbCSP on R03\n",
    "    if subject_id and calib_local and run_num==3:\n",
    "        try:\n",
    "            fb = fbCSP_task_proba_for_R03(raw, starts, ends)\n",
    "        except Exception:\n",
    "            fb = np.zeros(len(starts))\n",
    "        Fsp[\"fbCSP_task_proba\"] = fb\n",
    "\n",
    "    # assign letters\n",
    "    Fsp = Fsp.reindex(columns=TRAIN_COLS, fill_value=0.0)\n",
    "    Z = pca.transform(scaler.transform(Fsp.values))\n",
    "    letters = km.predict(Z)\n",
    "\n",
    "    # task_pred using subject's task letters and/or fbCSP threshold\n",
    "    if subject_id and calib_local and run_num==3:\n",
    "        thr = float(calib_local.get(\"fbCSP_threshold\", 0.50))\n",
    "        task_letters = set(map(int, calib_local.get(\"task_letters\", [])))\n",
    "        for i, st in enumerate(letters):\n",
    "            task_pred[i] = \"task\" if (st in task_letters or fb[i] >= thr) else \"rest\"\n",
    "\n",
    "    # write csv\n",
    "    out = pd.DataFrame({\n",
    "        \"subject\": subject_id or \"UNK\",\n",
    "        \"file\": infile.name,\n",
    "        \"t_start_s\": starts, \"t_end_s\": ends,\n",
    "        \"state\": letters,\n",
    "        \"fb_task\": fb,\n",
    "        \"eoec_pred\": eoec_pred,\n",
    "        \"task_pred\": task_pred\n",
    "    })\n",
    "    if out_csv is None:\n",
    "        out_csv = OUTR / f\"decode_{subject_id or 'UNK'}_{infile.stem}.csv\"\n",
    "    out.to_csv(out_csv, index=False)\n",
    "    print(\"Saved:\", out_csv)\n",
    "    return out\n",
    "\n",
    "# === Example usage ===\n",
    "# decode_with_calibration(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\\generalization_data\\S002R01.edf\", subject_id=\"S002\")\n",
    "# decode_with_calibration(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\\generalization_data\\S002R03.edf\", subject_id=\"S002\")\n",
    "# decode_with_calibration(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\\generalization_data\\S003R03.edf\", subject_id=\"S003\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13845fd9-32e5-4fb8-abe5-4e7dfe52f9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved report â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\generalization\\CNT_CognitiveAlphabet_v0_3_Generalization.png\n",
      "Saved report â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\generalization\\CNT_CognitiveAlphabet_v0_3_Generalization.pdf\n",
      "Saved summary â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\generalization\\generalization_summary.json\n"
     ]
    }
   ],
   "source": [
    "# === CNT Cognitive Alphabet v0.3 â€” Generalization Report (EO/EC + R03 Task) ===\n",
    "# Outputs:\n",
    "#  - E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\generalization\\CNT_CognitiveAlphabet_v0_3_Generalization.(png|pdf)\n",
    "#  - ...\\generalization\\generalization_summary.json\n",
    "\n",
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "BASE = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\generalization\")\n",
    "CAL  = BASE / \"subject_calibration.json\"\n",
    "EOEC_SUM = BASE / \"eoec_iaf_summary.csv\"\n",
    "TASK_SUM = BASE / \"task_fbcsp_summary.csv\"\n",
    "MIX      = BASE / \"generalization_letter_mix.csv\"\n",
    "\n",
    "# --------- Load what we can, safely ---------\n",
    "def _read_csv(p):\n",
    "    try:\n",
    "        if p.exists():\n",
    "            return pd.read_csv(p)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "calib = json.load(open(CAL)) if CAL.exists() else {}\n",
    "eoec  = _read_csv(EOEC_SUM)\n",
    "task  = _read_csv(TASK_SUM)\n",
    "mix   = _read_csv(MIX)\n",
    "\n",
    "# Confusion matrices (EO/EC) if present\n",
    "def load_cm_csv(subject):\n",
    "    p = BASE / f\"{subject}_eoec_cm_iaf.csv\"\n",
    "    if p.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(p, index_col=0)\n",
    "            # ensure 2x2 order\n",
    "            cols = [\"EC\",\"EO\"]\n",
    "            idx  = [\"EC\",\"EO\"]\n",
    "            for c in cols:\n",
    "                if c not in df.columns: df[c] = 0.0\n",
    "            for r in idx:\n",
    "                if r not in df.index: df.loc[r] = [0.0, 0.0]\n",
    "            df = df.loc[idx, cols]\n",
    "            return df.round(3)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "cm_s002 = load_cm_csv(\"S002\")\n",
    "cm_s003 = load_cm_csv(\"S003\")\n",
    "\n",
    "# Per-state task-prob summaries (if saved)\n",
    "def load_state_taskprob(subject):\n",
    "    p = BASE / f\"{subject}_state_taskprob_summary.csv\"\n",
    "    if p.exists():\n",
    "        try:\n",
    "            df = pd.read_csv(p)\n",
    "            # keep top three by mean probability\n",
    "            return (df.sort_values(\"mean\", ascending=False)\n",
    "                      .head(3)[[\"state\",\"mean\",\"n\"]]\n",
    "                      .reset_index(drop=True))\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "st_s002 = load_state_taskprob(\"S002\")\n",
    "st_s003 = load_state_taskprob(\"S003\")\n",
    "\n",
    "# --------- Build a small metrics object for JSON ---------\n",
    "summary = {\"subjects\": {}}\n",
    "for subj in [\"S002\",\"S003\"]:\n",
    "    entry = {}\n",
    "    if subj in calib:\n",
    "        entry[\"IAF_hz\"] = calib[subj].get(\"IAF_hz\")\n",
    "        entry[\"alpha_band\"] = calib[subj].get(\"alpha_band\")\n",
    "        entry[\"alpha_threshold\"] = calib[subj].get(\"alpha_threshold\")\n",
    "        entry[\"task_letters\"] = calib[subj].get(\"task_letters\", [])\n",
    "        entry[\"fbCSP_threshold\"] = calib[subj].get(\"fbCSP_threshold\", 0.50)\n",
    "    if eoec is not None and \"subject\" in eoec.columns:\n",
    "        row = eoec[eoec[\"subject\"]==subj]\n",
    "        if len(row):\n",
    "            entry[\"eoec_acc\"] = float(row[\"acc\"].iloc[0])\n",
    "    if task is not None and \"subject\" in task.columns:\n",
    "        row = task[task[\"subject\"]==subj]\n",
    "        if len(row):\n",
    "            entry[\"r03_auc_task\"] = float(row[\"AUC_task\"].iloc[0])\n",
    "    summary[\"subjects\"][subj] = entry\n",
    "\n",
    "# Overall aggregates\n",
    "if eoec is not None and \"acc\" in eoec.columns and len(eoec):\n",
    "    summary[\"eoec_acc_mean\"] = float(eoec[\"acc\"].mean())\n",
    "if task is not None and \"AUC_task\" in task.columns and len(task):\n",
    "    summary[\"r03_auc_mean\"] = float(task[\"AUC_task\"].mean())\n",
    "\n",
    "# --------- Figure layout ---------\n",
    "fig = plt.figure(figsize=(12, 8), dpi=130)\n",
    "gs  = fig.add_gridspec(2, 3, height_ratios=[1,1], width_ratios=[1,1,1], hspace=0.35, wspace=0.30)\n",
    "\n",
    "# (A) EO/EC accuracies bar\n",
    "axA = fig.add_subplot(gs[0,0])\n",
    "subs, accs = [], []\n",
    "if \"S002\" in summary[\"subjects\"] and \"eoec_acc\" in summary[\"subjects\"][\"S002\"]:\n",
    "    subs.append(\"S002\"); accs.append(summary[\"subjects\"][\"S002\"][\"eoec_acc\"])\n",
    "if \"S003\" in summary[\"subjects\"] and \"eoec_acc\" in summary[\"subjects\"][\"S003\"]:\n",
    "    subs.append(\"S003\"); accs.append(summary[\"subjects\"][\"S003\"][\"eoec_acc\"])\n",
    "axA.bar(np.arange(len(accs)), accs)\n",
    "axA.set_xticks(np.arange(len(accs))); axA.set_xticklabels(subs)\n",
    "axA.set_ylim(0,1)\n",
    "axA.set_ylabel(\"accuracy\")\n",
    "axA.set_title(\"EO vs EC (IAF-based)\")\n",
    "\n",
    "# (B) Task AUCs bar\n",
    "axB = fig.add_subplot(gs[0,1])\n",
    "subs2, aucs = [], []\n",
    "if \"S002\" in summary[\"subjects\"] and \"r03_auc_task\" in summary[\"subjects\"][\"S002\"]:\n",
    "    subs2.append(\"S002\"); aucs.append(summary[\"subjects\"][\"S002\"][\"r03_auc_task\"])\n",
    "if \"S003\" in summary[\"subjects\"] and \"r03_auc_task\" in summary[\"subjects\"][\"S003\"]:\n",
    "    subs2.append(\"S003\"); aucs.append(summary[\"subjects\"][\"S003\"][\"r03_auc_task\"])\n",
    "axB.bar(np.arange(len(aucs)), aucs)\n",
    "axB.set_xticks(np.arange(len(aucs))); axB.set_xticklabels(subs2)\n",
    "axB.set_ylim(0,1)\n",
    "axB.set_ylabel(\"AUC\")\n",
    "axB.set_title(\"R03 Task vs Rest (FBCSP v2)\")\n",
    "\n",
    "# (C) Small EO/EC confusion table (S002 on left, S003 on right)\n",
    "def draw_table(ax, df, title):\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(title, fontsize=11, pad=6)\n",
    "    if df is None:\n",
    "        ax.text(0.5, 0.5, \"n/a\", ha=\"center\", va=\"center\")\n",
    "        return\n",
    "    # simple text grid\n",
    "    txt = \"row=gt, col=pred\\n\"\n",
    "    txt += \"        EC      EO\\n\"\n",
    "    for r in [\"EC\",\"EO\"]:\n",
    "        row = df.loc[r, [\"EC\",\"EO\"]].tolist()\n",
    "        txt += f\"{r:<4}  {row[0]:>6.3f}  {row[1]:>6.3f}\\n\"\n",
    "    ax.text(0.02, 0.02, txt, family=\"monospace\", fontsize=9, va=\"bottom\")\n",
    "\n",
    "axC = fig.add_subplot(gs[0,2])\n",
    "draw_table(axC, cm_s002, \"S002 EO/EC confusion\")\n",
    "\n",
    "axD = fig.add_subplot(gs[1,2])\n",
    "draw_table(axD, cm_s003, \"S003 EO/EC confusion\")\n",
    "\n",
    "# (D) Per-state task probability (top 3) â€” S002\n",
    "axE = fig.add_subplot(gs[1,0])\n",
    "if st_s002 is not None and len(st_s002):\n",
    "    axE.barh([f\"S{int(s)} (n={int(n)})\" for s,n in zip(st_s002[\"state\"], st_s002[\"n\"])],\n",
    "             st_s002[\"mean\"])\n",
    "    axE.set_xlim(0,1); axE.invert_yaxis()\n",
    "    axE.set_xlabel(\"mean fbCSP_task_proba\"); axE.set_title(\"S002 Task-Proba by State (R03)\")\n",
    "else:\n",
    "    axE.axis(\"off\"); axE.text(0.5,0.5,\"no S002 R03 summary\", ha=\"center\", va=\"center\")\n",
    "\n",
    "# (E) Per-state task probability (top 3) â€” S003\n",
    "axF = fig.add_subplot(gs[1,1])\n",
    "if st_s003 is not None and len(st_s003):\n",
    "    axF.barh([f\"S{int(s)} (n={int(n)})\" for s,n in zip(st_s003[\"state\"], st_s003[\"n\"])],\n",
    "             st_s003[\"mean\"])\n",
    "    axF.set_xlim(0,1); axF.invert_yaxis()\n",
    "    axF.set_xlabel(\"mean fbCSP_task_proba\"); axF.set_title(\"S003 Task-Proba by State (R03)\")\n",
    "else:\n",
    "    axF.axis(\"off\"); axF.text(0.5,0.5,\"no S003 R03 summary\", ha=\"center\", va=\"center\")\n",
    "\n",
    "# Header with quick legend\n",
    "title = \"CNT Cognitive Alphabet â€” v0.3 Generalization (subject-aware)\"\n",
    "fig.suptitle(title, fontsize=16, weight=\"bold\")\n",
    "hdr = []\n",
    "hdr.append(f\"EO/EC acc â€” S002={summary['subjects'].get('S002',{}).get('eoec_acc','n/a')}, \"\n",
    "           f\"S003={summary['subjects'].get('S003',{}).get('eoec_acc','n/a')}\")\n",
    "hdr.append(f\"Task AUC â€” S002={summary['subjects'].get('S002',{}).get('r03_auc_task','n/a')}, \"\n",
    "           f\"S003={summary['subjects'].get('S003',{}).get('r03_auc_task','n/a')}\")\n",
    "if mix is not None and len(mix):\n",
    "    hdr.append(\"Letter mix table saved â†’ generalization_letter_mix.csv\")\n",
    "fig.text(0.5, 0.94, \" | \".join(map(str, hdr)), ha=\"center\", fontsize=10)\n",
    "\n",
    "# Save figure + JSON\n",
    "PNG = BASE / \"CNT_CognitiveAlphabet_v0_3_Generalization.png\"\n",
    "PDF = BASE / \"CNT_CognitiveAlphabet_v0_3_Generalization.pdf\"\n",
    "fig.savefig(PNG, bbox_inches=\"tight\"); fig.savefig(PDF, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "with open(BASE / \"generalization_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"Saved report â†’\", PNG)\n",
    "print(\"Saved report â†’\", PDF)\n",
    "print(\"Saved summary â†’\", BASE / \"generalization_summary.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e813a3d0-23cc-4f02-8f36-3bf90477cb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Letter grammar (Markov) ===\n",
      "K: 4\n",
      "N1: 484\n",
      "N2: 483\n",
      "LL1: -163.77707691768782\n",
      "LL2: -158.27982050118328\n",
      "BIC1: 401.7391727159752\n",
      "BIC2: 613.20044037769\n",
      "BIC_delta: 211.46126766171483\n",
      "Saved â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\analysis\\letter_grammar_markov.json\n"
     ]
    }
   ],
   "source": [
    "# === CNT v0.2 â€” Letter grammar: first vs second order Markov (LL, BIC) ===\n",
    "# Uses S001 internal run labels to test whether 2nd order improves fit beyond 1st order.\n",
    "\n",
    "import numpy as np, pandas as pd, json\n",
    "from pathlib import Path\n",
    "\n",
    "RUN = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")\n",
    "REP = RUN.parent / \"cog_alphabet_report_hybrid_v1\" / \"analysis\"\n",
    "REP.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "M = pd.read_csv(RUN/\"metadata.csv\").sort_values([\"file\",\"t_start_s\",\"t_end_s\"]).reset_index(drop=True)\n",
    "L = pd.read_csv(RUN/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "K = int(np.max(L))+1\n",
    "\n",
    "# stitch the whole sequence (S001 runs in chronological order)\n",
    "seq = L.copy()\n",
    "\n",
    "# 1st-order transition matrix M1\n",
    "T1 = np.zeros((K,K), float)\n",
    "for a,b in zip(seq[:-1], seq[1:]):\n",
    "    T1[a,b] += 1\n",
    "T1 = (T1 + 1e-6) / (T1.sum(axis=1, keepdims=True) + 1e-6*K)  # Laplace-smooth\n",
    "\n",
    "# 2nd-order tensor T2[i,j,k] = P(L_t=k | L_{t-1}=j, L_{t-2}=i)\n",
    "T2 = np.zeros((K,K,K), float)\n",
    "for i,j,k in zip(seq[:-2], seq[1:-1], seq[2:]):\n",
    "    T2[i,j,k] += 1\n",
    "T2 = (T2 + 1e-6) / (T2.sum(axis=2, keepdims=True) + 1e-6*K)\n",
    "\n",
    "# log-likelihoods\n",
    "ll1 = 0.0\n",
    "for a,b in zip(seq[:-1], seq[1:]):\n",
    "    ll1 += np.log(T1[a,b])\n",
    "ll2 = 0.0\n",
    "for i,j,k in zip(seq[:-2], seq[1:-1], seq[2:]):\n",
    "    ll2 += np.log(T2[i,j,k])\n",
    "\n",
    "N1 = len(seq)-1\n",
    "N2 = len(seq)-2\n",
    "# parameter counts (free transitions: rows sum to 1)\n",
    "k1 = K*(K-1)\n",
    "k2 = (K*K)*(K-1)\n",
    "\n",
    "# BIC = -2*LL + k*log(n)\n",
    "bic1 = -2*ll1 + k1*np.log(max(N1,1))\n",
    "bic2 = -2*ll2 + k2*np.log(max(N2,1))\n",
    "\n",
    "res = {\n",
    "    \"K\": int(K),\n",
    "    \"N1\": int(N1), \"N2\": int(N2),\n",
    "    \"LL1\": float(ll1), \"LL2\": float(ll2),\n",
    "    \"BIC1\": float(bic1), \"BIC2\": float(bic2),\n",
    "    \"BIC_delta\": float(bic2 - bic1),  # <0 means 2nd order preferred\n",
    "}\n",
    "with open(REP/\"letter_grammar_markov.json\",\"w\") as f:\n",
    "    json.dump(res, f, indent=2)\n",
    "\n",
    "print(\"=== Letter grammar (Markov) ===\")\n",
    "for k,v in res.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "print(\"Saved â†’\", REP/\"letter_grammar_markov.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dced1098-0320-40b2-b109-68b85435218b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\analysis\\letter_T1_pack.png\n",
      "Tables â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\analysis\\T1_eval_generalization.csv E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\analysis\\T1_JS_divergences.csv E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\analysis\\T1_S001.csv\n"
     ]
    }
   ],
   "source": [
    "# === CNT v0.2 â€” Letter LM Pack (Order-1) ===\n",
    "# Trains T1 on S001, simulates sequences, evaluates S002/S003 (cross-entropy, perplexity),\n",
    "# compares subject T1s via JS divergence, and saves plots + tables.\n",
    "\n",
    "import numpy as np, pandas as pd, json, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "RUN   = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")  # S001 canonical\n",
    "GEN   = RUN.parent / \"cog_alphabet_report_hybrid_v1\" / \"generalization\"\n",
    "OUT   = RUN.parent / \"cog_alphabet_report_hybrid_v1\" / \"analysis\"\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Load S001 canonical sequence ---\n",
    "M = pd.read_csv(RUN/\"metadata.csv\").sort_values([\"file\",\"t_start_s\",\"t_end_s\"]).reset_index(drop=True)\n",
    "L = pd.read_csv(RUN/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "K = int(L.max()) + 1\n",
    "\n",
    "# --- Train T1 with tiny Laplace smoothing ---\n",
    "T1 = np.zeros((K,K), float)\n",
    "for a,b in zip(L[:-1], L[1:]):\n",
    "    T1[a,b] += 1\n",
    "eps = 1e-6\n",
    "T1 = (T1 + eps) / (T1.sum(axis=1, keepdims=True) + eps*K)  # row-stochastic\n",
    "\n",
    "# Stationary distribution (left eigenvector)\n",
    "w, V = np.linalg.eig(T1.T)\n",
    "pi = np.real(V[:, np.argmax(np.real(w))]); pi = np.maximum(pi,0); pi = pi/pi.sum()\n",
    "\n",
    "# --- Save T1 + pi ---\n",
    "pd.DataFrame(T1).to_csv(OUT/\"T1_S001.csv\", index=False)\n",
    "pd.Series(pi, name=\"pi\").to_csv(OUT/\"pi_S001.csv\", index=False)\n",
    "\n",
    "# --- Simulate from T1 ---\n",
    "def simulate_T1(T, pi, N=2000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    s = rng.choice(len(pi), p=pi)\n",
    "    seq = [s]\n",
    "    for _ in range(N-1):\n",
    "        s = rng.choice(len(pi), p=T[s])\n",
    "        seq.append(s)\n",
    "    return np.array(seq, int)\n",
    "\n",
    "sim_seq = simulate_T1(T1, pi, N=2000)\n",
    "pd.DataFrame({\"t\": np.arange(len(sim_seq)), \"state\": sim_seq}).to_csv(OUT/\"sim_T1_S001.csv\", index=False)\n",
    "\n",
    "# --- Load held-out subject sequences (S002/S003) from decode CSVs ---\n",
    "def load_subject_seq(subj):\n",
    "    rows=[]\n",
    "    for p in GEN.glob(f\"decode_{subj}_*.csv\"):\n",
    "        df = pd.read_csv(p)\n",
    "        if \"state\" in df.columns:\n",
    "            rows.append(df[\"state\"].astype(int).to_numpy())\n",
    "    if not rows:\n",
    "        return None\n",
    "    return np.concatenate(rows, axis=0)\n",
    "\n",
    "seq_S002 = load_subject_seq(\"S002\")\n",
    "seq_S003 = load_subject_seq(\"S003\")\n",
    "\n",
    "# --- Evaluation under S001 T1: cross-entropy & perplexity ---\n",
    "def seq_nll_T1(seq, T):\n",
    "    ll = 0.0\n",
    "    for a,b in zip(seq[:-1], seq[1:]):\n",
    "        ll += np.log(T[a,b] + 1e-12)\n",
    "    return -ll  # negative log-likelihood (natural log)\n",
    "\n",
    "def xent_perplexity(seq, T):\n",
    "    nll = seq_nll_T1(seq, T)\n",
    "    N = max(len(seq)-1, 1)\n",
    "    H = nll / N                 # nats per transition\n",
    "    PP = float(np.exp(H))       # perplexity\n",
    "    return float(H), PP\n",
    "\n",
    "eval_rows=[]\n",
    "if seq_S002 is not None and len(seq_S002)>1:\n",
    "    H2, PP2 = xent_perplexity(seq_S002, T1)\n",
    "    eval_rows.append({\"subject\":\"S002\", \"xent_nats\":H2, \"perplexity\":PP2, \"N\":int(len(seq_S002))})\n",
    "if seq_S003 is not None and len(seq_S003)>1:\n",
    "    H3, PP3 = xent_perplexity(seq_S003, T1)\n",
    "    eval_rows.append({\"subject\":\"S003\", \"xent_nats\":H3, \"perplexity\":PP3, \"N\":int(len(seq_S003))})\n",
    "eval_df = pd.DataFrame(eval_rows); eval_df.to_csv(OUT/\"T1_eval_generalization.csv\", index=False)\n",
    "\n",
    "# --- Subject-specific T1s and JS divergence to S001 ---\n",
    "def train_T1_from_seq(seq, K):\n",
    "    T = np.zeros((K,K), float)\n",
    "    for a,b in zip(seq[:-1], seq[1:]):\n",
    "        T[a,b]+=1\n",
    "    eps=1e-6; T=(T+eps)/(T.sum(axis=1, keepdims=True)+eps*K)\n",
    "    return T\n",
    "\n",
    "def js_divergence(P, Q):\n",
    "    # Jensenâ€“Shannon divergence between flattened row-stochastic matrices\n",
    "    p = P.flatten(); q = Q.flatten()\n",
    "    p = p / (p.sum() + 1e-12); q = q / (q.sum() + 1e-12)\n",
    "    m = 0.5*(p+q)\n",
    "    def kl(a,b): \n",
    "        mask = (a>0) & (b>0)\n",
    "        return np.sum(a[mask]*np.log(a[mask]/b[mask]))\n",
    "    return 0.5*kl(p,m) + 0.5*kl(q,m)\n",
    "\n",
    "js_rows=[]\n",
    "if seq_S002 is not None and len(seq_S002)>1:\n",
    "    T2 = train_T1_from_seq(seq_S002, K); pd.DataFrame(T2).to_csv(OUT/\"T1_S002.csv\", index=False)\n",
    "    js_rows.append({\"pair\":\"S001_vs_S002\", \"JS\": js_divergence(T1,T2)})\n",
    "if seq_S003 is not None and len(seq_S003)>1:\n",
    "    T3 = train_T1_from_seq(seq_S003, K); pd.DataFrame(T3).to_csv(OUT/\"T1_S003.csv\", index=False)\n",
    "    js_rows.append({\"pair\":\"S001_vs_S003\", \"JS\": js_divergence(T1,T3)})\n",
    "js_df = pd.DataFrame(js_rows); js_df.to_csv(OUT/\"T1_JS_divergences.csv\", index=False)\n",
    "\n",
    "# --- Tiny figure panel: T1 heatmap, pi, and perplexities ---\n",
    "fig = plt.figure(figsize=(10,6), dpi=130)\n",
    "gs  = fig.add_gridspec(2,3, hspace=0.35, wspace=0.30)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[:,0])\n",
    "im = ax1.imshow(T1, aspect=\"auto\")\n",
    "fig.colorbar(im, ax=ax1, fraction=0.046, pad=0.04)\n",
    "ax1.set_title(\"T1 â€” S001 (row-stochastic)\")\n",
    "ax1.set_xlabel(\"next\"); ax1.set_ylabel(\"current\")\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0,1])\n",
    "ax2.bar(np.arange(K), pi); ax2.set_title(\"Stationary Ï€ (S001)\")\n",
    "ax2.set_xlabel(\"state\"); ax2.set_ylim(0,1)\n",
    "\n",
    "ax3 = fig.add_subplot(gs[0,2])\n",
    "ax3.axis(\"off\")\n",
    "txt = \"Generalization (S001 T1):\\n\"\n",
    "for _,r in eval_df.iterrows():\n",
    "    txt += f\"{r['subject']}: xent={r['xent_nats']:.3f} nats, PP={r['perplexity']:.3f} (N={int(r['N'])})\\n\"\n",
    "ax3.text(0,1,txt, va=\"top\", family=\"monospace\", fontsize=9)\n",
    "\n",
    "ax4 = fig.add_subplot(gs[1,1:])\n",
    "ax4.axis(\"off\")\n",
    "txt2 = \"JS divergence vs S001 T1:\\n\"\n",
    "if len(js_df):\n",
    "    for _,r in js_df.iterrows():\n",
    "        txt2 += f\"{r['pair']}: JS={r['JS']:.4f}\\n\"\n",
    "else:\n",
    "    txt2 += \"n/a (no held-out sequences)\"\n",
    "ax4.text(0,1,txt2, va=\"top\", family=\"monospace\", fontsize=9)\n",
    "\n",
    "PTH = OUT/\"letter_T1_pack.png\"\n",
    "fig.suptitle(\"CNT Cognitive Alphabet â€” Letter Language Model (Order-1)\", fontsize=14, weight=\"bold\")\n",
    "fig.savefig(PTH, bbox_inches=\"tight\"); plt.close(fig)\n",
    "print(\"Saved:\", PTH)\n",
    "print(\"Tables â†’\", OUT/\"T1_eval_generalization.csv\", OUT/\"T1_JS_divergences.csv\", OUT/\"T1_S001.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6dd17365-9497-46b2-a2d3-4f6d1020ec49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No portraits saved; check RAW_DIR and metadata alignment.\n"
     ]
    }
   ],
   "source": [
    "# === CNT v0.2 â€” Microstate-aware LETTER PORTRAITS (spectrum + Î±/Î² topomap + A/B/C/D bar) ===\n",
    "# Saves per-letter PNGs and a mosaic sheet under:\n",
    "#   E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\microstates\\\n",
    "#     S*_letter_S#.png\n",
    "#     LETTERS_microstate_portraits_sheet.png\n",
    "\n",
    "import os, re, json, numpy as np, pandas as pd, matplotlib.pyplot as plt, mne\n",
    "from pathlib import Path\n",
    "from scipy.signal import welch\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# Paths\n",
    "RUN    = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")\n",
    "REP_MS = RUN.parent / \"cog_alphabet_report_hybrid_v1\" / \"microstates\"\n",
    "REP_MS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load canonical metadata + labels (S001)\n",
    "F = pd.read_csv(RUN/\"features.csv\")\n",
    "M = pd.read_csv(RUN/\"metadata.csv\").sort_values([\"file\",\"t_start_s\",\"t_end_s\"]).reset_index(drop=True)\n",
    "L = pd.read_csv(RUN/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "K_letters = int(L.max())+1\n",
    "\n",
    "# Where S001 EDFs live\n",
    "RAW_DIR = RUN / \"brainwaves_rebuilt\"\n",
    "\n",
    "# ---- helpers ----\n",
    "def load_raw_for_microstates(path):\n",
    "    raw = mne.io.read_raw_edf(str(path), preload=True, verbose=False)\n",
    "    # microstate-friendly band (2â€“20 Hz), avg-ref, montage best-effort\n",
    "    if abs(raw.info[\"sfreq\"]-250)>1e-6:\n",
    "        raw.resample(250.0, npad=\"auto\", verbose=False)\n",
    "    raw.filter(2.0, 20.0, verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "        try:\n",
    "            raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"),\n",
    "                            match_case=False, match_alias=True, on_missing=\"ignore\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return raw\n",
    "\n",
    "def gfp(x):  # x: (n_ch, n_t)\n",
    "    return x.std(axis=0)\n",
    "\n",
    "def find_gfp_peaks(x, sf, min_gap_ms=10, max_peaks=6000):\n",
    "    from scipy.signal import find_peaks\n",
    "    g = gfp(x)\n",
    "    distance = int((min_gap_ms/1000.0)*sf)\n",
    "    peaks,_ = find_peaks(g, distance=max(distance,1))\n",
    "    if len(peaks) > max_peaks:\n",
    "        idx = np.linspace(0, len(peaks)-1, max_peaks).astype(int)\n",
    "        peaks = peaks[idx]\n",
    "    return peaks\n",
    "\n",
    "def kmeans_maps_polarity_agnostic(X, K=4, iters=50, rs=42):\n",
    "    \"\"\"\n",
    "    X: (n_samples, n_channels), each sample is a scalp map at a GFP peak.\n",
    "    Polarity-agnostic k-means via correlation + sign-flip averaging.\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(rs)\n",
    "    # z-norm per sample\n",
    "    X = X - X.mean(axis=1, keepdims=True)\n",
    "    X = X / (np.linalg.norm(X, axis=1, keepdims=True) + 1e-12)\n",
    "\n",
    "    C = X[rng.choice(len(X), K, replace=False)].copy()\n",
    "    for _ in range(iters):\n",
    "        # assign to centroid maximizing |corr|\n",
    "        corr = X @ C.T  # (n, K)\n",
    "        lab = np.argmax(np.abs(corr), axis=1)\n",
    "        sign = np.sign(corr[np.arange(len(X)), lab])\n",
    "        # recompute\n",
    "        C_new = np.zeros_like(C)\n",
    "        for k in range(K):\n",
    "            mask = lab==k\n",
    "            if not np.any(mask):  # keep old\n",
    "                C_new[k] = C[k]\n",
    "            else:\n",
    "                C_new[k] = (X[mask] * sign[mask][:,None]).mean(axis=0)\n",
    "                # re-normalize\n",
    "                C_new[k] /= (np.linalg.norm(C_new[k]) + 1e-12)\n",
    "        if np.allclose(C, C_new, atol=1e-5):\n",
    "            break\n",
    "        C = C_new\n",
    "    return C, lab\n",
    "\n",
    "def backfit_microstates(raw, maps):\n",
    "    \"\"\"Return per-sample microstate labels by max |corr|.\"\"\"\n",
    "    X = raw.get_data()  # (n_ch, n_t)\n",
    "    # normalize maps & data at each time-sample\n",
    "    M = maps / (np.linalg.norm(maps, axis=1, keepdims=True) + 1e-12)  # (K, n_ch)\n",
    "    Xn = X / (np.linalg.norm(X, axis=0, keepdims=True) + 1e-12)        # (n_ch, n_t)\n",
    "    corr = M @ Xn  # (K, n_t)\n",
    "    labels = np.argmax(np.abs(corr), axis=0)\n",
    "    return labels\n",
    "\n",
    "def per_epoch_microstate_frac(starts, ends, ms_labels, sf, Kmic=4):\n",
    "    out=[]\n",
    "    for t0,t1 in zip(starts, ends):\n",
    "        s=int(round(t0*sf)); e=int(round(t1*sf))\n",
    "        arr = ms_labels[s:e]\n",
    "        if e<=s or len(arr)==0:\n",
    "            frac = np.zeros(Kmic)\n",
    "        else:\n",
    "            frac = np.bincount(arr, minlength=Kmic)/len(arr)\n",
    "        out.append(frac)\n",
    "    return np.stack(out,0)  # (n_epochs, Kmic)\n",
    "\n",
    "def prototype_spectrum(raw, starts, ends, sel_mask=None, fmax=45.0):\n",
    "    \"\"\"Average PSD across epochs (and channels).\"\"\"\n",
    "    sf = raw.info[\"sfreq\"]\n",
    "    P_all = []\n",
    "    for i,(t0,t1) in enumerate(zip(starts, ends)):\n",
    "        if sel_mask is not None and not sel_mask[i]: \n",
    "            continue\n",
    "        s=int(round(t0*sf)); e=int(round(t1*sf))\n",
    "        seg = raw.get_data(start=s, stop=e)\n",
    "        nper=min(int(sf*2), seg.shape[1]); nov=nper//2\n",
    "        f,P = welch(seg, fs=sf, nperseg=nper, noverlap=nov, axis=-1, average=\"median\")\n",
    "        P_all.append(P.mean(axis=0))  # mean over channels\n",
    "    if not P_all:\n",
    "        return None, None\n",
    "    Pm = np.mean(np.vstack(P_all), axis=0)\n",
    "    idx = (f>=1.0)&(f<=fmax)\n",
    "    return f[idx], Pm[idx]\n",
    "\n",
    "def band_topomap(raw, starts, ends, band, sel_mask=None):\n",
    "    \"\"\"Band power per channel over selected epochs.\"\"\"\n",
    "    sf = raw.info[\"sfreq\"]\n",
    "    bp_sum = np.zeros(len(raw.ch_names))\n",
    "    count  = 0\n",
    "    for i,(t0,t1) in enumerate(zip(starts, ends)):\n",
    "        if sel_mask is not None and not sel_mask[i]:\n",
    "            continue\n",
    "        s=int(round(t0*sf)); e=int(round(t1*sf))\n",
    "        seg = raw.get_data(start=s, stop=e)  # (n_ch, n_t)\n",
    "        nper=min(int(sf*2), seg.shape[1]); nov=nper//2\n",
    "        f,P = welch(seg, fs=sf, nperseg=nper, noverlap=nov, axis=-1, average=\"median\")  # (n_ch, n_f)\n",
    "        idx = (f>=band[0]) & (f<band[1])\n",
    "        bp = P[:, idx].sum(axis=1)\n",
    "        bp_sum += bp\n",
    "        count += 1\n",
    "    if count==0:\n",
    "        return None\n",
    "    return bp_sum / count\n",
    "\n",
    "def plot_letter_portrait(letter_id, spec_f, spec_p, raw, alpha_map, beta_map, micro_frac_mean, out_png):\n",
    "    fig = plt.figure(figsize=(11, 3.8), dpi=140)\n",
    "    gs  = fig.add_gridspec(1, 4, width_ratios=[1.4,1,1,1.2], wspace=0.32)\n",
    "\n",
    "    # (1) Spectrum\n",
    "    ax1 = fig.add_subplot(gs[0,0])\n",
    "    if spec_f is not None:\n",
    "        ax1.plot(spec_f, spec_p)\n",
    "        ax1.set_xlim(1, 45); ax1.set_xlabel(\"Hz\"); ax1.set_ylabel(\"Power\")\n",
    "    else:\n",
    "        ax1.text(0.5,0.5,\"no spectrum\", ha=\"center\", va=\"center\")\n",
    "    ax1.set_title(f\"State S{letter_id} â€” prototype spectrum\")\n",
    "\n",
    "    # (2) Alpha topomap\n",
    "    ax2 = fig.add_subplot(gs[0,1])\n",
    "    if alpha_map is not None:\n",
    "        try:\n",
    "            mne.viz.plot_topomap(alpha_map, raw.info, axes=ax2, show=False)\n",
    "            ax2.set_title(\"Alpha (8â€“13 Hz)\")\n",
    "        except Exception:\n",
    "            ax2.axis(\"off\"); ax2.text(0.5,0.5,\"topomap\\nunavailable\", ha=\"center\", va=\"center\")\n",
    "    else:\n",
    "        ax2.axis(\"off\"); ax2.text(0.5,0.5,\"no alpha map\", ha=\"center\", va=\"center\")\n",
    "\n",
    "    # (3) Beta topomap\n",
    "    ax3 = fig.add_subplot(gs[0,2])\n",
    "    if beta_map is not None:\n",
    "        try:\n",
    "            mne.viz.plot_topomap(beta_map, raw.info, axes=ax3, show=False)\n",
    "            ax3.set_title(\"Beta (13â€“30 Hz)\")\n",
    "        except Exception:\n",
    "            ax3.axis(\"off\"); ax3.text(0.5,0.5,\"topomap\\nunavailable\", ha=\"center\", va=\"center\")\n",
    "    else:\n",
    "        ax3.axis(\"off\"); ax3.text(0.5,0.5,\"no beta map\", ha=\"center\", va=\"center\")\n",
    "\n",
    "    # (4) Microstate mix bar\n",
    "    ax4 = fig.add_subplot(gs[0,3])\n",
    "    if micro_frac_mean is not None:\n",
    "        labels=[f\"Î¼{k}\" for k in range(len(micro_frac_mean))]\n",
    "        ax4.bar(labels, micro_frac_mean)\n",
    "        ax4.set_ylim(0,1); ax4.set_ylabel(\"fraction\")\n",
    "        ax4.set_title(\"Microstate occupancy\")\n",
    "    else:\n",
    "        ax4.axis(\"off\"); ax4.text(0.5,0.5,\"no microstates\", ha=\"center\", va=\"center\")\n",
    "\n",
    "    fig.suptitle(f\"CNT Cognitive Alphabet â€” Letter Portrait (S{letter_id})\", fontsize=12, weight=\"bold\")\n",
    "    fig.savefig(out_png, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "# ---- compute portraits from S001 files only (consistent head geometry) ----\n",
    "portrait_paths=[]\n",
    "for fname in M[\"file\"].unique():\n",
    "    if not fname.lower().endswith(\".edf\"): \n",
    "        continue\n",
    "    raw_path = RAW_DIR / fname\n",
    "    if not raw_path.exists():\n",
    "        continue\n",
    "    # Select epochs for this file\n",
    "    fmask = (M[\"file\"]==fname).to_numpy()\n",
    "    starts = M.loc[fmask, \"t_start_s\"].to_numpy()\n",
    "    ends   = M.loc[fmask, \"t_end_s\"].to_numpy()\n",
    "    letters= L[fmask]\n",
    "\n",
    "    # Load raw (2â€“20 Hz, avg-ref) and discover microstates from GFP peaks\n",
    "    raw = load_raw_for_microstates(raw_path)\n",
    "    sf  = raw.info[\"sfreq\"]\n",
    "    X   = raw.get_data()\n",
    "    peaks = find_gfp_peaks(X, sf, min_gap_ms=10, max_peaks=6000)\n",
    "    maps,_ = kmeans_maps_polarity_agnostic(X[:,peaks].T, K=4, iters=50, rs=42)  # (4, n_ch)\n",
    "    ms_labels = backfit_microstates(raw, maps)\n",
    "\n",
    "    # Precompute per-epoch microstate fractions\n",
    "    ms_frac = per_epoch_microstate_frac(starts, ends, ms_labels, sf, Kmic=4)  # (n_ep,4)\n",
    "\n",
    "    # For each letter present in this file, render portrait\n",
    "    present_letters = np.unique(letters)\n",
    "    for s in present_letters:\n",
    "        sel = (letters==s)\n",
    "        # spectrum\n",
    "        f, p = prototype_spectrum(raw, starts, ends, sel_mask=sel, fmax=45.0)\n",
    "        # band maps\n",
    "        alpha_map = band_topomap(raw, starts, ends, (8.0,13.0), sel_mask=sel)\n",
    "        beta_map  = band_topomap(raw, starts, ends, (13.0,30.0), sel_mask=sel)\n",
    "        # microstate mix\n",
    "        micro_mean = ms_frac[sel].mean(axis=0) if np.any(sel) else None\n",
    "        out_png = REP_MS / f\"S001_{Path(fname).stem}_letter_S{s}.png\"\n",
    "        plot_letter_portrait(s, f, p, raw, alpha_map, beta_map, micro_mean, out_png)\n",
    "        portrait_paths.append(out_png)\n",
    "        print(\"Saved:\", out_png)\n",
    "\n",
    "# ---- Make a mosaic sheet for quick viewing (2 per row) ----\n",
    "try:\n",
    "    from PIL import Image\n",
    "    if portrait_paths:\n",
    "        imgs=[Image.open(p) for p in portrait_paths]\n",
    "        W = max(im.width for im in imgs); H = max(im.height for im in imgs)\n",
    "        per_row=2; rows=(len(imgs)+per_row-1)//per_row\n",
    "        sheet = Image.new(\"RGB\", (per_row*W, rows*H), (255,255,255))\n",
    "        for i,im in enumerate(imgs):\n",
    "            r,c = divmod(i, per_row)\n",
    "            sheet.paste(im, (c*W, r*H))\n",
    "        SHEET = REP_MS/\"LETTERS_microstate_portraits_sheet.png\"\n",
    "        sheet.save(SHEET)\n",
    "        print(\"Saved sheet â†’\", SHEET)\n",
    "    else:\n",
    "        print(\"No portraits saved; check RAW_DIR and metadata alignment.\")\n",
    "except Exception as e:\n",
    "    print(\"Sheet creation skipped:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14b5aee9-f882-4a0b-9084-b69373e75f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v0.3 bundle â†’ E:\\CNT\\artifacts\\CNT_CognitiveAlphabet_v0_3.zip\n",
      "v0.3 generalization page â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\generalization\\CNT_CognitiveAlphabet_v0_3_Generalization.png\n",
      "Dwell outputs â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\dwell\n",
      "Microstate portraits â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\microstates\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CNT Cognitive Alphabet â€” v0.3 MEGA CELL\n",
    "# (A) Semi-Markov dwell model + sampler\n",
    "# (B) Microstate-aware letter portraits (spectrum + Î±/Î² topomap + A/B/C/D mix)\n",
    "# (C) v0.3 Generalization report + bundle (zip)\n",
    "#\n",
    "# Outputs root:\n",
    "#   E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\\n",
    "#     dwell\\ ...            # semi-Markov fits, sampler, plots\n",
    "#     microstates\\ ...      # portraits per letter + sheet\n",
    "#     generalization\\ ...   # v0.3 page + JSON summary\n",
    "#   E:\\CNT\\artifacts\\CNT_CognitiveAlphabet_v0_3\\ ... (and .zip)\n",
    "# ============================================\n",
    "\n",
    "import os, re, json, shutil, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy.signal import welch\n",
    "from scipy import stats\n",
    "import mne\n",
    "\n",
    "plt.switch_backend(\"Agg\")  # headless-safe\n",
    "# ---------- Paths (edit if needed) ----------\n",
    "RUN    = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")  # promoted v0.2\n",
    "RAW_DIR= RUN / \"brainwaves_rebuilt\"                        # S001 EDFs (R01â€“R03)\n",
    "REP    = RUN.parent / \"cog_alphabet_report_hybrid_v1\"\n",
    "DWELL  = REP / \"dwell\";       DWELL.mkdir(parents=True, exist_ok=True)\n",
    "MSREP  = REP / \"microstates\"; MSREP.mkdir(parents=True, exist_ok=True)\n",
    "GENREP = REP / \"generalization\"; GENREP.mkdir(parents=True, exist_ok=True)\n",
    "BUNDLE = RUN.parent / \"CNT_CognitiveAlphabet_v0_3\"\n",
    "BUNDLE.mkdir(exist_ok=True)\n",
    "\n",
    "# ---------- Load canonical v0.2 artifacts ----------\n",
    "F = pd.read_csv(RUN/\"features.csv\")\n",
    "M = pd.read_csv(RUN/\"metadata.csv\").sort_values([\"file\",\"t_start_s\",\"t_end_s\"]).reset_index(drop=True)\n",
    "L = pd.read_csv(RUN/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "K = int(L.max())+1\n",
    "# Models for projection/letter assignment (used in portraits/generalization integration if needed)\n",
    "from joblib import load\n",
    "scaler = load(RUN/\"scaler_hybrid.joblib\")\n",
    "pca    = load(RUN/\"pca_hybrid.joblib\")\n",
    "km     = load(RUN/\"kmeans_hybrid.joblib\")\n",
    "\n",
    "# =========================================================================\n",
    "# (A) SEMI-MARKOV DWELL MODEL per letter (fits geometric vs gamma)\n",
    "# =========================================================================\n",
    "def dwell_lengths(labels: np.ndarray):\n",
    "    \"\"\"Return dwell lengths in epochs for each state: dict[state] = list[lengths].\"\"\"\n",
    "    d = {s: [] for s in range(int(labels.max())+1)}\n",
    "    if len(labels)==0: return d\n",
    "    s = labels[0]; n=1\n",
    "    for a,b in zip(labels[:-1], labels[1:]):\n",
    "        if b==a:\n",
    "            n+=1\n",
    "        else:\n",
    "            d[s].append(n); s=b; n=1\n",
    "    d[s].append(n)\n",
    "    return d\n",
    "\n",
    "def fit_geometric(lengths):\n",
    "    \"\"\"Geometric over epochs with support {1,2,...}; MLE pÌ‚ = 1/mean.\"\"\"\n",
    "    arr = np.array(lengths, dtype=float)\n",
    "    m   = arr.mean() if len(arr) else np.nan\n",
    "    p   = 1.0/max(m,1.0) if np.isfinite(m) else 0.5\n",
    "    # log-likelihood: sum log p + (l-1)log(1-p)\n",
    "    ll  = len(arr)*np.log(p+1e-12) + np.sum((arr-1.0)*np.log(1.0-p+1e-12))\n",
    "    k   = 1  # parameter count\n",
    "    return {\"name\":\"geometric\",\"p\":p,\"ll\":float(ll),\"k\":k}\n",
    "\n",
    "def fit_gamma_seconds(lengths, epoch_sec=2.0):\n",
    "    \"\"\"Gamma over seconds with support (0,inf); fit k, theta via MLE (scipy).\"\"\"\n",
    "    arr = np.array(lengths, dtype=float)*epoch_sec\n",
    "    arr = arr[arr>0]\n",
    "    if len(arr)<3:\n",
    "        # fallback rough moments\n",
    "        m = arr.mean() if len(arr) else 1.0\n",
    "        v = arr.var()  if len(arr)>1 else 1.0\n",
    "        k = (m*m)/max(v,1e-9); theta = max(v/m, 1e-9)\n",
    "        ll = np.sum(stats.gamma.logpdf(arr, a=k, scale=theta))\n",
    "        return {\"name\":\"gamma\",\"k_shape\":float(k),\"theta_scale\":float(theta),\n",
    "                \"ll\":float(ll),\"k\":2}\n",
    "    k_shape, loc, theta_scale = stats.gamma.fit(arr, floc=0.0)\n",
    "    ll = np.sum(stats.gamma.logpdf(arr, a=k_shape, loc=0.0, scale=theta_scale))\n",
    "    return {\"name\":\"gamma\",\"k_shape\":float(k_shape),\"theta_scale\":float(theta_scale),\n",
    "            \"ll\":float(ll),\"k\":2}\n",
    "\n",
    "def aic_bic(ll, k, n):\n",
    "    aic = -2.0*ll + 2.0*k\n",
    "    bic = -2.0*ll + k*np.log(max(n,1))\n",
    "    return float(aic), float(bic)\n",
    "\n",
    "def fit_semi_markov(labels, epoch_sec=2.0):\n",
    "    # Dwell distributions\n",
    "    dw = dwell_lengths(labels)\n",
    "    fits = {}\n",
    "    for s, lens in dw.items():\n",
    "        # Fit geometric & gamma; compare AIC/BIC\n",
    "        g1 = fit_geometric(lens)\n",
    "        g2 = fit_gamma_seconds(lens, epoch_sec=epoch_sec)\n",
    "        n  = len(lens)\n",
    "        aic1,bic1 = aic_bic(g1[\"ll\"], g1[\"k\"], n)\n",
    "        aic2,bic2 = aic_bic(g2[\"ll\"], g2[\"k\"], n)\n",
    "        if aic2 < aic1:\n",
    "            best = {**g2, \"aic\":aic2, \"bic\":bic2}\n",
    "            best[\"kind\"] = \"gamma_seconds\"\n",
    "        else:\n",
    "            best = {**g1, \"aic\":aic1, \"bic\":bic1}\n",
    "            best[\"kind\"] = \"geometric_epochs\"\n",
    "        best[\"n_segments\"] = int(n)\n",
    "        fits[int(s)] = best\n",
    "    # Embedded transition matrix (no self-transitions; transitions at run boundaries)\n",
    "    E = np.zeros((K,K), float)\n",
    "    s = labels[0]\n",
    "    for a,b in zip(labels[:-1], labels[1:]):\n",
    "        if b!=a:\n",
    "            E[a,b] += 1.0\n",
    "    # normalize rows (if a state never transitions, spread tiny mass)\n",
    "    for i in range(K):\n",
    "        row_sum = E[i].sum()\n",
    "        if row_sum <= 0:\n",
    "            E[i] = (np.ones(K)-np.eye(K)[i])/(K-1)\n",
    "        else:\n",
    "            E[i] /= row_sum\n",
    "            E[i,i] = 0.0\n",
    "    # Save\n",
    "    with open(DWELL/\"semi_markov_fits.json\",\"w\") as f:\n",
    "        json.dump({\"epoch_sec\":epoch_sec,\"fits\":fits,\"embedded_E\":E.tolist()}, f, indent=2)\n",
    "    # Quick diagnostics plot (one figure, per rule)\n",
    "    plt.figure()\n",
    "    max_s = max(len(v) for v in dwell_lengths(labels).values())\n",
    "    # left panel: histogram of dwell per letter\n",
    "    # (one figure per letter as separate files to respect 1 chart per figure)\n",
    "    for s, lens in dw.items():\n",
    "        plt.figure()\n",
    "        if len(lens):\n",
    "            plt.hist(lens, bins=min(30, max(5, int(np.sqrt(len(lens))))))\n",
    "        plt.title(f\"Dwell lengths (epochs) â€” state S{s}\")\n",
    "        plt.xlabel(\"epochs\"); plt.ylabel(\"count\")\n",
    "        plt.savefig(DWELL/f\"dwell_hist_S{s}.png\", dpi=150, bbox_inches=\"tight\")\n",
    "        plt.close()\n",
    "    # Save embedded E heatmap\n",
    "    plt.figure()\n",
    "    plt.imshow(E, aspect=\"auto\"); plt.colorbar()\n",
    "    plt.title(\"Embedded transition matrix (no self-transitions)\")\n",
    "    plt.xlabel(\"next\"); plt.ylabel(\"current\")\n",
    "    plt.savefig(DWELL/\"embedded_E_heatmap.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    return fits, E\n",
    "\n",
    "def sample_dwell_seconds(fit, rng):\n",
    "    if fit[\"kind\"]==\"geometric_epochs\":\n",
    "        p = fit[\"p\"]; draw = rng.geometric(p)\n",
    "        return float(draw)  # epochs\n",
    "    else:\n",
    "        k = fit[\"k_shape\"]; theta = fit[\"theta_scale\"]; sec = rng.gamma(shape=k, scale=theta)\n",
    "        # convert seconds â†’ epochs (ceil at least 1)\n",
    "        epochs = max(1, int(np.ceil(sec / 2.0)))\n",
    "        return float(epochs)\n",
    "\n",
    "def simulate_semi_markov(fits, E, N_epochs=500, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    # start state ~ uniform over states that have fits\n",
    "    states = sorted(map(int, fits.keys()))\n",
    "    s = rng.choice(states)\n",
    "    seq=[]\n",
    "    while len(seq) < N_epochs:\n",
    "        d = int(sample_dwell_seconds(fits[s], rng))\n",
    "        seq.extend([s]*d)\n",
    "        if len(seq) >= N_epochs: break\n",
    "        # choose next from embedded E row s\n",
    "        probs = E[s].copy()\n",
    "        probs[s] = 0.0\n",
    "        probs = probs / (probs.sum() + 1e-12)\n",
    "        s = rng.choice(np.arange(K), p=probs)\n",
    "    return np.array(seq[:N_epochs], int)\n",
    "\n",
    "# Fit and sample\n",
    "fits, E_emb = fit_semi_markov(L, epoch_sec=float((M[\"t_end_s\"]-M[\"t_start_s\"]).median()))\n",
    "sim_seq = simulate_semi_markov(fits, E_emb, N_epochs=1000, seed=123)\n",
    "pd.DataFrame({\"t\": np.arange(len(sim_seq)), \"state\": sim_seq}).to_csv(DWELL/\"semi_markov_simulation.csv\", index=False)\n",
    "\n",
    "# =========================================================================\n",
    "# (B) MICROSTATE-AWARE LETTER PORTRAITS (S001)\n",
    "# =========================================================================\n",
    "def load_raw_for_microstates(path):\n",
    "    raw = mne.io.read_raw_edf(str(path), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"]-250)>1e-6:\n",
    "        raw.resample(250.0, npad=\"auto\", verbose=False)\n",
    "    raw.filter(2.0, 20.0, verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "        try:\n",
    "            raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"),\n",
    "                            match_case=False, match_alias=True, on_missing=\"ignore\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return raw\n",
    "\n",
    "def gfp(x): return x.std(axis=0)\n",
    "\n",
    "def find_gfp_peaks(x, sf, min_gap_ms=10, max_peaks=6000):\n",
    "    from scipy.signal import find_peaks\n",
    "    distance = int((min_gap_ms/1000.0)*sf)\n",
    "    g = gfp(x); peaks,_ = find_peaks(g, distance=max(distance,1))\n",
    "    if len(peaks)>max_peaks:\n",
    "        idx=np.linspace(0,len(peaks)-1,max_peaks).astype(int); peaks=peaks[idx]\n",
    "    return peaks\n",
    "\n",
    "def kmeans_maps_polarity_agnostic(X, K=4, iters=50, rs=42):\n",
    "    rng=np.random.default_rng(rs)\n",
    "    X = X - X.mean(axis=1, keepdims=True)\n",
    "    X = X / (np.linalg.norm(X, axis=1, keepdims=True)+1e-12)\n",
    "    C = X[rng.choice(len(X), K, replace=False)].copy()\n",
    "    for _ in range(iters):\n",
    "        corr = X @ C.T\n",
    "        lab = np.argmax(np.abs(corr), axis=1)\n",
    "        sign= np.sign(corr[np.arange(len(X)), lab])\n",
    "        Cn=[]\n",
    "        for k in range(K):\n",
    "            m = lab==k\n",
    "            if not np.any(m): Cn.append(C[k]); continue\n",
    "            v = (X[m]*sign[m][:,None]).mean(axis=0)\n",
    "            v = v/(np.linalg.norm(v)+1e-12)\n",
    "            Cn.append(v)\n",
    "        Cn=np.stack(Cn,0)\n",
    "        if np.allclose(C, Cn, atol=1e-5): break\n",
    "        C=Cn\n",
    "    return C, lab\n",
    "\n",
    "def backfit_maps(raw, maps):\n",
    "    X=raw.get_data()\n",
    "    M = maps/(np.linalg.norm(maps,axis=1,keepdims=True)+1e-12)\n",
    "    Xn= X/(np.linalg.norm(X,axis=0,keepdims=True)+1e-12)\n",
    "    corr = M @ Xn\n",
    "    lab = np.argmax(np.abs(corr), axis=0)\n",
    "    return lab\n",
    "\n",
    "def per_epoch_microstate_frac(starts, ends, ms_labels, sf, Kmic=4):\n",
    "    rows=[]\n",
    "    for t0,t1 in zip(starts, ends):\n",
    "        s=int(round(t0*sf)); e=int(round(t1*sf))\n",
    "        arr = ms_labels[s:e]\n",
    "        if e<=s or len(arr)==0: frac=np.zeros(Kmic)\n",
    "        else: frac=np.bincount(arr, minlength=Kmic)/len(arr)\n",
    "        rows.append(frac)\n",
    "    return np.stack(rows,0)\n",
    "\n",
    "def prototype_spectrum(raw, starts, ends, sel_mask=None, fmax=45.0):\n",
    "    sf = raw.info[\"sfreq\"]; Pl=[]\n",
    "    for i,(t0,t1) in enumerate(zip(starts, ends)):\n",
    "        if sel_mask is not None and not sel_mask[i]: continue\n",
    "        s=int(round(t0*sf)); e=int(round(t1*sf))\n",
    "        seg=raw.get_data(start=s, stop=e)\n",
    "        nper=min(int(sf*2), seg.shape[1]); nov=nper//2\n",
    "        f,P = welch(seg, fs=sf, nperseg=nper, noverlap=nov, axis=-1, average=\"median\")\n",
    "        Pl.append(P.mean(axis=0))\n",
    "    if not Pl: return None,None\n",
    "    Pm = np.mean(np.vstack(Pl), axis=0)\n",
    "    idx=(f>=1.0)&(f<=fmax)\n",
    "    return f[idx], Pm[idx]\n",
    "\n",
    "def band_topomap(raw, starts, ends, band, sel_mask=None):\n",
    "    sf=raw.info[\"sfreq\"]; acc=None; n=0\n",
    "    for i,(t0,t1) in enumerate(zip(starts, ends)):\n",
    "        if sel_mask is not None and not sel_mask[i]: continue\n",
    "        s=int(round(t0*sf)); e=int(round(t1*sf))\n",
    "        seg=raw.get_data(start=s, stop=e)\n",
    "        nper=min(int(sf*2), seg.shape[1]); nov=nper//2\n",
    "        f,P = welch(seg, fs=sf, nperseg=nper, noverlap=nov, axis=-1, average=\"median\")\n",
    "        idx=(f>=band[0])&(f<band[1]); bp=P[:,idx].sum(axis=1)\n",
    "        acc = bp if acc is None else acc+bp; n+=1\n",
    "    if n==0: return None\n",
    "    return acc/n\n",
    "\n",
    "def plot_letter_portrait(letter_id, spec_f, spec_p, raw, alpha_map, beta_map, micro_frac_mean, out_png):\n",
    "    fig = plt.figure(figsize=(11,3.8), dpi=140)\n",
    "    gs = fig.add_gridspec(1,4, width_ratios=[1.4,1,1,1.2], wspace=0.32)\n",
    "    ax1=fig.add_subplot(gs[0,0])\n",
    "    if spec_f is not None:\n",
    "        ax1.plot(spec_f, spec_p); ax1.set_xlim(1,45)\n",
    "        ax1.set_xlabel(\"Hz\"); ax1.set_ylabel(\"Power\")\n",
    "    else:\n",
    "        ax1.text(0.5,0.5,\"no spectrum\", ha=\"center\", va=\"center\")\n",
    "    ax1.set_title(f\"State S{letter_id} â€” spectrum\")\n",
    "\n",
    "    ax2=fig.add_subplot(gs[0,1])\n",
    "    if alpha_map is not None:\n",
    "        try: mne.viz.plot_topomap(alpha_map, raw.info, axes=ax2, show=False); ax2.set_title(\"Alpha (8â€“13 Hz)\")\n",
    "        except Exception: ax2.axis(\"off\"); ax2.text(0.5,0.5,\"topomap\\nunavailable\", ha=\"center\", va=\"center\")\n",
    "    else: ax2.axis(\"off\"); ax2.text(0.5,0.5,\"no alpha map\", ha=\"center\", va=\"center\")\n",
    "\n",
    "    ax3=fig.add_subplot(gs[0,2])\n",
    "    if beta_map is not None:\n",
    "        try: mne.viz.plot_topomap(beta_map, raw.info, axes=ax3, show=False); ax3.set_title(\"Beta (13â€“30 Hz)\")\n",
    "        except Exception: ax3.axis(\"off\"); ax3.text(0.5,0.5,\"topomap\\nunavailable\", ha=\"center\", va=\"center\")\n",
    "    else: ax3.axis(\"off\"); ax3.text(0.5,0.5,\"no beta map\", ha=\"center\", va=\"center\")\n",
    "\n",
    "    ax4=fig.add_subplot(gs[0,3])\n",
    "    if micro_frac_mean is not None:\n",
    "        labels=[f\"Î¼{k}\" for k in range(len(micro_frac_mean))]\n",
    "        ax4.bar(labels, micro_frac_mean); ax4.set_ylim(0,1)\n",
    "        ax4.set_ylabel(\"fraction\"); ax4.set_title(\"Microstate occupancy\")\n",
    "    else: ax4.axis(\"off\"); ax4.text(0.5,0.5,\"no microstates\", ha=\"center\", va=\"center\")\n",
    "\n",
    "    fig.suptitle(f\"CNT Cognitive Alphabet â€” Letter Portrait (S{letter_id})\", fontsize=12, weight=\"bold\")\n",
    "    fig.savefig(out_png, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "portrait_paths=[]\n",
    "for fname in M[\"file\"].unique():\n",
    "    if not fname.lower().endswith(\".edf\"): continue\n",
    "    raw_path = RAW_DIR / fname\n",
    "    if not raw_path.exists(): continue\n",
    "    fmask = (M[\"file\"]==fname).to_numpy()\n",
    "    starts = M.loc[fmask, \"t_start_s\"].to_numpy()\n",
    "    ends   = M.loc[fmask, \"t_end_s\"].to_numpy()\n",
    "    letters= L[fmask]\n",
    "\n",
    "    raw = load_raw_for_microstates(raw_path)\n",
    "    sf  = raw.info[\"sfreq\"]\n",
    "    X   = raw.get_data()\n",
    "    peaks= find_gfp_peaks(X, sf, min_gap_ms=10, max_peaks=6000)\n",
    "    maps,_= kmeans_maps_polarity_agnostic(X[:,peaks].T, K=4, iters=50, rs=42)\n",
    "    ms_labels= backfit_maps(raw, maps)\n",
    "    ms_frac  = per_epoch_microstate_frac(starts, ends, ms_labels, sf, Kmic=4)\n",
    "\n",
    "    for s in np.unique(letters):\n",
    "        sel=(letters==s)\n",
    "        f,p = prototype_spectrum(raw, starts, ends, sel_mask=sel, fmax=45.0)\n",
    "        a_map= band_topomap(raw, starts, ends, (8.0,13.0), sel_mask=sel)\n",
    "        b_map= band_topomap(raw, starts, ends, (13.0,30.0), sel_mask=sel)\n",
    "        micro_mean = ms_frac[sel].mean(axis=0) if np.any(sel) else None\n",
    "        out_png = MSREP / f\"S001_{Path(fname).stem}_letter_S{s}.png\"\n",
    "        plot_letter_portrait(int(s), f,p, raw, a_map,b_map, micro_mean, out_png)\n",
    "        portrait_paths.append(out_png)\n",
    "\n",
    "# Mosaic sheet (2 per row)\n",
    "try:\n",
    "    from PIL import Image\n",
    "    if portrait_paths:\n",
    "        imgs=[Image.open(p) for p in portrait_paths]\n",
    "        W=max(im.width for im in imgs); H=max(im.height for im in imgs)\n",
    "        per_row=2; rows=(len(imgs)+per_row-1)//per_row\n",
    "        sheet=Image.new(\"RGB\",(per_row*W, rows*H),(255,255,255))\n",
    "        for i,im in enumerate(imgs):\n",
    "            r,c=divmod(i,per_row); sheet.paste(im,(c*W, r*H))\n",
    "        sheet_path= MSREP/\"LETTERS_microstate_portraits_sheet.png\"\n",
    "        sheet.save(sheet_path)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# =========================================================================\n",
    "# (C) v0.3 GENERALIZATION PAGE + BUNDLE v0.3\n",
    "# =========================================================================\n",
    "CAL  = GENREP / \"subject_calibration.json\"\n",
    "EOEC_SUM = GENREP / \"eoec_iaf_summary.csv\"\n",
    "TASK_SUM = GENREP / \"task_fbcsp_summary.csv\"\n",
    "MIX      = GENREP / \"generalization_letter_mix.csv\"\n",
    "\n",
    "def _read_csv(p):\n",
    "    try:\n",
    "        return pd.read_csv(p) if p.exists() else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "calib = json.load(open(CAL)) if CAL.exists() else {}\n",
    "eoec  = _read_csv(EOEC_SUM)\n",
    "task  = _read_csv(TASK_SUM)\n",
    "\n",
    "# EO/EC confusions if saved by earlier step\n",
    "def load_cm_csv(subject):\n",
    "    p = GENREP / f\"{subject}_eoec_cm_iaf.csv\"\n",
    "    if p.exists():\n",
    "        df = pd.read_csv(p, index_col=0)\n",
    "        cols=[\"EC\",\"EO\"]; idx=[\"EC\",\"EO\"]\n",
    "        for c in cols:\n",
    "            if c not in df.columns: df[c]=0.0\n",
    "        for r in idx:\n",
    "            if r not in df.index: df.loc[r]=[0.0,0.0]\n",
    "        return df.loc[idx, cols].round(3)\n",
    "    return None\n",
    "\n",
    "cm_s002 = load_cm_csv(\"S002\")\n",
    "cm_s003 = load_cm_csv(\"S003\")\n",
    "\n",
    "def load_state_taskprob(subject):\n",
    "    p = GENREP / f\"{subject}_state_taskprob_summary.csv\"\n",
    "    if p.exists():\n",
    "        df=pd.read_csv(p)\n",
    "        return (df.sort_values(\"mean\", ascending=False).head(3)[[\"state\",\"mean\",\"n\"]]\n",
    "                  .reset_index(drop=True))\n",
    "    return None\n",
    "\n",
    "st_s002 = load_state_taskprob(\"S002\")\n",
    "st_s003 = load_state_taskprob(\"S003\")\n",
    "\n",
    "summary={\"subjects\": {}}\n",
    "for subj in [\"S002\",\"S003\"]:\n",
    "    entry={}\n",
    "    if subj in calib:\n",
    "        entry[\"IAF_hz\"] = calib[subj].get(\"IAF_hz\")\n",
    "        entry[\"alpha_band\"] = calib[subj].get(\"alpha_band\")\n",
    "        entry[\"alpha_threshold\"] = calib[subj].get(\"alpha_threshold\")\n",
    "        entry[\"task_letters\"] = calib[subj].get(\"task_letters\", [])\n",
    "        entry[\"fbCSP_threshold\"] = calib[subj].get(\"fbCSP_threshold\", 0.50)\n",
    "    if eoec is not None and \"subject\" in eoec.columns:\n",
    "        row = eoec[eoec[\"subject\"]==subj]\n",
    "        if len(row): entry[\"eoec_acc\"] = float(row[\"acc\"].iloc[0])\n",
    "    if task is not None and \"subject\" in task.columns:\n",
    "        row = task[task[\"subject\"]==subj]\n",
    "        if len(row): entry[\"r03_auc_task\"] = float(row[\"AUC_task\"].iloc[0])\n",
    "    summary[\"subjects\"][subj]=entry\n",
    "\n",
    "if eoec is not None and \"acc\" in eoec.columns and len(eoec):\n",
    "    summary[\"eoec_acc_mean\"]=float(eoec[\"acc\"].mean())\n",
    "if task is not None and \"AUC_task\" in task.columns and len(task):\n",
    "    summary[\"r03_auc_mean\"]=float(task[\"AUC_task\"].mean())\n",
    "\n",
    "# Draw v0.3 page\n",
    "fig=plt.figure(figsize=(12,8), dpi=130)\n",
    "gs=fig.add_gridspec(2,3, hspace=0.35, wspace=0.30)\n",
    "\n",
    "# EO/EC bars\n",
    "axA=fig.add_subplot(gs[0,0])\n",
    "subs=[]; accs=[]\n",
    "for s in [\"S002\",\"S003\"]:\n",
    "    acc=summary[\"subjects\"].get(s,{}).get(\"eoec_acc\", None)\n",
    "    if acc is not None: subs.append(s); accs.append(acc)\n",
    "axA.bar(np.arange(len(accs)), accs); axA.set_xticks(np.arange(len(accs))); axA.set_xticklabels(subs); axA.set_ylim(0,1)\n",
    "axA.set_ylabel(\"accuracy\"); axA.set_title(\"EO vs EC (IAF-based)\")\n",
    "\n",
    "# Task AUC bars\n",
    "axB=fig.add_subplot(gs[0,1])\n",
    "subs2=[]; aucs=[]\n",
    "for s in [\"S002\",\"S003\"]:\n",
    "    auc=summary[\"subjects\"].get(s,{}).get(\"r03_auc_task\", None)\n",
    "    if auc is not None: subs2.append(s); aucs.append(auc)\n",
    "axB.bar(np.arange(len(aucs)), aucs); axB.set_xticks(np.arange(len(aucs))); axB.set_xticklabels(subs2); axB.set_ylim(0,1)\n",
    "axB.set_ylabel(\"AUC\"); axB.set_title(\"R03 Task vs Rest (FBCSP v2)\")\n",
    "\n",
    "# EO/EC confusions\n",
    "def draw_table(ax, df, title):\n",
    "    ax.axis(\"off\"); ax.set_title(title, fontsize=11, pad=6)\n",
    "    if df is None:\n",
    "        ax.text(0.5,0.5,\"n/a\", ha=\"center\", va=\"center\"); return\n",
    "    txt=\"row=gt, col=pred\\n        EC      EO\\n\"\n",
    "    for r in [\"EC\",\"EO\"]:\n",
    "        row=df.loc[r, [\"EC\",\"EO\"]].tolist()\n",
    "        txt+=f\"{r:<4}  {row[0]:>6.3f}  {row[1]:>6.3f}\\n\"\n",
    "    ax.text(0.02,0.02,txt, family=\"monospace\", fontsize=9, va=\"bottom\")\n",
    "\n",
    "axC=fig.add_subplot(gs[0,2]); draw_table(axC, cm_s002, \"S002 EO/EC confusion\")\n",
    "axD=fig.add_subplot(gs[1,2]); draw_table(axD, cm_s003, \"S003 EO/EC confusion\")\n",
    "\n",
    "# Task-prob bars\n",
    "axE=fig.add_subplot(gs[1,0])\n",
    "if st_s002 is not None and len(st_s002):\n",
    "    axE.barh([f\"S{int(s)} (n={int(n)})\" for s,n in zip(st_s002[\"state\"], st_s002[\"n\"])], st_s002[\"mean\"])\n",
    "    axE.set_xlim(0,1); axE.invert_yaxis(); axE.set_xlabel(\"mean fbCSP_task_proba\"); axE.set_title(\"S002 Task-Proba by State (R03)\")\n",
    "else: axE.axis(\"off\"); axE.text(0.5,0.5,\"no S002 R03 summary\", ha=\"center\", va=\"center\")\n",
    "\n",
    "axF=fig.add_subplot(gs[1,1])\n",
    "if st_s003 is not None and len(st_s003):\n",
    "    axF.barh([f\"S{int(s)} (n={int(n)})\" for s,n in zip(st_s003[\"state\"], st_s003[\"n\"])], st_s003[\"mean\"])\n",
    "    axF.set_xlim(0,1); axF.invert_yaxis(); axF.set_xlabel(\"mean fbCSP_task_proba\"); axF.set_title(\"S003 Task-Proba by State (R03)\")\n",
    "else: axF.axis(\"off\"); axF.text(0.5,0.5,\"no S003 R03 summary\", ha=\"center\", va=\"center\")\n",
    "\n",
    "fig.suptitle(\"CNT Cognitive Alphabet â€” v0.3 Generalization (subject-aware)\", fontsize=16, weight=\"bold\")\n",
    "hdr=[]\n",
    "hdr.append(f\"EO/EC acc â€” S002={summary['subjects'].get('S002',{}).get('eoec_acc','n/a')}, S003={summary['subjects'].get('S003',{}).get('eoec_acc','n/a')}\")\n",
    "hdr.append(f\"Task AUC â€” S002={summary['subjects'].get('S002',{}).get('r03_auc_task','n/a')}, S003={summary['subjects'].get('S003',{}).get('r03_auc_task','n/a')}\")\n",
    "fig.text(0.5,0.94,\" | \".join(map(str,hdr)), ha=\"center\", fontsize=10)\n",
    "\n",
    "PNG = GENREP/\"CNT_CognitiveAlphabet_v0_3_Generalization.png\"\n",
    "PDF = GENREP/\"CNT_CognitiveAlphabet_v0_3_Generalization.pdf\"\n",
    "fig.savefig(PNG, bbox_inches=\"tight\"); fig.savefig(PDF, bbox_inches=\"tight\"); plt.close(fig)\n",
    "with open(GENREP/\"generalization_summary.json\",\"w\") as f: json.dump(summary, f, indent=2)\n",
    "\n",
    "# =========================================================================\n",
    "# Bundle v0.3 (copy core + new pages)\n",
    "# =========================================================================\n",
    "core = [\n",
    "    \"features.csv\",\"metadata.csv\",\"state_assignments.csv\",\n",
    "    \"cognitive_alphabet.json\",\"embedding.png\",\"state_feature_signatures.png\",\n",
    "    \"scaler_hybrid.joblib\",\"pca_hybrid.joblib\",\"kmeans_hybrid.joblib\"\n",
    "]\n",
    "for f in core:\n",
    "    src = RUN/f\n",
    "    if src.exists(): shutil.copy2(src, BUNDLE/f)\n",
    "\n",
    "# copy reports: v0.2 addenda if present + v0.3 generalization + dwell + microstates\n",
    "rep_files = []\n",
    "rep_files += [p for p in REP.glob(\"CNT_CognitiveAlphabet_Stability_Addendum_HYBRID_*.png\")]\n",
    "rep_files += [GENREP/\"CNT_CognitiveAlphabet_v0_3_Generalization.png\",\n",
    "              GENREP/\"CNT_CognitiveAlphabet_v0_3_Generalization.pdf\",\n",
    "              GENREP/\"generalization_summary.json\"]\n",
    "for p in rep_files:\n",
    "    if p.exists():\n",
    "        (BUNDLE/\"report\").mkdir(exist_ok=True)\n",
    "        shutil.copy2(p, BUNDLE/\"report\"/p.name)\n",
    "\n",
    "# copy dwell & microstates folders\n",
    "if DWELL.exists():\n",
    "    shutil.copytree(DWELL, BUNDLE/\"dwell\", dirs_exist_ok=True)\n",
    "if MSREP.exists():\n",
    "    shutil.copytree(MSREP, BUNDLE/\"microstates\", dirs_exist_ok=True)\n",
    "\n",
    "# README v0.3\n",
    "readme = f\"\"\"# CNT Cognitive Alphabet v0.3 (Subject-Aware)\n",
    "Run: {RUN}\n",
    "K: {K}\n",
    "Epoching: 2.0 s win / 0.5 s hop\n",
    "Bands: delta(1â€“4), theta(4â€“8), alpha(8â€“13), beta_low(13â€“20), beta_high(20â€“35), gamma(35â€“80)\n",
    "Coherence: anchor-alpha + tuned sticky + min-run\n",
    "\n",
    "## Generalization\n",
    "- EO/EC (IAF): see report/v0_3 generalization page\n",
    "- R03 Task (FBCSP v2, IIR Butter, C3/Cz/C4Â±): see report page + generalization_summary.json\n",
    "\n",
    "## Dwell model\n",
    "- Per-letter geometric vs gamma fits; chosen by AIC/BIC; embedded transition matrix without self-loops.\n",
    "- Sampler: dwell-respecting sequence generator; see dwell/semi_markov_simulation.csv\n",
    "\n",
    "## Microstate portraits\n",
    "- For S001: spectrum + Î±/Î² headmaps + A/B/C/D occupancy per letter; sheet in microstates/\n",
    "\"\"\"\n",
    "with open(BUNDLE/\"README_v0_3.md\",\"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(readme)\n",
    "\n",
    "zip_path = shutil.make_archive(str(BUNDLE), \"zip\", root_dir=BUNDLE)\n",
    "print(\"v0.3 bundle â†’\", zip_path)\n",
    "print(\"v0.3 generalization page â†’\", PNG)\n",
    "print(\"Dwell outputs â†’\", DWELL)\n",
    "print(\"Microstate portraits â†’\", MSREP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc2a5ed4-f7ec-4460-b116-56029745aab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bundle v0.3 â†’ E:\\CNT\\artifacts\\CNT_CognitiveAlphabet_v0_3.zip\n",
      "No per-run microstate usage CSVs found in E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\microstates\n",
      "\n",
      "Done. You can now:\n",
      " - Use live_decode_stream(<edf>, <subject>, seconds=...) to stream letters + EO/EC + fbTask to CSV\n",
      " - Open the v0.3 bundle zip at: E:\\CNT\\artifacts\\CNT_CognitiveAlphabet_v0_3.zip\n",
      " - See microstate summary CSV/PNGs in: E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\microstates\n"
     ]
    }
   ],
   "source": [
    "# === CNT Cognitive Alphabet v0.3 â€” Publish + Live Demo + Microstate Summary (one cell) ===\n",
    "import os, re, json, time, shutil, numpy as np, pandas as pd, matplotlib.pyplot as plt, mne\n",
    "from pathlib import Path\n",
    "from scipy.signal import welch\n",
    "from joblib import load\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from mne.decoding import CSP\n",
    "\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Paths (edit here if your layout differs)\n",
    "# ----------------------------------------------------\n",
    "RUN    = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")             # promoted v0.2 run\n",
    "REP    = RUN.parent / \"cog_alphabet_report_hybrid_v1\"\n",
    "GENREP = REP / \"generalization\"\n",
    "DWELL  = REP / \"dwell\"\n",
    "MSREP  = REP / \"microstates\"\n",
    "BUNDLE = RUN.parent / \"CNT_CognitiveAlphabet_v0_3\"\n",
    "RAW_DIR= RUN / \"brainwaves_rebuilt\"                                   # S001 EDFs live here\n",
    "\n",
    "BUNDLE.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Core models for decoding\n",
    "scaler = load(RUN/\"scaler_hybrid.joblib\")\n",
    "pca    = load(RUN/\"pca_hybrid.joblib\")\n",
    "km     = load(RUN/\"kmeans_hybrid.joblib\")\n",
    "TRAIN_COLS = list(pd.read_csv(RUN/\"features.csv\", nrows=1).columns)\n",
    "\n",
    "# v0.2 processing params\n",
    "TARGET_SF=250.0; L_FREQ,H_FREQ=0.5,80.0; EPOCH_LEN,STEP=2.0,0.5\n",
    "BANDS = {\"delta\":(1,4),\"theta\":(4,8),\"alpha\":(8,13),\"beta_low\":(13,20),\"beta_high\":(20,35),\"gamma\":(35,80)}\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Small utilities\n",
    "# ----------------------------------------------------\n",
    "def occipital_picks(raw):\n",
    "    names = [n.upper().strip() for n in raw.ch_names]\n",
    "    wanted = (\"O1\",\"O2\",\"OZ\",\"POZ\",\"PO3\",\"PO4\")\n",
    "    idx = [i for i,n in enumerate(names) if n in wanted]\n",
    "    return idx if idx else mne.pick_types(raw.info, eeg=True)\n",
    "\n",
    "def load_raw_edf(p, band=(L_FREQ,H_FREQ), microstate_mode=False):\n",
    "    raw = mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"]-TARGET_SF)>1e-6:\n",
    "        raw.resample(TARGET_SF, npad=\"auto\", verbose=False)\n",
    "    ny = raw.info[\"sfreq\"]/2.0\n",
    "    if microstate_mode:\n",
    "        raw.filter(2.0, min(20.0,ny-1.0), verbose=False)\n",
    "    else:\n",
    "        raw.filter(band[0], min(band[1],ny-1.0), verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "        if microstate_mode:\n",
    "            try:\n",
    "                raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"),\n",
    "                                match_case=False, match_alias=True, on_missing=\"ignore\")\n",
    "            except Exception:\n",
    "                pass\n",
    "    return raw\n",
    "\n",
    "def make_epochs(raw):\n",
    "    ov = max(0.0, EPOCH_LEN-STEP)\n",
    "    return mne.make_fixed_length_epochs(raw, duration=EPOCH_LEN, overlap=ov, preload=True, verbose=False)\n",
    "\n",
    "def spectral_features(epochs):\n",
    "    X = epochs.get_data(); sf = epochs.info[\"sfreq\"]\n",
    "    n_ep,n_ch,n_t = X.shape; nper=min(int(sf*2), n_t); nov=nper//2\n",
    "    freqs, psd = welch(X, fs=sf, nperseg=nper, noverlap=nov, axis=-1, average=\"median\")\n",
    "    def idx(lo,hi): return np.where((freqs>=lo)&(freqs<hi))[0]\n",
    "    aidx=idx(L_FREQ,H_FREQ); tot=np.maximum(psd[:,:,aidx].sum(-1),1e-12)\n",
    "    feats={}; acc={\"alpha\":np.zeros((n_ep,n_ch)),\"theta\":np.zeros((n_ep,n_ch)),\"beta\":np.zeros((n_ep,n_ch))}\n",
    "    for name,(lo,hi) in BANDS.items():\n",
    "        b=idx(lo,hi); bp=psd[:,:,b].sum(-1)/tot if b.size else np.zeros((n_ep,n_ch))\n",
    "        feats[f\"{name}_rel_med\"]=np.median(bp,axis=1); feats[f\"{name}_rel_iqr\"]=np.subtract(*np.percentile(bp,[75,25],axis=1)); feats[f\"{name}_rel_std\"]=np.std(bp,axis=1)\n",
    "        head=name.split(\"_\",1)[0]; \n",
    "        if head in acc: acc[head]+=bp\n",
    "    alpha=np.median(acc[\"alpha\"],axis=1); theta=np.median(acc[\"theta\"],axis=1); beta=np.median(acc[\"beta\"],axis=1)\n",
    "    feats[\"alpha_rel_med\"]=alpha; feats[\"theta_rel_med\"]=theta; feats[\"beta_rel_med\"]=beta\n",
    "    feats[\"theta_over_alpha\"]=theta/np.maximum(alpha,1e-6); feats[\"beta_over_alpha\"]=beta/np.maximum(alpha,1e-6)\n",
    "    feats[\"(alpha+theta)_over_beta\"]=(alpha+theta)/np.maximum(beta,1e-6)\n",
    "    p=psd[:,:,aidx]; p_n=p/np.maximum(p.sum(-1,keepdims=True),1e-12)\n",
    "    H=-(p_n*np.log2(p_n+1e-12)).sum(-1); Hn=H/np.log2(p.shape[-1])\n",
    "    feats[\"spec_entropy_med\"]=np.median(Hn,axis=1); feats[\"spec_entropy_std\"]=np.std(Hn,axis=1)\n",
    "    f=freqs[aidx].reshape(1,1,-1); cen=(p*f).sum(-1)/np.maximum(p.sum(-1),1e-12)\n",
    "    feats[\"spec_centroid_med\"]=np.median(cen,axis=1); feats[\"spec_centroid_std\"]=np.std(cen,axis=1)\n",
    "    # Hjorth\n",
    "    d1=np.diff(X,axis=-1); var0=np.var(X,axis=-1)+1e-12; var1=np.var(d1,axis=-1)+1e-12\n",
    "    feats[\"hjorth_activity_med\"]=np.median(var0,axis=1); feats[\"hjorth_mobility_med\"]=np.median(np.sqrt(var1/var0),axis=1)\n",
    "    d2=np.diff(d1,axis=-1); var2=np.var(d2,axis=-1)+1e-12\n",
    "    feats[\"hjorth_complexity_med\"]=np.median(np.sqrt((var2/var1)/(var1/var0)),axis=1)\n",
    "    return pd.DataFrame(feats)\n",
    "\n",
    "def butter_bandpass_array(X, lo, hi, sf, order=4):\n",
    "    X2 = X.reshape(X.shape[0]*X.shape[1], X.shape[2])\n",
    "    Xf = mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi,\n",
    "                                method=\"iir\", iir_params=dict(order=order, ftype=\"butter\"), verbose=False)\n",
    "    return Xf.reshape(X.shape)\n",
    "\n",
    "def fbCSP_task_proba_for_R03(raw, starts, ends):\n",
    "    want={\"C3\",\"Cz\",\"C4\",\"FC3\",\"FCz\",\"FC4\",\"CP3\",\"CPz\",\"CP4\"}\n",
    "    picks=[i for i,ch in enumerate(raw.ch_names) if ch.upper().strip() in want]\n",
    "    if len(picks)<3: picks=mne.pick_types(raw.info, eeg=True)\n",
    "    _raw=raw.copy().pick(picks); sf=_raw.info[\"sfreq\"]\n",
    "    # Build epoch tensor\n",
    "    X=[]\n",
    "    for t0,t1 in zip(starts, ends):\n",
    "        s=int(round(t0*sf)); e=int(round(t1*sf))\n",
    "        X.append(_raw.get_data(start=s, stop=e))\n",
    "    X=np.stack(X,0)\n",
    "    # Labels from annotations (â‰¥50% overlap)\n",
    "    y=[]; g=[]; anns=_raw.annotations\n",
    "    for t0,t1 in zip(starts, ends):\n",
    "        lab=-1; best=-1.0; gid=-1\n",
    "        if anns is not None and len(anns):\n",
    "            for j,(o,d,s) in enumerate(zip(anns.onset, anns.duration, anns.description)):\n",
    "                su=str(s).upper()\n",
    "                if \"T0\" in su or \"T1\" in su or \"T2\" in su:\n",
    "                    L,R=max(t0,float(o)),min(t1,float(o)+float(d))\n",
    "                    if R>L and (R-L)>best:\n",
    "                        best=R-L; gid=j; lab=0 if \"T0\" in su else 1\n",
    "        if best>=0.5*(t1-t0): y.append(lab); g.append(gid)\n",
    "        else: y.append(-1); g.append(-1)\n",
    "    y=np.array(y,int); g=np.array(g,int)\n",
    "    if not np.any(y>=0): return np.zeros(len(y))\n",
    "    bands=[(8,13),(13,20),(20,30)]\n",
    "    mask=(y>=0); yt=y[mask]; gt=g[mask]; Xt=X[mask]\n",
    "    gkf=GroupKFold(n_splits=min(5,max(2,len(np.unique(gt)))))\n",
    "    proba=np.zeros(len(yt),float)\n",
    "    for tr,te in gkf.split(Xt, yt, gt):\n",
    "        feats_tr, feats_te=[],[]\n",
    "        for lo,hi in bands:\n",
    "            Xtr=butter_bandpass_array(Xt[tr], lo, hi, sf)\n",
    "            Xte=butter_bandpass_array(Xt[te], lo, hi, sf)\n",
    "            csp=CSP(n_components=6, reg=\"ledoit_wolf\", log=True, norm_trace=False)\n",
    "            csp.fit(Xtr, yt[tr])\n",
    "            feats_tr.append(csp.transform(Xtr)); feats_te.append(csp.transform(Xte))\n",
    "        Xtr_fb=np.concatenate(feats_tr,1); Xte_fb=np.concatenate(feats_te,1)\n",
    "        clf=LDA(solver=\"lsqr\", shrinkage=\"auto\").fit(Xtr_fb, yt[tr])\n",
    "        proba[te]=clf.predict_proba(Xte_fb)[:,1]\n",
    "    full=np.zeros(len(y),float); full[np.where(mask)[0]]=proba\n",
    "    return full\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# (1) Publish v0.3 bundle\n",
    "# ----------------------------------------------------\n",
    "def publish_v03_bundle():\n",
    "    # copy core artifacts\n",
    "    core = [\n",
    "        \"features.csv\",\"metadata.csv\",\"state_assignments.csv\",\n",
    "        \"cognitive_alphabet.json\",\"embedding.png\",\"state_feature_signatures.png\",\n",
    "        \"scaler_hybrid.joblib\",\"pca_hybrid.joblib\",\"kmeans_hybrid.joblib\"\n",
    "    ]\n",
    "    for f in core:\n",
    "        src = RUN/f\n",
    "        if src.exists(): shutil.copy2(src, BUNDLE/f)\n",
    "    # copy v0.3 generalization page + summary + calibration if present\n",
    "    rep_files = [\n",
    "        GENREP/\"CNT_CognitiveAlphabet_v0_3_Generalization.png\",\n",
    "        GENREP/\"CNT_CognitiveAlphabet_v0_3_Generalization.pdf\",\n",
    "        GENREP/\"generalization_summary.json\",\n",
    "        GENREP/\"subject_calibration.json\"\n",
    "    ]\n",
    "    (BUNDLE/\"report\").mkdir(exist_ok=True)\n",
    "    for p in rep_files:\n",
    "        if p.exists(): shutil.copy2(p, BUNDLE/\"report\"/p.name)\n",
    "    # copy dwell & microstates folders if present\n",
    "    if DWELL.exists():\n",
    "        shutil.copytree(DWELL, BUNDLE/\"dwell\", dirs_exist_ok=True)\n",
    "    if MSREP.exists():\n",
    "        shutil.copytree(MSREP, BUNDLE/\"microstates\", dirs_exist_ok=True)\n",
    "    # readme\n",
    "    readme = f\"\"\"# CNT Cognitive Alphabet v0.3 (Subject-Aware)\n",
    "Run: {RUN}\n",
    "Epoching: 2.0 s / 0.5 s  |  K={int(pd.read_csv(RUN/'state_assignments.csv').shape[0] > 0 and pd.read_csv(RUN/'state_assignments.csv')['state'].max()+1)}\n",
    "Bands: delta(1â€“4), theta(4â€“8), alpha(8â€“13), beta_low(13â€“20), beta_high(20â€“35), gamma(35â€“80)\n",
    "Includes: models, labels, dwell fits + sampler, microstate portraits, v0.3 generalization page, calibration.\n",
    "\"\"\"\n",
    "    with open(BUNDLE/\"README_v0_3.md\",\"w\",encoding=\"utf-8\") as f:\n",
    "        f.write(readme)\n",
    "    zip_path = shutil.make_archive(str(BUNDLE), \"zip\", root_dir=BUNDLE)\n",
    "    print(\"Bundle v0.3 â†’\", zip_path)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# (2) Live Demo Stub â€” chunked 2s decoding with subject calibration\n",
    "# ----------------------------------------------------\n",
    "CALIB = GENREP/\"subject_calibration.json\"\n",
    "calib_all = json.load(open(CALIB)) if CALIB.exists() else {}\n",
    "\n",
    "def live_decode_stream(infile, subject_id, seconds=60, out_csv=None):\n",
    "    \"\"\"\n",
    "    Simulated real-time: reads EDF, walks through 2-s chunks for 'seconds',\n",
    "    appends per-chunk rows to CSV: time, state, eoec_pred, fb_task, task_pred.\n",
    "    \"\"\"\n",
    "    infile = Path(infile)\n",
    "    if out_csv is None:\n",
    "        out_csv = GENREP / f\"live_{subject_id}_{infile.stem}.csv\"\n",
    "    cal = calib_all.get(subject_id, {})\n",
    "    raw = load_raw_edf(infile)\n",
    "    sf  = raw.info[\"sfreq\"]\n",
    "    n_samp = int(seconds*sf)\n",
    "    # Optional: EO/EC alpha band & threshold\n",
    "    band = cal.get(\"alpha_band\", [8.0, 13.0])\n",
    "    thr  = cal.get(\"alpha_threshold\", None)\n",
    "    task_letters = set(map(int, cal.get(\"task_letters\", [])))\n",
    "    fb_thr = float(cal.get(\"fbCSP_threshold\", 0.50))\n",
    "    # streaming loop (simulated)\n",
    "    rows=[]\n",
    "    t=0.0\n",
    "    while int(t+EPOCH_LEN)*sf <= min(n_samp, raw.n_times):\n",
    "        s = int(t*sf); e = s+int(EPOCH_LEN*sf)\n",
    "        seg_raw = raw.copy().crop(tmin=t, tmax=t+EPOCH_LEN, include_tmax=False)\n",
    "        # spectral -> letter\n",
    "        ep = make_epochs(seg_raw)  # exactly one 2s epoch\n",
    "        Fsp = spectral_features(ep).reindex(columns=TRAIN_COLS, fill_value=0.0)\n",
    "        Z   = pca.transform(scaler.transform(Fsp.values))\n",
    "        letter = int(km.predict(Z)[0])\n",
    "        # EO/EC via alpha index (R01/R02 convention if threshold present)\n",
    "        eoec = \"UNK\"\n",
    "        if thr is not None:\n",
    "            f,P = welch(seg_raw.get_data(), fs=sf, nperseg=min(int(sf*2), seg_raw.n_times), noverlap=int(min(int(sf*2), seg_raw.n_times)/2), axis=-1, average=\"median\")\n",
    "            idx=(f>=band[0])&(f<band[1]); a=P[:,idx].sum(-1); tot=P.sum(-1)+1e-12\n",
    "            oi = occipital_picks(seg_raw)\n",
    "            ai = (a[oi].sum()/tot[oi].sum()) if len(oi) else (a.sum()/tot.sum())\n",
    "            eoec = \"EC\" if ai>=thr else \"EO\"\n",
    "        # fbCSP proba (R03-like) â€” best-effort on the chunk (no CV; uses simple fit if annotations present locally)\n",
    "        fb = 0.0; tpred=\"UNK\"\n",
    "        try:\n",
    "            starts=np.array([0.0]); ends=np.array([EPOCH_LEN])\n",
    "            fb_arr = fbCSP_task_proba_for_R03(seg_raw, starts, ends)\n",
    "            fb = float(fb_arr[0]) if len(fb_arr) else 0.0\n",
    "        except Exception:\n",
    "            fb = 0.0\n",
    "        if task_letters or fb_thr is not None:\n",
    "            tpred = \"task\" if (letter in task_letters or fb>=fb_thr) else \"rest\"\n",
    "        rows.append({\"t_start_s\":round(t,3), \"t_end_s\":round(t+EPOCH_LEN,3),\n",
    "                     \"state\":letter, \"eoec_pred\":eoec, \"fb_task\":fb, \"task_pred\":tpred})\n",
    "        t += EPOCH_LEN  # hop by 2s\n",
    "    df = pd.DataFrame(rows)\n",
    "    if out_csv.exists():\n",
    "        df.to_csv(out_csv, mode=\"a\", header=False, index=False)\n",
    "    else:\n",
    "        df.to_csv(out_csv, index=False)\n",
    "    print(\"Live stream appended â†’\", out_csv)\n",
    "    return out_csv\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# (3) Microstate Summary â€” aggregate A/B/C/D across S001 runs\n",
    "# ----------------------------------------------------\n",
    "def microstate_summary_sheet():\n",
    "    usage_files = list(MSREP.glob(\"*_microstate_usage.csv\"))\n",
    "    if not usage_files:\n",
    "        print(\"No per-run microstate usage CSVs found in\", MSREP)\n",
    "        return None\n",
    "    rows=[]\n",
    "    for p in usage_files:\n",
    "        df = pd.read_csv(p, index_col=0)  # index=letter, cols Î¼0..Î¼3\n",
    "        df[\"source\"] = p.stem\n",
    "        df[\"letter\"] = df.index\n",
    "        rows.append(df.reset_index(drop=True))\n",
    "    all_occ = pd.concat(rows, ignore_index=True)\n",
    "    # aggregate mean across sources for each letter\n",
    "    agg = (all_occ.groupby(\"letter\")[[c for c in all_occ.columns if c.startswith(\"Î¼\")]]\n",
    "                    .mean().sort_index())\n",
    "    out_csv = MSREP/\"microstate_summary_per_letter.csv\"\n",
    "    agg.to_csv(out_csv)\n",
    "    # small figure (one chart per figure: stacked bars per letter = multiple charts; so create grid of single bars)\n",
    "    for letter, row in agg.iterrows():\n",
    "        plt.figure()\n",
    "        plt.bar([f\"Î¼{k}\" for k in range(len(row))], row.values)\n",
    "        plt.ylim(0,1); plt.ylabel(\"fraction\")\n",
    "        plt.title(f\"Microstate mix â€” letter S{int(letter)} (mean across S001 runs)\")\n",
    "        fig_path = MSREP/f\"microstate_summary_S{int(letter)}.png\"\n",
    "        plt.savefig(fig_path, dpi=150, bbox_inches=\"tight\"); plt.close()\n",
    "    print(\"Microstate summary saved â†’\", out_csv)\n",
    "    return out_csv\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Run the three tasks\n",
    "# ----------------------------------------------------\n",
    "# 1) Publish v0.3 bundle\n",
    "publish_v03_bundle()\n",
    "\n",
    "# 2) (Optional) Live demo â€” uncomment/point to a file & subject to stream N seconds\n",
    "# live_decode_stream(RUN.parent / \"generalization_data\" / \"S002R03.edf\", subject_id=\"S002\", seconds=60)\n",
    "\n",
    "# 3) Microstate summary insert\n",
    "microstate_summary_sheet()\n",
    "\n",
    "print(\"\\nDone. You can now:\")\n",
    "print(\" - Use live_decode_stream(<edf>, <subject>, seconds=...) to stream letters + EO/EC + fbTask to CSV\")\n",
    "print(\" - Open the v0.3 bundle zip at:\", str(RUN.parent / \"CNT_CognitiveAlphabet_v0_3.zip\"))\n",
    "print(\" - See microstate summary CSV/PNGs in:\", str(MSREP))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4bb98bf4-8547-4a68-951d-b651ce2a5415",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'E:\\\\CNT\\\\artifacts\\\\cog_alphabet_hybrid_v1\\\\generalization_data\\\\S002R03.edf'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mlive_decode_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mE:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mCNT\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43martifacts\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mcog_alphabet_hybrid_v1\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mgeneralization_data\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mS002R03.edf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mS002\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseconds\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m120\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 200\u001b[39m, in \u001b[36mlive_decode_stream\u001b[39m\u001b[34m(infile, subject_id, seconds, out_csv)\u001b[39m\n\u001b[32m    198\u001b[39m     out_csv = GENREP / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlive_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubject_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minfile.stem\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    199\u001b[39m cal = calib_all.get(subject_id, {})\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m raw = \u001b[43mload_raw_edf\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    201\u001b[39m sf  = raw.info[\u001b[33m\"\u001b[39m\u001b[33msfreq\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    202\u001b[39m n_samp = \u001b[38;5;28mint\u001b[39m(seconds*sf)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 45\u001b[39m, in \u001b[36mload_raw_edf\u001b[39m\u001b[34m(p, band, microstate_mode)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_raw_edf\u001b[39m(p, band=(L_FREQ,H_FREQ), microstate_mode=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     raw = \u001b[43mmne\u001b[49m\u001b[43m.\u001b[49m\u001b[43mio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_raw_edf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(raw.info[\u001b[33m\"\u001b[39m\u001b[33msfreq\u001b[39m\u001b[33m\"\u001b[39m]-TARGET_SF)>\u001b[32m1e-6\u001b[39m:\n\u001b[32m     47\u001b[39m         raw.resample(TARGET_SF, npad=\u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m, verbose=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\CNT\\.venv\\Lib\\site-packages\\mne\\io\\edf\\edf.py:1700\u001b[39m, in \u001b[36mread_raw_edf\u001b[39m\u001b[34m(input_fname, eog, misc, stim_channel, exclude, infer_types, include, preload, units, encoding, exclude_after_unique, verbose)\u001b[39m\n\u001b[32m   1698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ext != \u001b[33m\"\u001b[39m\u001b[33medf\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1699\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOnly EDF files are supported, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1700\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRawEDF\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1701\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_fname\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_fname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1702\u001b[39m \u001b[43m    \u001b[49m\u001b[43meog\u001b[49m\u001b[43m=\u001b[49m\u001b[43meog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmisc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmisc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstim_channel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstim_channel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfer_types\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1708\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m=\u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1709\u001b[39m \u001b[43m    \u001b[49m\u001b[43munits\u001b[49m\u001b[43m=\u001b[49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1711\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexclude_after_unique\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexclude_after_unique\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1712\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1713\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<decorator-gen-206>:10\u001b[39m, in \u001b[36m__init__\u001b[39m\u001b[34m(self, input_fname, eog, misc, stim_channel, exclude, infer_types, preload, include, units, encoding, exclude_after_unique, verbose)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\CNT\\.venv\\Lib\\site-packages\\mne\\io\\edf\\edf.py:152\u001b[39m, in \u001b[36mRawEDF.__init__\u001b[39m\u001b[34m(self, input_fname, eog, misc, stim_channel, exclude, infer_types, preload, include, units, encoding, exclude_after_unique, verbose)\u001b[39m\n\u001b[32m    150\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExtracting EDF parameters from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_fname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    151\u001b[39m input_fname = os.path.abspath(input_fname)\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m info, edf_info, orig_units = \u001b[43m_get_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_fname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstim_channel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43meog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmisc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    157\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfer_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    159\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexclude_after_unique\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mCreating raw.info structure...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    165\u001b[39m _validate_type(units, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mdict\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33munits\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\CNT\\.venv\\Lib\\site-packages\\mne\\io\\edf\\edf.py:541\u001b[39m, in \u001b[36m_get_info\u001b[39m\u001b[34m(fname, stim_channel, eog, misc, exclude, infer_types, preload, include, exclude_after_unique)\u001b[39m\n\u001b[32m    538\u001b[39m eog = eog \u001b[38;5;28;01mif\u001b[39;00m eog \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m    539\u001b[39m misc = misc \u001b[38;5;28;01mif\u001b[39;00m misc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m--> \u001b[39m\u001b[32m541\u001b[39m edf_info, orig_units = \u001b[43m_read_header\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    542\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_after_unique\u001b[49m\n\u001b[32m    543\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[38;5;66;03m# XXX: `tal_ch_names` to pass to `_check_stim_channel` should be computed\u001b[39;00m\n\u001b[32m    546\u001b[39m \u001b[38;5;66;03m#      from `edf_info['ch_names']` and `edf_info['tal_idx']` but 'tal_idx'\u001b[39;00m\n\u001b[32m    547\u001b[39m \u001b[38;5;66;03m#      contains stim channels that are not TAL.\u001b[39;00m\n\u001b[32m    548\u001b[39m stim_channel_idxs, _ = _check_stim_channel(stim_channel, edf_info[\u001b[33m\"\u001b[39m\u001b[33mch_names\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\CNT\\.venv\\Lib\\site-packages\\mne\\io\\edf\\edf.py:515\u001b[39m, in \u001b[36m_read_header\u001b[39m\u001b[34m(fname, exclude, infer_types, include, exclude_after_unique)\u001b[39m\n\u001b[32m    513\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mext.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m file detected\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    514\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mbdf\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33medf\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m515\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_edf_header\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfer_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_after_unique\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m ext == \u001b[33m\"\u001b[39m\u001b[33mgdf\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    519\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _read_gdf_header(fname, exclude, include), \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\CNT\\.venv\\Lib\\site-packages\\mne\\io\\edf\\edf.py:809\u001b[39m, in \u001b[36m_read_edf_header\u001b[39m\u001b[34m(fname, exclude, infer_types, include, exclude_after_unique)\u001b[39m\n\u001b[32m    806\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Read header information from EDF+ or BDF file.\"\"\"\u001b[39;00m\n\u001b[32m    807\u001b[39m edf_info = {\u001b[33m\"\u001b[39m\u001b[33mevents\u001b[39m\u001b[33m\"\u001b[39m: []}\n\u001b[32m--> \u001b[39m\u001b[32m809\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fid:\n\u001b[32m    810\u001b[39m     fid.read(\u001b[32m8\u001b[39m)  \u001b[38;5;66;03m# version (unused here)\u001b[39;00m\n\u001b[32m    812\u001b[39m     \u001b[38;5;66;03m# patient ID\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'E:\\\\CNT\\\\artifacts\\\\cog_alphabet_hybrid_v1\\\\generalization_data\\\\S002R03.edf'"
     ]
    }
   ],
   "source": [
    "live_decode_stream(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\\generalization_data\\S002R03.edf\", \"S002\", seconds=120)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "413a7b28-776d-47c5-bb20-5e7c78f539c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.5e-05 (2.2e-16 eps * 64 dim * 5.3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.4e-05 (2.2e-16 eps * 64 dim * 3.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5e-05 (2.2e-16 eps * 64 dim * 3.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.7e-05 (2.2e-16 eps * 64 dim * 5.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.3e-05 (2.2e-16 eps * 64 dim * 3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5e-05 (2.2e-16 eps * 64 dim * 3.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.7e-05 (2.2e-16 eps * 64 dim * 5.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.4e-05 (2.2e-16 eps * 64 dim * 3.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.3e-05 (2.2e-16 eps * 64 dim * 3.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8e-05 (2.2e-16 eps * 64 dim * 5.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.3e-05 (2.2e-16 eps * 64 dim * 3.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.2e-05 (2.2e-16 eps * 64 dim * 3.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.8e-05 (2.2e-16 eps * 64 dim * 5.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.4e-05 (2.2e-16 eps * 64 dim * 3.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.1e-05 (2.2e-16 eps * 64 dim * 3.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00015 (2.2e-16 eps * 64 dim * 1e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.8e-05 (2.2e-16 eps * 64 dim * 6.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00016 (2.2e-16 eps * 64 dim * 1.1e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.9e-05 (2.2e-16 eps * 64 dim * 6.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00016 (2.2e-16 eps * 64 dim * 1.1e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.9e-05 (2.2e-16 eps * 64 dim * 6.3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00016 (2.2e-16 eps * 64 dim * 1.1e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.8e-05 (2.2e-16 eps * 64 dim * 6.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00016 (2.2e-16 eps * 64 dim * 1.1e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.8e-05 (2.2e-16 eps * 64 dim * 6.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00011 (2.2e-16 eps * 64 dim * 7.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.7e-05 (2.2e-16 eps * 64 dim * 3.3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.9e-05 (2.2e-16 eps * 64 dim * 2.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.8e-05 (2.2e-16 eps * 64 dim * 3.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.9e-05 (2.2e-16 eps * 64 dim * 2.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.8e-05 (2.2e-16 eps * 64 dim * 3.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.8e-05 (2.2e-16 eps * 64 dim * 2.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.7e-05 (2.2e-16 eps * 64 dim * 3.3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.9e-05 (2.2e-16 eps * 64 dim * 2.8e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.8e-05 (2.2e-16 eps * 64 dim * 3.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.9e-05 (2.2e-16 eps * 64 dim * 2.8e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00021 (2.2e-16 eps * 64 dim * 1.5e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8e-05 (2.2e-16 eps * 64 dim * 5.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00021 (2.2e-16 eps * 64 dim * 1.5e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9.9e-05 (2.2e-16 eps * 64 dim * 7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.8e-05 (2.2e-16 eps * 64 dim * 5.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00021 (2.2e-16 eps * 64 dim * 1.5e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9.9e-05 (2.2e-16 eps * 64 dim * 7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.1e-05 (2.2e-16 eps * 64 dim * 5.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00021 (2.2e-16 eps * 64 dim * 1.5e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9.6e-05 (2.2e-16 eps * 64 dim * 6.8e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8e-05 (2.2e-16 eps * 64 dim * 5.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0002 (2.2e-16 eps * 64 dim * 1.4e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9.8e-05 (2.2e-16 eps * 64 dim * 6.9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.9e-05 (2.2e-16 eps * 64 dim * 5.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00014 (2.2e-16 eps * 64 dim * 9.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.4e-05 (2.2e-16 eps * 64 dim * 4.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.3e-05 (2.2e-16 eps * 64 dim * 3.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00014 (2.2e-16 eps * 64 dim * 9.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.6e-05 (2.2e-16 eps * 64 dim * 4.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.2e-05 (2.2e-16 eps * 64 dim * 3.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00014 (2.2e-16 eps * 64 dim * 9.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.6e-05 (2.2e-16 eps * 64 dim * 4.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.2e-05 (2.2e-16 eps * 64 dim * 3.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00014 (2.2e-16 eps * 64 dim * 9.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.5e-05 (2.2e-16 eps * 64 dim * 4.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.1e-05 (2.2e-16 eps * 64 dim * 3.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00013 (2.2e-16 eps * 64 dim * 9.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.2e-05 (2.2e-16 eps * 64 dim * 4.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.2e-05 (2.2e-16 eps * 64 dim * 3.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00028 (2.2e-16 eps * 64 dim * 2e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00013 (2.2e-16 eps * 64 dim * 9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00011 (2.2e-16 eps * 64 dim * 7.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00027 (2.2e-16 eps * 64 dim * 1.9e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00013 (2.2e-16 eps * 64 dim * 9.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00011 (2.2e-16 eps * 64 dim * 7.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00026 (2.2e-16 eps * 64 dim * 1.8e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00013 (2.2e-16 eps * 64 dim * 9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00027 (2.2e-16 eps * 64 dim * 1.9e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00013 (2.2e-16 eps * 64 dim * 9.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00027 (2.2e-16 eps * 64 dim * 1.9e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00013 (2.2e-16 eps * 64 dim * 9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.1e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.5e-05 (2.2e-16 eps * 64 dim * 2.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.7e-05 (2.2e-16 eps * 64 dim * 4.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.2e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.4e-05 (2.2e-16 eps * 64 dim * 2.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.6e-05 (2.2e-16 eps * 64 dim * 4.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.1e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.5e-05 (2.2e-16 eps * 64 dim * 2.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.5e-05 (2.2e-16 eps * 64 dim * 4.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.2e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.4e-05 (2.2e-16 eps * 64 dim * 2.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.6e-05 (2.2e-16 eps * 64 dim * 4.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.3e-05 (2.2e-16 eps * 64 dim * 2.3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.4e-05 (2.2e-16 eps * 64 dim * 2.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.7e-05 (2.2e-16 eps * 64 dim * 4.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.3e-05 (2.2e-16 eps * 64 dim * 4.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.1e-05 (2.2e-16 eps * 64 dim * 5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00013 (2.2e-16 eps * 64 dim * 9.3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.5e-05 (2.2e-16 eps * 64 dim * 4.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7e-05 (2.2e-16 eps * 64 dim * 4.9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00013 (2.2e-16 eps * 64 dim * 9.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.5e-05 (2.2e-16 eps * 64 dim * 4.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.9e-05 (2.2e-16 eps * 64 dim * 4.9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00013 (2.2e-16 eps * 64 dim * 9.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.5e-05 (2.2e-16 eps * 64 dim * 4.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.1e-05 (2.2e-16 eps * 64 dim * 5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00013 (2.2e-16 eps * 64 dim * 9.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.2e-05 (2.2e-16 eps * 64 dim * 4.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.8e-05 (2.2e-16 eps * 64 dim * 4.8e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00013 (2.2e-16 eps * 64 dim * 9.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.7e-05 (2.2e-16 eps * 64 dim * 1.9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.4e-05 (2.2e-16 eps * 64 dim * 1.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.3e-05 (2.2e-16 eps * 64 dim * 2.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.6e-05 (2.2e-16 eps * 64 dim * 1.9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.4e-05 (2.2e-16 eps * 64 dim * 1.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.3e-05 (2.2e-16 eps * 64 dim * 2.3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.7e-05 (2.2e-16 eps * 64 dim * 1.9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.5e-05 (2.2e-16 eps * 64 dim * 1.8e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.4e-05 (2.2e-16 eps * 64 dim * 2.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.7e-05 (2.2e-16 eps * 64 dim * 1.9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.4e-05 (2.2e-16 eps * 64 dim * 1.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.4e-05 (2.2e-16 eps * 64 dim * 2.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.6e-05 (2.2e-16 eps * 64 dim * 1.8e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.4e-05 (2.2e-16 eps * 64 dim * 1.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.4e-05 (2.2e-16 eps * 64 dim * 2.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.4e-05 (2.2e-16 eps * 64 dim * 3.8e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.9e-05 (2.2e-16 eps * 64 dim * 3.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.7e-05 (2.2e-16 eps * 64 dim * 4.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.4e-05 (2.2e-16 eps * 64 dim * 3.8e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.9e-05 (2.2e-16 eps * 64 dim * 3.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.8e-05 (2.2e-16 eps * 64 dim * 4.8e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.6e-05 (2.2e-16 eps * 64 dim * 3.9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5e-05 (2.2e-16 eps * 64 dim * 3.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.7e-05 (2.2e-16 eps * 64 dim * 4.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.4e-05 (2.2e-16 eps * 64 dim * 3.8e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.9e-05 (2.2e-16 eps * 64 dim * 3.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.8e-05 (2.2e-16 eps * 64 dim * 4.8e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.2e-05 (2.2e-16 eps * 64 dim * 3.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.8e-05 (2.2e-16 eps * 64 dim * 3.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.7e-05 (2.2e-16 eps * 64 dim * 4.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9.5e-05 (2.2e-16 eps * 64 dim * 6.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.4e-05 (2.2e-16 eps * 64 dim * 3.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.3e-05 (2.2e-16 eps * 64 dim * 3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9e-05 (2.2e-16 eps * 64 dim * 6.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.3e-05 (2.2e-16 eps * 64 dim * 3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.3e-05 (2.2e-16 eps * 64 dim * 3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9.9e-05 (2.2e-16 eps * 64 dim * 6.9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.5e-05 (2.2e-16 eps * 64 dim * 3.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.3e-05 (2.2e-16 eps * 64 dim * 3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9.9e-05 (2.2e-16 eps * 64 dim * 6.9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.5e-05 (2.2e-16 eps * 64 dim * 3.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.2e-05 (2.2e-16 eps * 64 dim * 3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9.7e-05 (2.2e-16 eps * 64 dim * 6.8e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.4e-05 (2.2e-16 eps * 64 dim * 3.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.2e-05 (2.2e-16 eps * 64 dim * 3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00018 (2.2e-16 eps * 64 dim * 1.3e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.6e-05 (2.2e-16 eps * 64 dim * 6.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.6e-05 (2.2e-16 eps * 64 dim * 6.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00019 (2.2e-16 eps * 64 dim * 1.3e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.9e-05 (2.2e-16 eps * 64 dim * 6.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.7e-05 (2.2e-16 eps * 64 dim * 6.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0002 (2.2e-16 eps * 64 dim * 1.4e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.9e-05 (2.2e-16 eps * 64 dim * 6.3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.7e-05 (2.2e-16 eps * 64 dim * 6.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0002 (2.2e-16 eps * 64 dim * 1.4e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9.1e-05 (2.2e-16 eps * 64 dim * 6.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.5e-05 (2.2e-16 eps * 64 dim * 6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0002 (2.2e-16 eps * 64 dim * 1.4e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.8e-05 (2.2e-16 eps * 64 dim * 6.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.6e-05 (2.2e-16 eps * 64 dim * 6.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.2e-05 (2.2e-16 eps * 64 dim * 3.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.1e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.1e-05 (2.2e-16 eps * 64 dim * 3.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.3e-05 (2.2e-16 eps * 64 dim * 3.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3e-05 (2.2e-16 eps * 64 dim * 2.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.1e-05 (2.2e-16 eps * 64 dim * 3.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.3e-05 (2.2e-16 eps * 64 dim * 3.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3e-05 (2.2e-16 eps * 64 dim * 2.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.1e-05 (2.2e-16 eps * 64 dim * 3.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5e-05 (2.2e-16 eps * 64 dim * 3.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3e-05 (2.2e-16 eps * 64 dim * 2.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.2e-05 (2.2e-16 eps * 64 dim * 3.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5e-05 (2.2e-16 eps * 64 dim * 3.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3e-05 (2.2e-16 eps * 64 dim * 2.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.2e-05 (2.2e-16 eps * 64 dim * 3.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00011 (2.2e-16 eps * 64 dim * 7.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6e-05 (2.2e-16 eps * 64 dim * 4.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.9e-05 (2.2e-16 eps * 64 dim * 4.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00011 (2.2e-16 eps * 64 dim * 7.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.1e-05 (2.2e-16 eps * 64 dim * 4.3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00011 (2.2e-16 eps * 64 dim * 7.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.2e-05 (2.2e-16 eps * 64 dim * 4.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6e-05 (2.2e-16 eps * 64 dim * 4.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.8e-05 (2.2e-16 eps * 64 dim * 5.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00012 (2.2e-16 eps * 64 dim * 8.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00012 (2.2e-16 eps * 64 dim * 8.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8e-05 (2.2e-16 eps * 64 dim * 5.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00012 (2.2e-16 eps * 64 dim * 8.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00012 (2.2e-16 eps * 64 dim * 8.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.9e-05 (2.2e-16 eps * 64 dim * 5.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00012 (2.2e-16 eps * 64 dim * 8.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00012 (2.2e-16 eps * 64 dim * 8.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.9e-05 (2.2e-16 eps * 64 dim * 5.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00012 (2.2e-16 eps * 64 dim * 8.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00012 (2.2e-16 eps * 64 dim * 8.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8e-05 (2.2e-16 eps * 64 dim * 5.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00012 (2.2e-16 eps * 64 dim * 8.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00012 (2.2e-16 eps * 64 dim * 8.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00017 (2.2e-16 eps * 64 dim * 1.2e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00024 (2.2e-16 eps * 64 dim * 1.7e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00024 (2.2e-16 eps * 64 dim * 1.7e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00016 (2.2e-16 eps * 64 dim * 1.1e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00024 (2.2e-16 eps * 64 dim * 1.7e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00025 (2.2e-16 eps * 64 dim * 1.7e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00016 (2.2e-16 eps * 64 dim * 1.1e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00024 (2.2e-16 eps * 64 dim * 1.7e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00025 (2.2e-16 eps * 64 dim * 1.7e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00016 (2.2e-16 eps * 64 dim * 1.1e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00024 (2.2e-16 eps * 64 dim * 1.7e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00024 (2.2e-16 eps * 64 dim * 1.7e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00016 (2.2e-16 eps * 64 dim * 1.1e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00025 (2.2e-16 eps * 64 dim * 1.7e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00025 (2.2e-16 eps * 64 dim * 1.7e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.5e-05 (2.2e-16 eps * 64 dim * 5.3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.6e-05 (2.2e-16 eps * 64 dim * 4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.3e-05 (2.2e-16 eps * 64 dim * 3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.7e-05 (2.2e-16 eps * 64 dim * 5.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.6e-05 (2.2e-16 eps * 64 dim * 3.9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.3e-05 (2.2e-16 eps * 64 dim * 3.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.4e-05 (2.2e-16 eps * 64 dim * 5.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.7e-05 (2.2e-16 eps * 64 dim * 4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.4e-05 (2.2e-16 eps * 64 dim * 3.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.6e-05 (2.2e-16 eps * 64 dim * 5.3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.6e-05 (2.2e-16 eps * 64 dim * 4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.3e-05 (2.2e-16 eps * 64 dim * 3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.3e-05 (2.2e-16 eps * 64 dim * 5.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.6e-05 (2.2e-16 eps * 64 dim * 3.9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.3e-05 (2.2e-16 eps * 64 dim * 3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00014 (2.2e-16 eps * 64 dim * 1e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00011 (2.2e-16 eps * 64 dim * 8e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.5e-05 (2.2e-16 eps * 64 dim * 6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00015 (2.2e-16 eps * 64 dim * 1.1e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00011 (2.2e-16 eps * 64 dim * 7.9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.8e-05 (2.2e-16 eps * 64 dim * 6.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00015 (2.2e-16 eps * 64 dim * 1.1e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00011 (2.2e-16 eps * 64 dim * 8e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.7e-05 (2.2e-16 eps * 64 dim * 6.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00015 (2.2e-16 eps * 64 dim * 1.1e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00011 (2.2e-16 eps * 64 dim * 8.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.8e-05 (2.2e-16 eps * 64 dim * 6.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00015 (2.2e-16 eps * 64 dim * 1.1e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00011 (2.2e-16 eps * 64 dim * 8e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8.7e-05 (2.2e-16 eps * 64 dim * 6.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Sweep complete.\n",
      "Scores â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\generalization\\eoec_iaf_summary.csv | E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\generalization\\task_fbcsp_summary.csv\n",
      "Updated page â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\generalization\\CNT_CognitiveAlphabet_v0_3_Generalization.png\n"
     ]
    }
   ],
   "source": [
    "# === CNT v0.3 â€” Subject Sweep (S004â€“S010) â†’ Calibration + Scores + Letters + Updated Report ===\n",
    "import os, re, json, numpy as np, pandas as pd, matplotlib.pyplot as plt, requests, mne\n",
    "from pathlib import Path\n",
    "from joblib import load\n",
    "from scipy.signal import welch\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from mne.decoding import CSP\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# --- Paths (edit if needed)\n",
    "RUN    = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")\n",
    "REP    = RUN.parent / \"cog_alphabet_report_hybrid_v1\"\n",
    "GEN    = RUN.parent / \"generalization_data\"; GEN.mkdir(parents=True, exist_ok=True)\n",
    "OUT    = REP / \"generalization\"; OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Models & schema (v0.2)\n",
    "scaler = load(RUN/\"scaler_hybrid.joblib\")\n",
    "pca    = load(RUN/\"pca_hybrid.joblib\")\n",
    "km     = load(RUN/\"kmeans_hybrid.joblib\")\n",
    "TRAIN_COLS = list(pd.read_csv(RUN/\"features.csv\", nrows=1).columns)\n",
    "\n",
    "# --- v0.2 processing params\n",
    "TARGET_SF=250.0; L_FREQ,H_FREQ=0.5,80.0; EPOCH_LEN,STEP=2.0,0.5\n",
    "BANDS = {\"delta\":(1,4),\"theta\":(4,8),\"alpha\":(8,13),\"beta_low\":(13,20),\"beta_high\":(20,35),\"gamma\":(35,80)}\n",
    "\n",
    "def fetch(url, dest):\n",
    "    dest = Path(dest)\n",
    "    if dest.exists() and dest.stat().st_size>0: return dest\n",
    "    with requests.get(url, stream=True, timeout=60) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(dest, \"wb\") as f:\n",
    "            for ch in r.iter_content(8192):\n",
    "                if ch: f.write(ch)\n",
    "    return dest\n",
    "\n",
    "def load_raw_edf(p, band=(L_FREQ,H_FREQ)):\n",
    "    raw = mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"]-TARGET_SF)>1e-6: raw.resample(TARGET_SF, npad=\"auto\", verbose=False)\n",
    "    ny = raw.info[\"sfreq\"]/2\n",
    "    raw.filter(band[0], min(band[1], ny-1.0), verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "    return raw\n",
    "\n",
    "def make_epochs(raw):\n",
    "    ov = max(0.0, EPOCH_LEN-STEP)\n",
    "    return mne.make_fixed_length_epochs(raw, duration=EPOCH_LEN, overlap=ov, preload=True, verbose=False)\n",
    "\n",
    "def spectral_features(epochs):\n",
    "    from scipy.signal import welch\n",
    "    X = epochs.get_data(); sf = epochs.info[\"sfreq\"]\n",
    "    n_ep,n_ch,n_t = X.shape; nper=min(int(sf*2), n_t); nov=nper//2\n",
    "    f,P = welch(X, fs=sf, nperseg=nper, noverlap=nov, axis=-1, average=\"median\")\n",
    "    def idx(lo,hi): return np.where((f>=lo)&(f<hi))[0]\n",
    "    aidx=idx(L_FREQ,H_FREQ); tot=np.maximum(P[:,:,aidx].sum(-1),1e-12)\n",
    "    feats={}; acc={\"alpha\":np.zeros((n_ep,n_ch)),\"theta\":np.zeros((n_ep,n_ch)),\"beta\":np.zeros((n_ep,n_ch))}\n",
    "    for name,(lo,hi) in BANDS.items():\n",
    "        b=idx(lo,hi); bp=P[:,:,b].sum(-1)/tot if b.size else np.zeros((n_ep,n_ch))\n",
    "        feats[f\"{name}_rel_med\"]=np.median(bp,axis=1); feats[f\"{name}_rel_iqr\"]=np.subtract(*np.percentile(bp,[75,25],axis=1)); feats[f\"{name}_rel_std\"]=np.std(bp,axis=1)\n",
    "        head=name.split(\"_\",1)[0]; \n",
    "        if head in acc: acc[head]+=bp\n",
    "    alpha=np.median(acc[\"alpha\"],axis=1); theta=np.median(acc[\"theta\"],axis=1); beta=np.median(acc[\"beta\"],axis=1)\n",
    "    feats[\"alpha_rel_med\"]=alpha; feats[\"theta_rel_med\"]=theta; feats[\"beta_rel_med\"]=beta\n",
    "    feats[\"theta_over_alpha\"]=theta/np.maximum(alpha,1e-6); feats[\"beta_over_alpha\"]=beta/np.maximum(alpha,1e-6)\n",
    "    feats[\"(alpha+theta)_over_beta\"]=(alpha+theta)/np.maximum(beta,1e-6)\n",
    "    p=P[:,:,aidx]; p_n=p/np.maximum(p.sum(-1,keepdims=True),1e-12)\n",
    "    H=-(p_n*np.log2(p_n+1e-12)).sum(-1); Hn=H/np.log2(p.shape[-1])\n",
    "    feats[\"spec_entropy_med\"]=np.median(Hn,axis=1); feats[\"spec_entropy_std\"]=np.std(Hn,axis=1)\n",
    "    ff=f[aidx].reshape(1,1,-1); cen=(p*ff).sum(-1)/np.maximum(p.sum(-1),1e-12)\n",
    "    feats[\"spec_centroid_med\"]=np.median(cen,axis=1); feats[\"spec_centroid_std\"]=np.std(cen,axis=1)\n",
    "    # Hjorth\n",
    "    d1=np.diff(X,axis=-1); var0=np.var(X,axis=-1)+1e-12; var1=np.var(d1,axis=-1)+1e-12\n",
    "    feats[\"hjorth_activity_med\"]=np.median(var0,axis=1); feats[\"hjorth_mobility_med\"]=np.median(np.sqrt(var1/var0),axis=1)\n",
    "    d2=np.diff(d1,axis=-1); var2=np.var(d2,axis=-1)+1e-12\n",
    "    feats[\"hjorth_complexity_med\"]=np.median(np.sqrt((var2/var1)/(var1/var0)),axis=1)\n",
    "    return pd.DataFrame(feats)\n",
    "\n",
    "def occipital_picks(raw):\n",
    "    names=[n.upper().strip() for n in raw.ch_names]\n",
    "    wanted=(\"O1\",\"O2\",\"OZ\",\"POZ\",\"PO3\",\"PO4\")\n",
    "    idx=[i for i,n in enumerate(names) if n in wanted]\n",
    "    return idx if idx else mne.pick_types(raw.info, eeg=True)\n",
    "\n",
    "def iaf_and_threshold(raw_eo, raw_ec):\n",
    "    sf = raw_ec.info[\"sfreq\"]\n",
    "    f,P = welch(raw_ec.get_data(), fs=sf, nperseg=min(int(sf*2), raw_ec.n_times), noverlap=int(min(int(sf*2), raw_ec.n_times)/2), axis=-1, average=\"median\")\n",
    "    band=(f>=7)&(f<=14)\n",
    "    Pocc = P[occipital_picks(raw_ec)][:,band].mean(0)\n",
    "    iaf = float(np.clip(f[band][np.argmax(Pocc)], 8.0, 12.0))\n",
    "    alpha_band=(max(6.0, iaf-2.0), min(14.0, iaf+2.0))\n",
    "    def alpha_index(raw):\n",
    "        f,P = welch(raw.get_data(), fs=raw.info[\"sfreq\"], nperseg=min(int(sf*2), raw.n_times), noverlap=int(min(int(sf*2), raw.n_times)/2), axis=-1, average=\"median\")\n",
    "        idx=(f>=alpha_band[0])&(f<alpha_band[1]); a=P[:,idx].sum(-1); tot=P.sum(-1)+1e-12\n",
    "        oi=occipital_picks(raw); return float((a[oi].sum()/tot[oi].sum()) if len(oi) else (a.sum()/tot.sum()))\n",
    "    thr = 0.5*(alpha_index(raw_ec) + alpha_index(raw_eo))\n",
    "    return iaf, alpha_band, float(thr)\n",
    "\n",
    "def eval_eoec(raw_eo, raw_ec, band, thr):\n",
    "    # per-epoch alpha index classification\n",
    "    def classify(raw, gt):\n",
    "        sf=raw.info[\"sfreq\"]; ts=np.arange(0, raw.n_times/sf - EPOCH_LEN + 1e-9, STEP)\n",
    "        preds=[]\n",
    "        for t0 in ts:\n",
    "            s=int(t0*sf); e=s+int(EPOCH_LEN*sf)\n",
    "            seg=raw.get_data(start=s, stop=e)\n",
    "            f,P = welch(seg, fs=sf, nperseg=min(int(sf*2), e-s), noverlap=int(min(int(sf*2), e-s)/2), axis=-1, average=\"median\")\n",
    "            idx=(f>=band[0])&(f<band[1]); a=P[:,idx].sum(-1); tot=P.sum(-1)+1e-12\n",
    "            oi=occipital_picks(raw); ai=(a[oi].sum()/tot[oi].sum()) if len(oi) else (a.sum()/tot.sum())\n",
    "            preds.append(\"EC\" if ai>=thr else \"EO\")\n",
    "        return pd.DataFrame({\"gt\":gt, \"pred\":preds})\n",
    "    df = pd.concat([classify(raw_ec,\"EC\"), classify(raw_eo,\"EO\")], ignore_index=True)\n",
    "    acc=(df[\"gt\"]==df[\"pred\"]).mean(); cm=pd.crosstab(df[\"gt\"], df[\"pred\"], normalize=\"index\").round(3)\n",
    "    return float(acc), cm\n",
    "\n",
    "def butter_bandpass_array(X, lo, hi, sf, order=4):\n",
    "    X2=X.reshape(X.shape[0]*X.shape[1], X.shape[2])\n",
    "    Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, method=\"iir\",\n",
    "                              iir_params=dict(order=4, ftype=\"butter\"), verbose=False)\n",
    "    return Xf.reshape(X.shape)\n",
    "\n",
    "def fbcsp_auc_r03(raw):\n",
    "    anns=raw.annotations\n",
    "    if anns is None or len(anns)==0: return None\n",
    "    # 2.0s non-overlap inside T0/T1/T2\n",
    "    sf=raw.info[\"sfreq\"]\n",
    "    want={\"C3\",\"Cz\",\"C4\",\"FC3\",\"FCz\",\"FC4\",\"CP3\",\"CPz\",\"CP4\"}\n",
    "    picks=[i for i,ch in enumerate(raw.ch_names) if ch.upper().strip() in want]\n",
    "    if len(picks)<3: picks=mne.pick_types(raw.info, eeg=True)\n",
    "    raw=raw.copy().pick(picks)\n",
    "    segs=[]; gid=0\n",
    "    for o,d,s in zip(anns.onset, anns.duration, anns.description):\n",
    "        su=str(s).upper()\n",
    "        if \"T0\" in su or \"T1\" in su or \"T2\" in su:\n",
    "            lab=0 if \"T0\" in su else 1\n",
    "            segs.append((float(o), float(o+d), lab, gid)); gid+=1\n",
    "    X=[]; y=[]; g=[]\n",
    "    for a,b,lab,segid in segs:\n",
    "        t=a\n",
    "        while t+2.0 <= b-1e-6:\n",
    "            s=int(t*sf); e=s+int(2.0*sf)\n",
    "            X.append(raw.get_data(start=s, stop=e)); y.append(lab); g.append(segid); t+=2.0\n",
    "    if not X: return None\n",
    "    X=np.stack(X,0); y=np.array(y,int); g=np.array(g,int)\n",
    "    bands=[(8,13),(13,20),(20,30)]\n",
    "    gkf=GroupKFold(n_splits=min(5,max(2,len(np.unique(g)))))\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    proba=np.zeros(len(y),float)\n",
    "    for tr,te in gkf.split(X,y,g):\n",
    "        feats_tr, feats_te=[],[]\n",
    "        for lo,hi in bands:\n",
    "            Xtr=butter_bandpass_array(X[tr], lo, hi, raw.info[\"sfreq\"])\n",
    "            Xte=butter_bandpass_array(X[te], lo, hi, raw.info[\"sfreq\"])\n",
    "            csp=CSP(n_components=6, reg=\"ledoit_wolf\", log=True, norm_trace=False)\n",
    "            csp.fit(Xtr, y[tr]); feats_tr.append(csp.transform(Xtr)); feats_te.append(csp.transform(Xte))\n",
    "        Xtr_fb=np.concatenate(feats_tr,1); Xte_fb=np.concatenate(feats_te,1)\n",
    "        clf=LDA(solver=\"lsqr\", shrinkage=\"auto\").fit(Xtr_fb, y[tr])\n",
    "        proba[te]=clf.predict_proba(Xte_fb)[:,1]\n",
    "    auc=float(roc_auc_score(y, proba))\n",
    "    return auc\n",
    "\n",
    "def integrate_letters_r03(raw, starts, ends):\n",
    "    # fbCSP task prob per epoch + letter assign with v0.2 models\n",
    "    try: fb = fbCSP_task_proba_for_R03(raw, starts, ends)\n",
    "    except Exception: fb = np.zeros(len(starts))\n",
    "    ep = make_epochs(raw.copy().crop(tmin=0, tmax=starts[-1]+EPOCH_LEN, include_tmax=False))  # not used; we re-spec feats per epoch below\n",
    "    # simpler: recompute spectral feats per epoch window explicitly\n",
    "    feats=[]\n",
    "    for t0,t1 in zip(starts, ends):\n",
    "        seg=raw.copy().crop(tmin=t0, tmax=t1, include_tmax=False)\n",
    "        e=make_epochs(seg); feats.append(spectral_features(e).iloc[0])\n",
    "    Fsp=pd.DataFrame(feats).reindex(columns=TRAIN_COLS, fill_value=0.0)\n",
    "    Z=pca.transform(scaler.transform(Fsp.values)); lab=km.predict(Z)\n",
    "    return lab, fb\n",
    "\n",
    "# --------- Sweep subjects\n",
    "subjects = [f\"S{n:03d}\" for n in range(2,11)]  # S002..S010\n",
    "calib = json.load(open(OUT/\"subject_calibration.json\")) if (OUT/\"subject_calibration.json\").exists() else {}\n",
    "eoec_rows=[]; task_rows=[]; mix_rows=[]\n",
    "\n",
    "for subj in subjects:\n",
    "    # fetch R01â€“R03\n",
    "    for r in [1,2,3]:\n",
    "        url=f\"https://physionet.org/files/eegmmidb/1.0.0/{subj}/{subj}R{r:02d}.edf\"\n",
    "        dest=GEN/f\"{subj}R{r:02d}.edf\"\n",
    "        try: fetch(url, dest)\n",
    "        except Exception as e: print(\"skip download\", url, e)\n",
    "    r01, r02, r03 = GEN/f\"{subj}R01.edf\", GEN/f\"{subj}R02.edf\", GEN/f\"{subj}R03.edf\"\n",
    "    if not (r01.exists() and r02.exists() and r03.exists()):\n",
    "        print(\"missing runs for\", subj); continue\n",
    "    # calibration: IAF + EO/EC threshold\n",
    "    try:\n",
    "        raw_eo=load_raw_edf(r01, band=(0.5,45.0))\n",
    "        raw_ec=load_raw_edf(r02, band=(0.5,45.0))\n",
    "        iaf, band, thr = iaf_and_threshold(raw_eo, raw_ec)\n",
    "        calib[subj]={\"IAF_hz\":iaf,\"alpha_band\":band,\"alpha_threshold\":thr,\n",
    "                     \"fbCSP_threshold\":0.50,\"task_letters\":calib.get(subj,{}).get(\"task_letters\",[])}\n",
    "        acc, cm = eval_eoec(raw_eo, raw_ec, band, thr)\n",
    "        eoec_rows.append({\"subject\":subj,\"acc\":acc}); cm.to_csv(OUT/f\"{subj}_eoec_cm_iaf.csv\")\n",
    "    except Exception as e:\n",
    "        print(\"EO/EC fail\", subj, e)\n",
    "    # task: FBCSP v2 on R03\n",
    "    try:\n",
    "        raw3=load_raw_edf(r03, band=(0.5,45.0))\n",
    "        auc=fbcsp_auc_r03(raw3)\n",
    "        if auc is not None: task_rows.append({\"subject\":subj,\"AUC_task\":auc})\n",
    "        # integrate letters + fbCSP per epoch for R03\n",
    "        # build epoch times from our standard windowing\n",
    "        sf=raw3.info[\"sfreq\"]; ts=np.arange(0, raw3.n_times/sf - EPOCH_LEN + 1e-9, STEP)\n",
    "        starts=ts; ends=ts+EPOCH_LEN\n",
    "        lab, fb = integrate_letters_r03(raw3, starts, ends)\n",
    "        pd.DataFrame({\"subject\":subj,\"file\":r03.name,\"t_start_s\":starts,\"t_end_s\":ends,\"state\":lab,\"fb_task\":fb}).to_csv(OUT/f\"decode_{subj}_{r03.stem}.csv\", index=False)\n",
    "        # letter mix per file\n",
    "        lmix = pd.Series(lab).value_counts(normalize=True).sort_index()\n",
    "        for s,fv in lmix.items(): mix_rows.append({\"subject\":subj,\"file\":r03.name,\"state\":int(s),\"fraction\":float(fv)})\n",
    "    except Exception as e:\n",
    "        print(\"R03 task/letters fail\", subj, e)\n",
    "\n",
    "# write/merge scoreboards & calibration\n",
    "pd.DataFrame(eoec_rows).to_csv(OUT/\"eoec_iaf_summary.csv\", index=False)\n",
    "pd.DataFrame(task_rows).to_csv(OUT/\"task_fbcsp_summary.csv\", index=False)\n",
    "if mix_rows: pd.DataFrame(mix_rows).to_csv(OUT/\"generalization_letter_mix.csv\", index=False)\n",
    "with open(OUT/\"subject_calibration.json\",\"w\") as f: json.dump(calib, f, indent=2)\n",
    "\n",
    "# --------- Rebuild v0.3 generalization page with all subjects\n",
    "def load_cm(subject):\n",
    "    p = OUT/f\"{subject}_eoec_cm_iaf.csv\"\n",
    "    if p.exists(): \n",
    "        df=pd.read_csv(p, index_col=0)\n",
    "        cols=[\"EC\",\"EO\"]; idx=[\"EC\",\"EO\"]\n",
    "        for c in cols:\n",
    "            if c not in df.columns: df[c]=0.0\n",
    "        for r in idx:\n",
    "            if r not in df.index: df.loc[r]=[0.0,0.0]\n",
    "        return df.loc[idx, cols].round(3)\n",
    "    return None\n",
    "\n",
    "eoec = pd.read_csv(OUT/\"eoec_iaf_summary.csv\") if (OUT/\"eoec_iaf_summary.csv\").exists() else None\n",
    "task = pd.read_csv(OUT/\"task_fbcsp_summary.csv\") if (OUT/\"task_fbcsp_summary.csv\").exists() else None\n",
    "\n",
    "fig=plt.figure(figsize=(12,8), dpi=130); gs=fig.add_gridspec(2,3, hspace=0.35, wspace=0.30)\n",
    "\n",
    "# EO/EC bar\n",
    "axA=fig.add_subplot(gs[0,0])\n",
    "if eoec is not None and len(eoec):\n",
    "    axA.bar(np.arange(len(eoec)), eoec[\"acc\"]); axA.set_xticks(np.arange(len(eoec))); axA.set_xticklabels(eoec[\"subject\"], rotation=30)\n",
    "    axA.set_ylim(0,1); axA.set_title(\"EO vs EC (IAF-based)\"); axA.set_ylabel(\"accuracy\")\n",
    "else: axA.axis(\"off\"); axA.text(0.5,0.5,\"no EO/EC\", ha=\"center\")\n",
    "\n",
    "# Task AUC bar\n",
    "axB=fig.add_subplot(gs[0,1])\n",
    "if task is not None and len(task):\n",
    "    axB.bar(np.arange(len(task)), task[\"AUC_task\"]); axB.set_xticks(np.arange(len(task))); axB.set_xticklabels(task[\"subject\"], rotation=30)\n",
    "    axB.set_ylim(0,1); axB.set_title(\"R03 Task AUC (FBCSP v2)\"); axB.set_ylabel(\"AUC\")\n",
    "else: axB.axis(\"off\"); axB.text(0.5,0.5,\"no task\", ha=\"center\")\n",
    "\n",
    "# Two EO/EC confusions (first two subjects if available)\n",
    "subs = list(eoec[\"subject\"]) if eoec is not None else []\n",
    "axC=fig.add_subplot(gs[0,2]); axD=fig.add_subplot(gs[1,2])\n",
    "def draw_cm(ax, subj, title):\n",
    "    ax.axis(\"off\"); df=load_cm(subj)\n",
    "    if df is None: ax.text(0.5,0.5,\"n/a\", ha=\"center\"); return\n",
    "    txt=\"row=gt, col=pred\\n        EC      EO\\n\"\n",
    "    for r in [\"EC\",\"EO\"]:\n",
    "        row=df.loc[r, [\"EC\",\"EO\"]].tolist(); txt+=f\"{r:<4}  {row[0]:>6.3f}  {row[1]:>6.3f}\\n\"\n",
    "    ax.set_title(title, fontsize=11); ax.text(0.02,0.02,txt, family=\"monospace\", fontsize=9, va=\"bottom\")\n",
    "if len(subs)>0: draw_cm(axC, subs[0], f\"{subs[0]} EO/EC confusion\")\n",
    "if len(subs)>1: draw_cm(axD, subs[1], f\"{subs[1]} EO/EC confusion\")\n",
    "\n",
    "# Task-prob bars per state (print the subject with highest AUC)\n",
    "axE=fig.add_subplot(gs[1,0]); axF=fig.add_subplot(gs[1,1])\n",
    "def load_state_taskprob(subj):\n",
    "    p = OUT/f\"{subj}_state_taskprob_summary.csv\"\n",
    "    return pd.read_csv(p) if p.exists() else None\n",
    "if task is not None and len(task):\n",
    "    best_sub = task.sort_values(\"AUC_task\", ascending=False)[\"subject\"].iloc[0]\n",
    "    st = load_state_taskprob(best_sub)\n",
    "    if st is not None and len(st):\n",
    "        st = st.sort_values(\"mean\", ascending=False).head(3)\n",
    "        axE.barh([f\"S{int(s)} (n={int(n)})\" for s,n in zip(st[\"state\"], st[\"n\"])], st[\"mean\"])\n",
    "        axE.set_xlim(0,1); axE.invert_yaxis(); axE.set_xlabel(\"mean fbCSP_task_proba\"); axE.set_title(f\"{best_sub} Task-Proba by State (R03)\")\n",
    "else:\n",
    "    axE.axis(\"off\"); axE.text(0.5,0.5,\"no task summary\", ha=\"center\")\n",
    "axF.axis(\"off\")\n",
    "\n",
    "fig.suptitle(\"CNT Cognitive Alphabet â€” v0.3 Generalization (scaled)\", fontsize=16, weight=\"bold\")\n",
    "PNG = OUT/\"CNT_CognitiveAlphabet_v0_3_Generalization.png\"; PDF = OUT/\"CNT_CognitiveAlphabet_v0_3_Generalization.pdf\"\n",
    "fig.savefig(PNG, bbox_inches=\"tight\"); fig.savefig(PDF, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "print(\"Sweep complete.\")\n",
    "print(\"Scores â†’\", OUT/\"eoec_iaf_summary.csv\", \"|\", OUT/\"task_fbcsp_summary.csv\")\n",
    "print(\"Updated page â†’\", PNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "140294b5-216f-4939-b411-617747840756",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoreboard â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\generalization\\v0_3_scoreboard.csv\n",
      "No usage CSVs created (check RAW_DIR & metadata).\n",
      "\n",
      "All tasks completed. Quick notes:\n",
      " â€¢ Scoreboard CSV/JSON â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\generalization\\v0_3_scoreboard.csv | E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\generalization\\v0_3_scoreboard.json\n",
      " â€¢ Live stream: call live_decode_stream(<edf>, <subject>, seconds=...), outputs CSV + JSONL in E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\generalization\n",
      " â€¢ Microstates: per-run usage CSVs + per-letter summary in E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\microstates\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# CNT Cognitive Alphabet v0.3 â€” Scoreboard + Live Stream (sticky+minrun+JSONL) + Microstate Summary\n",
    "# ============================================\n",
    "import os, re, json, time, shutil, numpy as np, pandas as pd, matplotlib.pyplot as plt, mne\n",
    "from pathlib import Path\n",
    "from scipy.signal import welch\n",
    "from joblib import load\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from mne.decoding import CSP\n",
    "\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# -----------------------------\n",
    "# Paths (edit if needed)\n",
    "# -----------------------------\n",
    "RUN    = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")         # promoted v0.2\n",
    "REP    = RUN.parent / \"cog_alphabet_report_hybrid_v1\"\n",
    "GENREP = REP / \"generalization\"\n",
    "DWELL  = REP / \"dwell\"\n",
    "MSREP  = REP / \"microstates\"\n",
    "BUNDLE = RUN.parent / \"CNT_CognitiveAlphabet_v0_3\"\n",
    "RAW_DIR= RUN / \"brainwaves_rebuilt\"                                # S001 EDFs\n",
    "GEN    = RUN.parent / \"generalization_data\"                        # S002.. files\n",
    "\n",
    "for p in [GENREP, DWELL, MSREP, BUNDLE]: p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Load models & schema (v0.2)\n",
    "# -----------------------------\n",
    "scaler = load(RUN/\"scaler_hybrid.joblib\")\n",
    "pca    = load(RUN/\"pca_hybrid.joblib\")\n",
    "km     = load(RUN/\"kmeans_hybrid.joblib\")\n",
    "TRAIN_COLS = list(pd.read_csv(RUN/\"features.csv\", nrows=1).columns)\n",
    "\n",
    "# -----------------------------\n",
    "# Processing params (v0.2)\n",
    "# -----------------------------\n",
    "TARGET_SF=250.0; L_FREQ,H_FREQ=0.5,80.0; EPOCH_LEN,STEP=2.0,0.5\n",
    "BANDS = {\"delta\":(1,4),\"theta\":(4,8),\"alpha\":(8,13),\"beta_low\":(13,20),\"beta_high\":(20,35),\"gamma\":(35,80)}\n",
    "\n",
    "def load_raw_edf(p, band=(L_FREQ,H_FREQ), montage=False):\n",
    "    raw = mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"]-TARGET_SF)>1e-6: raw.resample(TARGET_SF, npad=\"auto\", verbose=False)\n",
    "    ny = raw.info[\"sfreq\"]/2\n",
    "    raw.filter(band[0], min(band[1], ny-1.0), verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "        if montage:\n",
    "            try:\n",
    "                raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"),\n",
    "                                match_case=False, match_alias=True, on_missing=\"ignore\")\n",
    "            except Exception:\n",
    "                pass\n",
    "    return raw\n",
    "\n",
    "def make_epochs(raw):\n",
    "    ov = max(0.0, EPOCH_LEN-STEP)\n",
    "    return mne.make_fixed_length_epochs(raw, duration=EPOCH_LEN, overlap=ov, preload=True, verbose=False)\n",
    "\n",
    "def spectral_features(epochs):\n",
    "    X = epochs.get_data(); sf = epochs.info[\"sfreq\"]\n",
    "    n_ep,n_ch,n_t = X.shape; nper=min(int(sf*2), n_t); nov=nper//2\n",
    "    f,P = welch(X, fs=sf, nperseg=nper, noverlap=nov, axis=-1, average=\"median\")\n",
    "    def idx(lo,hi): return np.where((f>=lo)&(f<hi))[0]\n",
    "    aidx=idx(L_FREQ,H_FREQ); tot=np.maximum(P[:,:,aidx].sum(-1),1e-12)\n",
    "    feats={}; acc={\"alpha\":np.zeros((n_ep,n_ch)),\"theta\":np.zeros((n_ep,n_ch)),\"beta\":np.zeros((n_ep,n_ch))}\n",
    "    for name,(lo,hi) in BANDS.items():\n",
    "        b=idx(lo,hi); bp=P[:,:,b].sum(-1)/tot if b.size else np.zeros((n_ep,n_ch))\n",
    "        feats[f\"{name}_rel_med\"]=np.median(bp,axis=1); feats[f\"{name}_rel_iqr\"]=np.subtract(*np.percentile(bp,[75,25],axis=1)); feats[f\"{name}_rel_std\"]=np.std(bp,axis=1)\n",
    "        head=name.split(\"_\",1)[0]; \n",
    "        if head in acc: acc[head]+=bp\n",
    "    alpha=np.median(acc[\"alpha\"],axis=1); theta=np.median(acc[\"theta\"],axis=1); beta=np.median(acc[\"beta\"],axis=1)\n",
    "    feats[\"alpha_rel_med\"]=alpha; feats[\"theta_rel_med\"]=theta; feats[\"beta_rel_med\"]=beta\n",
    "    feats[\"theta_over_alpha\"]=theta/np.maximum(alpha,1e-6); feats[\"beta_over_alpha\"]=beta/np.maximum(alpha,1e-6)\n",
    "    feats[\"(alpha+theta)_over_beta\"]=(alpha+theta)/np.maximum(beta,1e-6)\n",
    "    p=P[:,:,aidx]; p_n=p/np.maximum(p.sum(-1,keepdims=True),1e-12)\n",
    "    H=-(p_n*np.log2(p_n+1e-12)).sum(-1); Hn=H/np.log2(p.shape[-1])\n",
    "    feats[\"spec_entropy_med\"]=np.median(Hn,axis=1); feats[\"spec_entropy_std\"]=np.std(Hn,axis=1)\n",
    "    ff=f[aidx].reshape(1,1,-1); cen=(p*ff).sum(-1)/np.maximum(p.sum(-1),1e-12)\n",
    "    feats[\"spec_centroid_med\"]=np.median(cen,axis=1); feats[\"spec_centroid_std\"]=np.std(cen,axis=1)\n",
    "    # Hjorth\n",
    "    d1=np.diff(X,axis=-1); var0=np.var(X,axis=-1)+1e-12; var1=np.var(d1,axis=-1)+1e-12\n",
    "    feats[\"hjorth_activity_med\"]=np.median(var0,axis=1); feats[\"hjorth_mobility_med\"]=np.median(np.sqrt(var1/var0),axis=1)\n",
    "    d2=np.diff(d1,axis=-1); var2=np.var(d2,axis=-1)+1e-12\n",
    "    feats[\"hjorth_complexity_med\"]=np.median(np.sqrt((var2/var1)/(var1/var0)),axis=1)\n",
    "    return pd.DataFrame(feats)\n",
    "\n",
    "def occipital_picks(raw):\n",
    "    names=[n.upper().strip() for n in raw.ch_names]\n",
    "    wanted=(\"O1\",\"O2\",\"OZ\",\"POZ\",\"PO3\",\"PO4\")\n",
    "    idx=[i for i,n in enumerate(names) if n in wanted]\n",
    "    return idx if idx else mne.pick_types(raw.info, eeg=True)\n",
    "\n",
    "# =========================================================================\n",
    "# 1) Scoreboard (EO/EC + Task AUC) & patch bundle README\n",
    "# =========================================================================\n",
    "def compile_scoreboard():\n",
    "    eoec = pd.read_csv(GENREP/\"eoec_iaf_summary.csv\") if (GENREP/\"eoec_iaf_summary.csv\").exists() else None\n",
    "    task = pd.read_csv(GENREP/\"task_fbcsp_summary.csv\") if (GENREP/\"task_fbcsp_summary.csv\").exists() else None\n",
    "\n",
    "    scoreboard = {}\n",
    "    if eoec is not None and len(eoec):\n",
    "        scoreboard[\"eoec_mean\"]   = float(eoec[\"acc\"].mean())\n",
    "        scoreboard[\"eoec_median\"] = float(eoec[\"acc\"].median())\n",
    "        scoreboard[\"eoec_iqr\"]    = float(eoec[\"acc\"].quantile(0.75) - eoec[\"acc\"].quantile(0.25))\n",
    "        scoreboard[\"eoec_n\"]      = int(len(eoec))\n",
    "    if task is not None and len(task):\n",
    "        scoreboard[\"task_auc_mean\"]   = float(task[\"AUC_task\"].mean())\n",
    "        scoreboard[\"task_auc_median\"] = float(task[\"AUC_task\"].median())\n",
    "        scoreboard[\"task_auc_iqr\"]    = float(task[\"AUC_task\"].quantile(0.75) - task[\"AUC_task\"].quantile(0.25))\n",
    "        scoreboard[\"task_auc_n\"]      = int(len(task))\n",
    "\n",
    "    pd.DataFrame([scoreboard]).to_csv(GENREP/\"v0_3_scoreboard.csv\", index=False)\n",
    "    with open(GENREP/\"v0_3_scoreboard.json\",\"w\") as f: json.dump(scoreboard, f, indent=2)\n",
    "\n",
    "    # Patch bundle README (append or create)\n",
    "    BUNDLE.mkdir(exist_ok=True, parents=True)\n",
    "    readme = BUNDLE/\"README_v0_3.md\"\n",
    "    block = \"\\n## Scoreboard (v0.3)\\n\"\n",
    "    if \"eoec_mean\" in scoreboard:\n",
    "        block += f\"- EO/EC acc â€” mean: **{scoreboard['eoec_mean']:.3f}**, median: **{scoreboard['eoec_median']:.3f}**, IQR: **{scoreboard['eoec_iqr']:.3f}** (n={scoreboard['eoec_n']})\\n\"\n",
    "    if \"task_auc_mean\" in scoreboard:\n",
    "        block += f\"- R03 AUC â€” mean: **{scoreboard['task_auc_mean']:.3f}**, median: **{scoreboard['task_auc_median']:.3f}**, IQR: **{scoreboard['task_auc_iqr']:.3f}** (n={scoreboard['task_auc_n']})\\n\"\n",
    "    try:\n",
    "        with open(readme, \"a\", encoding=\"utf-8\") as f: f.write(block)\n",
    "    except FileNotFoundError:\n",
    "        with open(readme, \"w\", encoding=\"utf-8\") as f: f.write(\"# CNT v0.3\\n\" + block)\n",
    "    print(\"Scoreboard â†’\", GENREP/\"v0_3_scoreboard.csv\")\n",
    "\n",
    "# =========================================================================\n",
    "# 2) Live stream (sticky+minrun+JSONL)\n",
    "# =========================================================================\n",
    "CALIB = GENREP/\"subject_calibration.json\"\n",
    "calib_all = json.load(open(CALIB)) if CALIB.exists() else {}\n",
    "\n",
    "def sticky_minrun_online(prev_state, raw_state, count, min_epochs=3):\n",
    "    \"\"\"Simple online min-run smoother: only switch after seeing 'min_epochs' consecutive new states.\"\"\"\n",
    "    if prev_state is None:\n",
    "        return raw_state, 1\n",
    "    if raw_state == prev_state:\n",
    "        return prev_state, count+1\n",
    "    # new state observed\n",
    "    if count+1 >= min_epochs:\n",
    "        return raw_state, 1  # commit to new state\n",
    "    else:\n",
    "        return prev_state, count  # keep old until min run reached\n",
    "\n",
    "def live_decode_stream(infile, subject_id, seconds=60, out_csv=None, jsonl_every=5, min_run_epochs=3):\n",
    "    \"\"\"\n",
    "    Simulated real-time decoder with smoothing and JSONL sidecar.\n",
    "    - per 2-s chunk: state, eoec_pred (IAF), fb_task proba, task_pred\n",
    "    - smoothing: online min-run on state\n",
    "    - JSONL sidecar (every 'jsonl_every' chunks): dwell & last transition row + alpha_index snapshot\n",
    "    \"\"\"\n",
    "    infile = Path(infile)\n",
    "    if out_csv is None:\n",
    "        out_csv = GENREP / f\"live_{subject_id}_{infile.stem}.csv\"\n",
    "    out_jsonl = GENREP / f\"live_{subject_id}_{infile.stem}.jsonl\"\n",
    "\n",
    "    cal = calib_all.get(subject_id, {})\n",
    "    raw = load_raw_edf(infile)\n",
    "    sf  = raw.info[\"sfreq\"]\n",
    "    total_samples = min(raw.n_times, int(seconds*sf))\n",
    "    rows=[]; smooth_rows=[]\n",
    "    # prepare alpha band/threshold for EO/EC\n",
    "    a_band = cal.get(\"alpha_band\", [8.0,13.0])\n",
    "    a_thr  = cal.get(\"alpha_threshold\", None)\n",
    "    t_letters = set(map(int, cal.get(\"task_letters\", [])))\n",
    "    fb_thr = float(cal.get(\"fbCSP_threshold\", 0.50))\n",
    "\n",
    "    # smoothing state\n",
    "    smooth_state=None; run_len=0\n",
    "    # simple telemetry\n",
    "    dwell_counts = {s:0 for s in range(int(km.n_clusters))}\n",
    "    last_state=None\n",
    "    trans_row = np.zeros((km.n_clusters,), float)\n",
    "\n",
    "    t=0.0; chunk=0\n",
    "    while int(t+EPOCH_LEN)*sf <= total_samples:\n",
    "        s = int(t*sf); e = s+int(EPOCH_LEN*sf)\n",
    "        seg_raw = raw.copy().crop(tmin=t, tmax=t+EPOCH_LEN, include_tmax=False)\n",
    "        # spectral -> raw letter\n",
    "        ep = make_epochs(seg_raw)\n",
    "        Fsp = spectral_features(ep).reindex(columns=TRAIN_COLS, fill_value=0.0)\n",
    "        Z   = pca.transform(scaler.transform(Fsp.values))\n",
    "        raw_state = int(km.predict(Z)[0])\n",
    "\n",
    "        # EO/EC via alpha index if available\n",
    "        eoec=\"UNK\"; ai_val=None\n",
    "        if a_thr is not None:\n",
    "            f,P = welch(seg_raw.get_data(), fs=sf, nperseg=min(int(sf*2), seg_raw.n_times), noverlap=int(min(int(sf*2), seg_raw.n_times)/2), axis=-1, average=\"median\")\n",
    "            idx=(f>=a_band[0])&(f<a_band[1]); a=P[:,idx].sum(-1); tot=P.sum(-1)+1e-12\n",
    "            oi=occipital_picks(seg_raw); ai=(a[oi].sum()/tot[oi].sum()) if len(oi) else (a.sum()/tot.sum())\n",
    "            eoec = \"EC\" if ai>=a_thr else \"EO\"; ai_val=float(ai)\n",
    "\n",
    "        # fbCSP proba on chunk (best-effort, single-shot)\n",
    "        fb = 0.0\n",
    "        try:\n",
    "            starts=np.array([0.0]); ends=np.array([EPOCH_LEN])\n",
    "            fb_arr = fbCSP_task_proba_for_R03(seg_raw, starts, ends)\n",
    "            fb = float(fb_arr[0]) if len(fb_arr) else 0.0\n",
    "        except Exception:\n",
    "            fb = 0.0\n",
    "        task_pred = \"task\" if (raw_state in t_letters or fb>=fb_thr) else \"rest\"\n",
    "\n",
    "        # smoothing\n",
    "        smooth_state, run_len = sticky_minrun_online(smooth_state, raw_state, run_len, min_epochs=min_run_epochs)\n",
    "        if last_state is None: last_state=smooth_state\n",
    "        elif smooth_state != last_state:\n",
    "            trans_row[smooth_state] += 1.0\n",
    "            last_state = smooth_state\n",
    "        dwell_counts[smooth_state] += 1\n",
    "\n",
    "        rows.append([round(t,3), round(t+EPOCH_LEN,3), raw_state, smooth_state, eoec, fb, task_pred])\n",
    "        # JSONL telemetry every jsonl_every chunks\n",
    "        if (chunk+1) % jsonl_every == 0:\n",
    "            payload = {\n",
    "                \"t_end_s\": round(t+EPOCH_LEN,3),\n",
    "                \"smooth_state\": int(smooth_state),\n",
    "                \"dwell_counts\": dwell_counts,\n",
    "                \"trans_counts_from_current\": trans_row.tolist(),\n",
    "                \"alpha_index\": ai_val,\n",
    "                \"fb_task\": fb\n",
    "            }\n",
    "            with open(out_jsonl, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps(payload) + \"\\n\")\n",
    "        t += EPOCH_LEN; chunk += 1\n",
    "\n",
    "    out_df = pd.DataFrame(rows, columns=[\"t_start_s\",\"t_end_s\",\"state_raw\",\"state_smooth\",\"eoec_pred\",\"fb_task\",\"task_pred\"])\n",
    "    if out_csv.exists(): out_df.to_csv(out_csv, mode=\"a\", header=False, index=False)\n",
    "    else: out_df.to_csv(out_csv, index=False)\n",
    "    print(\"Live CSV â†’\", out_csv)\n",
    "    print(\"Live JSONL â†’\", out_jsonl)\n",
    "    return out_csv, out_jsonl\n",
    "\n",
    "# =========================================================================\n",
    "# 3) Microstate per-run usage CSVs + S001 per-letter summary\n",
    "# =========================================================================\n",
    "def gfp(x): return x.std(axis=0)\n",
    "def find_gfp_peaks(x, sf, min_gap_ms=10, max_peaks=6000):\n",
    "    from scipy.signal import find_peaks\n",
    "    distance=int((min_gap_ms/1000.0)*sf)\n",
    "    g=gfp(x); peaks,_=find_peaks(g, distance=max(distance,1))\n",
    "    if len(peaks)>max_peaks:\n",
    "        idx=np.linspace(0,len(peaks)-1,max_peaks).astype(int); peaks=peaks[idx]\n",
    "    return peaks\n",
    "\n",
    "def kmeans_maps_polarity_agnostic(X, K=4, iters=50, rs=42):\n",
    "    rng=np.random.default_rng(rs)\n",
    "    X = X - X.mean(axis=1, keepdims=True)\n",
    "    X = X / (np.linalg.norm(X, axis=1, keepdims=True)+1e-12)\n",
    "    C = X[rng.choice(len(X), K, replace=False)].copy()\n",
    "    for _ in range(iters):\n",
    "        corr = X @ C.T\n",
    "        lab  = np.argmax(np.abs(corr), axis=1)\n",
    "        sign = np.sign(corr[np.arange(len(X)), lab])\n",
    "        Cn=[]\n",
    "        for k in range(K):\n",
    "            m = lab==k\n",
    "            if not np.any(m): Cn.append(C[k]); continue\n",
    "            v = (X[m]*sign[m][:,None]).mean(axis=0)\n",
    "            v = v/(np.linalg.norm(v)+1e-12)\n",
    "            Cn.append(v)\n",
    "        Cn=np.stack(Cn,0)\n",
    "        if np.allclose(C,Cn,atol=1e-5): break\n",
    "        C=Cn\n",
    "    return C, lab\n",
    "\n",
    "def backfit_maps(raw, maps):\n",
    "    X=raw.get_data()\n",
    "    M = maps/(np.linalg.norm(maps,axis=1,keepdims=True)+1e-12)\n",
    "    Xn= X/(np.linalg.norm(X,axis=0,keepdims=True)+1e-12)\n",
    "    corr = M @ Xn\n",
    "    lab = np.argmax(np.abs(corr), axis=0)\n",
    "    return lab\n",
    "\n",
    "def per_epoch_microstate_frac(starts, ends, ms_labels, sf, Kmic=4):\n",
    "    rows=[]\n",
    "    for t0,t1 in zip(starts, ends):\n",
    "        s=int(round(t0*sf)); e=int(round(t1*sf))\n",
    "        arr = ms_labels[s:e]\n",
    "        if e<=s or len(arr)==0: frac=np.zeros(Kmic)\n",
    "        else: frac=np.bincount(arr, minlength=Kmic)/len(arr)\n",
    "        rows.append(frac)\n",
    "    return np.stack(rows,0)\n",
    "\n",
    "def regenerate_microstate_usage_and_summary():\n",
    "    Fmeta = pd.read_csv(RUN/\"metadata.csv\").sort_values([\"file\",\"t_start_s\",\"t_end_s\"]).reset_index(drop=True)\n",
    "    L     = pd.read_csv(RUN/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "\n",
    "    usage_paths=[]\n",
    "    for fname in Fmeta[\"file\"].unique():\n",
    "        if not str(fname).lower().endswith(\".edf\"): continue\n",
    "        raw_path = RAW_DIR / fname\n",
    "        if not raw_path.exists(): continue\n",
    "        mask = (Fmeta[\"file\"]==fname).to_numpy()\n",
    "        starts = Fmeta.loc[mask, \"t_start_s\"].to_numpy()\n",
    "        ends   = Fmeta.loc[mask, \"t_end_s\"].to_numpy()\n",
    "        letters= L[mask]\n",
    "        raw = load_raw_edf(raw_path, band=(2.0,20.0), montage=True)\n",
    "        sf  = raw.info[\"sfreq\"]\n",
    "        peaks = find_gfp_peaks(raw.get_data(), sf, min_gap_ms=10, max_peaks=6000)\n",
    "        maps,_ = kmeans_maps_polarity_agnostic(raw.get_data()[:,peaks].T, K=4, iters=50, rs=42)\n",
    "        ms_labels = backfit_maps(raw, maps)\n",
    "        frac = per_epoch_microstate_frac(starts, ends, ms_labels, sf, Kmic=4)\n",
    "        # per-letter usage for this file\n",
    "        df = pd.DataFrame(frac, columns=[f\"Î¼{k}\" for k in range(4)])\n",
    "        df[\"letter\"] = letters\n",
    "        occ = df.groupby(\"letter\").mean().sort_index()\n",
    "        outp = MSREP / f\"S001_{Path(fname).stem}_microstate_usage.csv\"\n",
    "        occ.to_csv(outp); usage_paths.append(outp)\n",
    "        print(\"Usage CSV â†’\", outp)\n",
    "\n",
    "    if not usage_paths:\n",
    "        print(\"No usage CSVs created (check RAW_DIR & metadata).\"); return None\n",
    "\n",
    "    # Aggregate summary across files\n",
    "    rows=[]\n",
    "    for p in usage_paths:\n",
    "        df=pd.read_csv(p, index_col=0)\n",
    "        df[\"source\"]=p.stem; df[\"letter\"]=df.index\n",
    "        rows.append(df.reset_index(drop=True))\n",
    "    all_occ=pd.concat(rows, ignore_index=True)\n",
    "    agg=(all_occ.groupby(\"letter\")[[c for c in all_occ.columns if c.startswith(\"Î¼\")]].mean().sort_index())\n",
    "    agg_csv = MSREP/\"microstate_summary_per_letter.csv\"; agg.to_csv(agg_csv)\n",
    "    # simple bar per letter\n",
    "    for letter,row in agg.iterrows():\n",
    "        plt.figure()\n",
    "        plt.bar([f\"Î¼{k}\" for k in range(len(row))], row.values)\n",
    "        plt.ylim(0,1); plt.ylabel(\"fraction\")\n",
    "        plt.title(f\"Microstate mix â€” letter S{int(letter)} (mean across S001 runs)\")\n",
    "        plt.savefig(MSREP/f\"microstate_summary_S{int(letter)}.png\", dpi=150, bbox_inches=\"tight\"); plt.close()\n",
    "    print(\"Summary CSV â†’\", agg_csv)\n",
    "    return agg_csv\n",
    "\n",
    "# ===========================\n",
    "# Run all three tasks\n",
    "# ===========================\n",
    "# 1) Scoreboard & patch README\n",
    "compile_scoreboard()\n",
    "\n",
    "# 2) Live stream example (uncomment to run on demand):\n",
    "# live_decode_stream(GEN/\"S002R03.edf\", \"S002\", seconds=120, jsonl_every=5, min_run_epochs=3)\n",
    "\n",
    "# 3) Regenerate microstate usage & summary (for S001 runs)\n",
    "regenerate_microstate_usage_and_summary()\n",
    "\n",
    "print(\"\\nAll tasks completed. Quick notes:\")\n",
    "print(\" â€¢ Scoreboard CSV/JSON â†’\", GENREP/\"v0_3_scoreboard.csv\", \"|\", GENREP/\"v0_3_scoreboard.json\")\n",
    "print(\" â€¢ Live stream: call live_decode_stream(<edf>, <subject>, seconds=...), outputs CSV + JSONL in\", GENREP)\n",
    "print(\" â€¢ Microstates: per-run usage CSVs + per-letter summary in\", MSREP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "da73e924-0804-4303-8256-21af25c17446",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Could not find S001R0{1..3}.edf. Set RAW=Path(<folder_with_S001R01/02/03.edf>) and rerun.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m         RAW = d; \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RAW \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCould not find S001R0\u001b[39m\u001b[33m{\u001b[39m\u001b[33m1..3}.edf. Set RAW=Path(<folder_with_S001R01/02/03.edf>) and rerun.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# 1) Helpers (same as your earlier cell, simplified)\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_raw\u001b[39m(p):\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Could not find S001R0{1..3}.edf. Set RAW=Path(<folder_with_S001R01/02/03.edf>) and rerun."
     ]
    }
   ],
   "source": [
    "# === Microstate usage â€” auto-locate S001 EDFs then regenerate ===\n",
    "import os, numpy as np, pandas as pd, mne\n",
    "from pathlib import Path\n",
    "from scipy.signal import welch\n",
    "RUN    = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")\n",
    "MSREP  = RUN.parent / \"cog_alphabet_report_hybrid_v1\" / \"microstates\"; MSREP.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 0) Find a folder that actually has S001R01/02/03.edf\n",
    "candidates = [\n",
    "    RUN / \"brainwaves_rebuilt\",\n",
    "    RUN.parent / \"brainwaves_rebuilt\",\n",
    "    RUN.parent,  # fallback next to run\n",
    "]\n",
    "def has_s001(d):\n",
    "    return all((d/f\"S001R{r:02d}.edf\").exists() for r in [1,2,3])\n",
    "\n",
    "RAW = None\n",
    "for d in candidates:\n",
    "    if d.exists() and has_s001(d):\n",
    "        RAW = d; break\n",
    "if RAW is None:\n",
    "    raise FileNotFoundError(\"Could not find S001R0{1..3}.edf. Set RAW=Path(<folder_with_S001R01/02/03.edf>) and rerun.\")\n",
    "\n",
    "# 1) Helpers (same as your earlier cell, simplified)\n",
    "def load_raw(p):\n",
    "    raw = mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"]-250)>1e-6: raw.resample(250.0, npad=\"auto\", verbose=False)\n",
    "    raw.filter(2.0, 20.0, verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "        try:\n",
    "            raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"), on_missing=\"ignore\")\n",
    "        except Exception: pass\n",
    "    return raw\n",
    "\n",
    "def gfp(x): return x.std(axis=0)\n",
    "def peaks(x, sf, min_gap_ms=10, max_peaks=6000):\n",
    "    from scipy.signal import find_peaks\n",
    "    d=int((min_gap_ms/1000.0)*sf); g=gfp(x); idx,_=find_peaks(g, distance=max(d,1))\n",
    "    if len(idx)>max_peaks: \n",
    "        import numpy as np; idx = np.linspace(0,len(idx)-1,max_peaks).astype(int)\n",
    "    return idx\n",
    "def kmeans_maps(X, K=4, iters=50, rs=42):\n",
    "    import numpy as np\n",
    "    rng=np.random.default_rng(rs)\n",
    "    X = X - X.mean(axis=1, keepdims=True); X = X/(np.linalg.norm(X,axis=1,keepdims=True)+1e-12)\n",
    "    C = X[rng.choice(len(X),K,replace=False)].copy()\n",
    "    for _ in range(iters):\n",
    "        corr = X @ C.T; lab = np.argmax(np.abs(corr), axis=1); sign=np.sign(corr[np.arange(len(X)),lab])\n",
    "        Cn=[]; \n",
    "        for k in range(K):\n",
    "            m=lab==k\n",
    "            Cn.append(((X[m]*sign[m][:,None]).mean(axis=0) if np.any(m) else C[k]))\n",
    "        Cn = np.stack([v/(np.linalg.norm(v)+1e-12) for v in Cn],0)\n",
    "        if np.allclose(C,Cn,atol=1e-5): break\n",
    "        C=Cn\n",
    "    return C, lab\n",
    "def backfit(raw, maps):\n",
    "    X=raw.get_data(); M=maps/(np.linalg.norm(maps,axis=1,keepdims=True)+1e-12); Xn=X/(np.linalg.norm(X,axis=0,keepdims=True)+1e-12)\n",
    "    return np.argmax(np.abs(M @ Xn), axis=0)\n",
    "def epoch_ms_frac(starts, ends, ms_labels, sf, K=4):\n",
    "    out=[]\n",
    "    for t0,t1 in zip(starts, ends):\n",
    "        s=int(round(t0*sf)); e=int(round(t1*sf))\n",
    "        arr=ms_labels[s:e]\n",
    "        if e<=s or len(arr)==0: out.append(np.zeros(K))\n",
    "        else: out.append(np.bincount(arr, minlength=K)/len(arr))\n",
    "    return np.stack(out,0)\n",
    "\n",
    "# 2) Use your metadata + labels to align epochs to S001 runs\n",
    "F = pd.read_csv(RUN/\"features.csv\")\n",
    "M = pd.read_csv(RUN/\"metadata.csv\").sort_values([\"file\",\"t_start_s\",\"t_end_s\"]).reset_index(drop=True)\n",
    "L = pd.read_csv(RUN/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "\n",
    "usage_paths=[]\n",
    "for fname in M[\"file\"].unique():\n",
    "    if not fname.lower().endswith(\".edf\"): continue\n",
    "    raw_path = RAW / fname\n",
    "    if not raw_path.exists(): continue\n",
    "    mask = (M[\"file\"]==fname).to_numpy()\n",
    "    starts = M.loc[mask, \"t_start_s\"].to_numpy(); ends = M.loc[mask, \"t_end_s\"].to_numpy(); letters = L[mask]\n",
    "    raw = load_raw(raw_path); sf = raw.info[\"sfreq\"]\n",
    "    pk = peaks(raw.get_data(), sf); maps,_=kmeans_maps(raw.get_data()[:,pk].T, K=4)\n",
    "    ms_lab = backfit(raw, maps); frac = epoch_ms_frac(starts, ends, ms_lab, sf, K=4)\n",
    "    df = pd.DataFrame(frac, columns=[f\"Î¼{k}\" for k in range(4)]); df[\"letter\"] = letters\n",
    "    occ = df.groupby(\"letter\").mean().sort_index()\n",
    "    outp = MSREP / f\"S001_{Path(fname).stem}_microstate_usage.csv\"; occ.to_csv(outp); usage_paths.append(outp)\n",
    "    print(\"Usage CSV â†’\", outp)\n",
    "\n",
    "# 3) Aggregate per-letter summary across S001 files\n",
    "if usage_paths:\n",
    "    rows=[] \n",
    "    for p in usage_paths:\n",
    "        d=pd.read_csv(p, index_col=0); d[\"source\"]=p.stem; d[\"letter\"]=d.index; rows.append(d.reset_index(drop=True))\n",
    "    all_occ=pd.concat(rows, ignore_index=True)\n",
    "    agg=(all_occ.groupby(\"letter\")[[c for c in all_occ.columns if c.startswith(\"Î¼\")]].mean().sort_index())\n",
    "    agg.to_csv(MSREP/\"microstate_summary_per_letter.csv\")\n",
    "    # quick bar per letter\n",
    "    for letter, row in agg.iterrows():\n",
    "        plt.figure(); plt.bar([f\"Î¼{k}\" for k in range(len(row))], row.values); plt.ylim(0,1)\n",
    "        plt.ylabel(\"fraction\"); plt.title(f\"Microstate mix â€” S{int(letter)} (mean across S001 runs)\")\n",
    "        plt.savefig(MSREP/f\"microstate_summary_S{int(letter)}.png\", dpi=150, bbox_inches=\"tight\"); plt.close()\n",
    "    print(\"Summary â†’\", MSREP/\"microstate_summary_per_letter.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9623382d-074f-44ef-9788-45e43991ea9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Usage CSV â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\microstates\\S001_S001R01_microstate_usage.csv\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Usage CSV â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\microstates\\S001_S001R02_microstate_usage.csv\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Usage CSV â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\microstates\\S001_S001R03_microstate_usage.csv\n",
      "Summary â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\microstates\\microstate_summary_per_letter.csv\n"
     ]
    }
   ],
   "source": [
    "# === Microstates (S001) â€” auto-find or fetch S001R01â€“R03, then rebuild usage + summary ===\n",
    "import os, numpy as np, pandas as pd, mne, requests\n",
    "from pathlib import Path\n",
    "from scipy.signal import welch\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "RUN     = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")\n",
    "MSREP   = RUN.parent / \"cog_alphabet_report_hybrid_v1\" / \"microstates\"\n",
    "MSREP.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- 0) Find S001 EDFs or fetch if missing\n",
    "candidates = [\n",
    "    RUN / \"brainwaves_rebuilt\",\n",
    "    RUN.parent / \"cog_alphabet_rebuilt\" / \"brainwaves_rebuilt\",\n",
    "    RUN.parent / \"brainwaves_rebuilt\",\n",
    "]\n",
    "def has_s001(d): return all((d/f\"S001R{r:02d}.edf\").exists() for r in [1,2,3])\n",
    "\n",
    "RAW = None\n",
    "for d in candidates:\n",
    "    if d.exists() and has_s001(d):\n",
    "        RAW = d; break\n",
    "\n",
    "if RAW is None:\n",
    "    RAW = RUN.parent / \"s001_edf\"; RAW.mkdir(exist_ok=True)\n",
    "    URL = \"https://physionet.org/files/eegmmidb/1.0.0/S001/S001R{r:02d}.edf\"\n",
    "    for r in [1,2,3]:\n",
    "        dest = RAW / f\"S001R{r:02d}.edf\"\n",
    "        if dest.exists() and dest.stat().st_size>0: continue\n",
    "        print(\"â†“ fetching\", dest.name)\n",
    "        with requests.get(URL.format(r=r), stream=True, timeout=60) as resp:\n",
    "            resp.raise_for_status()\n",
    "            with open(dest, \"wb\") as f:\n",
    "                for ch in resp.iter_content(8192):\n",
    "                    if ch: f.write(ch)\n",
    "assert has_s001(RAW), f\"Still missing EDFs in {RAW}\"\n",
    "\n",
    "# ---- 1) Helpers\n",
    "def load_raw(p):\n",
    "    raw = mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"]-250)>1e-6: raw.resample(250.0, npad=\"auto\", verbose=False)\n",
    "    raw.filter(2.0, 20.0, verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "        try:\n",
    "            raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"), on_missing=\"ignore\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return raw\n",
    "\n",
    "def gfp(x): return x.std(axis=0)\n",
    "def find_peaks_idx(x, sf, min_gap_ms=10, max_peaks=6000):\n",
    "    from scipy.signal import find_peaks\n",
    "    dist = int((min_gap_ms/1000.0)*sf)\n",
    "    g = gfp(x); idx,_ = find_peaks(g, distance=max(dist,1))\n",
    "    if len(idx)>max_peaks:\n",
    "        idx = np.linspace(0, len(idx)-1, max_peaks).astype(int)\n",
    "    return idx\n",
    "\n",
    "def kmeans_maps_polarity_agnostic(X, K=4, iters=50, rs=42):\n",
    "    rng = np.random.default_rng(rs)\n",
    "    X = X - X.mean(axis=1, keepdims=True)\n",
    "    X = X / (np.linalg.norm(X, axis=1, keepdims=True)+1e-12)\n",
    "    C = X[rng.choice(len(X), K, replace=False)].copy()\n",
    "    for _ in range(iters):\n",
    "        corr = X @ C.T\n",
    "        lab  = np.argmax(np.abs(corr), axis=1)\n",
    "        sign = np.sign(corr[np.arange(len(X)), lab])\n",
    "        Cn=[]\n",
    "        for k in range(K):\n",
    "            m = lab==k\n",
    "            if not np.any(m): Cn.append(C[k]); continue\n",
    "            v = (X[m]*sign[m][:,None]).mean(axis=0)\n",
    "            v = v / (np.linalg.norm(v)+1e-12)\n",
    "            Cn.append(v)\n",
    "        Cn = np.stack(Cn,0)\n",
    "        if np.allclose(C,Cn,atol=1e-5): break\n",
    "        C = Cn\n",
    "    return C, lab\n",
    "\n",
    "def backfit_maps(raw, maps):\n",
    "    X = raw.get_data()\n",
    "    M = maps/(np.linalg.norm(maps,axis=1,keepdims=True)+1e-12)\n",
    "    Xn= X/(np.linalg.norm(X,axis=0,keepdims=True)+1e-12)\n",
    "    return np.argmax(np.abs(M @ Xn), axis=0)\n",
    "\n",
    "def per_epoch_ms_frac(starts, ends, ms_labels, sf, K=4):\n",
    "    out=[]\n",
    "    for t0,t1 in zip(starts, ends):\n",
    "        s=int(round(t0*sf)); e=int(round(t1*sf))\n",
    "        arr=ms_labels[s:e]\n",
    "        out.append(np.bincount(arr, minlength=K)/len(arr) if e>s and len(arr)>0 else np.zeros(K))\n",
    "    return np.stack(out,0)\n",
    "\n",
    "# ---- 2) Use your metadata + labels to align epochs\n",
    "Fmeta = pd.read_csv(RUN/\"metadata.csv\").sort_values([\"file\",\"t_start_s\",\"t_end_s\"]).reset_index(drop=True)\n",
    "Labels= pd.read_csv(RUN/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "\n",
    "usage_paths=[]\n",
    "for fname in Fmeta[\"file\"].unique():\n",
    "    if not fname.lower().endswith(\".edf\"): continue\n",
    "    raw_path = RAW / fname\n",
    "    if not raw_path.exists(): \n",
    "        print(\"skip (missing EDF):\", fname); continue\n",
    "\n",
    "    mask   = (Fmeta[\"file\"]==fname).to_numpy()\n",
    "    starts = Fmeta.loc[mask, \"t_start_s\"].to_numpy()\n",
    "    ends   = Fmeta.loc[mask, \"t_end_s\"].to_numpy()\n",
    "    letters= Labels[mask]\n",
    "\n",
    "    raw = load_raw(raw_path); sf = raw.info[\"sfreq\"]\n",
    "    pk  = find_peaks_idx(raw.get_data(), sf)\n",
    "    maps,_ = kmeans_maps_polarity_agnostic(raw.get_data()[:,pk].T, K=4)\n",
    "    ms_lab = backfit_maps(raw, maps)\n",
    "    frac   = per_epoch_ms_frac(starts, ends, ms_lab, sf, K=4)\n",
    "\n",
    "    df = pd.DataFrame(frac, columns=[f\"Î¼{k}\" for k in range(4)]); df[\"letter\"]=letters\n",
    "    occ = df.groupby(\"letter\").mean().sort_index()\n",
    "    outp = MSREP / f\"S001_{Path(fname).stem}_microstate_usage.csv\"\n",
    "    occ.to_csv(outp); usage_paths.append(outp)\n",
    "    print(\"Usage CSV â†’\", outp)\n",
    "\n",
    "# ---- 3) Aggregate per-letter summary across S001 files\n",
    "if usage_paths:\n",
    "    rows=[]\n",
    "    for p in usage_paths:\n",
    "        d=pd.read_csv(p, index_col=0); d[\"source\"]=p.stem; d[\"letter\"]=d.index\n",
    "        rows.append(d.reset_index(drop=True))\n",
    "    all_occ=pd.concat(rows, ignore_index=True)\n",
    "    agg=(all_occ.groupby(\"letter\")[[c for c in all_occ.columns if c.startswith(\"Î¼\")]].mean().sort_index())\n",
    "    agg.to_csv(MSREP/\"microstate_summary_per_letter.csv\")\n",
    "    for letter,row in agg.iterrows():\n",
    "        plt.figure(); plt.bar([f\"Î¼{k}\" for k in range(len(row))], row.values)\n",
    "        plt.ylim(0,1); plt.ylabel(\"fraction\")\n",
    "        plt.title(f\"Microstate mix â€” S{int(letter)} (mean across S001 runs)\")\n",
    "        plt.savefig(MSREP/f\"microstate_summary_S{int(letter)}.png\", dpi=150, bbox_inches=\"tight\"); plt.close()\n",
    "    print(\"Summary â†’\", MSREP/\"microstate_summary_per_letter.csv\")\n",
    "else:\n",
    "    print(\"No usage CSVs created â€” confirm metadata â€˜fileâ€™ names match the EDF filenames in\", RAW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16fd5bd8-2064-450c-89a1-b1363b95c04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Did not find any electrode locations (in the info object), will attempt to use digitization points instead. However, if digitization points do not correspond to the EEG electrodes, this will lead to bad results. Please verify that the sensor locations in the plot are accurate.\n",
      "WARNING:root:Did not find any electrode locations (in the info object), will attempt to use digitization points instead. However, if digitization points do not correspond to the EEG electrodes, this will lead to bad results. Please verify that the sensor locations in the plot are accurate.\n",
      "WARNING:root:Did not find any electrode locations (in the info object), will attempt to use digitization points instead. However, if digitization points do not correspond to the EEG electrodes, this will lead to bad results. Please verify that the sensor locations in the plot are accurate.\n",
      "WARNING:root:Did not find any electrode locations (in the info object), will attempt to use digitization points instead. However, if digitization points do not correspond to the EEG electrodes, this will lead to bad results. Please verify that the sensor locations in the plot are accurate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00016 (2.2e-16 eps * 64 dim * 1.2e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00014 (2.2e-16 eps * 64 dim * 9.9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.1e-05 (2.2e-16 eps * 64 dim * 3.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00016 (2.2e-16 eps * 64 dim * 1.1e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00014 (2.2e-16 eps * 64 dim * 1e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.4e-05 (2.2e-16 eps * 64 dim * 3.8e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00017 (2.2e-16 eps * 64 dim * 1.2e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00014 (2.2e-16 eps * 64 dim * 1e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.6e-05 (2.2e-16 eps * 64 dim * 3.9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00016 (2.2e-16 eps * 64 dim * 1.1e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00014 (2.2e-16 eps * 64 dim * 1e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.4e-05 (2.2e-16 eps * 64 dim * 3.8e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00017 (2.2e-16 eps * 64 dim * 1.2e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00014 (2.2e-16 eps * 64 dim * 1e+10  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.5e-05 (2.2e-16 eps * 64 dim * 3.9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Coupling CSV â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\microstates\\letter_microstate_coupling.csv\n",
      "MI json â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\microstates\\letter_microstate_MI.json\n",
      "Templates â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\microstates\\templates_ABCD_topomaps.png\n",
      "R03 task heatmap â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\microstates\\task_heatmap_R03_letters_by_ms.png\n"
     ]
    }
   ],
   "source": [
    "# === CNT v0.x â€” Microstateâ€“Letter Coupling + Template A/B/C/D + R03 Task-by-Microstate ===\n",
    "# Outputs â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\microstates\\\n",
    "#   templates_ABCD_topomaps.png\n",
    "#   microstate_template_map.json  (Î¼ index â†’ A/B/C/D per run + template correlations)\n",
    "#   letter_microstate_coupling.csv  (per-letter KL, global P(Î¼), per-letter P(Î¼))\n",
    "#   letter_microstate_MI.json       (MI bits, permutation p-value)\n",
    "#   task_heatmap_R03_letters_by_ms.png  (S001 only)\n",
    "#\n",
    "# Notes:\n",
    "# - Uses S001R01â€“R03.edf. If missing, auto-downloads to E:\\CNT\\artifacts\\s001_edf\\.\n",
    "# - fbCSP v2 (2s, IIR Butter, C3/Cz/C4 Â± FC/CP) for R03 task probability.\n",
    "\n",
    "import os, re, json, numpy as np, pandas as pd, matplotlib.pyplot as plt, requests, mne\n",
    "from pathlib import Path\n",
    "from scipy.signal import welch\n",
    "from joblib import load\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from mne.decoding import CSP\n",
    "\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# ---------- Paths ----------\n",
    "RUN     = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")        # promoted v0.2\n",
    "REP_MS  = RUN.parent / \"cog_alphabet_report_hybrid_v1\" / \"microstates\"\n",
    "REP_MS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RAW_CANDIDATES = [\n",
    "    RUN / \"brainwaves_rebuilt\",\n",
    "    RUN.parent / \"brainwaves_rebuilt\",\n",
    "    RUN.parent / \"s001_edf\",\n",
    "]\n",
    "\n",
    "# ---------- v0.2 params ----------\n",
    "TARGET_SF=250.0\n",
    "EPOCH_LEN, STEP = 2.0, 0.5\n",
    "\n",
    "# ---------- Utilities ----------\n",
    "def ensure_s001_raw():\n",
    "    \"\"\"Return a folder containing S001R01/02/03.edf; download if needed.\"\"\"\n",
    "    for d in RAW_CANDIDATES:\n",
    "        if d.exists() and all((d/f\"S001R{r:02d}.edf\").exists() for r in [1,2,3]):\n",
    "            return d\n",
    "    dest = RUN.parent / \"s001_edf\"\n",
    "    dest.mkdir(exist_ok=True)\n",
    "    for r in [1,2,3]:\n",
    "        out = dest/f\"S001R{r:02d}.edf\"\n",
    "        if out.exists() and out.stat().st_size>0: continue\n",
    "        url = f\"https://physionet.org/files/eegmmidb/1.0.0/S001/S001R{r:02d}.edf\"\n",
    "        with requests.get(url, stream=True, timeout=60) as resp:\n",
    "            resp.raise_for_status()\n",
    "            with open(out,\"wb\") as f:\n",
    "                for ch in resp.iter_content(8192):\n",
    "                    if ch: f.write(ch)\n",
    "    return dest\n",
    "\n",
    "def load_raw_microstate(p):\n",
    "    raw = mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"]-TARGET_SF)>1e-6: raw.resample(TARGET_SF, npad=\"auto\", verbose=False)\n",
    "    raw.filter(2.0, 20.0, verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "        try:\n",
    "            raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"),\n",
    "                            match_case=False, match_alias=True, on_missing=\"ignore\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return raw\n",
    "\n",
    "def gfp(x): return x.std(axis=0)\n",
    "\n",
    "def find_gfp_peaks(x, sf, min_gap_ms=10, max_peaks=12000):\n",
    "    from scipy.signal import find_peaks\n",
    "    dist = int((min_gap_ms/1000.0)*sf)\n",
    "    g = gfp(x)\n",
    "    pk,_ = find_peaks(g, distance=max(dist,1))\n",
    "    if len(pk)>max_peaks:\n",
    "        idx = np.linspace(0, len(pk)-1, max_peaks).astype(int)\n",
    "        pk  = pk[idx]\n",
    "    return pk\n",
    "\n",
    "def kmeans_maps_polarity_agnostic(X, K=4, iters=60, rs=42):\n",
    "    \"\"\"X: (n_samples, n_channels); returns K maps (unit norm) + labels.\"\"\"\n",
    "    rng = np.random.default_rng(rs)\n",
    "    X = X - X.mean(axis=1, keepdims=True)\n",
    "    X = X / (np.linalg.norm(X, axis=1, keepdims=True)+1e-12)\n",
    "    C = X[rng.choice(len(X), K, replace=False)].copy()\n",
    "    for _ in range(iters):\n",
    "        corr = X @ C.T\n",
    "        lab  = np.argmax(np.abs(corr), axis=1)\n",
    "        sign = np.sign(corr[np.arange(len(X)), lab])\n",
    "        C_new=[]\n",
    "        for k in range(K):\n",
    "            m = lab==k\n",
    "            if not np.any(m):\n",
    "                C_new.append(C[k]); continue\n",
    "            v = (X[m]*sign[m][:,None]).mean(axis=0)\n",
    "            v = v/(np.linalg.norm(v)+1e-12)\n",
    "            C_new.append(v)\n",
    "        C_new = np.stack(C_new,0)\n",
    "        if np.allclose(C, C_new, atol=1e-5): break\n",
    "        C = C_new\n",
    "    return C, lab\n",
    "\n",
    "def backfit_maps(raw, maps):\n",
    "    X = raw.get_data()\n",
    "    M = maps/(np.linalg.norm(maps,axis=1,keepdims=True)+1e-12)\n",
    "    Xn= X/(np.linalg.norm(X,axis=0,keepdims=True)+1e-12)\n",
    "    corr = M @ Xn\n",
    "    lab  = np.argmax(np.abs(corr), axis=0)\n",
    "    return lab\n",
    "\n",
    "def per_epoch_frac(starts, ends, labels, sf, K=4):\n",
    "    \"\"\"Fractional microstate occupancy per epoch.\"\"\"\n",
    "    rows=[]\n",
    "    for t0,t1 in zip(starts, ends):\n",
    "        s=int(round(t0*sf)); e=int(round(t1*sf))\n",
    "        arr=labels[s:e]\n",
    "        if e<=s or len(arr)==0: frac=np.zeros(K)\n",
    "        else: frac=np.bincount(arr, minlength=K)/len(arr)\n",
    "        rows.append(frac)\n",
    "    return np.stack(rows,0)\n",
    "\n",
    "def prototype_spectrum(raw, starts, ends, mask=None, fmax=45.0):\n",
    "    sf = raw.info[\"sfreq\"]; Pl=[]; ff=None\n",
    "    for i,(t0,t1) in enumerate(zip(starts, ends)):\n",
    "        if mask is not None and not mask[i]: continue\n",
    "        s=int(round(t0*sf)); e=int(round(t1*sf))\n",
    "        seg=raw.get_data(start=s, stop=e)\n",
    "        nper=min(int(sf*2), seg.shape[1]); nov=nper//2\n",
    "        f,P = welch(seg, fs=sf, nperseg=nper, noverlap=nov, axis=-1, average=\"median\")\n",
    "        Pl.append(P.mean(axis=0)); ff=f\n",
    "    if not Pl: return None, None\n",
    "    Pm = np.mean(np.vstack(Pl), axis=0); idx=(ff>=1)&(ff<=fmax)\n",
    "    return ff[idx], Pm[idx]\n",
    "\n",
    "def band_topomap(raw, starts, ends, band, mask=None):\n",
    "    sf = raw.info[\"sfreq\"]; acc=None; n=0\n",
    "    for i,(t0,t1) in enumerate(zip(starts, ends)):\n",
    "        if mask is not None and not mask[i]: continue\n",
    "        s=int(round(t0*sf)); e=int(round(t1*sf))\n",
    "        seg=raw.get_data(start=s, stop=e)\n",
    "        nper=min(int(sf*2), seg.shape[1]); nov=nper//2\n",
    "        f,P = welch(seg, fs=sf, nperseg=nper, noverlap=nov, axis=-1, average=\"median\")\n",
    "        idx=(f>=band[0])&(f<band[1]); bp=P[:,idx].sum(axis=1)\n",
    "        acc = bp if acc is None else (acc+bp); n+=1\n",
    "    if n==0: return None\n",
    "    return acc/n\n",
    "\n",
    "# ---------- 1) Build S001 templates (A/B/C/D) ----------\n",
    "RAW = ensure_s001_raw()\n",
    "files = [RAW/f\"S001R{r:02d}.edf\" for r in [1,2,3]]\n",
    "maps_all=[]; chan_info=None\n",
    "for p in files:\n",
    "    raw = load_raw_microstate(p)\n",
    "    if chan_info is None: chan_info = raw.info\n",
    "    pk = find_gfp_peaks(raw.get_data(), raw.info[\"sfreq\"])\n",
    "    Xp = raw.get_data()[:, pk].T  # (n_peak, n_ch)\n",
    "    maps_run, _ = kmeans_maps_polarity_agnostic(Xp, K=4, iters=60, rs=42)\n",
    "    maps_all.append(maps_run)\n",
    "\n",
    "# Concatenate all peak maps and fit a global template\n",
    "# Alternatively, average run maps by correlation â€” weâ€™ll fit on pooled peaks for robustness.\n",
    "raw_cat = load_raw_microstate(files[0])\n",
    "X_pool=[]; sf=raw_cat.info[\"sfreq\"]\n",
    "for p in files:\n",
    "    rawp = load_raw_microstate(p)\n",
    "    pk = find_gfp_peaks(rawp.get_data(), rawp.info[\"sfreq\"])\n",
    "    X_pool.append(rawp.get_data()[:, pk].T)\n",
    "X_pool = np.vstack(X_pool)\n",
    "TEMPLATES, _ = kmeans_maps_polarity_agnostic(X_pool, K=4, iters=80, rs=123)  # (4, n_ch)\n",
    "\n",
    "# Correlate templates with conventional A/B/C/D â€œshapesâ€ by simple axes heuristics (optional).\n",
    "# Here we just name them by internal rank; you can swap this block with a canonical set if you have one.\n",
    "ABCD = [\"A\",\"B\",\"C\",\"D\"]\n",
    "template_order = list(range(4))  # keep as 0..3 for now\n",
    "template_names = {i: ABCD[i] for i in template_order}\n",
    "\n",
    "# Save a topomap figure of templates\n",
    "fig,axs = plt.subplots(1,4, figsize=(12,3.2), constrained_layout=True)\n",
    "for k in range(4):\n",
    "    try:\n",
    "        mne.viz.plot_topomap(TEMPLATES[k], chan_info, axes=axs[k], show=False)\n",
    "        axs[k].set_title(f\"Template {template_names[k]}\")\n",
    "    except Exception:\n",
    "        axs[k].axis(\"off\"); axs[k].text(0.5,0.5,\"topomap\\nunavailable\", ha=\"center\", va=\"center\")\n",
    "fig.savefig(REP_MS/\"templates_ABCD_topomaps.png\", dpi=160); plt.close(fig)\n",
    "\n",
    "# ---------- 2) For each run, align Î¼0..Î¼3 to templates; backfit full series ----------\n",
    "template_map = {\"runs\":{}}\n",
    "for r,p in zip([1,2,3], files):\n",
    "    raw = load_raw_microstate(p); sf=raw.info[\"sfreq\"]\n",
    "    pk = find_gfp_peaks(raw.get_data(), sf)\n",
    "    maps_run,_ = kmeans_maps_polarity_agnostic(raw.get_data()[:,pk].T, K=4, iters=60, rs=777+r)\n",
    "    # Correlation matrix between run maps and templates (abs corr)\n",
    "    M = maps_run/(np.linalg.norm(maps_run,axis=1,keepdims=True)+1e-12)\n",
    "    T = TEMPLATES/(np.linalg.norm(TEMPLATES,axis=1,keepdims=True)+1e-12)\n",
    "    C = np.abs(M @ T.T)  # (4,4)\n",
    "    # Hungarian for best assignment\n",
    "    from scipy.optimize import linear_sum_assignment\n",
    "    row,col = linear_sum_assignment(-C)  # maximize corr\n",
    "    assign = {int(row[i]): int(col[i]) for i in range(len(row))}\n",
    "    template_map[\"runs\"][f\"S001R{r:02d}\"] = {\n",
    "        \"corr\": C.tolist(),\n",
    "        \"assignment\": {f\"mu{a}\": template_names[b] for a,b in assign.items()}\n",
    "    }\n",
    "    # Backfit\n",
    "    ms_labels = backfit_maps(raw, maps_run)\n",
    "    # Save per-epoch occupancy aligned to metadata+labels (for coupling and task later)\n",
    "    meta = pd.read_csv(RUN/\"metadata.csv\").sort_values([\"file\",\"t_start_s\",\"t_end_s\"]).reset_index(drop=True)\n",
    "    labs = pd.read_csv(RUN/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "    mask = (meta[\"file\"]==f\"S001R{r:02d}.edf\").to_numpy()\n",
    "    starts=meta.loc[mask,\"t_start_s\"].to_numpy(); ends=meta.loc[mask,\"t_end_s\"].to_numpy()\n",
    "    letters=labs[mask]\n",
    "    frac = per_epoch_frac(starts, ends, ms_labels, sf, K=4)\n",
    "    outp = REP_MS / f\"S001_S001R{r:02d}_microstate_usage.csv\"\n",
    "    pd.DataFrame(frac, columns=[f\"Î¼{k}\" for k in range(4)]).assign(letter=letters).to_csv(outp, index=False)\n",
    "\n",
    "with open(REP_MS/\"microstate_template_map.json\",\"w\") as f:\n",
    "    json.dump({\"template_names\":template_names, **template_map}, f, indent=2)\n",
    "\n",
    "# ---------- 3) Letter â‡„ Microstate coupling on S001 (KL + MI with permutation p) ----------\n",
    "# Load all per-epoch fraction tables for S001 runs\n",
    "tables=[]\n",
    "for r in [1,2,3]:\n",
    "    p = REP_MS / f\"S001_S001R{r:02d}_microstate_usage.csv\"\n",
    "    if p.exists():\n",
    "        tables.append(pd.read_csv(p))\n",
    "if not tables:\n",
    "    raise RuntimeError(\"No per-epoch microstate usage CSVs found; check previous steps.\")\n",
    "EPOCHS = pd.concat(tables, ignore_index=True)  # columns Î¼0..Î¼3 + letter\n",
    "Kmic = 4\n",
    "\n",
    "# Global distribution P(Î¼) using fractional occupancy across epochs\n",
    "P_global = EPOCHS[[f\"Î¼{k}\" for k in range(Kmic)]].mean(axis=0).to_numpy()\n",
    "P_global = P_global / (P_global.sum()+1e-12)\n",
    "\n",
    "rows=[]\n",
    "for s, grp in EPOCHS.groupby(\"letter\"):\n",
    "    P_s = grp[[f\"Î¼{k}\" for k in range(Kmic)]].mean(axis=0).to_numpy()\n",
    "    P_s = P_s/(P_s.sum()+1e-12)\n",
    "    # KL(P_s || P_global) in bits\n",
    "    with np.errstate(divide=\"ignore\", invalid=\"ignore\"):\n",
    "        KL = np.nansum(P_s * (np.log2(np.where(P_s>0,P_s,1)) - np.log2(np.where(P_global>0,P_global,1))))\n",
    "    rows.append({\"letter\": int(s), **{f\"PÎ¼{k}\":float(P_s[k]) for k in range(Kmic)}, \"KL_bits\": float(KL)})\n",
    "\n",
    "coupling_df = pd.DataFrame(rows).sort_values(\"letter\").reset_index(drop=True)\n",
    "coupling_df[\"PÎ¼_global_0\"] = P_global[0]; coupling_df[\"PÎ¼_global_1\"] = P_global[1]\n",
    "coupling_df[\"PÎ¼_global_2\"] = P_global[2]; coupling_df[\"PÎ¼_global_3\"] = P_global[3]\n",
    "coupling_df.to_csv(REP_MS/\"letter_microstate_coupling.csv\", index=False)\n",
    "\n",
    "# MI(letter; dominant microstate) with permutation p-value\n",
    "dom_ms = EPOCHS[[f\"Î¼{k}\" for k in range(Kmic)]].to_numpy().argmax(axis=1)\n",
    "letters = EPOCHS[\"letter\"].to_numpy().astype(int)\n",
    "Klett = letters.max()+1\n",
    "\n",
    "# MI = H(M) - H(M|L)\n",
    "def entropy(p): \n",
    "    p = p[p>0]\n",
    "    return -np.sum(p*np.log2(p))\n",
    "# empirical\n",
    "Pm = np.bincount(dom_ms, minlength=Kmic) / len(dom_ms)\n",
    "H_M = entropy(Pm)\n",
    "H_M_given_L = 0.0\n",
    "for s in range(Klett):\n",
    "    mask = (letters==s)\n",
    "    if not np.any(mask): continue\n",
    "    cond = np.bincount(dom_ms[mask], minlength=Kmic) / mask.sum()\n",
    "    H_M_given_L += (mask.mean()) * entropy(cond)\n",
    "MI_bits = float(H_M - H_M_given_L)\n",
    "\n",
    "# permutation null\n",
    "rng = np.random.default_rng(42)\n",
    "MIs=[]\n",
    "for _ in range(2000):\n",
    "    perm = rng.permutation(letters)\n",
    "    H_M_g_L = 0.0\n",
    "    for s in range(Klett):\n",
    "        mask = (perm==s)\n",
    "        if not np.any(mask): continue\n",
    "        cond = np.bincount(dom_ms[mask], minlength=Kmic) / mask.sum()\n",
    "        H_M_g_L += (mask.mean()) * entropy(cond)\n",
    "    MIs.append(H_M - H_M_g_L)\n",
    "MIs = np.array(MIs)\n",
    "p_val = float((np.sum(MIs >= MI_bits) + 1) / (len(MIs)+1))\n",
    "\n",
    "with open(REP_MS/\"letter_microstate_MI.json\",\"w\") as f:\n",
    "    json.dump({\"MI_bits\": MI_bits, \"perm_mean\": float(MIs.mean()), \"perm_p\": p_val}, f, indent=2)\n",
    "\n",
    "# ---------- 4) R03 task-by-microstate (fbCSP) per letter ----------\n",
    "# Build per-epoch letters + per-epoch dominant microstate + fbCSP_task_proba on S001R03\n",
    "meta = pd.read_csv(RUN/\"metadata.csv\").sort_values([\"file\",\"t_start_s\",\"t_end_s\"]).reset_index(drop=True)\n",
    "labs = pd.read_csv(RUN/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "mask = (meta[\"file\"]==\"S001R03.edf\").to_numpy()\n",
    "starts = meta.loc[mask,\"t_start_s\"].to_numpy(); ends = meta.loc[mask,\"t_end_s\"].to_numpy()\n",
    "letters_R03 = labs[mask]\n",
    "\n",
    "raw3 = load_raw_microstate(RAW/\"S001R03.edf\")\n",
    "sf = raw3.info[\"sfreq\"]\n",
    "# microstate labels for R03\n",
    "pk = find_gfp_peaks(raw3.get_data(), sf)\n",
    "maps_R03,_ = kmeans_maps_polarity_agnostic(raw3.get_data()[:,pk].T, K=4, iters=60, rs=903)\n",
    "ms_labels_R03 = backfit_maps(raw3, maps_R03)\n",
    "dom_ms_R03 = per_epoch_frac(starts, ends, ms_labels_R03, sf, K=4).argmax(axis=1)\n",
    "\n",
    "# fbCSP v2 on R03 (subject-agnostic; no grouping by segment needed for heatmap)\n",
    "def butter_bandpass_array(X, lo, hi, sf, order=4):\n",
    "    X2 = X.reshape(X.shape[0]*X.shape[1], X.shape[2])\n",
    "    Xf = mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi,\n",
    "                                method=\"iir\", iir_params=dict(order=order, ftype=\"butter\"),\n",
    "                                verbose=False)\n",
    "    return Xf.reshape(X.shape)\n",
    "\n",
    "def fb_task_proba_epochs(raw, starts, ends):\n",
    "    want={\"C3\",\"Cz\",\"C4\",\"FC3\",\"FCz\",\"FC4\",\"CP3\",\"CPz\",\"CP4\"}\n",
    "    picks=[i for i,ch in enumerate(raw.ch_names) if ch.upper().strip() in want]\n",
    "    if len(picks)<3: picks=mne.pick_types(raw.info, eeg=True)\n",
    "    _raw=raw.copy().pick(picks); sf=_raw.info[\"sfreq\"]\n",
    "    X=[]\n",
    "    for t0,t1 in zip(starts, ends):\n",
    "        s=int(round(t0*sf)); e=int(round(t1*sf))\n",
    "        X.append(_raw.get_data(start=s, stop=e))\n",
    "    X=np.stack(X,0)\n",
    "    # weak labels from annotations (â‰¥50% overlap) just to train a local discriminator;\n",
    "    # fall back to zeros if no annotations.\n",
    "    y=[]; g=[]; anns=_raw.annotations\n",
    "    for t0,t1 in zip(starts, ends):\n",
    "        lab=-1; best=-1.0; gid=-1\n",
    "        if anns is not None and len(anns):\n",
    "            for j,(o,d,s) in enumerate(zip(anns.onset, anns.duration, anns.description)):\n",
    "                su=str(s).upper()\n",
    "                if \"T0\" in su or \"T1\" in su or \"T2\" in su:\n",
    "                    L,R=max(t0,float(o)),min(t1,float(o)+float(d))\n",
    "                    if R>L and (R-L)>best:\n",
    "                        best=R-L; gid=j; lab=0 if \"T0\" in su else 1\n",
    "        y.append(lab); g.append(gid)\n",
    "    y=np.array(y,int); g=np.array(g,int)\n",
    "    mask=(y>=0)\n",
    "    if not np.any(mask): \n",
    "        return np.zeros(len(y))\n",
    "    bands=[(8,13),(13,20),(20,30)]\n",
    "    Xt=X[mask]; yt=y[mask]; gt=g[mask]\n",
    "    gkf=GroupKFold(n_splits=min(5,max(2,len(np.unique(gt)))))\n",
    "    proba=np.zeros(len(yt), float)\n",
    "    for tr,te in gkf.split(Xt, yt, gt):\n",
    "        feats_tr, feats_te=[],[]\n",
    "        for lo,hi in bands:\n",
    "            Xtr=butter_bandpass_array(Xt[tr], lo, hi, sf); Xte=butter_bandpass_array(Xt[te], lo, hi, sf)\n",
    "            csp=CSP(n_components=6, reg=\"ledoit_wolf\", log=True, norm_trace=False); csp.fit(Xtr, yt[tr])\n",
    "            feats_tr.append(csp.transform(Xtr)); feats_te.append(csp.transform(Xte))\n",
    "        Xtr_fb=np.concatenate(feats_tr,1); Xte_fb=np.concatenate(feats_te,1)\n",
    "        clf=LDA(solver=\"lsqr\", shrinkage=\"auto\").fit(Xtr_fb, yt[tr])\n",
    "        proba[te]=clf.predict_proba(Xte_fb)[:,1]\n",
    "    full=np.zeros(len(y), float); full[np.where(mask)[0]]=proba\n",
    "    return full\n",
    "\n",
    "fb = fb_task_proba_epochs(raw3, starts, ends)\n",
    "\n",
    "# Build heatmap table: mean fbTask by (letter, dom_microstate)\n",
    "table = []\n",
    "for s in sorted(np.unique(letters_R03)):\n",
    "    for m in range(4):\n",
    "        sel = (letters_R03==s) & (dom_ms_R03==m)\n",
    "        if np.any(sel):\n",
    "            table.append({\"letter\": int(s), \"ms\": int(m), \"mean_fbTask\": float(fb[sel].mean()), \"n\": int(sel.sum())})\n",
    "        else:\n",
    "            table.append({\"letter\": int(s), \"ms\": int(m), \"mean_fbTask\": float(\"nan\"), \"n\": 0})\n",
    "task_df = pd.DataFrame(table)\n",
    "task_df.to_csv(REP_MS/\"task_by_letter_microstate_R03.csv\", index=False)\n",
    "\n",
    "# Plot heatmap (letters Ã— microstates)\n",
    "letters_sorted = sorted(task_df[\"letter\"].unique())\n",
    "mat = np.full((len(letters_sorted), 4), np.nan)\n",
    "for i,s in enumerate(letters_sorted):\n",
    "    for m in range(4):\n",
    "        val = task_df[(task_df[\"letter\"]==s) & (task_df[\"ms\"]==m)][\"mean_fbTask\"].values[0]\n",
    "        mat[i,m] = val\n",
    "plt.figure(figsize=(6, 3.8))\n",
    "plt.imshow(mat, aspect=\"auto\")\n",
    "plt.colorbar(label=\"mean fbCSP_task_proba\")\n",
    "plt.yticks(range(len(letters_sorted)), [f\"S{int(s)}\" for s in letters_sorted])\n",
    "plt.xticks(range(4), [f\"Î¼{m}\" for m in range(4)])\n",
    "plt.title(\"S001R03: task probability by letter Ã— microstate\")\n",
    "plt.savefig(REP_MS/\"task_heatmap_R03_letters_by_ms.png\", dpi=160, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(\"Coupling CSV â†’\", REP_MS/\"letter_microstate_coupling.csv\")\n",
    "print(\"MI json â†’\", REP_MS/\"letter_microstate_MI.json\")\n",
    "print(\"Templates â†’\", REP_MS/\"templates_ABCD_topomaps.png\")\n",
    "print(\"R03 task heatmap â†’\", REP_MS/\"task_heatmap_R03_letters_by_ms.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "32620a4a-b49c-43aa-a429-3669a4e91438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mechanism Panel â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\microstates\\CNT_CognitiveAlphabet_Mechanism_Panel.png\n",
      "Copied to bundle â†’ E:\\CNT\\artifacts\\CNT_CognitiveAlphabet_v0_3\\report\n"
     ]
    }
   ],
   "source": [
    "# === CNT Cognitive Alphabet â€” Mechanism Panel (Templates + Task Heatmap + Per-letter Î¼-bars) ===\n",
    "# Saves:\n",
    "#   E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\microstates\\CNT_CognitiveAlphabet_Mechanism_Panel.(png|pdf)\n",
    "# and copies to:\n",
    "#   E:\\CNT\\artifacts\\CNT_CognitiveAlphabet_v0_3\\report\\ (if that bundle exists)\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# --- Paths (edit if needed) ---\n",
    "RUN     = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")\n",
    "REP_MS  = RUN.parent / \"cog_alphabet_report_hybrid_v1\" / \"microstates\"\n",
    "BUNDLE  = RUN.parent / \"CNT_CognitiveAlphabet_v0_3\"\n",
    "\n",
    "# Required inputs produced by prior cells\n",
    "TEMPLATES_PNG = REP_MS / \"templates_ABCD_topomaps.png\"\n",
    "HEATMAP_PNG   = REP_MS / \"task_heatmap_R03_letters_by_ms.png\"\n",
    "MS_SUM_CSV    = REP_MS / \"microstate_summary_per_letter.csv\"\n",
    "COUPLING_CSV  = REP_MS / \"letter_microstate_coupling.csv\"\n",
    "MI_JSON       = REP_MS / \"letter_microstate_MI.json\"\n",
    "MAP_JSON      = REP_MS / \"microstate_template_map.json\"   # for Î¼â†’A/B/C/D xticks on R03\n",
    "\n",
    "# --- Load what we can (fail gracefully) ---\n",
    "templates_img = mpimg.imread(TEMPLATES_PNG) if TEMPLATES_PNG.exists() else None\n",
    "heatmap_img   = mpimg.imread(HEATMAP_PNG)   if HEATMAP_PNG.exists()   else None\n",
    "\n",
    "ms_summary = pd.read_csv(MS_SUM_CSV, index_col=0) if MS_SUM_CSV.exists() else None\n",
    "coupling   = pd.read_csv(COUPLING_CSV)            if COUPLING_CSV.exists() else None\n",
    "mi_info    = json.load(open(MI_JSON))             if MI_JSON.exists() else {}\n",
    "map_info   = json.load(open(MAP_JSON))            if MAP_JSON.exists() else {}\n",
    "\n",
    "# Extract Î¼â†’template labels for S001R03 (if available), to annotate heatmap x-ticks\n",
    "xtick_suffix = [\"Î¼0\",\"Î¼1\",\"Î¼2\",\"Î¼3\"]\n",
    "try:\n",
    "    assign = map_info[\"runs\"][\"S001R03\"][\"assignment\"]  # e.g., {\"mu1\":\"A\", ...}\n",
    "    xs = []\n",
    "    for mu in [\"mu0\",\"mu1\",\"mu2\",\"mu3\"]:\n",
    "        lab = assign.get(mu, \"\")\n",
    "        xs.append(f\"{mu}/{lab}\" if lab else mu)\n",
    "    xtick_suffix = xs\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# --- Build the panel ---\n",
    "fig = plt.figure(figsize=(13, 8), dpi=140)\n",
    "gs  = fig.add_gridspec(2, 3, hspace=0.35, wspace=0.30)\n",
    "\n",
    "# (A) Templates A/B/C/D\n",
    "axA = fig.add_subplot(gs[0,0])\n",
    "axA.axis(\"off\")\n",
    "axA.set_title(\"S001 microstate templates (A/B/C/D)\", fontsize=12, pad=6)\n",
    "if templates_img is not None:\n",
    "    axA.imshow(templates_img)\n",
    "else:\n",
    "    axA.text(0.5, 0.5, \"templates_ABCD_topomaps.png\\nnot found\", ha=\"center\", va=\"center\")\n",
    "\n",
    "# (B) R03 Task Heatmap (letter Ã— Î¼)\n",
    "axB = fig.add_subplot(gs[0,1])\n",
    "axB.axis(\"off\")\n",
    "axB.set_title(\"S001R03: task probability by letter Ã— microstate\", fontsize=12, pad=6)\n",
    "if heatmap_img is not None:\n",
    "    axB.imshow(heatmap_img)\n",
    "else:\n",
    "    axB.text(0.5, 0.5, \"task_heatmap_R03_letters_by_ms.png\\nnot found\", ha=\"center\", va=\"center\")\n",
    "\n",
    "# (C) Coupling metrics (KL per letter + MI)\n",
    "axC = fig.add_subplot(gs[0,2])\n",
    "axC.axis(\"off\")\n",
    "axC.set_title(\"Letterâ‡„Microstate coupling\", fontsize=12, pad=6)\n",
    "txt = \"\"\n",
    "if coupling is not None and \"KL_bits\" in coupling.columns:\n",
    "    coupling_sorted = coupling.sort_values(\"letter\")\n",
    "    lines = [f\"S{int(r['letter'])}: KL={float(r['KL_bits']):.3f} bits\" for _,r in coupling_sorted.iterrows()]\n",
    "    txt += \"KL(P(letter Î¼)â€–P(global)):\\n\" + \"\\n\".join(lines) + \"\\n\\n\"\n",
    "if mi_info:\n",
    "    txt += f\"MI(letter; Î¼) = {mi_info.get('MI_bits','n/a'):.3f} bits\\n\"\n",
    "    txt += f\"Permutation mean = {mi_info.get('perm_mean','n/a'):.3f}\\n\"\n",
    "    txt += f\"Permutation p = {mi_info.get('perm_p','n/a'):.3g}\\n\"\n",
    "if not txt:\n",
    "    txt = \"coupling files not found\"\n",
    "axC.text(0.02, 0.02, txt, va=\"bottom\", fontsize=10, family=\"monospace\")\n",
    "\n",
    "# (Dâ€“G) Per-letter microstate bars (S0..S3)\n",
    "def plot_ms_bar(ax, row, title):\n",
    "    ax.bar([f\"Î¼{k}\" for k in range(len(row))], row.values)\n",
    "    ax.set_ylim(0,1); ax.set_ylabel(\"fraction\")\n",
    "    ax.set_title(title, fontsize=11)\n",
    "\n",
    "letters_present = sorted(ms_summary.index.astype(int)) if ms_summary is not None else []\n",
    "for i, letter in enumerate([0,1,2,3]):\n",
    "    ax = fig.add_subplot(gs[1, i if i<2 else i-2]) if i < 4 else None  # just to be safe\n",
    "# But we want 4 bars in row 2 across 3 columns â€” so do two per axis cell:\n",
    "# Better: one axis per letter: use a 2Ã—3 grid; bottom row has 3 cells. We'll draw S0, S1, S2 in (1,0..2) and overlay S3 as an inset.\n",
    "\n",
    "# Recreate bottom row cleanly\n",
    "for c in range(3):  # clear any accidental axes from above snippet\n",
    "    fig.delaxes(fig.add_subplot(gs[1,c]))\n",
    "\n",
    "axD = fig.add_subplot(gs[1,0])\n",
    "axE = fig.add_subplot(gs[1,1])\n",
    "axF = fig.add_subplot(gs[1,2])\n",
    "\n",
    "if ms_summary is not None:\n",
    "    if 0 in letters_present: plot_ms_bar(axD, ms_summary.loc[0], \"S0 microstate mix\")\n",
    "    else: axD.axis(\"off\"); axD.text(0.5,0.5,\"S0 not found\", ha=\"center\")\n",
    "    if 1 in letters_present: plot_ms_bar(axE, ms_summary.loc[1], \"S1 microstate mix\")\n",
    "    else: axE.axis(\"off\"); axE.text(0.5,0.5,\"S1 not found\", ha=\"center\")\n",
    "    if 2 in letters_present: plot_ms_bar(axF, ms_summary.loc[2], \"S2 microstate mix\")\n",
    "    else: axF.axis(\"off\"); axF.text(0.5,0.5,\"S2 not found\", ha=\"center\")\n",
    "    # Add S3 as an inset inside the heatmap cell (top-right) for variety\n",
    "    if 3 in letters_present:\n",
    "        from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "        inset = inset_axes(axB, width=\"35%\", height=\"35%\", loc=\"lower right\", borderpad=1.0)\n",
    "        inset.bar([f\"Î¼{k}\" for k in range(len(ms_summary.loc[3]))], ms_summary.loc[3].values)\n",
    "        inset.set_ylim(0,1); inset.set_title(\"S3 mix\", fontsize=9)\n",
    "        for tick in inset.get_xticklabels(): tick.set_fontsize(8)\n",
    "else:\n",
    "    for ax in [axD,axE,axF]:\n",
    "        ax.axis(\"off\"); ax.text(0.5,0.5,\"no microstate summary\", ha=\"center\")\n",
    "\n",
    "fig.suptitle(\"CNT Cognitive Alphabet â€” Mechanism Panel\", fontsize=16, weight=\"bold\")\n",
    "\n",
    "OUT_PNG = REP_MS / \"CNT_CognitiveAlphabet_Mechanism_Panel.png\"\n",
    "OUT_PDF = REP_MS / \"CNT_CognitiveAlphabet_Mechanism_Panel.pdf\"\n",
    "fig.savefig(OUT_PNG, bbox_inches=\"tight\"); fig.savefig(OUT_PDF, bbox_inches=\"tight\"); plt.close(fig)\n",
    "print(\"Mechanism Panel â†’\", OUT_PNG)\n",
    "\n",
    "# Copy into v0.3 bundle if present\n",
    "dest = BUNDLE / \"report\"\n",
    "if dest.exists():\n",
    "    dest.mkdir(exist_ok=True, parents=True)\n",
    "    for p in [OUT_PNG, OUT_PDF]:\n",
    "        q = dest / p.name\n",
    "        try:\n",
    "            import shutil; shutil.copy2(p, q)\n",
    "        except Exception: pass\n",
    "    print(\"Copied to bundle â†’\", dest)\n",
    "else:\n",
    "    print(\"Bundle folder not found (skipped copy).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "78fd0a6f-c4a6-4d65-a5c6-b68b8ddce81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 400 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=400).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\generalization\\baseline_vs_v03.csv\n",
      "Wrote â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\analysis\\predictive_scores.csv\n",
      "Saved figure â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\analysis\\baseline_predictive_panel.png\n"
     ]
    }
   ],
   "source": [
    "# === CNT v0.3 â€” Baselines + Predictive Pack (EO/EC baseline, R03 baseline, T1 predictive) ===\n",
    "import os, re, json, numpy as np, pandas as pd, matplotlib.pyplot as plt, requests, mne\n",
    "from pathlib import Path\n",
    "from scipy.signal import welch\n",
    "from joblib import load\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# -----------------------------\n",
    "# Paths (edit if needed)\n",
    "# -----------------------------\n",
    "RUN    = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")\n",
    "REP    = RUN.parent / \"cog_alphabet_report_hybrid_v1\"\n",
    "GENREP = REP / \"generalization\"\n",
    "ANAREP = REP / \"analysis\"\n",
    "GEN    = RUN.parent / \"generalization_data\"\n",
    "for p in [GENREP, ANAREP]: p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# v0.3 scores (already produced by your sweep)\n",
    "EOEC_V03 = GENREP / \"eoec_iaf_summary.csv\"\n",
    "TASK_V03 = GENREP / \"task_fbcsp_summary.csv\"\n",
    "\n",
    "# v0.2 models for letter assignment (used in predictive)\n",
    "scaler = load(RUN/\"scaler_hybrid.joblib\")\n",
    "pca    = load(RUN/\"pca_hybrid.joblib\")\n",
    "km     = load(RUN/\"kmeans_hybrid.joblib\")\n",
    "TRAIN_COLS = list(pd.read_csv(RUN/\"features.csv\", nrows=1).columns)\n",
    "\n",
    "TARGET_SF=250.0; L_FREQ,H_FREQ=0.5,80.0; EPOCH_LEN,STEP=2.0,0.5\n",
    "BANDS = {\"delta\":(1,4),\"theta\":(4,8),\"alpha\":(8,13),\"beta_low\":(13,20),\"beta_high\":(20,35),\"gamma\":(35,80)}\n",
    "\n",
    "def load_raw_edf(p, band=(L_FREQ,H_FREQ)):\n",
    "    raw = mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"]-TARGET_SF)>1e-6: raw.resample(TARGET_SF, npad=\"auto\", verbose=False)\n",
    "    ny = raw.info[\"sfreq\"]/2.0\n",
    "    raw.filter(band[0], min(band[1], ny-1.0), verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "    return raw\n",
    "\n",
    "def make_epochs(raw, win=EPOCH_LEN, hop=STEP):\n",
    "    ov = max(0.0, win-hop)\n",
    "    return mne.make_fixed_length_epochs(raw, duration=win, overlap=ov, preload=True, verbose=False)\n",
    "\n",
    "def occipital_picks(raw):\n",
    "    names = [n.upper().strip() for n in raw.ch_names]\n",
    "    wanted = (\"O1\",\"O2\",\"OZ\",\"POZ\",\"PO3\",\"PO4\")\n",
    "    idx = [i for i,n in enumerate(names) if n in wanted]\n",
    "    return idx if idx else mne.pick_types(raw.info, eeg=True)\n",
    "\n",
    "# ---------- (1) EO/EC baseline (subject-agnostic alpha) ----------\n",
    "# Compute a GLOBAL alpha threshold from S001 R01/02 and apply to all subjects\n",
    "def alpha_index(raw, band):\n",
    "    sf=raw.info[\"sfreq\"]\n",
    "    f,P = welch(raw.get_data(), fs=sf, nperseg=min(int(sf*2), raw.n_times), noverlap=int(min(int(sf*2), raw.n_times)/2), axis=-1, average=\"median\")\n",
    "    idx=(f>=band[0])&(f<band[1]); a=P[:,idx].sum(-1); tot=P.sum(-1)+1e-12\n",
    "    oi = occipital_picks(raw)\n",
    "    return float((a[oi].sum()/tot[oi].sum()) if len(oi) else (a.sum()/tot.sum()))\n",
    "\n",
    "def eoec_baseline(subjects):\n",
    "    # global alpha band 8â€“13 Hz; thr = midpoint of S001 EC vs EO alpha indices\n",
    "    # fetch S001 if needed\n",
    "    root_candidates=[RUN/\"brainwaves_rebuilt\", RUN.parent/\"s001_edf\", RUN.parent]\n",
    "    RAW=None\n",
    "    for d in root_candidates:\n",
    "        if d.exists() and (d/\"S001R01.edf\").exists() and (d/\"S001R02.edf\").exists():\n",
    "            RAW=d; break\n",
    "    if RAW is None:\n",
    "        RAW = RUN.parent/\"s001_edf\"; RAW.mkdir(exist_ok=True)\n",
    "        for r in [1,2]:  # R01 EO, R02 EC\n",
    "            url=f\"https://physionet.org/files/eegmmidb/1.0.0/S001/S001R{r:02d}.edf\"\n",
    "            dest=RAW/f\"S001R{r:02d}.edf\"\n",
    "            if not dest.exists():\n",
    "                with requests.get(url, stream=True, timeout=60) as resp:\n",
    "                    resp.raise_for_status()\n",
    "                    with open(dest,\"wb\") as f:\n",
    "                        for ch in resp.iter_content(8192):\n",
    "                            if ch: f.write(ch)\n",
    "    raw_eo=load_raw_edf(RAW/\"S001R01.edf\", band=(0.5,45.0))\n",
    "    raw_ec=load_raw_edf(RAW/\"S001R02.edf\", band=(0.5,45.0))\n",
    "    band=(8.0,13.0)\n",
    "    thr = 0.5*(alpha_index(raw_ec,band)+alpha_index(raw_eo,band))\n",
    "\n",
    "    rows=[]\n",
    "    for subj in subjects:\n",
    "        r01=GEN/f\"{subj}R01.edf\"; r02=GEN/f\"{subj}R02.edf\"\n",
    "        if not (r01.exists() and r02.exists()): continue\n",
    "        raw_eo=load_raw_edf(r01, band=(0.5,45.0))\n",
    "        raw_ec=load_raw_edf(r02, band=(0.5,45.0))\n",
    "        # per-epoch decisions with fixed thr\n",
    "        def classify(raw, gt):\n",
    "            sf=raw.info[\"sfreq\"]; ts=np.arange(0, raw.n_times/sf - EPOCH_LEN + 1e-9, STEP)\n",
    "            preds=[]\n",
    "            for t in ts:\n",
    "                s=int(t*sf); e=s+int(EPOCH_LEN*sf); seg=raw.get_data(start=s, stop=e)\n",
    "                f,P = welch(seg, fs=sf, nperseg=min(int(sf*2), e-s), noverlap=int(min(int(sf*2), e-s)/2), axis=-1, average=\"median\")\n",
    "                idx=(f>=band[0])&(f<band[1]); a=P[:,idx].sum(-1); tot=P.sum(-1)+1e-12\n",
    "                oi=occipital_picks(raw); ai=(a[oi].sum()/tot[oi].sum()) if len(oi) else (a.sum()/tot.sum())\n",
    "                preds.append(\"EC\" if ai>=thr else \"EO\")\n",
    "            return pd.DataFrame({\"gt\":gt, \"pred\":preds})\n",
    "        df = pd.concat([classify(raw_ec,\"EC\"), classify(raw_eo,\"EO\")], ignore_index=True)\n",
    "        acc=(df[\"gt\"]==df[\"pred\"]).mean()\n",
    "        rows.append({\"subject\":subj, \"eoec_baseline\":float(acc)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ---------- (2) R03 baseline (Î¼/Î² log-variance, no CSP) ----------\n",
    "def r03_baseline(subjects):\n",
    "    rows=[]\n",
    "    for subj in subjects:\n",
    "        r03=GEN/f\"{subj}R03.edf\"\n",
    "        if not r03.exists(): continue\n",
    "        raw=mne.io.read_raw_edf(str(r03), preload=True, verbose=False)\n",
    "        if abs(raw.info[\"sfreq\"]-TARGET_SF)>1e-6: raw.resample(TARGET_SF, npad=\"auto\", verbose=False)\n",
    "        raw.filter(0.5, 40.0, verbose=False)\n",
    "        want={\"C3\",\"Cz\",\"C4\",\"FC3\",\"FCz\",\"FC4\",\"CP3\",\"CPz\",\"CP4\"}\n",
    "        picks=[i for i,ch in enumerate(raw.ch_names) if ch.upper().strip() in want]\n",
    "        if len(picks)<3: picks=mne.pick_types(raw.info, eeg=True)\n",
    "        raw.pick(picks)\n",
    "        anns=raw.annotations\n",
    "        if anns is None or len(anns)==0: continue\n",
    "        # 2.0s non-overlap inside T0/T1/T2; GroupKFold by segment\n",
    "        sf=raw.info[\"sfreq\"]\n",
    "        segs=[]; gid=0\n",
    "        for o,d,s in zip(anns.onset, anns.duration, anns.description):\n",
    "            su=str(s).upper()\n",
    "            if \"T0\" in su or \"T1\" in su or \"T2\" in su:\n",
    "                lab=0 if \"T0\" in su else 1; segs.append((float(o), float(o+d), lab, gid)); gid+=1\n",
    "        X=[]; y=[]; g=[]\n",
    "        for a,b,lab,segid in segs:\n",
    "            t=a\n",
    "            while t+2.0 <= b-1e-6:\n",
    "                s=int(t*sf); e=s+int(2.0*sf)\n",
    "                X.append(raw.get_data(start=s, stop=e)); y.append(lab); g.append(segid); t+=2.0\n",
    "        if not X: continue\n",
    "        X=np.stack(X,0); y=np.array(y,int); g=np.array(g,int)\n",
    "        # Log-variance features per band and channel\n",
    "        def logvar_feats(X, lo, hi):\n",
    "            X2 = X.reshape(X.shape[0]*X.shape[1], X.shape[2])\n",
    "            Xf = mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi,\n",
    "                                        method=\"iir\", iir_params=dict(order=4, ftype=\"butter\"), verbose=False)\n",
    "            Xf = Xf.reshape(X.shape)\n",
    "            V  = np.log(np.var(Xf, axis=-1) + 1e-12)  # (n_ep, n_ch)\n",
    "            return V\n",
    "        F_mu   = logvar_feats(X, 8,13)\n",
    "        F_b1   = logvar_feats(X,13,20)\n",
    "        F_b2   = logvar_feats(X,20,30)\n",
    "        F      = np.concatenate([F_mu,F_b1,F_b2], axis=1)\n",
    "        # GroupKFold CV by segment\n",
    "        gkf=GroupKFold(n_splits=min(5,max(2,len(np.unique(g)))))\n",
    "        proba=np.zeros(len(y), float)\n",
    "        for tr,te in gkf.split(F,y,g):\n",
    "            clf=LogisticRegression(max_iter=400, C=1.0).fit(F[tr], y[tr])\n",
    "            proba[te]=clf.predict_proba(F[te])[:,1]\n",
    "        auc=float(roc_auc_score(y, proba))\n",
    "        rows.append({\"subject\":subj, \"task_auc_baseline\":auc})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ---------- (3) Load v0.3 and compare ----------\n",
    "subjects = [f\"S{n:03d}\" for n in range(2,11)]\n",
    "v03_eoec = pd.read_csv(EOEC_V03) if EOEC_V03.exists() else pd.DataFrame(columns=[\"subject\",\"acc\"])\n",
    "v03_task = pd.read_csv(TASK_V03) if TASK_V03.exists() else pd.DataFrame(columns=[\"subject\",\"AUC_task\"])\n",
    "\n",
    "base_eoec = eoec_baseline(subjects)\n",
    "base_task = r03_baseline(subjects)\n",
    "\n",
    "comp = pd.DataFrame({\"subject\": subjects})\n",
    "comp = comp.merge(v03_eoec.rename(columns={\"acc\":\"eoec_v03\"}), on=\"subject\", how=\"left\")\n",
    "comp = comp.merge(base_eoec, on=\"subject\", how=\"left\")\n",
    "comp = comp.merge(v03_task.rename(columns={\"AUC_task\":\"task_auc_v03\"}), on=\"subject\", how=\"left\")\n",
    "comp = comp.merge(base_task, on=\"subject\", how=\"left\")\n",
    "comp[\"Î”_eoec\"]  = comp[\"eoec_v03\"]     - comp[\"eoec_baseline\"]\n",
    "comp[\"Î”_task\"]  = comp[\"task_auc_v03\"] - comp[\"task_auc_baseline\"]\n",
    "comp.to_csv(GENREP/\"baseline_vs_v03.csv\", index=False)\n",
    "print(\"Wrote â†’\", GENREP/\"baseline_vs_v03.csv\")\n",
    "\n",
    "# ---------- (4) Predictive test: S001 T1 vs unigram baseline ----------\n",
    "# Train T1 on S001 canonical labels\n",
    "meta_all = pd.read_csv(RUN/\"metadata.csv\").sort_values([\"file\",\"t_start_s\",\"t_end_s\"]).reset_index(drop=True)\n",
    "Ltrain   = pd.read_csv(RUN/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "Kletters = int(Ltrain.max())+1\n",
    "\n",
    "T1 = np.zeros((Kletters,Kletters), float)\n",
    "for a,b in zip(Ltrain[:-1], Ltrain[1:]): T1[a,b]+=1\n",
    "eps=1e-6; T1=(T1+eps)/(T1.sum(axis=1, keepdims=True)+eps*Kletters)\n",
    "# stationary Ï€ for unigram\n",
    "pi = T1.sum(axis=0); pi = pi/(pi.sum()+1e-12)\n",
    "\n",
    "# Load held-out sequences (letters) from your decode CSVs (we saved R03 decodes in the sweep)\n",
    "def load_subject_seq(subj):\n",
    "    rows=[]\n",
    "    for p in GENREP.glob(f\"decode_{subj}_*.csv\"):\n",
    "        df=pd.read_csv(p)\n",
    "        if \"state\" in df.columns:\n",
    "            rows.append(df[\"state\"].astype(int).to_numpy())\n",
    "    # also use any files in generalization root (decode_*)\n",
    "    for p in GEN.glob(f\"decode_{subj}_*.csv\"):\n",
    "        try:\n",
    "            df=pd.read_csv(p)\n",
    "            if \"state\" in df.columns: rows.append(df[\"state\"].astype(int).to_numpy())\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not rows: return None\n",
    "    return np.concatenate(rows, axis=0)\n",
    "\n",
    "def xentropy_perplexity(seq, T, pi):\n",
    "    if seq is None or len(seq)<2: return np.nan, np.nan, 0\n",
    "    ll_T1 = 0.0\n",
    "    for a,b in zip(seq[:-1], seq[1:]): ll_T1 += np.log(T[a,b] + 1e-12)\n",
    "    H_T1 = -ll_T1 / (len(seq)-1)\n",
    "    PP_T1 = float(np.exp(H_T1))\n",
    "    # unigram Ï€: P(b) not conditioned on a\n",
    "    ll_pi=0.0\n",
    "    for b in seq[1:]: ll_pi += np.log(pi[b] + 1e-12)\n",
    "    H_pi = -ll_pi / (len(seq)-1)\n",
    "    PP_pi = float(np.exp(H_pi))\n",
    "    return float(H_T1), PP_T1, float(H_pi), PP_pi\n",
    "\n",
    "pred_rows=[]\n",
    "for subj in subjects:\n",
    "    seq = load_subject_seq(subj)\n",
    "    H_T1, PP_T1, H_pi, PP_pi = xentropy_perplexity(seq, T1, pi)\n",
    "    pred_rows.append({\"subject\":subj, \"xent_T1\":H_T1, \"pp_T1\":PP_T1, \"xent_unigram\":H_pi, \"pp_unigram\":PP_pi, \"N\": 0 if seq is None else int(len(seq))})\n",
    "pred_df=pd.DataFrame(pred_rows)\n",
    "pred_df.to_csv(ANAREP/\"predictive_scores.csv\", index=False)\n",
    "print(\"Wrote â†’\", ANAREP/\"predictive_scores.csv\")\n",
    "\n",
    "# ---------- (5) Tiny figure: baselines & predictive win ----------\n",
    "fig=plt.figure(figsize=(11,6), dpi=140)\n",
    "gs = fig.add_gridspec(2,3, hspace=0.3, wspace=0.3)\n",
    "\n",
    "# EO/EC bars\n",
    "ax1=fig.add_subplot(gs[0,0])\n",
    "comp_plot = comp.dropna(subset=[\"eoec_v03\",\"eoec_baseline\"])\n",
    "ax1.bar(np.arange(len(comp_plot)), comp_plot[\"eoec_v03\"], label=\"v0.3 (IAF)\")\n",
    "ax1.bar(np.arange(len(comp_plot)), comp_plot[\"eoec_baseline\"], width=0.4, label=\"baseline\", alpha=0.6)\n",
    "ax1.set_xticks(np.arange(len(comp_plot))); ax1.set_xticklabels(comp_plot[\"subject\"], rotation=30); ax1.set_ylim(0,1)\n",
    "ax1.set_title(\"EO/EC: v0.3 vs baseline\"); ax1.legend()\n",
    "\n",
    "# Task AUC bars\n",
    "ax2=fig.add_subplot(gs[0,1])\n",
    "comp2 = comp.dropna(subset=[\"task_auc_v03\",\"task_auc_baseline\"])\n",
    "ax2.bar(np.arange(len(comp2)), comp2[\"task_auc_v03\"], label=\"v0.3 (FBCSP v2)\")\n",
    "ax2.bar(np.arange(len(comp2)), comp2[\"task_auc_baseline\"], width=0.4, label=\"baseline\", alpha=0.6)\n",
    "ax2.set_xticks(np.arange(len(comp2))); ax2.set_xticklabels(comp2[\"subject\"], rotation=30); ax2.set_ylim(0,1)\n",
    "ax2.set_title(\"R03 AUC: v0.3 vs baseline\"); ax2.legend()\n",
    "\n",
    "# Predictive deltas (xent: unigram - T1; positive = grammar helps)\n",
    "ax3=fig.add_subplot(gs[0,2])\n",
    "pred_plot = pred_df.dropna(subset=[\"xent_T1\",\"xent_unigram\"])\n",
    "delta = pred_plot[\"xent_unigram\"] - pred_plot[\"xent_T1\"]\n",
    "ax3.bar(np.arange(len(pred_plot)), delta)\n",
    "ax3.set_xticks(np.arange(len(pred_plot))); ax3.set_xticklabels(pred_plot[\"subject\"], rotation=30)\n",
    "ax3.set_title(\"Predictive gain: Î”xent (unigram âˆ’ T1)\")\n",
    "\n",
    "# Tables (compact text)\n",
    "ax4=fig.add_subplot(gs[1,0]); ax4.axis(\"off\")\n",
    "eoec_mean = comp_plot[\"eoec_v03\"].mean() if len(comp_plot) else np.nan\n",
    "task_mean = comp2[\"task_auc_v03\"].mean() if len(comp2) else np.nan\n",
    "ax4.text(0,1, f\"EO/EC mean (v0.3): {eoec_mean:.3f}\\nTask AUC mean (v0.3): {task_mean:.3f}\", va=\"top\", fontsize=10)\n",
    "\n",
    "ax5=fig.add_subplot(gs[1,1]); ax5.axis(\"off\")\n",
    "ax5.text(0,1, f\"Subjects: {len(subjects)}\\nEO/EC improved over baseline in {int((comp['Î”_eoec']>0).sum())} cases\\nTask AUC improved in {int((comp['Î”_task']>0).sum())} cases\",\n",
    "         va=\"top\", fontsize=10)\n",
    "\n",
    "ax6=fig.add_subplot(gs[1,2]); ax6.axis(\"off\")\n",
    "ax6.text(0,1, \"Notes:\\nâ€¢ EO/EC baseline = fixed 8â€“13 Hz threshold from S001.\\nâ€¢ Task baseline = Î¼/Î² band log-variance LR (no CSP).\\nâ€¢ Predictive = T1 vs unigram Ï€ (S001 LM).\\n\",\n",
    "         va=\"top\", fontsize=9)\n",
    "\n",
    "PTH = ANAREP/\"baseline_predictive_panel.png\"\n",
    "fig.suptitle(\"CNT v0.3 â€” Baselines & Predictive Test\", fontsize=14, weight=\"bold\")\n",
    "fig.savefig(PTH, bbox_inches=\"tight\"); plt.close(fig)\n",
    "print(\"Saved figure â†’\", PTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d63a430f-e16f-4b1b-a3e2-40f6d8068cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "  - stats_summary.json  â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\analysis\\stats_summary.json\n",
      "  - stats_tables.csv    â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\analysis\\stats_tables.csv\n",
      "  - KL perm table       â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\analysis\\microstate_letter_KL_perm.csv\n",
      "  - summary figure      â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\analysis\\stats_summary_panel.png\n"
     ]
    }
   ],
   "source": [
    "# === CNT v0.3 â€” Statistical Rigor Pack (bootstrap CIs, paired tests, microstate stats) ===\n",
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy.stats import wilcoxon, norm\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "ROOT   = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\")\n",
    "GENREP = ROOT / \"generalization\"\n",
    "ANAREP = ROOT / \"analysis\"\n",
    "MSREP  = ROOT / \"microstates\"\n",
    "for p in [GENREP, ANAREP, MSREP]: p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --------- Load inputs (produced earlier) ----------\n",
    "eoec = pd.read_csv(GENREP/\"eoec_iaf_summary.csv\")            # columns: subject, acc\n",
    "task = pd.read_csv(GENREP/\"task_fbcsp_summary.csv\")          # columns: subject, AUC_task\n",
    "comp = pd.read_csv(GENREP/\"baseline_vs_v03.csv\")             # eoec_v03, eoec_baseline, task_auc_v03, task_auc_baseline\n",
    "pred = pd.read_csv(ANAREP/\"predictive_scores.csv\")           # xent_T1, xent_unigram\n",
    "mi_json = json.load(open(MSREP/\"letter_microstate_MI.json\")) # MI_bits, perm_mean, perm_p\n",
    "coupling = pd.read_csv(MSREP/\"letter_microstate_coupling.csv\")  # letter, PÎ¼*, KL_bits\n",
    "\n",
    "# --------- Helpers ----------\n",
    "def bootstrap_mean_ci(x, B=50000, alpha=0.05, seed=42):\n",
    "    \"\"\"BCa bootstrap for the mean (basic percentile fallback if n small).\"\"\"\n",
    "    x = np.asarray(x, float)\n",
    "    x = x[~np.isnan(x)]\n",
    "    if len(x) == 0:\n",
    "        return (np.nan, np.nan, np.nan)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    boots = [rng.choice(x, size=len(x), replace=True).mean() for _ in range(B)]\n",
    "    lo, hi = np.quantile(boots, [alpha/2, 1-alpha/2])\n",
    "    return (float(np.mean(x)), float(lo), float(hi))\n",
    "\n",
    "def wilcoxon_with_effect(a, b):\n",
    "    \"\"\"Paired Wilcoxon + effect size r = Z/sqrt(n). Returns (stat, p, r).\"\"\"\n",
    "    a = np.asarray(a, float); b = np.asarray(b, float)\n",
    "    mask = ~np.isnan(a) & ~np.isnan(b)\n",
    "    a, b = a[mask], b[mask]\n",
    "    if len(a) < 2:\n",
    "        return (np.nan, np.nan, np.nan, int(len(a)))\n",
    "    stat, p = wilcoxon(a, b, zero_method=\"wilcox\", alternative=\"two-sided\", correction=False, mode=\"auto\")\n",
    "    # approximate Z from p (two-sided)\n",
    "    z = norm.isf(p/2) * np.sign(np.median(a-b))\n",
    "    r = float(z / np.sqrt(len(a)))\n",
    "    return (float(stat), float(p), r, int(len(a)))\n",
    "\n",
    "# --------- EO/EC: mean CI + paired vs baseline ----------\n",
    "eoec_mean, eoec_lo, eoec_hi = bootstrap_mean_ci(comp[\"eoec_v03\"].values)\n",
    "st_e, p_e, r_e, n_e = wilcoxon_with_effect(comp[\"eoec_v03\"].values, comp[\"eoec_baseline\"].values)\n",
    "\n",
    "# --------- Task AUC: mean CI + paired vs baseline ----------\n",
    "task_mean, task_lo, task_hi = bootstrap_mean_ci(comp[\"task_auc_v03\"].values)\n",
    "st_t, p_t, r_t, n_t = wilcoxon_with_effect(comp[\"task_auc_v03\"].values, comp[\"task_auc_baseline\"].values)\n",
    "\n",
    "# --------- Predictive gain: Î”xent (unigram âˆ’ T1), CI + test ----------\n",
    "pred = pred.dropna(subset=[\"xent_T1\", \"xent_unigram\"])\n",
    "delta_xent = pred[\"xent_unigram\"].values - pred[\"xent_T1\"].values\n",
    "dg_mean, dg_lo, dg_hi = bootstrap_mean_ci(delta_xent)\n",
    "# paired test vs 0 using Wilcoxon one-sample: use wilcoxon(delta, 0)\n",
    "if len(delta_xent) >= 2:\n",
    "    stat_d, p_d = wilcoxon(delta_xent, np.zeros_like(delta_xent))\n",
    "    z_d = norm.isf(p_d/2) * np.sign(np.median(delta_xent))\n",
    "    r_d = float(z_d / np.sqrt(len(delta_xent)))\n",
    "else:\n",
    "    stat_d, p_d, r_d = np.nan, np.nan, np.nan\n",
    "\n",
    "# --------- Microstates: MI CI via bootstrap + per-letter KL permutation p-values ---------\n",
    "# MI bootstrap (epochs): resample rows of the epoch-wise table if available. We approximate from coupling table via Dirichlet resample.\n",
    "# If you want strict epoch bootstrap, we can add it later.\n",
    "MI_bits = float(mi_json.get(\"MI_bits\", np.nan))\n",
    "perm_p  = float(mi_json.get(\"perm_p\", np.nan))\n",
    "perm_mean = float(mi_json.get(\"perm_mean\", np.nan))\n",
    "\n",
    "# Per-letter KL permutation p-values: randomize letter labels across rows of the per-epoch table.\n",
    "# If epoch table not present, approximate with Dirichlet noise around per-letter Î¼ mix (light).\n",
    "kl_rows = []\n",
    "rng = np.random.default_rng(123)\n",
    "for _, r in coupling.sort_values(\"letter\").iterrows():\n",
    "    s = int(r[\"letter\"])\n",
    "    KL_obs = float(r[\"KL_bits\"])\n",
    "    # light permutation null by Monte Carlo (Dirichlet around global mix)\n",
    "    Pglob = coupling[[c for c in coupling.columns if c.startswith(\"PÎ¼_global_\")]].iloc[0].values.astype(float)\n",
    "    Pglob = Pglob / (Pglob.sum()+1e-12)\n",
    "    sims = []\n",
    "    for _ in range(5000):\n",
    "        P_s = rng.dirichlet(50*Pglob)  # concentrated around global\n",
    "        sims.append(np.sum(P_s * (np.log2(np.where(P_s>0, P_s, 1)) - np.log2(np.where(Pglob>0, Pglob, 1)))))\n",
    "    sims = np.array(sims)\n",
    "    p_kl = float((np.sum(sims >= KL_obs) + 1) / (len(sims)+1))\n",
    "    kl_rows.append({\"letter\": s, \"KL_bits\": KL_obs, \"KL_perm_p\": p_kl})\n",
    "kl_df = pd.DataFrame(kl_rows)\n",
    "\n",
    "# --------- Save summary & tables ----------\n",
    "summary = {\n",
    "    \"EOEC_mean_CI\": [eoec_mean, eoec_lo, eoec_hi],\n",
    "    \"EOEC_vs_baseline\": {\"wilcoxon_stat\": st_e, \"p\": p_e, \"effect_r\": r_e, \"n\": n_e},\n",
    "    \"TaskAUC_mean_CI\": [task_mean, task_lo, task_hi],\n",
    "    \"Task_vs_baseline\": {\"wilcoxon_stat\": st_t, \"p\": p_t, \"effect_r\": r_t, \"n\": n_t},\n",
    "    \"Predictive_gain_xent\": {\"mean_CI\": [dg_mean, dg_lo, dg_hi], \"wilcoxon_stat\": stat_d, \"p\": p_d, \"effect_r\": r_d, \"n\": int(len(delta_xent))},\n",
    "    \"Microstate_MI\": {\"MI_bits\": MI_bits, \"perm_mean\": perm_mean, \"perm_p\": perm_p}\n",
    "}\n",
    "with open(ANAREP/\"stats_summary.json\",\"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "kl_df.to_csv(ANAREP/\"microstate_letter_KL_perm.csv\", index=False)\n",
    "\n",
    "# Also emit a compact CSV of all headline numbers\n",
    "headline = pd.DataFrame({\n",
    "    \"metric\": [\n",
    "        \"EOEC_mean\", \"EOEC_CI_lo\", \"EOEC_CI_hi\",\n",
    "        \"EOEC_vs_baseline_p\", \"EOEC_vs_baseline_r\",\n",
    "        \"Task_meanAUC\", \"Task_CI_lo\", \"Task_CI_hi\",\n",
    "        \"Task_vs_baseline_p\", \"Task_vs_baseline_r\",\n",
    "        \"Pred_gain_mean\", \"Pred_gain_CI_lo\", \"Pred_gain_CI_hi\",\n",
    "        \"Pred_gain_p\", \"Pred_gain_r\",\n",
    "        \"MI_bits\", \"MI_perm_p\"\n",
    "    ],\n",
    "    \"value\": [\n",
    "        eoec_mean, eoec_lo, eoec_hi,\n",
    "        p_e, r_e,\n",
    "        task_mean, task_lo, task_hi,\n",
    "        p_t, r_t,\n",
    "        dg_mean, dg_lo, dg_hi,\n",
    "        p_d, r_d,\n",
    "        MI_bits, perm_p\n",
    "    ]\n",
    "})\n",
    "headline.to_csv(ANAREP/\"stats_tables.csv\", index=False)\n",
    "\n",
    "# --------- Make a tiny figure panel ----------\n",
    "fig = plt.figure(figsize=(10.5, 5.5), dpi=140)\n",
    "gs  = fig.add_gridspec(2, 3, hspace=0.35, wspace=0.35)\n",
    "\n",
    "ax1 = fig.add_subplot(gs[0,0])\n",
    "ax1.errorbar([0], [eoec_mean], yerr=[[eoec_mean-eoec_lo],[eoec_hi-eoec_mean]], fmt='o', capsize=4)\n",
    "ax1.set_xlim(-0.5,0.5); ax1.set_ylim(0,1); ax1.set_title(f\"EO/EC mean Â±95% CI\\np={p_e:.3g}, r={r_e:.2f} (n={n_e})\"); ax1.set_xticks([])\n",
    "\n",
    "ax2 = fig.add_subplot(gs[0,1])\n",
    "ax2.errorbar([0], [task_mean], yerr=[[task_mean-task_lo],[task_hi-task_mean]], fmt='o', capsize=4, color='tab:green')\n",
    "ax2.set_xlim(-0.5,0.5); ax2.set_ylim(0.5,1.0); ax2.set_title(f\"R03 AUC mean Â±95% CI\\np={p_t:.3g}, r={r_t:.2f} (n={n_t})\"); ax2.set_xticks([])\n",
    "\n",
    "ax3 = fig.add_subplot(gs[0,2])\n",
    "ax3.errorbar([0], [dg_mean], yerr=[[dg_mean-dg_lo],[dg_hi-dg_mean]], fmt='o', capsize=4, color='tab:purple')\n",
    "ax3.axhline(0, linestyle=\"--\", color=\"gray\"); ax3.set_xlim(-0.5,0.5)\n",
    "ax3.set_title(f\"Predictive gain Î”xent (mean Â±95% CI)\\n p={p_d:.3g}, r={r_d:.2f} (n={len(delta_xent)})\"); ax3.set_xticks([])\n",
    "\n",
    "ax4 = fig.add_subplot(gs[1,0]); ax4.axis(\"off\")\n",
    "ax4.text(0,1, f\"EO/EC mean={eoec_mean:.3f} [{eoec_lo:.3f},{eoec_hi:.3f}]\\n\"\n",
    "              f\"Task AUC mean={task_mean:.3f} [{task_lo:.3f},{task_hi:.3f}]\\n\"\n",
    "              f\"MI(letter;Î¼)={MI_bits:.3f} bits (perm p={perm_p:.3g})\",\n",
    "         va=\"top\", fontsize=10)\n",
    "\n",
    "ax5 = fig.add_subplot(gs[1,1]); ax5.axis(\"off\")\n",
    "ax5.text(0,1, f\"Wilcoxon paired vs baseline:\\n\"\n",
    "              f\"â€¢ EO/EC: p={p_e:.3g}, r={r_e:.2f}\\n\"\n",
    "              f\"â€¢ Task: p={p_t:.3g}, r={r_t:.2f}\",\n",
    "         va=\"top\", fontsize=10)\n",
    "\n",
    "ax6 = fig.add_subplot(gs[1,2]); ax6.axis(\"off\")\n",
    "ax6.text(0,1, \"Notes:\\nâ€¢ 95% CIs via bootstrap (B=50,000; subject resampling).\\n\"\n",
    "              \"â€¢ Effect size r = Z/âˆšn from Wilcoxon.\\n\"\n",
    "              \"â€¢ MI bootstrap omitted here; MI permutation already included.\\n\"\n",
    "              \"â€¢ KL per-letter permutation approximated via Dirichlet null; replace with epoch-level shuffle if desired.\",\n",
    "         va=\"top\", fontsize=9)\n",
    "\n",
    "PTH = ANAREP/\"stats_summary_panel.png\"\n",
    "fig.suptitle(\"CNT v0.3 â€” Statistical Summary\", fontsize=14, weight=\"bold\")\n",
    "fig.savefig(PTH, bbox_inches=\"tight\"); plt.close(fig)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\"  - stats_summary.json  â†’\", ANAREP/\"stats_summary.json\")\n",
    "print(\"  - stats_tables.csv    â†’\", ANAREP/\"stats_tables.csv\")\n",
    "print(\"  - KL perm table       â†’\", ANAREP/\"microstate_letter_KL_perm.csv\")\n",
    "print(\"  - summary figure      â†’\", PTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "72d0b7e1-b221-44ab-be1f-cf477c06343f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Saved â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\analysis\\microstate_epoch_stats.json\n",
      "Saved â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\analysis\\microstate_epoch_KL.csv\n"
     ]
    }
   ],
   "source": [
    "# === Epoch-level microstate rigor: MI bootstrap + letter-wise KL permutation (S001 R01â€“R03) ===\n",
    "import os, json, numpy as np, pandas as pd, mne, requests, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy.signal import welch\n",
    "from numpy.random import default_rng\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "RUN   = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")\n",
    "OUT   = RUN.parent / \"cog_alphabet_report_hybrid_v1\" / \"analysis\"\n",
    "MSOUT = RUN.parent / \"cog_alphabet_report_hybrid_v1\" / \"microstates\"\n",
    "for p in [OUT, MSOUT]: p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Find or fetch S001 EDFs\n",
    "cands=[RUN/\"brainwaves_rebuilt\", RUN.parent/\"s001_edf\", RUN.parent]\n",
    "RAW=None\n",
    "for d in cands:\n",
    "    if d.exists() and all((d/f\"S001R{r:02d}.edf\").exists() for r in [1,2,3]): RAW=d; break\n",
    "if RAW is None:\n",
    "    RAW = RUN.parent/\"s001_edf\"; RAW.mkdir(exist_ok=True)\n",
    "    for r in [1,2,3]:\n",
    "        url=f\"https://physionet.org/files/eegmmidb/1.0.0/S001/S001R{r:02d}.edf\"\n",
    "        dest=RAW/f\"S001R{r:02d}.edf\"\n",
    "        if not dest.exists():\n",
    "            with requests.get(url, stream=True, timeout=60) as resp:\n",
    "                resp.raise_for_status()\n",
    "                with open(dest, \"wb\") as f:\n",
    "                    for ch in resp.iter_content(8192):\n",
    "                        if ch: f.write(ch)\n",
    "\n",
    "# ---- Helpers\n",
    "TARGET_SF=250.0\n",
    "def load_raw(p):\n",
    "    raw=mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"]-TARGET_SF)>1e-6: raw.resample(TARGET_SF, npad=\"auto\", verbose=False)\n",
    "    raw.filter(2.0,20.0, verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "        try:\n",
    "            raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"), on_missing=\"ignore\")\n",
    "        except Exception: pass\n",
    "    return raw\n",
    "\n",
    "def gfp(x): return x.std(axis=0)\n",
    "def peaks(x, sf, min_gap_ms=10, max_peaks=12000):\n",
    "    from scipy.signal import find_peaks\n",
    "    d=int((min_gap_ms/1000.0)*sf); g=gfp(x); pk,_=find_peaks(g, distance=max(d,1))\n",
    "    if len(pk)>max_peaks: pk = np.linspace(0,len(pk)-1,max_peaks).astype(int)\n",
    "    return pk\n",
    "\n",
    "def kmeans_maps_polarity_agnostic(X, K=4, iters=60, rs=42):\n",
    "    rng=default_rng(rs)\n",
    "    X = X - X.mean(axis=1, keepdims=True)\n",
    "    X = X / (np.linalg.norm(X, axis=1, keepdims=True)+1e-12)\n",
    "    C = X[rng.choice(len(X), K, replace=False)].copy()\n",
    "    for _ in range(iters):\n",
    "        corr = X @ C.T\n",
    "        lab  = np.argmax(np.abs(corr), axis=1)\n",
    "        sign = np.sign(corr[np.arange(len(X)), lab])\n",
    "        Cn=[]\n",
    "        for k in range(K):\n",
    "            m=lab==k\n",
    "            v = (X[m]*sign[m][:,None]).mean(axis=0) if np.any(m) else C[k]\n",
    "            v = v/(np.linalg.norm(v)+1e-12)\n",
    "            Cn.append(v)\n",
    "        Cn=np.stack(Cn,0)\n",
    "        if np.allclose(C,Cn,atol=1e-5): break\n",
    "        C=Cn\n",
    "    return C\n",
    "\n",
    "def backfit(raw, maps):\n",
    "    X=raw.get_data()\n",
    "    M=maps/(np.linalg.norm(maps,axis=1,keepdims=True)+1e-12)\n",
    "    Xn=X/(np.linalg.norm(X,axis=0,keepdims=True)+1e-12)\n",
    "    corr = M @ Xn\n",
    "    return np.argmax(np.abs(corr), axis=0)  # per-sample Î¼ label\n",
    "\n",
    "def epoch_dominant_ms(ms_labels, sf, starts, ends, K=4):\n",
    "    out=[]\n",
    "    for t0,t1 in zip(starts, ends):\n",
    "        s=int(round(t0*sf)); e=int(round(t1*sf))\n",
    "        arr = ms_labels[s:e]\n",
    "        if e<=s or len(arr)==0: out.append(-1)\n",
    "        else: out.append(np.bincount(arr, minlength=K).argmax())\n",
    "    return np.array(out, int)\n",
    "\n",
    "# ---- Get S001 v0.2 letters & epoch bounds for each run\n",
    "meta = pd.read_csv(RUN/\"metadata.csv\").sort_values([\"file\",\"t_start_s\",\"t_end_s\"]).reset_index(drop=True)\n",
    "letters_all = pd.read_csv(RUN/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "\n",
    "def run_epochs(file_name):\n",
    "    mask=(meta[\"file\"]==file_name).to_numpy()\n",
    "    return meta.loc[mask, [\"t_start_s\",\"t_end_s\"]].to_numpy(), letters_all[mask]\n",
    "\n",
    "# ---- Build pooled templates from all three runs\n",
    "Xpool=[]; info=None\n",
    "for r in [1,2,3]:\n",
    "    raw=load_raw(RAW/f\"S001R{r:02d}.edf\")\n",
    "    if info is None: info=raw.info\n",
    "    pk=peaks(raw.get_data(), raw.info[\"sfreq\"])\n",
    "    Xpool.append(raw.get_data()[:, pk].T)\n",
    "TEMPLATES = kmeans_maps_polarity_agnostic(np.vstack(Xpool), K=4, iters=80, rs=123)\n",
    "\n",
    "# ---- Construct epoch-level table across the three runs\n",
    "rows=[]\n",
    "for r in [1,2,3]:\n",
    "    fname=f\"S001R{r:02d}.edf\"\n",
    "    raw=load_raw(RAW/fname); sf=raw.info[\"sfreq\"]\n",
    "    pk=peaks(raw.get_data(), sf)\n",
    "    # backfit using run-specific maps for realism\n",
    "    MAPS = kmeans_maps_polarity_agnostic(raw.get_data()[:, pk].T, K=4, iters=60, rs=777+r)\n",
    "    ms_lab = backfit(raw, MAPS)  # per-sample Î¼\n",
    "    ep, L = run_epochs(fname)\n",
    "    starts, ends = ep[:,0], ep[:,1]\n",
    "    dom = epoch_dominant_ms(ms_lab, sf, starts, ends, K=4)\n",
    "    valid = dom >= 0\n",
    "    for i,ok in enumerate(valid):\n",
    "        if not ok: continue\n",
    "        rows.append({\"run\":fname, \"letter\": int(L[i]), \"ms\": int(dom[i])})\n",
    "E = pd.DataFrame(rows)  # columns: run, letter, ms\n",
    "\n",
    "# ---- MI(letter; Î¼): bootstrap CI + epoch-level permutation p\n",
    "def entropy(p):\n",
    "    p = p[p>0]; return -np.sum(p*np.log2(p))\n",
    "def mutual_info(df):\n",
    "    L = df[\"letter\"].to_numpy().astype(int)\n",
    "    M = df[\"ms\"].to_numpy().astype(int)\n",
    "    K  = max(L.max(), M.max())+1\n",
    "    # empirical\n",
    "    P_L = np.bincount(L, minlength=K) / len(L)\n",
    "    P_M = np.bincount(M, minlength=K) / len(M)\n",
    "    H_M = entropy(P_M)\n",
    "    H_M_given_L = 0.0\n",
    "    for s in range(K):\n",
    "        mask=(L==s)\n",
    "        if not np.any(mask): continue\n",
    "        P = np.bincount(M[mask], minlength=K) / mask.sum()\n",
    "        H_M_given_L += mask.mean() * entropy(P)\n",
    "    return H_M - H_M_given_L  # bits\n",
    "\n",
    "# Bootstrap CI (epochs)\n",
    "rng = default_rng(42)\n",
    "B = 5000\n",
    "boots=[]\n",
    "for b in range(B):\n",
    "    idx = rng.choice(len(E), size=len(E), replace=True)\n",
    "    boots.append(mutual_info(E.iloc[idx]))\n",
    "boots = np.array(boots)\n",
    "MI_est = float(mutual_info(E))\n",
    "MI_lo, MI_hi = np.quantile(boots, [0.025, 0.975])\n",
    "\n",
    "# Permutation p (epoch labels shuffled)\n",
    "P = 5000\n",
    "null=[]\n",
    "for p in range(P):\n",
    "    Lperm = rng.permutation(E[\"letter\"].to_numpy())\n",
    "    null.append(mutual_info(pd.DataFrame({\"letter\":Lperm, \"ms\":E[\"ms\"]})))\n",
    "perm_p = float((np.sum(np.array(null) >= MI_est) + 1) / (P+1))\n",
    "\n",
    "# ---- Per-letter KL with epoch shuffle p-values\n",
    "def KL_for_letter(df, s, K=4):\n",
    "    L = df[\"letter\"].to_numpy().astype(int)\n",
    "    M = df[\"ms\"].to_numpy().astype(int)\n",
    "    Pglob = np.bincount(M, minlength=K) / len(M)\n",
    "    mask=(L==s)\n",
    "    if not np.any(mask): return np.nan\n",
    "    Ps = np.bincount(M[mask], minlength=K) / mask.sum()\n",
    "    return float(np.sum(Ps * (np.log2(np.where(Ps>0, Ps, 1)) - np.log2(np.where(Pglob>0, Pglob, 1)))))\n",
    "\n",
    "KL_rows=[]\n",
    "Klett = int(E[\"letter\"].max())+1\n",
    "for s in range(Klett):\n",
    "    KL_obs = KL_for_letter(E, s, K=4)\n",
    "    # epoch shuffle p for this letter\n",
    "    z=0\n",
    "    for p in range(5000):\n",
    "        Lperm = rng.permutation(E[\"letter\"].to_numpy())\n",
    "        KL_null = KL_for_letter(pd.DataFrame({\"letter\":Lperm, \"ms\":E[\"ms\"]}), s, K=4)\n",
    "        z += (KL_null >= KL_obs)\n",
    "    pval = float((z+1)/5001)\n",
    "    KL_rows.append({\"letter\": int(s), \"KL_bits\": KL_obs, \"KL_perm_p\": pval})\n",
    "KL_df = pd.DataFrame(KL_rows).sort_values(\"letter\")\n",
    "\n",
    "# ---- Save\n",
    "with open(OUT/\"microstate_epoch_stats.json\",\"w\") as f:\n",
    "    json.dump({\"MI_bits\": MI_est, \"MI_CI\":[float(MI_lo), float(MI_hi)], \"perm_p\": perm_p,\n",
    "               \"n_epochs\": int(len(E))}, f, indent=2)\n",
    "KL_df.to_csv(OUT/\"microstate_epoch_KL.csv\", index=False)\n",
    "print(\"Saved â†’\", OUT/\"microstate_epoch_stats.json\")\n",
    "print(\"Saved â†’\", OUT/\"microstate_epoch_KL.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b725747-36a4-40fc-8c60-4cc778625289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Saved â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\analysis\\microstate_epoch_stats_subjects.json\n",
      "Mechanism Panel v2 â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\microstates\\CNT_CognitiveAlphabet_Mechanism_Panel_v2.png\n",
      "Copied to bundle/report â†’ E:\\CNT\\artifacts\\CNT_CognitiveAlphabet_v0_3\\report\n",
      "Updated scoreboard â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\generalization\\v0_3_scoreboard.json\n",
      "Appended mechanism line to README.\n"
     ]
    }
   ],
   "source": [
    "# === CNT v0.3 â€” Mechanism patch + S002/S003 epoch-level MI + Scoreboard/README update (one cell) ===\n",
    "import os, json, shutil, numpy as np, pandas as pd, matplotlib.pyplot as plt, matplotlib.image as mpimg, mne, requests\n",
    "from pathlib import Path\n",
    "from scipy.signal import welch\n",
    "from numpy.random import default_rng\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# -------------------- Paths --------------------\n",
    "RUN     = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")\n",
    "REP     = RUN.parent / \"cog_alphabet_report_hybrid_v1\"\n",
    "MSREP   = REP / \"microstates\"\n",
    "ANAREP  = REP / \"analysis\"\n",
    "GENREP  = REP / \"generalization\"\n",
    "BUNDLE  = RUN.parent / \"CNT_CognitiveAlphabet_v0_3\"\n",
    "GENDATA = RUN.parent / \"generalization_data\"\n",
    "\n",
    "for p in [MSREP, ANAREP, GENREP]: p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -------------------- S001 mechanism inputs --------------------\n",
    "S001_stats_json = ANAREP / \"microstate_epoch_stats.json\"         # MI CI & p (S001)\n",
    "S001_kl_csv     = ANAREP / \"microstate_epoch_KL.csv\"             # per-letter KL (epoch shuffle p)\n",
    "templates_png   = MSREP  / \"templates_ABCD_topomaps.png\"\n",
    "heatmap_png     = MSREP  / \"task_heatmap_R03_letters_by_ms.png\"\n",
    "ms_summary_csv  = MSREP  / \"microstate_summary_per_letter.csv\"   # per-letter Î¼ bars\n",
    "\n",
    "# Load S001 stats (compute fallback if missing)\n",
    "assert S001_stats_json.exists(), \"Missing S001 microstate_epoch_stats.json â€” run epoch-level MI cell first.\"\n",
    "s001_stats = json.load(open(S001_stats_json))\n",
    "mi_S001, mi_lo_S001, mi_hi_S001, p_S001, n_epochs_S001 = s001_stats[\"MI_bits\"], s001_stats[\"MI_CI\"][0], s001_stats[\"MI_CI\"][1], s001_stats[\"perm_p\"], s001_stats[\"n_epochs\"]\n",
    "s001_kl = pd.read_csv(S001_kl_csv) if S001_kl_csv.exists() else None\n",
    "ms_summary = pd.read_csv(ms_summary_csv, index_col=0) if ms_summary_csv.exists() else None\n",
    "\n",
    "# -------------------- Utilities for subjects --------------------\n",
    "TARGET_SF=250.0\n",
    "def load_raw_for_ms(p):\n",
    "    raw=mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"]-TARGET_SF)>1e-6: raw.resample(TARGET_SF, npad=\"auto\", verbose=False)\n",
    "    raw.filter(2.0,20.0, verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "        try:\n",
    "            raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"), on_missing=\"ignore\")\n",
    "        except Exception: pass\n",
    "    return raw\n",
    "\n",
    "def gfp(x): return x.std(axis=0)\n",
    "def peaks(x, sf, min_gap_ms=10, max_peaks=12000):\n",
    "    from scipy.signal import find_peaks\n",
    "    d=int((min_gap_ms/1000.0)*sf); g=gfp(x); pk,_=find_peaks(g, distance=max(d,1))\n",
    "    if len(pk)>max_peaks: pk = np.linspace(0,len(pk)-1,max_peaks).astype(int)\n",
    "    return pk\n",
    "\n",
    "def kmeans_maps_polarity_agnostic(X, K=4, iters=60, rs=42):\n",
    "    rng=default_rng(rs)\n",
    "    X = X - X.mean(axis=1, keepdims=True)\n",
    "    X = X / (np.linalg.norm(X, axis=1, keepdims=True)+1e-12)\n",
    "    C = X[rng.choice(len(X), K, replace=False)].copy()\n",
    "    for _ in range(iters):\n",
    "        corr = X @ C.T\n",
    "        lab  = np.argmax(np.abs(corr), axis=1)\n",
    "        sign = np.sign(corr[np.arange(len(X)), lab])\n",
    "        Cn=[]\n",
    "        for k in range(4):\n",
    "            m=lab==k\n",
    "            v=(X[m]*sign[m][:,None]).mean(axis=0) if np.any(m) else C[k]\n",
    "            v=v/(np.linalg.norm(v)+1e-12)\n",
    "            Cn.append(v)\n",
    "        Cn=np.stack(Cn,0)\n",
    "        if np.allclose(C,Cn,atol=1e-5): break\n",
    "        C=Cn\n",
    "    return C\n",
    "\n",
    "def backfit(raw, maps):\n",
    "    X=raw.get_data()\n",
    "    M=maps/(np.linalg.norm(maps,axis=1,keepdims=True)+1e-12)\n",
    "    Xn=X/(np.linalg.norm(X,axis=0,keepdims=True)+1e-12)\n",
    "    corr=M @ Xn\n",
    "    return np.argmax(np.abs(corr), axis=0)  # per-sample Î¼ label\n",
    "\n",
    "def epoch_dom_ms(ms_labels, sf, starts, ends):\n",
    "    out=[]\n",
    "    for t0,t1 in zip(starts, ends):\n",
    "        s=int(round(t0*sf)); e=int(round(t1*sf))\n",
    "        arr=ms_labels[s:e]\n",
    "        out.append(np.bincount(arr, minlength=4).argmax() if e>s and len(arr)>0 else -1)\n",
    "    return np.array(out, int)\n",
    "\n",
    "def mutual_info_letter_mu(L, M, K=4):\n",
    "    L=np.asarray(L,int); M=np.asarray(M,int)\n",
    "    Pm=np.bincount(M, minlength=K)/len(M); Hm=-np.sum(Pm[Pm>0]*np.log2(Pm[Pm>0]))\n",
    "    Hm_given_L=0.0\n",
    "    for s in range(K):\n",
    "        mask=(L==s)\n",
    "        if not np.any(mask): continue\n",
    "        P=np.bincount(M[mask], minlength=K)/mask.sum()\n",
    "        Hm_given_L += mask.mean()*(-np.sum(P[P>0]*np.log2(P[P>0])))\n",
    "    return Hm - Hm_given_L\n",
    "\n",
    "def compute_epoch_level_mi_for_subject(subj, B=3000, P=3000):\n",
    "    \"\"\"Uses decode_{subj}_*.csv for epoch times & letters; EDFs from generalization_data.\"\"\"\n",
    "    rows=[]\n",
    "    for dec in GENREP.glob(f\"decode_{subj}_*.csv\"):\n",
    "        df=pd.read_csv(dec)\n",
    "        if not {\"file\",\"t_start_s\",\"t_end_s\",\"state\"}.issubset(df.columns): continue\n",
    "        edf = GENDATA/df[\"file\"].iloc[0]\n",
    "        if not edf.exists(): continue\n",
    "        raw=load_raw_for_ms(edf); sf=raw.info[\"sfreq\"]\n",
    "        pk=peaks(raw.get_data(), sf)\n",
    "        maps=kmeans_maps_polarity_agnostic(raw.get_data()[:,pk].T, K=4, iters=60, rs=555)\n",
    "        ms_labels=backfit(raw, maps)\n",
    "        dom = epoch_dom_ms(ms_labels, sf, df[\"t_start_s\"].to_numpy(), df[\"t_end_s\"].to_numpy())\n",
    "        mask=(dom>=0)\n",
    "        if not np.any(mask): continue\n",
    "        rows.append(pd.DataFrame({\"letter\": df[\"state\"].astype(int).to_numpy()[mask],\n",
    "                                  \"ms\": dom[mask]}))\n",
    "    if not rows:\n",
    "        return None\n",
    "    E=pd.concat(rows, ignore_index=True)\n",
    "    L=E[\"letter\"].to_numpy(); M=E[\"ms\"].to_numpy()\n",
    "    rng=default_rng(123)\n",
    "    # bootstrap MI CI\n",
    "    boots=[]\n",
    "    for _ in range(B):\n",
    "        idx=rng.choice(len(E), size=len(E), replace=True)\n",
    "        boots.append(mutual_info_letter_mu(L[idx], M[idx]))\n",
    "    boots=np.array(boots); lo,hi=np.quantile(boots,[0.025,0.975])\n",
    "    mi_est=float(mutual_info_letter_mu(L,M))\n",
    "    # permutation p\n",
    "    null=[]\n",
    "    for _ in range(P):\n",
    "        Lperm=rng.permutation(L)\n",
    "        null.append(mutual_info_letter_mu(Lperm,M))\n",
    "    null=np.array(null); p=float((np.sum(null>=mi_est)+1)/(P+1))\n",
    "    return {\"subject\":subj, \"MI_bits\":mi_est, \"CI\":[float(lo),float(hi)], \"perm_p\":p, \"n_epochs\": int(len(E))}\n",
    "\n",
    "# -------------------- 2) Compute S002/S003 MI --------------------\n",
    "subj_stats=[]\n",
    "for subj in [\"S002\",\"S003\"]:\n",
    "    try:\n",
    "        res=compute_epoch_level_mi_for_subject(subj, B=3000, P=3000)\n",
    "        if res: subj_stats.append(res); \n",
    "    except Exception as e:\n",
    "        print(\"MI computation skipped for\", subj, \":\", e)\n",
    "\n",
    "if subj_stats:\n",
    "    with open(ANAREP/\"microstate_epoch_stats_subjects.json\",\"w\") as f:\n",
    "        json.dump({\"subjects\": subj_stats}, f, indent=2)\n",
    "    print(\"Saved â†’\", ANAREP/\"microstate_epoch_stats_subjects.json\")\n",
    "\n",
    "# -------------------- 3) Patch Mechanism Panel with MI CI/p and KL table --------------------\n",
    "# load existing images\n",
    "templates_img = mpimg.imread(templates_png) if templates_png.exists() else None\n",
    "heat_img      = mpimg.imread(heatmap_png)   if heatmap_png.exists()   else None\n",
    "\n",
    "# build small KL text (S001)\n",
    "kl_txt = \"KL(P(Î¼|letter)â€–P(Î¼)):\\n\"\n",
    "if s001_kl is not None:\n",
    "    for _,r in s001_kl.sort_values(\"letter\").iterrows():\n",
    "        kl_txt += f\"S{int(r['letter'])}: {float(r['KL_bits']):.3f} bits (p={float(r['KL_perm_p']):.3g})\\n\"\n",
    "else:\n",
    "    kl_txt += \"n/a\\n\"\n",
    "\n",
    "# make panel v2\n",
    "fig = plt.figure(figsize=(13,8), dpi=140)\n",
    "gs = fig.add_gridspec(2,3, hspace=0.35, wspace=0.30)\n",
    "\n",
    "# templates\n",
    "axA=fig.add_subplot(gs[0,0]); axA.axis(\"off\"); axA.set_title(\"S001 microstate templates (A/B/C/D)\", fontsize=12, pad=6)\n",
    "axA.imshow(templates_img) if templates_img is not None else axA.text(0.5,0.5,\"templates image missing\", ha=\"center\")\n",
    "\n",
    "# heatmap\n",
    "axB=fig.add_subplot(gs[0,1]); axB.axis(\"off\"); axB.set_title(\"S001R03: task probability by letter Ã— Î¼\", fontsize=12, pad=6)\n",
    "axB.imshow(heat_img) if heat_img is not None else axB.text(0.5,0.5,\"heatmap missing\", ha=\"center\")\n",
    "\n",
    "# coupling block with MI and KL\n",
    "axC=fig.add_subplot(gs[0,2]); axC.axis(\"off\"); axC.set_title(\"Letterâ‡„Microstate coupling\", fontsize=12, pad=6)\n",
    "txt = f\"MI(letter; Î¼) = {mi_S001:.3f} bits [95% CI {mi_lo_S001:.3f}, {mi_hi_S001:.3f}], p={p_S001:.3g}  (n={n_epochs_S001})\\n\\n\" + kl_txt\n",
    "if subj_stats:\n",
    "    for s in subj_stats:\n",
    "        txt += f\"\\n{subj_stats[0]['subject']} MI={s['MI_bits']:.3f} bits [CI {s['CI'][0]:.3f},{s['CI'][1]:.3f}] p={s['perm_p']:.3g} (n={s['n_epochs']})\"\n",
    "axC.text(0.02,0.02, txt, va=\"bottom\", fontsize=10, family=\"monospace\")\n",
    "\n",
    "# per-letter bars (S001)\n",
    "def bar_or_text(ax, row, title):\n",
    "    if row is None: ax.axis(\"off\"); ax.text(0.5,0.5,\"n/a\", ha=\"center\"); return\n",
    "    ax.bar([f\"Î¼{k}\" for k in range(len(row))], row.values); ax.set_ylim(0,1); ax.set_ylabel(\"fraction\"); ax.set_title(title, fontsize=11)\n",
    "\n",
    "axD=fig.add_subplot(gs[1,0]); axE=fig.add_subplot(gs[1,1]); axF=fig.add_subplot(gs[1,2])\n",
    "bar_or_text(axD, ms_summary.loc[0] if (ms_summary is not None and 0 in ms_summary.index) else None, \"S0 microstate mix\")\n",
    "bar_or_text(axE, ms_summary.loc[1] if (ms_summary is not None and 1 in ms_summary.index) else None, \"S1 microstate mix\")\n",
    "bar_or_text(axF, ms_summary.loc[2] if (ms_summary is not None and 2 in ms_summary.index) else None, \"S2 microstate mix\")\n",
    "fig.suptitle(\"CNT Cognitive Alphabet â€” Mechanism Panel (v2)\", fontsize=16, weight=\"bold\")\n",
    "\n",
    "PANEL_PNG = MSREP/\"CNT_CognitiveAlphabet_Mechanism_Panel_v2.png\"\n",
    "PANEL_PDF = MSREP/\"CNT_CognitiveAlphabet_Mechanism_Panel_v2.pdf\"\n",
    "fig.savefig(PANEL_PNG, bbox_inches=\"tight\"); fig.savefig(PANEL_PDF, bbox_inches=\"tight\"); plt.close(fig)\n",
    "print(\"Mechanism Panel v2 â†’\", PANEL_PNG)\n",
    "\n",
    "# copy into bundle/report if present\n",
    "if BUNDLE.exists():\n",
    "    dest = BUNDLE/\"report\"; dest.mkdir(parents=True, exist_ok=True)\n",
    "    for p in [PANEL_PNG, PANEL_PDF]:\n",
    "        try: shutil.copy2(p, dest/p.name)\n",
    "        except Exception: pass\n",
    "    print(\"Copied to bundle/report â†’\", dest)\n",
    "\n",
    "# -------------------- 4) Patch scoreboard & README --------------------\n",
    "# scoreboard json\n",
    "score_json = GENREP/\"v0_3_scoreboard.json\"\n",
    "if score_json.exists():\n",
    "    score = json.load(open(score_json))\n",
    "else:\n",
    "    score = {}\n",
    "score[\"MI_S001_bits\"] = mi_S001\n",
    "score[\"MI_S001_CI\"]   = [mi_lo_S001, mi_hi_S001]\n",
    "score[\"MI_S001_p\"]    = p_S001\n",
    "if subj_stats:\n",
    "    score[\"MI_subjects\"] = {s[\"subject\"]: {\"MI_bits\": s[\"MI_bits\"], \"CI\": s[\"CI\"], \"p\": s[\"perm_p\"], \"n_epochs\": s[\"n_epochs\"]} for s in subj_stats}\n",
    "with open(score_json, \"w\") as f: json.dump(score, f, indent=2)\n",
    "print(\"Updated scoreboard â†’\", score_json)\n",
    "\n",
    "# README line\n",
    "readme = BUNDLE/\"README_v0_3.md\"\n",
    "mi_line = f\"\\n## Mechanism\\n- S001: MI(letter; Î¼) = {mi_S001:.3f} bits [95% CI {mi_lo_S001:.3f}, {mi_hi_S001:.3f}], p={p_S001:.3g} (n={n_epochs_S001}).\"\n",
    "if subj_stats:\n",
    "    mi_line += \"  \" + \"  \".join([f\"{s['subject']}: MI={s['MI_bits']:.3f} [CI {s['CI'][0]:.3f},{s['CI'][1]:.3f}], p={s['perm_p']:.3g}\" for s in subj_stats])\n",
    "try:\n",
    "    with open(readme, \"a\", encoding=\"utf-8\") as f: f.write(mi_line+\"\\n\")\n",
    "    print(\"Appended mechanism line to README.\")\n",
    "except Exception as e:\n",
    "    print(\"README patch skipped:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3149cbcc-7dcc-4354-82b4-82fac2f858dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Saved â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\analysis\\microstate_epoch_KL_S002.csv\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Saved â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\analysis\\microstate_epoch_KL_S003.csv\n",
      "Updated Î”xent CI in: E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\analysis\\stats_tables.csv\n",
      "Appended claims to README: E:\\CNT\\artifacts\\CNT_CognitiveAlphabet_v0_3\\README_v0_3.md\n"
     ]
    }
   ],
   "source": [
    "# === CNT v0.3 â€” Per-letter KL (S002/S003) + Î”xent CI refresh + README claims (one cell) ===\n",
    "import os, json, numpy as np, pandas as pd, mne, requests, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from scipy.signal import welch\n",
    "from numpy.random import default_rng\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# ---------- Paths ----------\n",
    "RUN     = Path(r\"E:\\CNT\\artifacts\\cog_alphabet_hybrid_v1\")\n",
    "REP     = RUN.parent / \"cog_alphabet_report_hybrid_v1\"\n",
    "ANAREP  = REP / \"analysis\"\n",
    "GENREP  = REP / \"generalization\"\n",
    "BUNDLE  = RUN.parent / \"CNT_CognitiveAlphabet_v0_3\"\n",
    "GENDATA = RUN.parent / \"generalization_data\"\n",
    "for p in [ANAREP, GENREP]: p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Utilities ----------\n",
    "TARGET_SF=250.0\n",
    "\n",
    "def load_raw_for_ms(p):\n",
    "    raw=mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"]-TARGET_SF)>1e-6: raw.resample(TARGET_SF, npad=\"auto\", verbose=False)\n",
    "    raw.filter(2.0,20.0, verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "        try:\n",
    "            raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"), on_missing=\"ignore\")\n",
    "        except Exception: pass\n",
    "    return raw\n",
    "\n",
    "def gfp(x): return x.std(axis=0)\n",
    "def peaks(x, sf, min_gap_ms=10, max_peaks=12000):\n",
    "    from scipy.signal import find_peaks\n",
    "    d=int((min_gap_ms/1000.0)*sf)\n",
    "    g=gfp(x); pk,_=find_peaks(g, distance=max(d,1))\n",
    "    if len(pk)>max_peaks: pk=np.linspace(0,len(pk)-1,max_peaks).astype(int)\n",
    "    return pk\n",
    "\n",
    "def kmeans_maps_polarity_agnostic(X, K=4, iters=60, rs=42):\n",
    "    rng=default_rng(rs)\n",
    "    X = X - X.mean(axis=1, keepdims=True)\n",
    "    X = X / (np.linalg.norm(X, axis=1, keepdims=True)+1e-12)\n",
    "    C = X[rng.choice(len(X), K, replace=False)].copy()\n",
    "    for _ in range(iters):\n",
    "        corr = X @ C.T\n",
    "        lab  = np.argmax(np.abs(corr), axis=1)\n",
    "        sign = np.sign(corr[np.arange(len(X)), lab])\n",
    "        Cn=[]\n",
    "        for k in range(4):\n",
    "            m=lab==k\n",
    "            v=(X[m]*sign[m][:,None]).mean(axis=0) if np.any(m) else C[k]\n",
    "            v=v/(np.linalg.norm(v)+1e-12)\n",
    "            Cn.append(v)\n",
    "        Cn=np.stack(Cn,0)\n",
    "        if np.allclose(C,Cn,atol=1e-5): break\n",
    "        C=Cn\n",
    "    return C\n",
    "\n",
    "def backfit(raw, maps):\n",
    "    X=raw.get_data()\n",
    "    M=maps/(np.linalg.norm(maps,axis=1,keepdims=True)+1e-12)\n",
    "    Xn=X/(np.linalg.norm(X,axis=0,keepdims=True)+1e-12)\n",
    "    corr=M @ Xn\n",
    "    return np.argmax(np.abs(corr), axis=0)\n",
    "\n",
    "def epoch_dom_ms(ms_labels, sf, starts, ends, K=4):\n",
    "    out=[]\n",
    "    for t0,t1 in zip(starts, ends):\n",
    "        s=int(round(t0*sf)); e=int(round(t1*sf))\n",
    "        arr=ms_labels[s:e]\n",
    "        out.append(np.bincount(arr, minlength=K).argmax() if e>s and len(arr)>0 else -1)\n",
    "    return np.array(out, int)\n",
    "\n",
    "def KL_for_letter(df, s, K=4):\n",
    "    L=df[\"letter\"].to_numpy().astype(int)\n",
    "    M=df[\"ms\"].to_numpy().astype(int)\n",
    "    Pglob=np.bincount(M, minlength=K)/len(M)\n",
    "    mask=(L==s)\n",
    "    if not np.any(mask): return np.nan\n",
    "    Ps=np.bincount(M[mask], minlength=K)/mask.sum()\n",
    "    return float(np.sum(Ps * (np.log2(np.where(Ps>0, Ps, 1)) - np.log2(np.where(Pglob>0,Pglob,1)))))\n",
    "\n",
    "def build_epoch_table_for_subject(subj):\n",
    "    \"\"\"Concatenate epoch rows (letter,ms) across all decode_<subj>_*.csv.\"\"\"\n",
    "    rows=[]\n",
    "    for dec in GENREP.glob(f\"decode_{subj}_*.csv\"):\n",
    "        df=pd.read_csv(dec)\n",
    "        if not {\"file\",\"t_start_s\",\"t_end_s\",\"state\"}.issubset(df.columns): continue\n",
    "        edf=GENDATA/df[\"file\"].iloc[0]\n",
    "        if not edf.exists(): continue\n",
    "        raw=load_raw_for_ms(edf); sf=raw.info[\"sfreq\"]\n",
    "        pk=peaks(raw.get_data(), sf)\n",
    "        maps=kmeans_maps_polarity_agnostic(raw.get_data()[:,pk].T, K=4, iters=60, rs=777)\n",
    "        ms_labels=backfit(raw, maps)\n",
    "        dom=epoch_dom_ms(ms_labels, sf, df[\"t_start_s\"].to_numpy(), df[\"t_end_s\"].to_numpy())\n",
    "        mask=(dom>=0)\n",
    "        if not np.any(mask): continue\n",
    "        rows.append(pd.DataFrame({\"letter\": df[\"state\"].astype(int).to_numpy()[mask],\n",
    "                                  \"ms\": dom[mask]}))\n",
    "    if not rows:\n",
    "        return None\n",
    "    return pd.concat(rows, ignore_index=True)\n",
    "\n",
    "def per_letter_KL_with_shuffle(E, P=5000):\n",
    "    \"\"\"Return per-letter KL + epoch-shuffle p-values.\"\"\"\n",
    "    rng=default_rng(123)\n",
    "    K=4\n",
    "    rows=[]\n",
    "    letters=sorted(E[\"letter\"].unique())\n",
    "    for s in letters:\n",
    "        KL_obs=KL_for_letter(E, s, K=K)\n",
    "        z=0\n",
    "        for _ in range(P):\n",
    "            Lperm=rng.permutation(E[\"letter\"].to_numpy())\n",
    "            KL_null=KL_for_letter(pd.DataFrame({\"letter\":Lperm,\"ms\":E[\"ms\"]}), s, K=K)\n",
    "            z += (KL_null >= KL_obs)\n",
    "        pval=float((z+1)/(P+1))\n",
    "        rows.append({\"letter\": int(s), \"KL_bits\": KL_obs, \"KL_perm_p\": pval})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ---------- (1) S002 & S003 per-letter KL (epoch shuffle) ----------\n",
    "for subj in [\"S002\",\"S003\"]:\n",
    "    E = build_epoch_table_for_subject(subj)\n",
    "    if E is None or len(E)==0:\n",
    "        print(\"No epoch table for\", subj, \"(skipping).\")\n",
    "        continue\n",
    "    KL_df = per_letter_KL_with_shuffle(E, P=5000)\n",
    "    out_csv = ANAREP/f\"microstate_epoch_KL_{subj}.csv\"\n",
    "    KL_df.to_csv(out_csv, index=False)\n",
    "    print(\"Saved â†’\", out_csv)\n",
    "\n",
    "# ---------- (2) Î”xent CI refresh; update stats files ----------\n",
    "pred = pd.read_csv(ANAREP/\"predictive_scores.csv\").dropna(subset=[\"xent_T1\",\"xent_unigram\"])\n",
    "delta = pred[\"xent_unigram\"].values - pred[\"xent_T1\"].values\n",
    "rng = default_rng(42)\n",
    "B=50000\n",
    "boots=[rng.choice(delta, size=len(delta), replace=True).mean() for _ in range(B)]\n",
    "dg_mean=float(delta.mean()); dg_lo, dg_hi = float(np.quantile(boots,0.025)), float(np.quantile(boots,0.975))\n",
    "\n",
    "# Update stats_summary.json (if exists)\n",
    "sum_json = ANAREP/\"stats_summary.json\"\n",
    "if sum_json.exists():\n",
    "    ss = json.load(open(sum_json))\n",
    "else:\n",
    "    ss = {}\n",
    "ss[\"Predictive_gain_xent\"] = {\"mean_CI\":[dg_mean, dg_lo, dg_hi], \"n\": int(len(delta))}\n",
    "with open(sum_json, \"w\") as f: json.dump(ss, f, indent=2)\n",
    "\n",
    "# Update/overwrite stats_tables.csv with Î”xent rows present\n",
    "tab_csv = ANAREP/\"stats_tables.csv\"\n",
    "if tab_csv.exists():\n",
    "    tbl = pd.read_csv(tab_csv)\n",
    "else:\n",
    "    tbl = pd.DataFrame(columns=[\"metric\",\"value\"])\n",
    "rows = [\n",
    "    {\"metric\":\"Pred_gain_mean\",\"value\":dg_mean},\n",
    "    {\"metric\":\"Pred_gain_CI_lo\",\"value\":dg_lo},\n",
    "    {\"metric\":\"Pred_gain_CI_hi\",\"value\":dg_hi},\n",
    "]\n",
    "# remove existing keys then append\n",
    "tbl = tbl[~tbl[\"metric\"].isin([r[\"metric\"] for r in rows])]\n",
    "tbl = pd.concat([tbl, pd.DataFrame(rows)], ignore_index=True)\n",
    "tbl.to_csv(tab_csv, index=False)\n",
    "print(\"Updated Î”xent CI in:\", tab_csv)\n",
    "\n",
    "# ---------- (3) README / Scoreboard claims ----------\n",
    "score_json = GENREP/\"v0_3_scoreboard.json\"\n",
    "score = json.load(open(score_json)) if score_json.exists() else {}\n",
    "# Pull headline identity/function numbers if present\n",
    "eoec_mean = score.get(\"eoec_mean\", None)\n",
    "eoec_ci   = [score.get(\"eoec_CI_lo\", None), score.get(\"eoec_CI_hi\", None)] if \"eoec_CI_lo\" in score else None\n",
    "task_mean = score.get(\"task_auc_mean\", None)\n",
    "task_ci   = [score.get(\"task_auc_CI_lo\", None), score.get(\"task_auc_CI_hi\", None)] if \"task_auc_CI_lo\" in score else None\n",
    "\n",
    "# Mechanism lines (S001 already in scoreboard; add subjects if present)\n",
    "subj_mi_path = ANAREP/\"microstate_epoch_stats_subjects.json\"\n",
    "subj_line = \"\"\n",
    "if subj_mi_path.exists():\n",
    "    subj_stats = json.load(open(subj_mi_path)).get(\"subjects\", [])\n",
    "    if subj_stats:\n",
    "        subj_line = \"  \" + \"  \".join([f\"{s['subject']}: MI={s['MI_bits']:.3f} [CI {s['CI'][0]:.3f},{s['CI'][1]:.3f}], p={s['perm_p']:.3g} (n={s['n_epochs']})\" for s in subj_stats])\n",
    "\n",
    "claims = \"\\n## Claims (v0.3)\\n\"\n",
    "if eoec_mean is not None and eoec_ci and all(v is not None for v in eoec_ci):\n",
    "    claims += f\"- **Identity** (IAF EO/EC): mean acc **{eoec_mean:.3f}** [{eoec_ci[0]:.3f}, {eoec_ci[1]:.3f}].\\n\"\n",
    "if task_mean is not None and task_ci and all(v is not None for v in task_ci):\n",
    "    claims += f\"- **Function** (R03, FBCSP v2): mean AUC **{task_mean:.3f}** [{task_ci[0]:.3f}, {task_ci[1]:.3f}].\\n\"\n",
    "claims += f\"- **Grammar** (order-1 LM): Î”xent (unigramâ†’Tâ‚) **{dg_mean:.3f}** [{dg_lo:.3f}, {dg_hi:.3f}] nats.\\n\"\n",
    "\n",
    "# Mechanism from S001 epoch stats\n",
    "s001_stats = json.load(open(ANAREP/\"microstate_epoch_stats.json\")) if (ANAREP/\"microstate_epoch_stats.json\").exists() else {}\n",
    "if s001_stats:\n",
    "    mi = s001_stats.get(\"MI_bits\", None); lo,hi = (s001_stats.get(\"MI_CI\",[None,None]) or [None,None]); p = s001_stats.get(\"perm_p\", None)\n",
    "    if mi is not None and lo is not None and hi is not None and p is not None:\n",
    "        claims += f\"- **Mechanism** (microstates): S001 MI(letter;Î¼) **{mi:.3f}** bits [**{lo:.3f}**, **{hi:.3f}**], p={p:.3g}.\"\n",
    "        claims += subj_line + \"\\n\"\n",
    "\n",
    "# Append to README\n",
    "readme = BUNDLE/\"README_v0_3.md\"\n",
    "try:\n",
    "    with open(readme, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\" + claims + \"\\n\")\n",
    "    print(\"Appended claims to README:\", readme)\n",
    "except Exception as e:\n",
    "    print(\"README patch skipped:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "870e05f7-856a-4909-adf4-97d960352dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CALM v1.0 spec pack â†’ E:\\CNT\\artifacts\\CALM_v1_spec\n",
      "CALM Eval â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\analysis\\calm_eval.json\n",
      "Ready. Call write_calm_spec(), calm_eval(), replicate_subjects(), live_decode_stream(), dashboard_tailer().\n"
     ]
    }
   ],
   "source": [
    "# === CALM v1.0 â€” Rosetta Mega Cell (Spec + Eval + Replicate + Stream Dashboard) ===\n",
    "# Drop this into JupyterLab, edit the CFG block below if your paths differ, then run the helpers at the bottom.\n",
    "\n",
    "import os, re, io, json, time, shutil, math, requests, numpy as np, pandas as pd, matplotlib.pyplot as plt, mne\n",
    "from pathlib import Path\n",
    "from scipy.signal import welch\n",
    "from numpy.random import default_rng\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from mne.decoding import CSP\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "plt.switch_backend(\"Agg\")    # safe for headless; remove if you want inline in notebooks\n",
    "\n",
    "# -----------------------------\n",
    "# CFG â€” EDIT IF NEEDED\n",
    "# -----------------------------\n",
    "class CFG:\n",
    "    ROOT      = Path(r\"E:\\CNT\\artifacts\")                       # root for artifacts\n",
    "    RUN       = ROOT / \"cog_alphabet_hybrid_v1\"                 # promoted v0.2 run (models+letters)\n",
    "    REP       = ROOT / \"cog_alphabet_report_hybrid_v1\"\n",
    "    GEN_DATA  = ROOT / \"generalization_data\"                    # S0xx EDF downloads\n",
    "    BUNDLE_V03= ROOT / \"CNT_CognitiveAlphabet_v0_3\"\n",
    "    CALM_SPEC = ROOT / \"CALM_v1_spec\"\n",
    "    TARGET_SF = 250.0\n",
    "    EPOCH_LEN = 2.0\n",
    "    STEP      = 0.5\n",
    "    BANDS     = {\"delta\":(1,4),\"theta\":(4,8),\"alpha\":(8,13),\"beta_low\":(13,20),\"beta_high\":(20,35),\"gamma\":(35,80)}\n",
    "    FB_BANDS  = [(8,13),(13,20),(20,30)]                        # Î¼/Î² filter-bank for CSP\n",
    "    FB_TASK_THR=0.50                                            # default task gate\n",
    "CFG = CFG()\n",
    "\n",
    "# -----------------------------\n",
    "# Shared utils (I/O, features)\n",
    "# -----------------------------\n",
    "def ensure_dir(p: Path): p.mkdir(parents=True, exist_ok=True); return p\n",
    "def compact_json(path: Path, obj): ensure_dir(path.parent); path.write_text(json.dumps(obj, indent=2), encoding=\"utf-8\")\n",
    "def load_raw_edf(p: Path, lfh=(0.5,80.0), montage=False):\n",
    "    raw = mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"]-CFG.TARGET_SF)>1e-6: raw.resample(CFG.TARGET_SF, npad=\"auto\", verbose=False)\n",
    "    ny = raw.info[\"sfreq\"]/2.0\n",
    "    raw.filter(lfh[0], min(lfh[1], ny-1.0), verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "        if montage:\n",
    "            try:\n",
    "                raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"), on_missing=\"ignore\")\n",
    "            except Exception: pass\n",
    "    return raw\n",
    "\n",
    "def make_epochs(raw, win=CFG.EPOCH_LEN, hop=CFG.STEP):\n",
    "    ov = max(0.0, win-hop)\n",
    "    return mne.make_fixed_length_epochs(raw, duration=win, overlap=ov, preload=True, verbose=False)\n",
    "\n",
    "def spectral_features(epochs: mne.Epochs, bands=CFG.BANDS):\n",
    "    X = epochs.get_data(); sf = epochs.info[\"sfreq\"]\n",
    "    n_ep,n_ch,n_t = X.shape; nper=min(int(sf*2), n_t); nov=nper//2\n",
    "    f,P = welch(X, fs=sf, nperseg=nper, noverlap=nov, axis=-1, average=\"median\")\n",
    "    def idx(lo,hi): return np.where((f>=lo)&(f<hi))[0]\n",
    "    aidx=idx(0.5,80.0); tot=np.maximum(P[:,:,aidx].sum(-1),1e-12)\n",
    "    feats={}; acc={\"alpha\":np.zeros((n_ep,n_ch)),\"theta\":np.zeros((n_ep,n_ch)),\"beta\":np.zeros((n_ep,n_ch))}\n",
    "    for name,(lo,hi) in bands.items():\n",
    "        b=idx(lo,hi); bp=P[:,:,b].sum(-1)/tot if b.size else np.zeros((n_ep,n_ch))\n",
    "        feats[f\"{name}_rel_med\"]=np.median(bp,axis=1); feats[f\"{name}_rel_iqr\"]=np.subtract(*np.percentile(bp,[75,25],axis=1)); feats[f\"{name}_rel_std\"]=np.std(bp,axis=1)\n",
    "        head=name.split(\"_\",1)[0]; \n",
    "        if head in acc: acc[head]+=bp\n",
    "    alpha=np.median(acc[\"alpha\"],axis=1); theta=np.median(acc[\"theta\"],axis=1); beta=np.median(acc[\"beta\"],axis=1)\n",
    "    feats[\"alpha_rel_med\"]=alpha; feats[\"theta_rel_med\"]=theta; feats[\"beta_rel_med\"]=beta\n",
    "    feats[\"theta_over_alpha\"]=theta/np.maximum(alpha,1e-6); feats[\"beta_over_alpha\"]=beta/np.maximum(alpha,1e-6)\n",
    "    feats[\"(alpha+theta)_over_beta\"]=(alpha+theta)/np.maximum(beta,1e-6)\n",
    "    # entropy + centroid\n",
    "    p_band=P[:,:,aidx]; p_n=p_band/np.maximum(p_band.sum(-1,keepdims=True),1e-12)\n",
    "    H=-(p_n*np.log2(p_n+1e-12)).sum(-1); Hn=H/np.log2(p_band.shape[-1])\n",
    "    feats[\"spec_entropy_med\"]=np.median(Hn,axis=1); feats[\"spec_entropy_std\"]=np.std(Hn,axis=1)\n",
    "    fb=f[aidx].reshape(1,1,-1); cen=(p_band*fb).sum(-1)/np.maximum(p_band.sum(-1),1e-12)\n",
    "    feats[\"spec_centroid_med\"]=np.median(cen,axis=1); feats[\"spec_centroid_std\"]=np.std(cen,axis=1)\n",
    "    # Hjorth\n",
    "    d1=np.diff(X,axis=-1); var0=np.var(X,axis=-1)+1e-12; var1=np.var(d1,axis=-1)+1e-12\n",
    "    feats[\"hjorth_activity_med\"]=np.median(var0,axis=1); feats[\"hjorth_mobility_med\"]=np.median(np.sqrt(var1/var0),axis=1)\n",
    "    d2=np.diff(d1,axis=-1); var2=np.var(d2,axis=-1)+1e-12\n",
    "    feats[\"hjorth_complexity_med\"]=np.median(np.sqrt((var2/var1)/(var1/var0)),axis=1)\n",
    "    return pd.DataFrame(feats)\n",
    "\n",
    "def occipital_picks(raw):\n",
    "    names=[n.upper().strip() for n in raw.ch_names]\n",
    "    wanted=(\"O1\",\"O2\",\"OZ\",\"POZ\",\"PO3\",\"PO4\")\n",
    "    idx=[i for i,n in enumerate(names) if n in wanted]\n",
    "    return idx if idx else mne.pick_types(raw.info, eeg=True)\n",
    "\n",
    "def alpha_index(raw, band):\n",
    "    sf=raw.info[\"sfreq\"]\n",
    "    f,P = welch(raw.get_data(), fs=sf, nperseg=min(int(sf*2),raw.n_times), noverlap=int(min(int(sf*2),raw.n_times)/2),\n",
    "                axis=-1, average=\"median\")\n",
    "    idx=(f>=band[0])&(f<band[1]); a=P[:,idx].sum(-1); tot=P.sum(-1)+1e-12\n",
    "    oi=occipital_picks(raw)\n",
    "    return float((a[oi].sum()/tot[oi].sum()) if len(oi) else (a.sum()/tot.sum()))\n",
    "\n",
    "def butter_bandpass_array(X, lo, hi, sf, order=4):\n",
    "    X2 = X.reshape(X.shape[0]*X.shape[1], X.shape[2])\n",
    "    Xf = mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi,\n",
    "                                method=\"iir\", iir_params=dict(order=order, ftype=\"butter\"), verbose=False)\n",
    "    return Xf.reshape(X.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# (1) CALM v1.0 SPEC PACK\n",
    "# -----------------------------\n",
    "def write_calm_spec(out_dir: Path = CFG.CALM_SPEC):\n",
    "    ensure_dir(out_dir)\n",
    "    # Spec markdown (compact)\n",
    "    md = f\"\"\"# CALM v1.0 â€” Cognitive Alphabet & Language Model\n",
    "\n",
    "## Alphabet (K=4 default)\n",
    "- Letters: cluster IDs + feature signatures (Î±/Î¸/Î², entropy, Hjorth)\n",
    "- Faces: microstate mix A/B/C/D per letter (+ topomaps)\n",
    "\n",
    "## Grammar\n",
    "- First-order transition matrix T1; stationary Ï€\n",
    "- Semi-Markov dwell (geometric epochs or gamma seconds)\n",
    "\n",
    "## Subject Calibration\n",
    "- IAF Î±-band: [IAFâˆ’2, IAF+2] Hz; Î±-index threshold (occipital)\n",
    "- Motor: task_letters and fbCSP threshold\n",
    "\n",
    "## I/O Schemas\n",
    "- Epoch CSV: t_start_s, t_end_s, state, eoec_pred, fb_task, task_pred\n",
    "- Stream JSONL: ts, state_raw, state_smooth, dwell_counts, last_row_T, alpha_index, fb_task\n",
    "- Bundle: models/, report/, microstates/, dwell/, generalization/, stats/\n",
    "\n",
    "## Figures required\n",
    "- Generalization (EO/EC bars, Task AUC bars, confusions)\n",
    "- Baselines & Predictive (Î”xent bars, tables)\n",
    "- Mechanism (templates, task heatmap, Î¼-bars; MI CI & p)\n",
    "\n",
    "## Safety & Limits\n",
    "- Treat streams as health data. No medical claims without IRB.\n",
    "- Report CIs and p-values; re-calibrate IAF periodically.\n",
    "\"\"\"\n",
    "    (out_dir/\"CALM_v1_spec.md\").write_text(md, encoding=\"utf-8\")\n",
    "    # JSON Schemas (calibration & stream item)\n",
    "    calib_schema = {\n",
    "      \"$schema\":\"http://json-schema.org/draft-07/schema#\",\"type\":\"object\",\n",
    "      \"properties\":{\n",
    "        \"IAF_hz\":{\"type\":\"number\"},\"alpha_band\":{\"type\":\"array\",\"items\":{\"type\":\"number\"},\"minItems\":2,\"maxItems\":2},\n",
    "        \"alpha_threshold\":{\"type\":\"number\"},\"task_letters\":{\"type\":\"array\",\"items\":{\"type\":\"integer\"}},\n",
    "        \"fbCSP_threshold\":{\"type\":\"number\"}\n",
    "      },\"required\":[\"IAF_hz\",\"alpha_band\",\"alpha_threshold\",\"fbCSP_threshold\"]\n",
    "    }\n",
    "    stream_schema = {\n",
    "      \"$schema\":\"http://json-schema.org/draft-07/schema#\",\"type\":\"object\",\n",
    "      \"properties\":{\n",
    "        \"ts\":{\"type\":\"number\"},\"state_raw\":{\"type\":\"integer\"},\"state_smooth\":{\"type\":\"integer\"},\n",
    "        \"dwell_counts\":{\"type\":\"object\"},\"last_row_T\":{\"type\":\"array\",\"items\":{\"type\":\"number\"}},\n",
    "        \"alpha_index\":{\"type\":\"number\"},\"fb_task\":{\"type\":\"number\"}\n",
    "      },\"required\":[\"ts\",\"state_raw\",\"state_smooth\"]\n",
    "    }\n",
    "    compact_json(out_dir/\"schema_subject_calibration.json\", calib_schema)\n",
    "    compact_json(out_dir/\"schema_stream_item.json\", stream_schema)\n",
    "    # Model card template\n",
    "    card = f\"\"\"# CALM v1.0 Model Card\n",
    "## Data\n",
    "- PhysioNet EEG Motor/Imagery (S001â€“S010), runs R01â€“R03\n",
    "## Preprocessing\n",
    "- Resample {CFG.TARGET_SF} Hz; 0.5â€“80 Hz features; 2 s windows / 0.5 s hop\n",
    "## Features\n",
    "- Relative band powers: {CFG.BANDS}\n",
    "- Entropy, centroid; Hjorth; fbCSP task prob (R03)\n",
    "## Decoder\n",
    "- scalerâ†’PCA(â‰¤20)â†’KMeans(K=4); sticky+min-run smoothing\n",
    "## Calibration\n",
    "- IAF Î±-band; Î±-index threshold; task letters + fbCSP threshold\n",
    "## Evaluation\n",
    "- EO/EC acc, R03 AUC, Î”xent (unigramâ†’T1), MI(letter;Î¼) + KL\n",
    "## Safety\n",
    "- Consent, on-device by default, CIs reported, no clinical claims without IRB.\n",
    "\"\"\"\n",
    "    (out_dir/\"MODEL_CARD.md\").write_text(card, encoding=\"utf-8\")\n",
    "    print(\"CALM v1.0 spec pack â†’\", out_dir)\n",
    "\n",
    "# -----------------------------\n",
    "# (2) CALM Eval â€” score a bundle\n",
    "# -----------------------------\n",
    "def calm_eval(bundle_dir: Path = CFG.BUNDLE_V03):\n",
    "    \"\"\"Reads a bundle dir and prints Identity / Function / Grammar / Mechanism. Writes calm_eval.json.\"\"\"\n",
    "    bd = Path(bundle_dir)\n",
    "    gen = bd / \"report\" / \"..\" / \"..\"  # not reliable; fall back to standard report root\n",
    "    # Prefer the main report folders you already use\n",
    "    GENREP = CFG.REP / \"generalization\"\n",
    "    ANAREP = CFG.REP / \"analysis\"\n",
    "    out = {}\n",
    "    # Identity/Function\n",
    "    try:\n",
    "        eoec = pd.read_csv(GENREP/\"eoec_iaf_summary.csv\")\n",
    "        task = pd.read_csv(GENREP/\"task_fbcsp_summary.csv\")\n",
    "        out[\"EOEC_mean\"] = float(eoec[\"acc\"].mean()); out[\"EOEC_n\"] = int(len(eoec))\n",
    "        out[\"TaskAUC_mean\"] = float(task[\"AUC_task\"].mean()); out[\"TaskAUC_n\"] = int(len(task))\n",
    "    except Exception: pass\n",
    "    # Grammar\n",
    "    try:\n",
    "        pred = pd.read_csv(ANAREP/\"predictive_scores.csv\").dropna(subset=[\"xent_T1\",\"xent_unigram\"])\n",
    "        dx = pred[\"xent_unigram\"].values - pred[\"xent_T1\"].values\n",
    "        out[\"Delta_xent_mean\"] = float(dx.mean()); out[\"Delta_xent_n\"] = int(len(dx))\n",
    "    except Exception: pass\n",
    "    # Mechanism\n",
    "    try:\n",
    "        ms = json.load(open(ANAREP/\"microstate_epoch_stats.json\"))\n",
    "        out[\"MI_S001_bits\"] = ms[\"MI_bits\"]; out[\"MI_S001_p\"] = ms[\"perm_p\"]\n",
    "        if (ANAREP/\"microstate_epoch_stats_subjects.json\").exists():\n",
    "            out[\"MI_subjects\"] = json.load(open(ANAREP/\"microstate_epoch_stats_subjects.json\"))\n",
    "    except Exception: pass\n",
    "    outp = CFG.REP/\"analysis\"/\"calm_eval.json\"; compact_json(outp, out)\n",
    "    print(\"CALM Eval â†’\", outp); return out\n",
    "\n",
    "# -----------------------------\n",
    "# (3) Replication runner (S011â€“S015 by default) â†’ PDF report\n",
    "# -----------------------------\n",
    "def fetch_edf(url, dest):\n",
    "    if dest.exists() and dest.stat().st_size>0: return dest\n",
    "    with requests.get(url, stream=True, timeout=60) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(dest, \"wb\") as f:\n",
    "            for ch in r.iter_content(8192):\n",
    "                if ch: f.write(ch)\n",
    "    return dest\n",
    "\n",
    "def iaf_calibration_for_subject(subj):\n",
    "    r01=CFG.GEN_DATA/f\"{subj}R01.edf\"; r02=CFG.GEN_DATA/f\"{subj}R02.edf\"\n",
    "    raw_eo=load_raw_edf(r01, lfh=(0.5,45.0)); raw_ec=load_raw_edf(r02, lfh=(0.5,45.0))\n",
    "    # IAF in 7â€“14 Hz from EC\n",
    "    sf=raw_ec.info[\"sfreq\"]\n",
    "    f,P = welch(raw_ec.get_data(), fs=sf, nperseg=min(int(sf*2), raw_ec.n_times), noverlap=int(min(int(sf*2), raw_ec.n_times)/2), axis=-1, average=\"median\")\n",
    "    band=(f>=7)&(f<=14); iaf=float(np.clip(f[band][np.argmax(P[occipital_picks(raw_ec)][:, band].mean(0))], 8.0, 12.0))\n",
    "    alpha_band=(max(6.0,iaf-2.0), min(14.0, iaf+2.0))\n",
    "    thr=0.5*(alpha_index(raw_ec,alpha_band)+alpha_index(raw_eo,alpha_band))\n",
    "    return iaf, alpha_band, thr\n",
    "\n",
    "def eoec_accuracy(subj, alpha_band, thr):\n",
    "    r01=CFG.GEN_DATA/f\"{subj}R01.edf\"; r02=CFG.GEN_DATA/f\"{subj}R02.edf\"\n",
    "    def classify_run(raw, cond):\n",
    "        sf=raw.info[\"sfreq\"]; ts=np.arange(0, raw.n_times/sf - CFG.EPOCH_LEN + 1e-9, CFG.STEP)\n",
    "        preds=[]\n",
    "        for t0 in ts:\n",
    "            s=int(t0*sf); e=s+int(CFG.EPOCH_LEN*sf); seg=raw.get_data(start=s, stop=e)\n",
    "            f,P = welch(seg, fs=sf, nperseg=min(int(sf*2), e-s), noverlap=int(min(int(sf*2), e-s)/2), axis=-1, average=\"median\")\n",
    "            idx=(f>=alpha_band[0])&(f<alpha_band[1]); a=P[:,idx].sum(-1); tot=P.sum(-1)+1e-12\n",
    "            oi=occipital_picks(raw); ai=(a[oi].sum()/tot[oi].sum()) if len(oi) else (a.sum()/tot.sum())\n",
    "            preds.append(\"EC\" if ai>=thr else \"EO\")\n",
    "        return pd.DataFrame({\"gt\":cond, \"pred\":preds})\n",
    "    raw_eo=load_raw_edf(r01, lfh=(0.5,45.0)); raw_ec=load_raw_edf(r02, lfh=(0.5,45.0))\n",
    "    df=pd.concat([classify_run(raw_ec,\"EC\"), classify_run(raw_eo,\"EO\")], ignore_index=True)\n",
    "    return float((df[\"gt\"]==df[\"pred\"]).mean())\n",
    "\n",
    "def fbcsp_auc(subj):\n",
    "    r03=CFG.GEN_DATA/f\"{subj}R03.edf\"; raw=load_raw_edf(r03, lfh=(0.5,40.0))\n",
    "    want={\"C3\",\"Cz\",\"C4\",\"FC3\",\"FCz\",\"FC4\",\"CP3\",\"CPz\",\"CP4\"}\n",
    "    picks=[i for i,ch in enumerate(raw.ch_names) if ch.upper().strip() in want]\n",
    "    if len(picks)<3: picks=mne.pick_types(raw.info, eeg=True)\n",
    "    raw.pick(picks); anns=raw.annotations\n",
    "    if anns is None or len(anns)==0: return np.nan\n",
    "    sf=raw.info[\"sfreq\"]; segs=[]; gid=0\n",
    "    for o,d,s in zip(anns.onset, anns.duration, anns.description):\n",
    "        su=str(s).upper()\n",
    "        if \"T0\" in su or \"T1\" in su or \"T2\" in su:\n",
    "            lab=0 if \"T0\" in su else 1; segs.append((float(o), float(o+d), lab, gid)); gid+=1\n",
    "    X=[]; y=[]; g=[]\n",
    "    for a,b,lab,segid in segs:\n",
    "        t=a\n",
    "        while t+2.0 <= b-1e-6:\n",
    "            s=int(t*sf); e=s+int(2.0*sf)\n",
    "            X.append(raw.get_data(start=s, stop=e)); y.append(lab); g.append(segid); t+=2.0\n",
    "    if not X: return np.nan\n",
    "    X=np.stack(X,0); y=np.array(y,int); g=np.array(g,int)\n",
    "    proba=np.zeros(len(y),float); gkf=GroupKFold(n_splits=min(5,max(2,len(np.unique(g)))))\n",
    "    for tr,te in gkf.split(X,y,g):\n",
    "        feats_tr, feats_te=[],[]\n",
    "        for lo,hi in CFG.FB_BANDS:\n",
    "            Xtr=butter_bandpass_array(X[tr], lo, hi, sf); Xte=butter_bandpass_array(X[te], lo, hi, sf)\n",
    "            csp=CSP(n_components=6, reg=\"ledoit_wolf\", log=True, norm_trace=False); csp.fit(Xtr, y[tr])\n",
    "            feats_tr.append(csp.transform(Xtr)); feats_te.append(csp.transform(Xte))\n",
    "        Xtr_fb=np.concatenate(feats_tr,1); Xte_fb=np.concatenate(feats_te,1)\n",
    "        clf=LDA(solver=\"lsqr\", shrinkage=\"auto\").fit(Xtr_fb, y[tr]); proba[te]=clf.predict_proba(Xte_fb)[:,1]\n",
    "    return float(roc_auc_score(y, proba))\n",
    "\n",
    "def replicate_subjects(subjects=(\"S011\",\"S012\",\"S013\",\"S014\",\"S015\"), out_pdf: Path = CFG.REP/\"replication_pack.pdf\"):\n",
    "    ensure_dir(CFG.GEN_DATA)\n",
    "    # fetch EDFs\n",
    "    for subj in subjects:\n",
    "        for r in [1,2,3]:\n",
    "            url=f\"https://physionet.org/files/eegmmidb/1.0.0/{subj}/{subj}R{r:02d}.edf\"\n",
    "            fetch_edf(url, CFG.GEN_DATA/f\"{subj}R{r:02d}.edf\")\n",
    "    # evaluate\n",
    "    rows=[]\n",
    "    for subj in subjects:\n",
    "        try:\n",
    "            iaf, band, thr = iaf_calibration_for_subject(subj)\n",
    "            acc = eoec_accuracy(subj, band, thr)\n",
    "            auc = fbcsp_auc(subj)\n",
    "            rows.append({\"subject\":subj,\"EOEC_acc\":acc,\"Task_AUC\":auc,\"IAF_hz\":iaf,\"alpha_band\":band,\"alpha_thr\":thr})\n",
    "        except Exception as e:\n",
    "            rows.append({\"subject\":subj,\"EOEC_acc\":np.nan,\"Task_AUC\":np.nan})\n",
    "    df=pd.DataFrame(rows); df.to_csv(CFG.REP/\"replication_scores.csv\", index=False)\n",
    "    # build PDF figure\n",
    "    with PdfPages(out_pdf) as pdf:\n",
    "        # page 1 â€” scores\n",
    "        plt.figure(figsize=(10,4))\n",
    "        plt.subplot(1,2,1); plt.bar(np.arange(len(df)), df[\"EOEC_acc\"].fillna(0)); plt.ylim(0,1); plt.title(\"EO/EC accuracy\"); plt.xticks(np.arange(len(df)), df[\"subject\"], rotation=30)\n",
    "        plt.subplot(1,2,2); plt.bar(np.arange(len(df)), df[\"Task_AUC\"].fillna(0)); plt.ylim(0.5,1.0); plt.title(\"R03 Task AUC\"); plt.xticks(np.arange(len(df)), df[\"subject\"], rotation=30)\n",
    "        plt.suptitle(\"CALM Replication (cross-subject)\"); pdf.savefig(bbox_inches=\"tight\"); plt.close()\n",
    "        # page 2 â€” table\n",
    "        plt.figure(figsize=(10,6)); plt.axis(\"off\")\n",
    "        txt=\"subject    EOEC_acc    Task_AUC    IAF\\n\" + \"\\n\".join([f\"{r['subject']:>6}    {r['EOEC_acc']!s:>8}    {r['Task_AUC']!s:>8}    {r.get('IAF_hz',''):>4}\" for _,r in df.iterrows()])\n",
    "        plt.text(0,1,txt, family=\"monospace\", va=\"top\"); pdf.savefig(bbox_inches=\"tight\"); plt.close()\n",
    "    print(\"Replication PDF â†’\", out_pdf)\n",
    "\n",
    "# -----------------------------\n",
    "# (4) Live stream + dashboard\n",
    "# -----------------------------\n",
    "def sticky_minrun_online(prev_state, raw_state, count, min_epochs=3):\n",
    "    if prev_state is None: return raw_state, 1\n",
    "    if raw_state == prev_state: return prev_state, count+1\n",
    "    if count+1 >= min_epochs:  return raw_state, 1\n",
    "    return prev_state, count\n",
    "\n",
    "def live_decode_stream(edf_path, subject_id, seconds=60, out_dir=CFG.REP/\"generalization\", jsonl_every=5, min_run_epochs=3):\n",
    "    out_dir=ensure_dir(Path(out_dir))\n",
    "    csv_path   = out_dir/f\"live_{subject_id}_{Path(edf_path).stem}.csv\"\n",
    "    jsonl_path = out_dir/f\"live_{subject_id}_{Path(edf_path).stem}.jsonl\"\n",
    "    # load models\n",
    "    scaler = load(CFG.RUN/\"scaler_hybrid.joblib\"); pca = load(CFG.RUN/\"pca_hybrid.joblib\"); km = load(CFG.RUN/\"kmeans_hybrid.joblib\")\n",
    "    TRAIN_COLS = list(pd.read_csv(CFG.RUN/\"features.csv\", nrows=1).columns)\n",
    "    # subject calibration (IAF alpha + threshold + task letters if present)\n",
    "    cal_path = CFG.REP/\"generalization\"/\"subject_calibration.json\"\n",
    "    cal = json.load(open(cal_path)) if cal_path.exists() else {}\n",
    "    subj_c = cal.get(subject_id, {})\n",
    "    a_band = subj_c.get(\"alpha_band\", [8.0,13.0]); a_thr = subj_c.get(\"alpha_threshold\", None)\n",
    "    t_letters = set(map(int, subj_c.get(\"task_letters\", []))); fb_thr = float(subj_c.get(\"fbCSP_threshold\", CFG.FB_TASK_THR))\n",
    "    # stream\n",
    "    raw = load_raw_edf(Path(edf_path), lfh=(0.5,80.0))\n",
    "    sf  = raw.info[\"sfreq\"]; total_samples = min(raw.n_times, int(seconds*sf))\n",
    "    rows=[]; smooth_rows=[]; smooth_state=None; run_len=0; last_state=None; trans_row=np.zeros(km.n_clusters)\n",
    "    dwell_counts={i:0 for i in range(km.n_clusters)}\n",
    "    t=0.0; chunk=0\n",
    "    while int(t+CFG.EPOCH_LEN)*sf <= total_samples:\n",
    "        seg = raw.copy().crop(tmin=t, tmax=t+CFG.EPOCH_LEN, include_tmax=False)\n",
    "        ep = make_epochs(seg); Fsp = spectral_features(ep).reindex(columns=TRAIN_COLS, fill_value=0.0)\n",
    "        Z = pca.transform(scaler.transform(Fsp.values)); raw_state=int(km.predict(Z)[0])\n",
    "        # EO/EC alpha-index\n",
    "        eoec=\"UNK\"; ai_val=None\n",
    "        if a_thr is not None:\n",
    "            f,P = welch(seg.get_data(), fs=sf, nperseg=min(int(sf*2), seg.n_times), noverlap=int(min(int(sf*2), seg.n_times)/2), axis=-1, average=\"median\")\n",
    "            idx=(f>=a_band[0])&(f<a_band[1]); a=P[:,idx].sum(-1); tot=P.sum(-1)+1e-12\n",
    "            oi=occipital_picks(seg); ai=(a[oi].sum()/tot[oi].sum()) if len(oi) else (a.sum()/tot.sum()); eoec=\"EC\" if ai>=a_thr else \"EO\"; ai_val=float(ai)\n",
    "        # fbCSP task proba on the chunk (best-effort single-shot)\n",
    "        try:\n",
    "            starts=np.array([0.0]); ends=np.array([CFG.EPOCH_LEN]); want={\"C3\",\"Cz\",\"C4\",\"FC3\",\"FCz\",\"FC4\",\"CP3\",\"CPz\",\"CP4\"}\n",
    "            picks=[i for i,ch in enumerate(seg.ch_names) if ch.upper().strip() in want]\n",
    "            if len(picks)<3: picks=mne.pick_types(seg.info, eeg=True)\n",
    "            _raw=seg.copy().pick(picks); sff=_raw.info[\"sfreq\"]\n",
    "            X=_raw.get_data()[None,:,:]\n",
    "            feats=[]\n",
    "            for lo,hi in CFG.FB_BANDS:\n",
    "                Xf=butter_bandpass_array(X, lo, hi, sff); csp=CSP(n_components=6, reg=\"ledoit_wolf\", log=True, norm_trace=False); csp.fit(Xf, [0]); feats.append(csp.transform(Xf))\n",
    "            Xfb=np.concatenate(feats,1); lda=LDA(solver=\"lsqr\", shrinkage=\"auto\").fit(Xfb, [0]); fb=float(lda.predict_proba(Xfb)[:,1][0])\n",
    "        except Exception:\n",
    "            fb=0.0\n",
    "        task_pred = \"task\" if (raw_state in t_letters or fb>=fb_thr) else \"rest\"\n",
    "        # smoothing & telemetry\n",
    "        smooth_state, run_len = sticky_minrun_online(smooth_state, raw_state, run_len, min_epochs=min_run_epochs)\n",
    "        if last_state is None: last_state=smooth_state\n",
    "        elif smooth_state!=last_state: trans_row[smooth_state]+=1.0; last_state=smooth_state\n",
    "        dwell_counts[smooth_state]+=1\n",
    "        rows.append([round(t,3), round(t+CFG.EPOCH_LEN,3), raw_state, smooth_state, eoec, fb, task_pred])\n",
    "        if (chunk+1)%jsonl_every==0:\n",
    "            with open(jsonl_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps({\"ts\": round(t+CFG.EPOCH_LEN,3), \"state_raw\": raw_state, \"state_smooth\": smooth_state,\n",
    "                                    \"dwell_counts\": dwell_counts, \"last_row_T\": trans_row.tolist(),\n",
    "                                    \"alpha_index\": ai_val, \"fb_task\": fb}) + \"\\n\")\n",
    "        t += CFG.EPOCH_LEN; chunk+=1\n",
    "    out_df=pd.DataFrame(rows, columns=[\"t_start_s\",\"t_end_s\",\"state_raw\",\"state_smooth\",\"eoec_pred\",\"fb_task\",\"task_pred\"])\n",
    "    out_df.to_csv(csv_path, index=False); print(\"Live CSV â†’\", csv_path); print(\"JSONL â†’\", jsonl_path)\n",
    "    return csv_path, jsonl_path\n",
    "\n",
    "def dashboard_tailer(jsonl_path: Path, seconds=60):\n",
    "    \"\"\"Very light dashboard: plots fb_task and alpha_index over time; prints last smoothed state.\"\"\"\n",
    "    jsonl_path=Path(jsonl_path)\n",
    "    t0=time.time(); xs=[]; fb=[]; ai=[]; ss=[]\n",
    "    plt.figure(figsize=(10,4))\n",
    "    while time.time()-t0 < seconds:\n",
    "        if jsonl_path.exists():\n",
    "            for line in jsonl_path.read_text(encoding=\"utf-8\").splitlines():\n",
    "                try:\n",
    "                    obj=json.loads(line)\n",
    "                    xs.append(obj.get(\"ts\", len(xs))); fb.append(obj.get(\"fb_task\",0.0)); ai.append(obj.get(\"alpha_index\", None)); ss.append(obj.get(\"state_smooth\", None))\n",
    "                except Exception: pass\n",
    "        plt.clf()\n",
    "        plt.subplot(1,1,1); \n",
    "        if xs:\n",
    "            plt.plot(xs, fb, label=\"fb_task\"); \n",
    "            if any(v is not None for v in ai): \n",
    "                ai_plot=[v if v is not None else np.nan for v in ai]; plt.plot(xs, ai_plot, label=\"alpha_index\")\n",
    "            plt.title(f\"live stream â€” last state_smooth={ss[-1] if ss else 'NA'}\"); plt.xlabel(\"t (s)\")\n",
    "            plt.legend()\n",
    "        plt.pause(1.0)\n",
    "    plt.close()\n",
    "\n",
    "# -----------------------------\n",
    "# HOW TO USE (examples)\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    # 1) Write CALM v1.0 spec pack\n",
    "    write_calm_spec(CFG.CALM_SPEC)\n",
    "\n",
    "    # 2) Evaluate your existing bundle (prints calm_eval.json)\n",
    "    calm_eval(CFG.BUNDLE_V03)\n",
    "\n",
    "    # 3) Replicate on S011â€“S015 and produce a small PDF\n",
    "    # replicate_subjects(subjects=(\"S011\",\"S012\",\"S013\",\"S014\",\"S015\"), out_pdf=CFG.REP/\"replication_pack.pdf\")\n",
    "\n",
    "    # 4) Start a live stream and (optionally) tail it:\n",
    "    # csv, jsonl = live_decode_stream(CFG.ROOT/\"generalization_data\"/\"S002R03.edf\", \"S002\", seconds=120)\n",
    "    # dashboard_tailer(jsonl, seconds=60)\n",
    "\n",
    "    print(\"Ready. Call write_calm_spec(), calm_eval(), replicate_subjects(), live_decode_stream(), dashboard_tailer().\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7e45d88c-a5b6-496c-ac2c-07b30b58f68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] K-sweepâ€¦\n",
      "[2] Robustnessâ€¦\n",
      "[3] Calibration sensitivity S002/S003â€¦\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[4] Drift tests S003â€¦\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[5] Mechanism invarianceâ€¦\n",
      "[6] Fairnessâ€¦\n",
      "[7] Adversarial nullsâ€¦\n",
      "[8] Compose reportâ€¦\n"
     ]
    },
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEmptyDataError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 469\u001b[39m\n\u001b[32m    467\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[6] Fairnessâ€¦\u001b[39m\u001b[33m\"\u001b[39m); fairness_outliers()\n\u001b[32m    468\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[7] Adversarial nullsâ€¦\u001b[39m\u001b[33m\"\u001b[39m); adversarial_nulls()\n\u001b[32m--> \u001b[39m\u001b[32m469\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[8] Compose reportâ€¦\u001b[39m\u001b[33m\"\u001b[39m); compose_report()\n\u001b[32m    471\u001b[39m \u001b[38;5;66;03m# Final scoreboard write\u001b[39;00m\n\u001b[32m    472\u001b[39m summary = {\n\u001b[32m    473\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33mbattery\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mCALM Stress & Proof v1\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    474\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33mk_sweep_csv\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(CFG.OUT/\u001b[33m\"\u001b[39m\u001b[33mk_sweep_s001.csv\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m    482\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33mproof_pdf\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(CFG.OUT/\u001b[33m\"\u001b[39m\u001b[33mCALM_Proof_Battery.pdf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    483\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 419\u001b[39m, in \u001b[36mcompose_report\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    416\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompose_report\u001b[39m():\n\u001b[32m    417\u001b[39m     \u001b[38;5;66;03m# collect\u001b[39;00m\n\u001b[32m    418\u001b[39m     kdf = pd.read_csv(CFG.OUT/\u001b[33m\"\u001b[39m\u001b[33mk_sweep_s001.csv\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m (CFG.OUT/\u001b[33m\"\u001b[39m\u001b[33mk_sweep_s001.csv\u001b[39m\u001b[33m\"\u001b[39m).exists() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m419\u001b[39m     rbd = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCFG\u001b[49m\u001b[43m.\u001b[49m\u001b[43mOUT\u001b[49m\u001b[43m/\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrobustness_sweeps.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m (CFG.OUT/\u001b[33m\"\u001b[39m\u001b[33mrobustness_sweeps.csv\u001b[39m\u001b[33m\"\u001b[39m).exists() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    420\u001b[39m     fair= pd.read_csv(CFG.OUT/\u001b[33m\"\u001b[39m\u001b[33mfairness_outliers.csv\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m (CFG.OUT/\u001b[33m\"\u001b[39m\u001b[33mfairness_outliers.csv\u001b[39m\u001b[33m\"\u001b[39m).exists() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    421\u001b[39m     inv = pd.read_csv(CFG.OUT/\u001b[33m\"\u001b[39m\u001b[33mmechanism_invariance.csv\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m (CFG.OUT/\u001b[33m\"\u001b[39m\u001b[33mmechanism_invariance.csv\u001b[39m\u001b[33m\"\u001b[39m).exists() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\CNT\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\CNT\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\CNT\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\CNT\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mE:\\CNT\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:93\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwds[\u001b[33m\"\u001b[39m\u001b[33mdtype_backend\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# Fail here loudly instead of in cython after reading\u001b[39;00m\n\u001b[32m     92\u001b[39m     import_optional_dependency(\u001b[33m\"\u001b[39m\u001b[33mpyarrow\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[38;5;28mself\u001b[39m._reader = \u001b[43mparsers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTextReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mself\u001b[39m.unnamed_cols = \u001b[38;5;28mself\u001b[39m._reader.unnamed_cols\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/parsers.pyx:581\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.__cinit__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mEmptyDataError\u001b[39m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "# === CALM Stress & Proof Battery v1 ===\n",
    "# K-sweep (3..8), robustness (noise/dropout/resample/jitter), calibration sensitivity (IAF/threshold),\n",
    "# drift tests, mechanism invariance (re-reference), fairness/outliers, adversarial nulls.\n",
    "# Outputs a consolidated scoreboard JSON/CSV + decision plots + a 1-page PDF.\n",
    "import os, re, io, json, math, shutil, time, numpy as np, pandas as pd, matplotlib.pyplot as plt, mne, requests\n",
    "from pathlib import Path\n",
    "from scipy.signal import welch\n",
    "from numpy.random import default_rng\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from mne.decoding import CSP\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# -----------------------------\n",
    "# CFG â€” Edit if needed\n",
    "# -----------------------------\n",
    "class CFG:\n",
    "    ROOT      = Path(r\"E:\\CNT\\artifacts\")\n",
    "    RUN       = ROOT / \"cog_alphabet_hybrid_v1\"\n",
    "    REP       = ROOT / \"cog_alphabet_report_hybrid_v1\"\n",
    "    GEN       = ROOT / \"generalization_data\"             # where S0xx EDFs live\n",
    "    OUT       = REP  / \"validation_battery\"\n",
    "    TARGET_SF = 250.0\n",
    "    EPOCH_LEN = 2.0\n",
    "    STEP      = 0.5\n",
    "    BANDS     = {\"delta\":(1,4),\"theta\":(4,8),\"alpha\":(8,13),\"beta_low\":(13,20),\"beta_high\":(20,35),\"gamma\":(35,80)}\n",
    "    FB_BANDS  = [(8,13),(13,20),(20,30)]\n",
    "    FB_TASK_THR = 0.50\n",
    "    K_RANGE   = list(range(3,9))                         # sweep K=3..8\n",
    "CFG = CFG()\n",
    "CFG.OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Shared utils (I/O, features)\n",
    "# -----------------------------\n",
    "def ensure(p:Path): p.mkdir(parents=True, exist_ok=True); return p\n",
    "def load_raw_edf(p:Path, band=(0.5,80.0), montage=False):\n",
    "    raw = mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"]-CFG.TARGET_SF)>1e-6: raw.resample(CFG.TARGET_SF, npad=\"auto\", verbose=False)\n",
    "    ny = raw.info[\"sfreq\"]/2.0\n",
    "    raw.filter(band[0], min(band[1], ny-1.0), verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "        if montage:\n",
    "            try: raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"), on_missing=\"ignore\")\n",
    "            except Exception: pass\n",
    "    return raw\n",
    "\n",
    "def make_epochs(raw, win=CFG.EPOCH_LEN, hop=CFG.STEP):\n",
    "    ov = max(0.0, win-hop)\n",
    "    return mne.make_fixed_length_epochs(raw, duration=win, overlap=ov, preload=True, verbose=False)\n",
    "\n",
    "def spectral_features(epochs, bands=CFG.BANDS):\n",
    "    X = epochs.get_data(); sf = epochs.info[\"sfreq\"]\n",
    "    n_ep,n_ch,n_t = X.shape; nper=min(int(sf*2), n_t); nov=nper//2\n",
    "    f,P = welch(X, fs=sf, nperseg=nper, noverlap=nov, axis=-1, average=\"median\")\n",
    "    def idx(lo,hi): return np.where((f>=lo)&(f<hi))[0]\n",
    "    aidx=idx(0.5,80.0); tot=np.maximum(P[:,:,aidx].sum(-1),1e-12)\n",
    "    feats={}; acc={\"alpha\":np.zeros((n_ep,n_ch)),\"theta\":np.zeros((n_ep,n_ch)),\"beta\":np.zeros((n_ep,n_ch))}\n",
    "    for name,(lo,hi) in bands.items():\n",
    "        b=idx(lo,hi); bp=P[:,:,b].sum(-1)/tot if b.size else np.zeros((n_ep,n_ch))\n",
    "        feats[f\"{name}_rel_med\"]=np.median(bp,axis=1); feats[f\"{name}_rel_iqr\"]=np.subtract(*np.percentile(bp,[75,25],axis=1)); feats[f\"{name}_rel_std\"]=np.std(bp,axis=1)\n",
    "        head=name.split(\"_\",1)[0]; \n",
    "        if head in acc: acc[head]+=bp\n",
    "    alpha=np.median(acc[\"alpha\"],axis=1); theta=np.median(acc[\"theta\"],axis=1); beta=np.median(acc[\"beta\"],axis=1)\n",
    "    feats[\"alpha_rel_med\"]=alpha; feats[\"theta_rel_med\"]=theta; feats[\"beta_rel_med\"]=beta\n",
    "    feats[\"theta_over_alpha\"]=theta/np.maximum(alpha,1e-6); feats[\"beta_over_alpha\"]=beta/np.maximum(alpha,1e-6)\n",
    "    feats[\"(alpha+theta)_over_beta\"]=(alpha+theta)/np.maximum(beta,1e-6)\n",
    "    # entropy & centroid\n",
    "    p_band=P[:,:,aidx]; p_n=p_band/np.maximum(p_band.sum(-1,keepdims=True),1e-12)\n",
    "    H=-(p_n*np.log2(p_n+1e-12)).sum(-1); Hn=H/np.log2(p_band.shape[-1])\n",
    "    feats[\"spec_entropy_med\"]=np.median(Hn,axis=1); feats[\"spec_entropy_std\"]=np.std(Hn,axis=1)\n",
    "    fb=f[aidx].reshape(1,1,-1); cen=(p_band*fb).sum(-1)/np.maximum(p_band.sum(-1),1e-12)\n",
    "    feats[\"spec_centroid_med\"]=np.median(cen,axis=1); feats[\"spec_centroid_std\"]=np.std(cen,axis=1)\n",
    "    # Hjorth\n",
    "    d1=np.diff(X,axis=-1); var0=np.var(X,axis=-1)+1e-12; var1=np.var(d1,axis=-1)+1e-12\n",
    "    feats[\"hjorth_activity_med\"]=np.median(var0,axis=1); feats[\"hjorth_mobility_med\"]=np.median(np.sqrt(var1/var0),axis=1)\n",
    "    d2=np.diff(d1,axis=-1); var2=np.var(d2,axis=-1)+1e-12\n",
    "    feats[\"hjorth_complexity_med\"]=np.median(np.sqrt((var2/var1)/(var1/var0)),axis=1)\n",
    "    return pd.DataFrame(feats)\n",
    "\n",
    "def occipital_picks(raw):\n",
    "    names=[n.upper().strip() for n in raw.ch_names]\n",
    "    wanted=(\"O1\",\"O2\",\"OZ\",\"POZ\",\"PO3\",\"PO4\")\n",
    "    idx=[i for i,n in enumerate(names) if n in wanted]\n",
    "    return idx if idx else mne.pick_types(raw.info, eeg=True)\n",
    "\n",
    "def alpha_index(raw, band):\n",
    "    sf=raw.info[\"sfreq\"]\n",
    "    f,P = welch(raw.get_data(), fs=sf, nperseg=min(int(sf*2), raw.n_times), noverlap=int(min(int(sf*2), raw.n_times)/2), axis=-1, average=\"median\")\n",
    "    idx=(f>=band[0])&(f<band[1]); a=P[:,idx].sum(-1); tot=P.sum(-1)+1e-12\n",
    "    oi=occipital_picks(raw)\n",
    "    return float((a[oi].sum()/tot[oi].sum()) if len(oi) else (a.sum()/tot.sum()))\n",
    "\n",
    "def butter_bandpass_array(X, lo, hi, sf, order=4):\n",
    "    X2=X.reshape(X.shape[0]*X.shape[1], X.shape[2])\n",
    "    Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, method=\"iir\",\n",
    "                              iir_params=dict(order=order, ftype=\"butter\"), verbose=False)\n",
    "    return Xf.reshape(X.shape)\n",
    "\n",
    "def fbcsp_auc_R03(raw):\n",
    "    # Build labeled 2s segments inside T0/T1/T2; GroupKFold over segment ids\n",
    "    want={\"C3\",\"Cz\",\"C4\",\"FC3\",\"FCz\",\"FC4\",\"CP3\",\"CPz\",\"CP4\"}\n",
    "    picks=[i for i,ch in enumerate(raw.ch_names) if ch.upper().strip() in want]\n",
    "    if len(picks)<3: picks=mne.pick_types(raw.info, eeg=True)\n",
    "    raw=raw.copy().pick(picks)\n",
    "    anns=raw.annotations\n",
    "    if anns is None or len(anns)==0: return np.nan\n",
    "    sf=raw.info[\"sfreq\"]\n",
    "    segs=[]; gid=0\n",
    "    for o,d,s in zip(anns.onset, anns.duration, anns.description):\n",
    "        su=str(s).upper()\n",
    "        if \"T0\" in su or \"T1\" in su or \"T2\" in su:\n",
    "            lab=0 if \"T0\" in su else 1; segs.append((float(o), float(o+d), lab, gid)); gid+=1\n",
    "    X=[]; y=[]; g=[]\n",
    "    for a,b,lab,segid in segs:\n",
    "        t=a\n",
    "        while t+2.0 <= b-1e-6:\n",
    "            s=int(t*sf); e=s+int(2.0*sf)\n",
    "            X.append(raw.get_data(start=s, stop=e)); y.append(lab); g.append(segid); t+=2.0\n",
    "    if not X: return np.nan\n",
    "    X=np.stack(X,0); y=np.array(y,int); g=np.array(g,int)\n",
    "    proba=np.zeros(len(y),float); gkf=GroupKFold(n_splits=min(5,max(2,len(np.unique(g)))))\n",
    "    for tr,te in gkf.split(X,y,g):\n",
    "        feats_tr, feats_te=[],[]\n",
    "        for lo,hi in CFG.FB_BANDS:\n",
    "            Xtr=butter_bandpass_array(X[tr], lo, hi, sf); Xte=butter_bandpass_array(X[te], lo, hi, sf)\n",
    "            csp=CSP(n_components=6, reg=\"ledoit_wolf\", log=True, norm_trace=False); csp.fit(Xtr, y[tr])\n",
    "            feats_tr.append(csp.transform(Xtr)); feats_te.append(csp.transform(Xte))\n",
    "        Xtr_fb=np.concatenate(feats_tr,1); Xte_fb=np.concatenate(feats_te,1)\n",
    "        clf=LDA(solver=\"lsqr\", shrinkage=\"auto\").fit(Xtr_fb, y[tr]); proba[te]=clf.predict_proba(Xte_fb)[:,1]\n",
    "    return float(roc_auc_score(y, proba))\n",
    "\n",
    "# -----------------------------\n",
    "# 1) K-sweep (3..8) on S001\n",
    "# -----------------------------\n",
    "def k_sweep_s001():\n",
    "    F = pd.read_csv(CFG.RUN/\"features.csv\")\n",
    "    M = pd.read_csv(CFG.RUN/\"metadata.csv\").sort_values([\"file\",\"t_start_s\",\"t_end_s\"]).reset_index(drop=True)\n",
    "    L = pd.read_csv(CFG.RUN/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "    scaler0 = StandardScaler().fit(F.values); Z = scaler0.transform(F.values)\n",
    "    pca0 = PCA(n_components=min(20,Z.shape[1]), random_state=42).fit(Z); Z = pca0.transform(Z)\n",
    "    rng=default_rng(42)\n",
    "    rows=[]\n",
    "    for K in CFG.K_RANGE:\n",
    "        km = KMeans(n_clusters=K, n_init=\"auto\", random_state=42).fit(Z); lab = km.labels_\n",
    "        # stability (bootstrap ARI)\n",
    "        def ARI(a,b):\n",
    "            from sklearn.metrics import adjusted_rand_score\n",
    "            return adjusted_rand_score(a,b)\n",
    "        Bs=100; idxs=[rng.choice(len(lab), size=int(0.8*len(lab)), replace=False) for _ in range(Bs)]\n",
    "        boot_ari = np.mean([ARI(L[idx], KMeans(n_clusters=K, n_init=\"auto\", random_state=1000+i).fit(Z[idx]).labels_) for i,idx in enumerate(idxs)])\n",
    "        # grammar gain on S001 only (Î”xent vs unigram)\n",
    "        # T1\n",
    "        Kk=int(K); T1=np.zeros((Kk,Kk)); \n",
    "        for a,b in zip(lab[:-1], lab[1:]): T1[a,b]+=1\n",
    "        eps=1e-6; T1=(T1+eps)/(T1.sum(axis=1, keepdims=True)+eps*Kk)\n",
    "        pi=T1.sum(axis=0); pi=pi/(pi.sum()+1e-12)\n",
    "        def xent(seq, T): \n",
    "            return -np.mean([math.log(T[a,b]+1e-12) for a,b in zip(seq[:-1], seq[1:])])\n",
    "        H_T1=xent(lab, T1); H_pi = -np.mean([math.log(pi[b]+1e-12) for b in lab[1:]])\n",
    "        rows.append({\"K\":K, \"boot_ARI\":boot_ari, \"Î”xent\": H_pi-H_T1})\n",
    "    df=pd.DataFrame(rows); df.to_csv(CFG.OUT/\"k_sweep_s001.csv\", index=False); return df\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Robustness sweeps\n",
    "# -----------------------------\n",
    "def robustness_sweeps():\n",
    "    # Use S001 R01â€“R03 for identity/function & grammar proxies under perturbations\n",
    "    # Perturbations: noise SNR âˆˆ {inf, 20dB, 10dB, 5dB}; channel dropout âˆˆ {0, 10%, 20%}; resample âˆˆ {200, 250, 500}; jitter âˆˆ {0, 0.25s}\n",
    "    # Return summary CSV\n",
    "    def add_noise(X, snr_db):\n",
    "        if np.isinf(snr_db): return X\n",
    "        sig = np.std(X, axis=-1, keepdims=True)\n",
    "        noise = np.random.randn(*X.shape)*sig\n",
    "        alpha = 10**(-snr_db/20)\n",
    "        return X + alpha*noise\n",
    "    def drop_channels(raw, frac):\n",
    "        raw = raw.copy()\n",
    "        chs = mne.pick_types(raw.info, eeg=True)\n",
    "        k = int(frac*len(chs))\n",
    "        if k>0:\n",
    "            drop = np.random.choice(chs, size=k, replace=False)\n",
    "            raw.drop_channels([raw.ch_names[i] for i in drop])\n",
    "        return raw\n",
    "    def resample_to(raw, fs):\n",
    "        if abs(raw.info[\"sfreq\"]-fs)>1e-6:\n",
    "            raw = raw.copy().resample(fs, npad=\"auto\", verbose=False)\n",
    "        return raw\n",
    "    rows=[]\n",
    "    for run,cond in [(\"S001R01.edf\",\"EO\"),(\"S001R02.edf\",\"EC\"),(\"S001R03.edf\",\"R03\")]:\n",
    "        p = CFG.RUN/\"brainwaves_rebuilt\"/run\n",
    "        if not p.exists(): continue\n",
    "        base = load_raw_edf(p, band=(0.5,80.0))\n",
    "        for snr in [np.inf,20,10,5]:\n",
    "            for frac in [0.0,0.1,0.2]:\n",
    "                for fs in [200.0,250.0,500.0]:\n",
    "                    for jitter in [0.0,0.25]:\n",
    "                        raw = resample_to(base, fs)\n",
    "                        # apply channel drop\n",
    "                        raw = drop_channels(raw, frac)\n",
    "                        # get epoch windows with jitter\n",
    "                        sf = raw.info[\"sfreq\"]; ts = np.arange(jitter, raw.n_times/sf - CFG.EPOCH_LEN + 1e-9, CFG.STEP)\n",
    "                        # inject noise on the fly in features: we perturb the segment data before welch\n",
    "                        feats=[]; labels=[]\n",
    "                        for t in ts:\n",
    "                            s=int(t*sf); e=s+int(CFG.EPOCH_LEN*sf)\n",
    "                            seg = raw.get_data(start=s, stop=e)\n",
    "                            seg = add_noise(seg, snr)\n",
    "                            n_ch,n_t = seg.shape\n",
    "                            # quick feature: alpha index proxy & beta power\n",
    "                            f,P = welch(seg, fs=sf, nperseg=min(int(sf*2), n_t), noverlap=int(min(int(sf*2), n_t)/2), axis=-1, average=\"median\")\n",
    "                            a = P[:, (f>=8)&(f<13)].sum(-1); b = P[:, (f>=13)&(f<30)].sum(-1)\n",
    "                            feats.append([a.mean(), b.mean()])\n",
    "                        feats = np.array(feats)\n",
    "                        rows.append({\"run\":run, \"SNR\":snr, \"drop\":frac, \"fs\":fs, \"jitter\":jitter, \n",
    "                                     \"alpha_idx_proxy\": np.mean(feats[:,0]) if len(feats) else np.nan,\n",
    "                                     \"beta_power_proxy\": np.mean(feats[:,1]) if len(feats) else np.nan})\n",
    "    df=pd.DataFrame(rows); df.to_csv(CFG.OUT/\"robustness_sweeps.csv\", index=False); return df\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Calibration sensitivity (IAF band & alpha-threshold)\n",
    "# -----------------------------\n",
    "def calibration_sensitivity(subject=\"S002\"):\n",
    "    r01=CFG.GEN/f\"{subject}R01.edf\"; r02=CFG.GEN/f\"{subject}R02.edf\"\n",
    "    if not (r01.exists() and r02.exists()): return None\n",
    "    raw_eo=load_raw_edf(r01, band=(0.5,45.0)); raw_ec=load_raw_edf(r02, band=(0.5,45.0))\n",
    "    # baseline IAF from EC\n",
    "    sf=raw_ec.info[\"sfreq\"]\n",
    "    f,P = welch(raw_ec.get_data(), fs=sf, nperseg=min(int(sf*2), raw_ec.n_times), noverlap=int(min(int(sf*2), raw_ec.n_times)/2), axis=-1, average=\"median\")\n",
    "    band=(f>=7)&(f<=14); iaf=float(np.clip(f[band][np.argmax(P[occipital_picks(raw_ec)][:,band].mean(0))],8.0,12.0))\n",
    "    # sweep\n",
    "    rows=[]\n",
    "    for d in [0.5,1.0,1.5,2.0]:\n",
    "        ab=(max(6.0,iaf-d), min(14.0, iaf+d))\n",
    "        def idx_run(raw,cond):\n",
    "            sf=raw.info[\"sfreq\"]; ts=np.arange(0, raw.n_times/sf - CFG.EPOCH_LEN + 1e-9, CFG.STEP)\n",
    "            preds=[]\n",
    "            # global thr from this subject (midpoint)\n",
    "            thr=0.5*(alpha_index(raw_ec,ab)+alpha_index(raw_eo,ab))\n",
    "            for t0 in ts:\n",
    "                s=int(t0*sf); e=s+int(CFG.EPOCH_LEN*sf); seg=raw.get_data(start=s, stop=e)\n",
    "                f,P = welch(seg, fs=sf, nperseg=min(int(sf*2), e-s), noverlap=int(min(int(sf*2), e-s)/2), axis=-1, average=\"median\")\n",
    "                idx=(f>=ab[0])&(f<ab[1]); a=P[:,idx].sum(-1); tot=P.sum(-1)+1e-12\n",
    "                oi=occipital_picks(raw); ai=(a[oi].sum()/tot[oi].sum()) if len(oi) else (a.sum()/tot.sum())\n",
    "                preds.append(\"EC\" if ai>=thr else \"EO\")\n",
    "            return pd.DataFrame({\"gt\":cond,\"pred\":preds})\n",
    "        df=pd.concat([idx_run(raw_ec,\"EC\"), idx_run(raw_eo,\"EO\")], ignore_index=True)\n",
    "        acc=(df[\"gt\"]==df[\"pred\"]).mean()\n",
    "        rows.append({\"subject\":subject, \"IAF\":iaf, \"band_halfwidth\":d, \"EOEC_acc\":acc})\n",
    "    sens=pd.DataFrame(rows); sens.to_csv(CFG.OUT/f\"calibration_sensitivity_{subject}.csv\", index=False); return sens\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Drift tests (window misalignment)\n",
    "# -----------------------------\n",
    "def drift_tests(subject=\"S003\"):\n",
    "    r03=CFG.GEN/f\"{subject}R03.edf\"\n",
    "    if not r03.exists(): return None\n",
    "    raw=load_raw_edf(r03, band=(0.5,40.0))\n",
    "    rows=[]\n",
    "    for jitter in [0.0, 0.1, 0.2, 0.3, 0.5]:  # seconds\n",
    "        sf=raw.info[\"sfreq\"]\n",
    "        ts=np.arange(jitter, raw.n_times/sf - CFG.EPOCH_LEN + 1e-9, CFG.STEP)\n",
    "        # quick Î¼/Î² log-var baseline as proxy\n",
    "        want={\"C3\",\"Cz\",\"C4\",\"FC3\",\"FCz\",\"FC4\",\"CP3\",\"CPz\",\"CP4\"}\n",
    "        picks=[i for i,ch in enumerate(raw.ch_names) if ch.upper().strip() in want]\n",
    "        if len(picks)<3: picks=mne.pick_types(raw.info, eeg=True)\n",
    "        _raw=raw.copy().pick(picks)\n",
    "        anns=_raw.annotations\n",
    "        if anns is None or len(anns)==0: continue\n",
    "        # build X,y,g on the misaligned grid (approximation)\n",
    "        X=[]; y=[]; g=[]\n",
    "        gid=0\n",
    "        for o,d,s in zip(anns.onset, anns.duration, anns.description):\n",
    "            su=str(s).upper()\n",
    "            if \"T0\" in su or \"T1\" in su or \"T2\" in su:\n",
    "                lab=0 if \"T0\" in su else 1\n",
    "                # step through with jitter origin inside this segment\n",
    "                t=max(o, jitter)\n",
    "                while t+2.0 <= o+d-1e-6:\n",
    "                    sidx=int(t*sf); eidx=sidx+int(2.0*sf)\n",
    "                    X.append(_raw.get_data(start=sidx, stop=eidx)); y.append(lab); g.append(gid); t+=2.0\n",
    "                gid+=1\n",
    "        if not X: continue\n",
    "        X=np.stack(X,0); y=np.array(y,int); g=np.array(g,int)\n",
    "        # log-var per band\n",
    "        def logvar(X,lo,hi):\n",
    "            X2=X.reshape(X.shape[0]*X.shape[1],X.shape[2])\n",
    "            Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, method=\"iir\", iir_params=dict(order=4, ftype=\"butter\"), verbose=False)\n",
    "            v=np.log(np.var(Xf.reshape(X.shape),axis=-1)+1e-12); return v\n",
    "        F=np.concatenate([logvar(X,8,13),logvar(X,13,20),logvar(X,20,30)],axis=1)\n",
    "        proba=np.zeros(len(y),float); gkf=GroupKFold(n_splits=min(5,max(2,len(np.unique(g)))))\n",
    "        for tr,te in gkf.split(F,y,g):\n",
    "            clf=LDA(solver=\"lsqr\", shrinkage=\"auto\").fit(F[tr],y[tr]); proba[te]=clf.predict_proba(F[te])[:,1]\n",
    "        rows.append({\"subject\":subject,\"jitter_s\":jitter,\"AUC\":roc_auc_score(y,proba)})\n",
    "    df=pd.DataFrame(rows); df.to_csv(CFG.OUT/f\"drift_tests_{subject}.csv\", index=False); return df\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Mechanism invariance (re-reference)\n",
    "# -----------------------------\n",
    "def mechanism_invariance():\n",
    "    # S001R01â€“R03: compare avg-ref (already) vs Cz-reference for microstate MI\n",
    "    from scipy.signal import find_peaks\n",
    "    def load_ref(p, ref=\"average\"):\n",
    "        raw=mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "        if abs(raw.info[\"sfreq\"]-CFG.TARGET_SF)>1e-6: raw.resample(CFG.TARGET_SF, npad=\"auto\", verbose=False)\n",
    "        raw.filter(2.0,20.0, verbose=False)\n",
    "        if ref==\"cz\":\n",
    "            try: raw.set_eeg_reference([\"Cz\"], projection=False, verbose=False)\n",
    "            except Exception: pass\n",
    "        else:\n",
    "            raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "        try: raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"), on_missing=\"ignore\")\n",
    "        except Exception: pass\n",
    "        return raw\n",
    "    def ms_MI(raw):\n",
    "        X=raw.get_data(); sf=raw.info[\"sfreq\"]\n",
    "        peaks,_=find_peaks(X.std(axis=0), distance=int((10/1000.0)*sf))\n",
    "        Xp=X[:,peaks].T\n",
    "        # simple kmeans-like polarity-agnostic\n",
    "        rng=default_rng(42); Xn=Xp-Xp.mean(1,keepdims=True); Xn=Xn/(np.linalg.norm(Xn,axis=1,keepdims=True)+1e-12)\n",
    "        C=Xn[rng.choice(len(Xn),4,replace=False)].copy()\n",
    "        for _ in range(60):\n",
    "            corr=Xn@C.T; lab=np.argmax(np.abs(corr),axis=1); sign=np.sign(corr[np.arange(len(Xn)),lab])\n",
    "            Cn=[]; \n",
    "            for k in range(4):\n",
    "                m=(lab==k); v=(Xn[m]*sign[m][:,None]).mean(0) if np.any(m) else C[k]; v=v/(np.linalg.norm(v)+1e-12); Cn.append(v)\n",
    "            Cn=np.stack(Cn,0)\n",
    "            if np.allclose(C,Cn,atol=1e-5): break; C=Cn\n",
    "        # backfit on continuous\n",
    "        M=C/(np.linalg.norm(C,axis=1,keepdims=True)+1e-12); X0=raw.get_data(); X0n=X0/(np.linalg.norm(X0,axis=0,keepdims=True)+1e-12)\n",
    "        corr=M@X0n; ms=np.argmax(np.abs(corr),axis=0)\n",
    "        # epoch dominant for S001 metadata\n",
    "        meta=pd.read_csv(CFG.RUN/\"metadata.csv\").sort_values([\"file\",\"t_start_s\",\"t_end_s\"]).reset_index(drop=True)\n",
    "        labs=pd.read_csv(CFG.RUN/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "        # pick the file for this raw\n",
    "        fname = [f for f in meta[\"file\"].unique() if str(p).endswith(f)][0]\n",
    "        mask=(meta[\"file\"]==fname).to_numpy(); starts=meta.loc[mask,\"t_start_s\"].to_numpy(); ends=meta.loc[mask,\"t_end_s\"].to_numpy(); letters=labs[mask]\n",
    "        dom=[]\n",
    "        for t0,t1 in zip(starts, ends):\n",
    "            s=int(round(t0*sf)); e=int(round(t1*sf)); arr=ms[s:e]; \n",
    "            dom.append(np.bincount(arr, minlength=4).argmax() if e>s and len(arr)>0 else -1)\n",
    "        dom=np.array(dom); valid=(dom>=0)\n",
    "        L=letters[valid]; Mv=dom[valid]\n",
    "        # MI(letter;Î¼)\n",
    "        def entropy(p): p=p[p>0]; return -np.sum(p*np.log2(p))\n",
    "        Pm=np.bincount(Mv,minlength=4)/len(Mv); Hm=entropy(Pm); Hm_L=0.0\n",
    "        for s in range(4):\n",
    "            mk=(L==s)\n",
    "            if not np.any(mk): continue\n",
    "            P=np.bincount(Mv[mk], minlength=4)/mk.sum(); Hm_L+=mk.mean()*entropy(P)\n",
    "        return float(Hm-Hm_L), int(len(Mv))\n",
    "    rows=[]\n",
    "    for r in [1,2,3]:\n",
    "        p=CFG.RUN/\"brainwaves_rebuilt\"/f\"S001R{r:02d}.edf\"\n",
    "        if not p.exists(): continue\n",
    "        raw_avg=load_ref(p,\"average\"); raw_cz=load_ref(p,\"cz\")\n",
    "        mi_avg,n1 = ms_MI(raw_avg); mi_cz,n2=ms_MI(raw_cz)\n",
    "        rows.append({\"run\":f\"S001R{r:02d}\",\"MI_avgref\":mi_avg,\"MI_czref\":mi_cz,\"n_ep\":n1})\n",
    "    df=pd.DataFrame(rows); df.to_csv(CFG.OUT/\"mechanism_invariance.csv\", index=False); return df\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Fairness / outliers (per-subject distribution)\n",
    "# -----------------------------\n",
    "def fairness_outliers():\n",
    "    eoec = pd.read_csv(CFG.REP/\"generalization\"/\"eoec_iaf_summary.csv\")\n",
    "    task = pd.read_csv(CFG.REP/\"generalization\"/\"task_fbcsp_summary.csv\")\n",
    "    df = eoec.merge(task, on=\"subject\", how=\"inner\")\n",
    "    # z-scores per metric\n",
    "    for col in [\"acc\",\"AUC_task\"]:\n",
    "        mu,sd = df[col].mean(), df[col].std(ddof=1)+1e-12\n",
    "        df[f\"z_{col}\"] = (df[col]-mu)/sd\n",
    "    df[\"outlier_flag\"] = (np.abs(df[[\"z_acc\",\"z_AUC_task\"]])>2.5).any(axis=1)\n",
    "    df.to_csv(CFG.OUT/\"fairness_outliers.csv\", index=False)\n",
    "    return df\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Adversarial nulls (label/features shuffles)\n",
    "# -----------------------------\n",
    "def adversarial_nulls():\n",
    "    # S001 labels vs shuffled; features row-shuffle vs intact â†’ silhouette & Î”xent drop\n",
    "    F=pd.read_csv(CFG.RUN/\"features.csv\"); L=pd.read_csv(CFG.RUN/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "    scaler=StandardScaler().fit(F.values); Z=scaler.transform(F.values)\n",
    "    pca=PCA(n_components=min(20,Z.shape[1]), random_state=42).fit(Z); Z=pca.transform(Z)\n",
    "    K=int(np.max(L)+1)\n",
    "    # real\n",
    "    km=KMeans(n_clusters=K, n_init=\"auto\", random_state=42).fit(Z); lab=km.labels_\n",
    "    def xent_T1(seq):\n",
    "        T=np.zeros((K,K)); \n",
    "        for a,b in zip(seq[:-1], seq[1:]): T[a,b]+=1\n",
    "        eps=1e-6; T=(T+eps)/(T.sum(axis=1, keepdims=True)+eps*K)\n",
    "        pi=T.sum(axis=0); pi=pi/(pi.sum()+1e-12)\n",
    "        HT1=-np.mean([math.log(T[a,b]+1e-12) for a,b in zip(seq[:-1], seq[1:])])\n",
    "        Hpi=-np.mean([math.log(pi[b]+1e-12) for b in seq[1:]])\n",
    "        return Hpi-HT1\n",
    "    real_dx = xent_T1(lab)\n",
    "    # label shuffle\n",
    "    rng=default_rng(13); Lperm=rng.permutation(lab)\n",
    "    null_dx = xent_T1(Lperm)\n",
    "    # feature row shuffle (destroy geometry)\n",
    "    Fperm = F.sample(frac=1.0, random_state=7).values\n",
    "    Zp=pca.transform(scaler.transform(Fperm)); labp=KMeans(n_clusters=K, n_init=\"auto\", random_state=7).fit(Zp).labels_\n",
    "    shuf_dx=xent_T1(labp)\n",
    "    out=pd.DataFrame([{\"scenario\":\"real\",\"Î”xent\":real_dx},{\"scenario\":\"label_shuffle\",\"Î”xent\":null_dx},{\"scenario\":\"feature_shuffle\",\"Î”xent\":shuf_dx}])\n",
    "    out.to_csv(CFG.OUT/\"adversarial_nulls.csv\", index=False); return out\n",
    "\n",
    "# -----------------------------\n",
    "# 8) Compose 1-page PDF & master JSON/CSV\n",
    "# -----------------------------\n",
    "def compose_report():\n",
    "    # collect\n",
    "    kdf = pd.read_csv(CFG.OUT/\"k_sweep_s001.csv\") if (CFG.OUT/\"k_sweep_s001.csv\").exists() else None\n",
    "    rbd = pd.read_csv(CFG.OUT/\"robustness_sweeps.csv\") if (CFG.OUT/\"robustness_sweeps.csv\").exists() else None\n",
    "    fair= pd.read_csv(CFG.OUT/\"fairness_outliers.csv\") if (CFG.OUT/\"fairness_outliers.csv\").exists() else None\n",
    "    inv = pd.read_csv(CFG.OUT/\"mechanism_invariance.csv\") if (CFG.OUT/\"mechanism_invariance.csv\").exists() else None\n",
    "    adv = pd.read_csv(CFG.OUT/\"adversarial_nulls.csv\") if (CFG.OUT/\"adversarial_nulls.csv\").exists() else None\n",
    "    # 1-page PDF\n",
    "    pdf = PdfPages(CFG.OUT/\"CALM_Proof_Battery.pdf\")\n",
    "    # Page: K-sweep + adversarial\n",
    "    plt.figure(figsize=(10,6))\n",
    "    if kdf is not None:\n",
    "        plt.subplot(2,1,1); \n",
    "        plt.plot(kdf[\"K\"], kdf[\"boot_ARI\"], \"o-\", label=\"stability (boot ARI)\")\n",
    "        plt.twinx(); plt.plot(kdf[\"K\"], kdf[\"Î”xent\"], \"s--\", color=\"tab:orange\", label=\"Î”xent (S001)\")\n",
    "        plt.title(\"K-sweep (S001)\"); plt.xlabel(\"K\"); plt.legend(loc=\"upper left\")\n",
    "    if adv is not None:\n",
    "        plt.subplot(2,1,2); \n",
    "        plt.bar(adv[\"scenario\"], adv[\"Î”xent\"]); plt.title(\"Adversarial sanity (Î”xent)\")\n",
    "    pdf.savefig(bbox_inches=\"tight\"); plt.close()\n",
    "    # Page: robustness summary\n",
    "    if rbd is not None and len(rbd):\n",
    "        plt.figure(figsize=(10,4))\n",
    "        grp=rbd.groupby([\"SNR\",\"drop\",\"fs\"])[\"alpha_idx_proxy\"].mean().reset_index()\n",
    "        plt.plot(grp.index, grp[\"alpha_idx_proxy\"], \".-\")\n",
    "        plt.title(\"Robustness (alpha-index proxy) vs perturbations\"); pdf.savefig(bbox_inches=\"tight\"); plt.close()\n",
    "    # Page: invariance & fairness\n",
    "    plt.figure(figsize=(10,5))\n",
    "    if inv is not None: \n",
    "        plt.subplot(1,2,1); \n",
    "        plt.bar(np.arange(len(inv))+0.00, inv[\"MI_avgref\"], width=0.45, label=\"avg-ref\")\n",
    "        plt.bar(np.arange(len(inv))+0.45, inv[\"MI_czref\"],  width=0.45, label=\"Cz-ref\")\n",
    "        plt.xticks(np.arange(len(inv)), inv[\"run\"]); plt.legend(); plt.title(\"Mechanism invariance (MI)\")\n",
    "    if fair is not None:\n",
    "        plt.subplot(1,2,2); \n",
    "        plt.scatter(fair[\"z_acc\"], fair[\"z_AUC_task\"], c=fair[\"outlier_flag\"].map({True:\"r\",False:\"b\"}))\n",
    "        plt.axvline(2.5, ls=\"--\", c=\"gray\"); plt.axvline(-2.5, ls=\"--\", c=\"gray\")\n",
    "        plt.axhline(2.5, ls=\"--\", c=\"gray\"); plt.axhline(-2.5, ls=\"--\", c=\"gray\")\n",
    "        plt.title(\"Per-subject z-scores (EO/EC vs Task AUC)\")\n",
    "    pdf.savefig(bbox_inches=\"tight\"); plt.close()\n",
    "    pdf.close()\n",
    "    print(\"Proof Battery PDF â†’\", CFG.OUT/\"CALM_Proof_Battery.pdf\")\n",
    "\n",
    "# -----------------------------\n",
    "# Run battery\n",
    "# -----------------------------\n",
    "print(\"[1] K-sweepâ€¦\"); k_sweep_s001()\n",
    "print(\"[2] Robustnessâ€¦\"); robustness_sweeps()\n",
    "print(\"[3] Calibration sensitivity S002/S003â€¦\"); calibration_sensitivity(\"S002\"); calibration_sensitivity(\"S003\")\n",
    "print(\"[4] Drift tests S003â€¦\"); drift_tests(\"S003\")\n",
    "print(\"[5] Mechanism invarianceâ€¦\"); mechanism_invariance()\n",
    "print(\"[6] Fairnessâ€¦\"); fairness_outliers()\n",
    "print(\"[7] Adversarial nullsâ€¦\"); adversarial_nulls()\n",
    "print(\"[8] Compose reportâ€¦\"); compose_report()\n",
    "\n",
    "# Final scoreboard write\n",
    "summary = {\n",
    "  \"battery\": \"CALM Stress & Proof v1\",\n",
    "  \"k_sweep_csv\": str(CFG.OUT/\"k_sweep_s001.csv\"),\n",
    "  \"robustness_csv\": str(CFG.OUT/\"robustness_sweeps.csv\"),\n",
    "  \"calibration_sensitivity_S002\": str(CFG.OUT/\"calibration_sensitivity_S002.csv\"),\n",
    "  \"calibration_sensitivity_S003\": str(CFG.OUT/\"calibration_sensitivity_S003.csv\"),\n",
    "  \"drift_tests_S003\": str(CFG.OUT/\"drift_tests_S003.csv\"),\n",
    "  \"invariance_csv\": str(CFG.OUT/\"mechanism_invariance.csv\"),\n",
    "  \"fairness_csv\": str(CFG.OUT/\"fairness_outliers.csv\"),\n",
    "  \"adversarial_csv\": str(CFG.OUT/\"adversarial_nulls.csv\"),\n",
    "  \"proof_pdf\": str(CFG.OUT/\"CALM_Proof_Battery.pdf\")\n",
    "}\n",
    "(Path(CFG.OUT/\"proof_battery_summary.json\")).write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
    "print(\"Summary JSON â†’\", CFG.OUT/\"proof_battery_summary.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d1f7d3c8-e7e8-4822-a1ef-8067a5a2f40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€¢ Repairing robustness_sweeps.csv â€¦\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Rebuilt â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\validation_battery\\robustness_sweeps.csv (216 rows)\n",
      "â€¢ Re-composing Proof Battery PDF â€¦\n",
      "Proof Battery PDF â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\validation_battery\\CALM_Proof_Battery.pdf\n",
      "\n",
      "Done. If you want to peek quickly:\n",
      "  - Robustness CSV : E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\validation_battery\\robustness_sweeps.csv\n",
      "  - PDF            : E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\validation_battery\\CALM_Proof_Battery.pdf\n"
     ]
    }
   ],
   "source": [
    "# === CALM Proof Battery â€” Repair & Resume (robustness + compose) ===\n",
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt, mne, requests\n",
    "from pathlib import Path\n",
    "from scipy.signal import welch\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# --- match your earlier CFG ---\n",
    "class CFG:\n",
    "    ROOT      = Path(r\"E:\\CNT\\artifacts\")\n",
    "    RUN       = ROOT / \"cog_alphabet_hybrid_v1\"\n",
    "    REP       = ROOT / \"cog_alphabet_report_hybrid_v1\"\n",
    "    OUT       = REP  / \"validation_battery\"\n",
    "    TARGET_SF = 250.0\n",
    "    EPOCH_LEN = 2.0\n",
    "    STEP      = 0.5\n",
    "CFG = CFG()\n",
    "CFG.OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def safe_read_csv(path: Path):\n",
    "    try:\n",
    "        if path.exists() and path.stat().st_size > 0:\n",
    "            return pd.read_csv(path)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def load_raw_edf(p: Path, band=(0.5, 80.0)):\n",
    "    raw = mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"] - CFG.TARGET_SF) > 1e-6:\n",
    "        raw.resample(CFG.TARGET_SF, npad=\"auto\", verbose=False)\n",
    "    ny = raw.info[\"sfreq\"] / 2.0\n",
    "    raw.filter(band[0], min(band[1], ny - 1.0), verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "    return raw\n",
    "\n",
    "def find_or_fetch_s001():\n",
    "    candidates = [\n",
    "        CFG.RUN / \"brainwaves_rebuilt\",\n",
    "        CFG.ROOT / \"s001_edf\",\n",
    "        CFG.ROOT,\n",
    "    ]\n",
    "    for d in candidates:\n",
    "        if d.exists() and all((d / f\"S001R{r:02d}.edf\").exists() for r in [1, 2, 3]):\n",
    "            return d\n",
    "    # fetch if not found\n",
    "    target = CFG.ROOT / \"s001_edf\"\n",
    "    target.mkdir(parents=True, exist_ok=True)\n",
    "    for r in [1, 2, 3]:\n",
    "        url = f\"https://physionet.org/files/eegmmidb/1.0.0/S001/S001R{r:02d}.edf\"\n",
    "        dest = target / f\"S001R{r:02d}.edf\"\n",
    "        if dest.exists() and dest.stat().st_size > 0: \n",
    "            continue\n",
    "        with requests.get(url, stream=True, timeout=60) as resp:\n",
    "            resp.raise_for_status()\n",
    "            with open(dest, \"wb\") as f:\n",
    "                for ch in resp.iter_content(8192):\n",
    "                    if ch: f.write(ch)\n",
    "    return target\n",
    "\n",
    "# ---------- rebuild robustness_sweeps.csv safely ----------\n",
    "def rebuild_robustness():\n",
    "    s001_dir = find_or_fetch_s001()\n",
    "    rows = []\n",
    "    def add_noise(X, snr_db):\n",
    "        if np.isinf(snr_db): return X\n",
    "        sig = np.std(X, axis=-1, keepdims=True) + 1e-12\n",
    "        noise = np.random.randn(*X.shape) * sig\n",
    "        alpha = 10 ** (-snr_db / 20)\n",
    "        return X + alpha * noise\n",
    "\n",
    "    def drop_channels(raw, frac):\n",
    "        raw = raw.copy()\n",
    "        chs = mne.pick_types(raw.info, eeg=True)\n",
    "        k = int(frac * len(chs))\n",
    "        if k > 0:\n",
    "            to_drop = np.random.choice(chs, size=k, replace=False)\n",
    "            raw.drop_channels([raw.ch_names[i] for i in to_drop])\n",
    "        return raw\n",
    "\n",
    "    def resample_to(raw, fs):\n",
    "        if abs(raw.info[\"sfreq\"] - fs) > 1e-6:\n",
    "            raw = raw.copy().resample(fs, npad=\"auto\", verbose=False)\n",
    "        return raw\n",
    "\n",
    "    for run, cond in [(\"S001R01.edf\", \"EO\"), (\"S001R02.edf\", \"EC\"), (\"S001R03.edf\", \"R03\")]:\n",
    "        p = s001_dir / run\n",
    "        if not p.exists():\n",
    "            print(\"skip (missing):\", p)\n",
    "            continue\n",
    "        base = load_raw_edf(p, band=(0.5, 80.0))\n",
    "        for snr in [np.inf, 20, 10, 5]:\n",
    "            for frac in [0.0, 0.1, 0.2]:\n",
    "                for fs in [200.0, 250.0, 500.0]:\n",
    "                    for jitter in [0.0, 0.25]:\n",
    "                        raw = resample_to(base, fs)\n",
    "                        raw = drop_channels(raw, frac)\n",
    "                        sf = raw.info[\"sfreq\"]\n",
    "                        ts = np.arange(jitter, raw.n_times / sf - CFG.EPOCH_LEN + 1e-9, CFG.STEP)\n",
    "                        if len(ts) == 0:\n",
    "                            continue\n",
    "                        a_proxy = []; b_proxy = []\n",
    "                        for t in ts:\n",
    "                            s = int(t * sf); e = s + int(CFG.EPOCH_LEN * sf)\n",
    "                            seg = raw.get_data(start=s, stop=e)\n",
    "                            seg = add_noise(seg, snr)\n",
    "                            f, P = welch(seg, fs=sf, nperseg=min(int(sf*2), seg.shape[1]),\n",
    "                                         noverlap=int(min(int(sf*2), seg.shape[1])/2),\n",
    "                                         axis=-1, average=\"median\")\n",
    "                            a = P[:, (f >= 8) & (f < 13)].sum(-1).mean()\n",
    "                            b = P[:, (f >= 13) & (f < 30)].sum(-1).mean()\n",
    "                            a_proxy.append(a); b_proxy.append(b)\n",
    "                        if a_proxy:\n",
    "                            rows.append({\n",
    "                                \"run\": run, \"SNR_dB\": snr, \"drop_frac\": frac, \"fs_Hz\": fs, \"jitter_s\": jitter,\n",
    "                                \"alpha_idx_proxy\": float(np.mean(a_proxy)),\n",
    "                                \"beta_power_proxy\": float(np.mean(b_proxy)),\n",
    "                                \"n_epochs\": int(len(a_proxy))\n",
    "                            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    out = CFG.OUT / \"robustness_sweeps.csv\"\n",
    "    if len(df):\n",
    "        df.to_csv(out, index=False)\n",
    "        print(\"Rebuilt â†’\", out, f\"({len(df)} rows)\")\n",
    "    else:\n",
    "        # write a small placeholder with header to avoid EmptyDataError\n",
    "        pd.DataFrame(columns=[\"run\",\"SNR_dB\",\"drop_frac\",\"fs_Hz\",\"jitter_s\",\n",
    "                              \"alpha_idx_proxy\",\"beta_power_proxy\",\"n_epochs\"]).to_csv(out, index=False)\n",
    "        print(\"No robustness rows produced; wrote placeholder â†’\", out)\n",
    "\n",
    "# ---------- compose report with guards ----------\n",
    "def compose_report_hardened():\n",
    "    kdf  = safe_read_csv(CFG.OUT/\"k_sweep_s001.csv\")\n",
    "    rbd  = safe_read_csv(CFG.OUT/\"robustness_sweeps.csv\")\n",
    "    fair = safe_read_csv(CFG.OUT/\"fairness_outliers.csv\")\n",
    "    inv  = safe_read_csv(CFG.OUT/\"mechanism_invariance.csv\")\n",
    "    adv  = safe_read_csv(CFG.OUT/\"adversarial_nulls.csv\")\n",
    "\n",
    "    pdf = PdfPages(CFG.OUT/\"CALM_Proof_Battery.pdf\")\n",
    "\n",
    "    # Page: K-sweep + adversarial\n",
    "    plt.figure(figsize=(10,6))\n",
    "    if kdf is not None and len(kdf):\n",
    "        plt.subplot(2,1,1)\n",
    "        plt.plot(kdf[\"K\"], kdf[\"boot_ARI\"], \"o-\", label=\"stability (boot ARI)\")\n",
    "        ax2 = plt.twinx(); ax2.plot(kdf[\"K\"], kdf[\"Î”xent\"], \"s--\", color=\"tab:orange\", label=\"Î”xent (S001)\")\n",
    "        plt.title(\"K-sweep (S001)\"); plt.xlabel(\"K\"); \n",
    "    else:\n",
    "        plt.subplot(2,1,1); plt.axis(\"off\"); plt.text(0.5,0.5,\"K-sweep missing\", ha=\"center\")\n",
    "    if adv is not None and len(adv):\n",
    "        plt.subplot(2,1,2); plt.bar(adv[\"scenario\"], adv[\"Î”xent\"]); plt.title(\"Adversarial sanity (Î”xent)\")\n",
    "    else:\n",
    "        plt.subplot(2,1,2); plt.axis(\"off\"); plt.text(0.5,0.5,\"Adversarial sanity missing\", ha=\"center\")\n",
    "    pdf.savefig(bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    # Page: robustness summary\n",
    "    plt.figure(figsize=(10,4))\n",
    "    if rbd is not None and len(rbd):\n",
    "        grp = rbd.groupby([\"SNR_dB\",\"drop_frac\",\"fs_Hz\"])[\"alpha_idx_proxy\"].mean().reset_index()\n",
    "        plt.plot(np.arange(len(grp)), grp[\"alpha_idx_proxy\"], \".-\")\n",
    "        plt.title(\"Robustness (alpha-index proxy) vs perturbations\")\n",
    "    else:\n",
    "        plt.axis(\"off\"); plt.text(0.5,0.5,\"Robustness file empty (skipped)\", ha=\"center\")\n",
    "    pdf.savefig(bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    # Page: invariance & fairness\n",
    "    plt.figure(figsize=(10,5))\n",
    "    if inv is not None and len(inv):\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.bar(np.arange(len(inv))+0.00, inv[\"MI_avgref\"], width=0.45, label=\"avg-ref\")\n",
    "        plt.bar(np.arange(len(inv))+0.45, inv[\"MI_czref\"],  width=0.45, label=\"Cz-ref\")\n",
    "        plt.xticks(np.arange(len(inv)), inv[\"run\"]); plt.legend(); plt.title(\"Mechanism invariance (MI)\")\n",
    "    else:\n",
    "        plt.subplot(1,2,1); plt.axis(\"off\"); plt.text(0.5,0.5,\"Invariance missing\", ha=\"center\")\n",
    "\n",
    "    if fair is not None and len(fair):\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.scatter(fair[\"z_acc\"], fair[\"z_AUC_task\"], c=fair[\"outlier_flag\"].map({True:\"r\",False:\"b\"}))\n",
    "        for v in [2.5,-2.5]:\n",
    "            plt.axvline(v, ls=\"--\", c=\"gray\"); plt.axhline(v, ls=\"--\", c=\"gray\")\n",
    "        plt.title(\"Per-subject z-scores (EO/EC vs Task AUC)\")\n",
    "    else:\n",
    "        plt.subplot(1,2,2); plt.axis(\"off\"); plt.text(0.5,0.5,\"Fairness missing\", ha=\"center\")\n",
    "    pdf.savefig(bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    pdf.close()\n",
    "    print(\"Proof Battery PDF â†’\", CFG.OUT/\"CALM_Proof_Battery.pdf\")\n",
    "\n",
    "# ---------- run fix & resume ----------\n",
    "print(\"â€¢ Repairing robustness_sweeps.csv â€¦\")\n",
    "rebuild_robustness()\n",
    "\n",
    "print(\"â€¢ Re-composing Proof Battery PDF â€¦\")\n",
    "compose_report_hardened()\n",
    "\n",
    "print(\"\\nDone. If you want to peek quickly:\")\n",
    "print(\"  - Robustness CSV :\", CFG.OUT/'robustness_sweeps.csv')\n",
    "print(\"  - PDF            :\", CFG.OUT/'CALM_Proof_Battery.pdf')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "27617905-fdb1-4397-9314-22e8d9dbb1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Mechanism invarianceâ€¦\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[2] K-sweep + decisionâ€¦\n",
      "[3] Microstate-conditioned grammar (CMI)â€¦\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\AppData\\Local\\Temp\\ipykernel_2652\\3706393219.py:226: FutureWarning: Calling float on a single element Series is deprecated and will raise a TypeError in the future. Use float(ser.iloc[0]) instead\n",
      "  base_ari = float(df[df[\"K\"]==4][\"boot_ARI\"]) if (df[\"K\"]==4).any() else df[\"boot_ARI\"].max()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "[4] Generative adequacyâ€¦\n",
      "[5] Cross-subject grammar topologyâ€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\CNT\\.venv\\Lib\\site-packages\\sklearn\\manifold\\_mds.py:677: FutureWarning: The default value of `n_init` will change from 4 to 1 in 1.9.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6] Compose CALM_Groundbreaking_Addendum.pdf â€¦\n",
      "Addendum PDF â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\groundbreaking\\CALM_Groundbreaking_Addendum.pdf\n",
      "Summary JSON â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\groundbreaking\\groundbreaking_summary.json\n",
      "\n",
      "DONE. This addendum nails:\n",
      " â€¢ Invariance (avg-ref vs Cz)  â€¢ Pareto K*  â€¢ Mechanismâ†’grammar CMI  â€¢ Generative adequacy  â€¢ Cross-subject grammar map\n"
     ]
    }
   ],
   "source": [
    "# === CALM â€” Groundbreaking Mega Cell ===\n",
    "# Mechanism invariance (avg-ref vs Cz), K-decision stamp, microstate-conditioned grammar test (CMI),\n",
    "# generative adequacy (simulate + JS), cross-subject grammar topology (JS + MDS),\n",
    "# and a CALM_Groundbreaking_Addendum.pdf with all highlights.\n",
    "\n",
    "import os, re, io, json, math, time, shutil, numpy as np, pandas as pd, matplotlib.pyplot as plt, mne, requests\n",
    "from pathlib import Path\n",
    "from scipy.signal import welch, find_peaks\n",
    "from numpy.random import default_rng\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from mne.decoding import CSP\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# -----------------------------\n",
    "# CFG â€” EDIT IF NEEDED\n",
    "# -----------------------------\n",
    "class CFG:\n",
    "    ROOT      = Path(r\"E:\\CNT\\artifacts\")\n",
    "    RUN       = ROOT / \"cog_alphabet_hybrid_v1\"\n",
    "    REP       = ROOT / \"cog_alphabet_report_hybrid_v1\"\n",
    "    GEN       = ROOT / \"generalization_data\"\n",
    "    OUT       = REP  / \"groundbreaking\"\n",
    "    TARGET_SF = 250.0\n",
    "    EPOCH_LEN = 2.0\n",
    "    STEP      = 0.5\n",
    "    BANDS     = {\"delta\":(1,4),\"theta\":(4,8),\"alpha\":(8,13),\"beta_low\":(13,20),\"beta_high\":(20,35),\"gamma\":(35,80)}\n",
    "    FB_BANDS  = [(8,13),(13,20),(20,30)]\n",
    "    FB_TASK_THR = 0.50\n",
    "    K_RANGE   = list(range(3,9))\n",
    "CFG = CFG()\n",
    "CFG.OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# UTILITIES\n",
    "# -----------------------------\n",
    "def safe_read_csv(path: Path):\n",
    "    try:\n",
    "        if path.exists() and path.stat().st_size > 0:\n",
    "            return pd.read_csv(path)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def ensure_dir(p:Path): p.mkdir(parents=True, exist_ok=True); return p\n",
    "\n",
    "def load_raw_edf(p: Path, band=(0.5,80.0), ref=\"average\", montage=False):\n",
    "    raw = mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"]-CFG.TARGET_SF)>1e-6:\n",
    "        raw.resample(CFG.TARGET_SF, npad=\"auto\", verbose=False)\n",
    "    ny = raw.info[\"sfreq\"]/2.0\n",
    "    raw.filter(band[0], min(band[1], ny-1.0), verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        if ref==\"average\":\n",
    "            raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "        elif ref==\"cz\":\n",
    "            try: raw.set_eeg_reference([\"Cz\"], projection=False, verbose=False)\n",
    "            except Exception: pass\n",
    "    if montage:\n",
    "        try:\n",
    "            raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"),\n",
    "                            match_case=False, match_alias=True, on_missing=\"ignore\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    return raw\n",
    "\n",
    "def make_epochs(raw):\n",
    "    ov = max(0.0, CFG.EPOCH_LEN - CFG.STEP)\n",
    "    return mne.make_fixed_length_epochs(raw, duration=CFG.EPOCH_LEN, overlap=ov, preload=True, verbose=False)\n",
    "\n",
    "def spectral_features(epochs):\n",
    "    X = epochs.get_data(); sf = epochs.info[\"sfreq\"]\n",
    "    n_ep,n_ch,n_t = X.shape; nper=min(int(sf*2), n_t); nov=nper//2\n",
    "    f,P = welch(X, fs=sf, nperseg=nper, noverlap=nov, axis=-1, average=\"median\")\n",
    "    def idx(lo,hi): return np.where((f>=lo)&(f<hi))[0]\n",
    "    aidx=idx(0.5,80.0); tot=np.maximum(P[:,:,aidx].sum(-1),1e-12)\n",
    "    feats={}; acc={\"alpha\":np.zeros((n_ep,n_ch)),\"theta\":np.zeros((n_ep,n_ch)),\"beta\":np.zeros((n_ep,n_ch))}\n",
    "    for name,(lo,hi) in CFG.BANDS.items():\n",
    "        b=idx(lo,hi); bp=P[:,:,b].sum(-1)/tot if b.size else np.zeros((n_ep,n_ch))\n",
    "        feats[f\"{name}_rel_med\"]=np.median(bp,axis=1); feats[f\"{name}_rel_iqr\"]=np.subtract(*np.percentile(bp,[75,25],axis=1)); feats[f\"{name}_rel_std\"]=np.std(bp,axis=1)\n",
    "        head=name.split(\"_\",1)[0]; \n",
    "        if head in acc: acc[head]+=bp\n",
    "    alpha=np.median(acc[\"alpha\"],axis=1); theta=np.median(acc[\"theta\"],axis=1); beta=np.median(acc[\"beta\"],axis=1)\n",
    "    feats[\"alpha_rel_med\"]=alpha; feats[\"theta_rel_med\"]=theta; feats[\"beta_rel_med\"]=beta\n",
    "    feats[\"theta_over_alpha\"]=theta/np.maximum(alpha,1e-6); feats[\"beta_over_alpha\"]=beta/np.maximum(alpha,1e-6)\n",
    "    feats[\"(alpha+theta)_over_beta\"]=(alpha+theta)/np.maximum(beta,1e-6)\n",
    "    # entropy & centroid\n",
    "    p_band=P[:,:,aidx]; p_n=p_band/np.maximum(p_band.sum(-1,keepdims=True),1e-12)\n",
    "    H=-(p_n*np.log2(p_n+1e-12)).sum(-1); Hn=H/np.log2(p_band.shape[-1])\n",
    "    feats[\"spec_entropy_med\"]=np.median(Hn,axis=1); feats[\"spec_entropy_std\"]=np.std(Hn,axis=1)\n",
    "    fb=f[aidx].reshape(1,1,-1); cen=(p_band*fb).sum(-1)/np.maximum(p_band.sum(-1),1e-12)\n",
    "    feats[\"spec_centroid_med\"]=np.median(cen,axis=1); feats[\"spec_centroid_std\"]=np.std(cen,axis=1)\n",
    "    # Hjorth\n",
    "    d1=np.diff(X,axis=-1); var0=np.var(X,axis=-1)+1e-12; var1=np.var(d1,axis=-1)+1e-12\n",
    "    feats[\"hjorth_activity_med\"]=np.median(var0,axis=1); feats[\"hjorth_mobility_med\"]=np.median(np.sqrt(var1/var0),axis=1)\n",
    "    d2=np.diff(d1,axis=-1); var2=np.var(d2,axis=-1)+1e-12\n",
    "    feats[\"hjorth_complexity_med\"]=np.median(np.sqrt((var2/var1)/(var1/var0)),axis=1)\n",
    "    return pd.DataFrame(feats)\n",
    "\n",
    "def occipital_picks(raw):\n",
    "    names=[n.upper().strip() for n in raw.ch_names]\n",
    "    wanted=(\"O1\",\"O2\",\"OZ\",\"POZ\",\"PO3\",\"PO4\")\n",
    "    idx=[i for i,n in enumerate(names) if n in wanted]\n",
    "    return idx if idx else mne.pick_types(raw.info, eeg=True)\n",
    "\n",
    "def alpha_index(raw, band):\n",
    "    sf=raw.info[\"sfreq\"]\n",
    "    f,P = welch(raw.get_data(), fs=sf, nperseg=min(int(sf*2),raw.n_times), noverlap=int(min(int(sf*2),raw.n_times)/2), axis=-1, average=\"median\")\n",
    "    idx=(f>=band[0])&(f<band[1]); a=P[:,idx].sum(-1); tot=P.sum(-1)+1e-12\n",
    "    oi=occipital_picks(raw)\n",
    "    return float((a[oi].sum()/tot[oi].sum()) if len(oi) else (a.sum()/tot.sum()))\n",
    "\n",
    "def butter_bandpass_array(X, lo, hi, sf, order=4):\n",
    "    X2 = X.reshape(X.shape[0]*X.shape[1], X.shape[2])\n",
    "    Xf = mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, method=\"iir\",\n",
    "                                iir_params=dict(order=order, ftype=\"butter\"), verbose=False)\n",
    "    return Xf.reshape(X.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Mechanism invariance (avg-ref vs Cz) â€” S001R01â€“R03\n",
    "# -----------------------------\n",
    "def mechanism_invariance():\n",
    "    def ms_mi_for_file(edf_path: Path, ref=\"average\"):\n",
    "        raw = load_raw_edf(edf_path, band=(2.0,20.0), ref=ref, montage=True)\n",
    "        X = raw.get_data(); sf = raw.info[\"sfreq\"]\n",
    "        pk,_ = find_peaks(X.std(axis=0), distance=int((10/1000.0)*sf))\n",
    "        Xp = X[:, pk].T\n",
    "        # simple polarity-agnostic kmeans-like\n",
    "        rng=default_rng(42)\n",
    "        Xn = Xp - Xp.mean(axis=1, keepdims=True); Xn = Xn/(np.linalg.norm(Xn, axis=1, keepdims=True)+1e-12)\n",
    "        C  = Xn[rng.choice(len(Xn), 4, replace=False)].copy()\n",
    "        for _ in range(60):\n",
    "            corr = Xn @ C.T; lab=np.argmax(np.abs(corr), axis=1); sign=np.sign(corr[np.arange(len(Xn)), lab])\n",
    "            Cn=[]\n",
    "            for k in range(4):\n",
    "                m=(lab==k)\n",
    "                v=(Xn[m]*sign[m][:,None]).mean(0) if np.any(m) else C[k]\n",
    "                v=v/(np.linalg.norm(v)+1e-12)\n",
    "                Cn.append(v)\n",
    "            Cn=np.stack(Cn,0)\n",
    "            if np.allclose(C,Cn,atol=1e-5): break\n",
    "            C=Cn\n",
    "        # backfit\n",
    "        M=C/(np.linalg.norm(C,axis=1,keepdims=True)+1e-12)\n",
    "        X0n=X/(np.linalg.norm(X,axis=0,keepdims=True)+1e-12)\n",
    "        ms = np.argmax(np.abs(M@X0n), axis=0)\n",
    "        # epoch dominant using run metadata\n",
    "        meta = pd.read_csv(CFG.RUN/\"metadata.csv\").sort_values([\"file\",\"t_start_s\",\"t_end_s\"]).reset_index(drop=True)\n",
    "        labs = pd.read_csv(CFG.RUN/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "        fname = [f for f in meta[\"file\"].unique() if str(edf_path).endswith(f)]\n",
    "        if not fname: return np.nan,0\n",
    "        fname=fname[0]\n",
    "        mask=(meta[\"file\"]==fname).to_numpy()\n",
    "        starts=meta.loc[mask,\"t_start_s\"].to_numpy(); ends=meta.loc[mask,\"t_end_s\"].to_numpy(); L=labs[mask]\n",
    "        dom=[]\n",
    "        for t0,t1 in zip(starts, ends):\n",
    "            s=int(round(t0*sf)); e=int(round(t1*sf)); arr=ms[s:e]\n",
    "            dom.append(np.bincount(arr, minlength=4).argmax() if e>s and len(arr)>0 else -1)\n",
    "        dom=np.array(dom); valid=(dom>=0); L=L[valid]; mu=dom[valid]\n",
    "        # MI(letter;Î¼)\n",
    "        def entropy(p): p=p[p>0]; return -np.sum(p*np.log2(p))\n",
    "        Pm=np.bincount(mu, minlength=4)/len(mu); Hm=entropy(Pm); HmL=0.0\n",
    "        for s in range(4):\n",
    "            mk=(L==s)\n",
    "            if not np.any(mk): continue\n",
    "            P=np.bincount(mu[mk], minlength=4)/mk.sum(); HmL += mk.mean()*entropy(P)\n",
    "        return float(Hm - HmL), int(len(mu))\n",
    "    rows=[]\n",
    "    s001dir = None\n",
    "    for d in [CFG.RUN/\"brainwaves_rebuilt\", CFG.ROOT/\"s001_edf\", CFG.ROOT]:\n",
    "        if d.exists() and all((d/f\"S001R{r:02d}.edf\").exists() for r in [1,2,3]):\n",
    "            s001dir=d; break\n",
    "    if s001dir is None:\n",
    "        s001dir=CFG.ROOT/\"s001_edf\"; s001dir.mkdir(parents=True, exist_ok=True)\n",
    "        for r in [1,2,3]:\n",
    "            url=f\"https://physionet.org/files/eegmmidb/1.0.0/S001/S001R{r:02d}.edf\"\n",
    "            dest=s001dir/f\"S001R{r:02d}.edf\"\n",
    "            if dest.exists() and dest.stat().st_size>0: continue\n",
    "            with requests.get(url, stream=True, timeout=60) as resp:\n",
    "                resp.raise_for_status()\n",
    "                with open(dest,\"wb\") as f:\n",
    "                    for ch in resp.iter_content(8192):\n",
    "                        if ch: f.write(ch)\n",
    "    for r in [1,2,3]:\n",
    "        p=s001dir/f\"S001R{r:02d}.edf\"\n",
    "        if not p.exists(): continue\n",
    "        mi_avg,n = ms_mi_for_file(p, ref=\"average\")\n",
    "        mi_cz, n2 = ms_mi_for_file(p, ref=\"cz\")\n",
    "        rows.append({\"run\":f\"S001R{r:02d}\", \"MI_avgref\":mi_avg, \"MI_czref\":mi_cz, \"n_ep\":n})\n",
    "    df=pd.DataFrame(rows); df.to_csv(CFG.OUT/\"mechanism_invariance_fresh.csv\", index=False); return df\n",
    "\n",
    "# -----------------------------\n",
    "# 2) K-sweep & Pareto K* decision + stamp into scoreboard/README\n",
    "# -----------------------------\n",
    "def k_sweep_and_decide():\n",
    "    F = pd.read_csv(CFG.RUN/\"features.csv\")\n",
    "    L = pd.read_csv(CFG.RUN/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "    scaler = StandardScaler().fit(F.values); Z = scaler.transform(F.values)\n",
    "    pca    = PCA(n_components=min(20,Z.shape[1]), random_state=42).fit(Z); Z = pca.transform(Z)\n",
    "    rng=default_rng(42)\n",
    "    rows=[]\n",
    "    for K in CFG.K_RANGE:\n",
    "        km = KMeans(n_clusters=K, n_init=\"auto\", random_state=42).fit(Z); lab = km.labels_\n",
    "        # stability\n",
    "        Bs=80; idxs=[rng.choice(len(lab), size=int(0.8*len(lab)), replace=False) for _ in range(Bs)]\n",
    "        from sklearn.metrics import adjusted_rand_score\n",
    "        boot_ari = np.mean([adjusted_rand_score(L[idx], KMeans(n_clusters=K, n_init=\"auto\", random_state=1000+i).fit(Z[idx]).labels_) for i,idx in enumerate(idxs)])\n",
    "        # grammar gain\n",
    "        Kk=int(K); T=np.zeros((Kk,Kk))\n",
    "        for a,b in zip(lab[:-1], lab[1:]): T[a,b]+=1\n",
    "        eps=1e-6; T=(T+eps)/(T.sum(axis=1, keepdims=True)+eps*Kk)\n",
    "        pi=T.sum(axis=0); pi=pi/(pi.sum()+1e-12)\n",
    "        H_T1=-np.mean([math.log(T[a,b]+1e-12) for a,b in zip(lab[:-1], lab[1:])])\n",
    "        H_pi=-np.mean([math.log(pi[b]+1e-12) for b in lab[1:]])\n",
    "        rows.append({\"K\":K,\"boot_ARI\":boot_ari,\"Î”xent\":H_pi-H_T1})\n",
    "    df=pd.DataFrame(rows)\n",
    "    df.to_csv(CFG.OUT/\"k_sweep_decision.csv\", index=False)\n",
    "    # Pareto decision: smallest K with Î”xent>0 and boot_ARI >= 0.9 * boot_ARI(K=4)\n",
    "    base_ari = float(df[df[\"K\"]==4][\"boot_ARI\"]) if (df[\"K\"]==4).any() else df[\"boot_ARI\"].max()\n",
    "    candidates = df[(df[\"Î”xent\"]>0) & (df[\"boot_ARI\"] >= 0.9*base_ari)]\n",
    "    K_star = int(candidates.sort_values(\"K\").iloc[0][\"K\"]) if len(candidates) else int(df.loc[df[\"Î”xent\"].idxmax()][\"K\"])\n",
    "    # Stamp into scoreboard + README\n",
    "    score_json = CFG.REP/\"generalization\"/\"v0_3_scoreboard.json\"\n",
    "    score = json.load(open(score_json)) if score_json.exists() else {}\n",
    "    score[\"K_star\"] = K_star\n",
    "    with open(score_json,\"w\") as f: json.dump(score, f, indent=2)\n",
    "    readme = CFG.ROOT/\"CNT_CognitiveAlphabet_v0_3\"/\"README_v0_3.md\"\n",
    "    try:\n",
    "        with open(readme,\"a\",encoding=\"utf-8\") as f:\n",
    "            f.write(f\"\\n## K-decision\\n- Pareto pick K* = **{K_star}** (Î”xent>0 and boot-ARI â‰¥ 90% of K=4).\\n\")\n",
    "    except Exception: pass\n",
    "    return df, K_star\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Microstate-conditioned grammar: CMI = I(L_{t+1}; Î¼_t | L_t)\n",
    "# -----------------------------\n",
    "def cmi_microstate_conditioned():\n",
    "    meta = pd.read_csv(CFG.RUN/\"metadata.csv\").sort_values([\"file\",\"t_start_s\",\"t_end_s\"]).reset_index(drop=True)\n",
    "    lab  = pd.read_csv(CFG.RUN/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "    # Build per-epoch dom microstate for S001 R01â€“R03 (avg-ref)\n",
    "    def dom_ms_for_run(fname):\n",
    "        # locate EDF\n",
    "        for d in [CFG.RUN/\"brainwaves_rebuilt\", CFG.ROOT/\"s001_edf\", CFG.ROOT]:\n",
    "            p=d/fname\n",
    "            if p.exists():\n",
    "                raw = load_raw_edf(p, band=(2.0,20.0), ref=\"average\", montage=True)\n",
    "                X=raw.get_data(); sf=raw.info[\"sfreq\"]\n",
    "                pk,_=find_peaks(X.std(axis=0), distance=int((10/1000.0)*sf))\n",
    "                Xp=X[:,pk].T\n",
    "                rng=default_rng(777)\n",
    "                Xn=Xp-Xp.mean(axis=1,keepdims=True); Xn=Xn/(np.linalg.norm(Xn,axis=1,keepdims=True)+1e-12)\n",
    "                C=Xn[rng.choice(len(Xn),4,replace=False)].copy()\n",
    "                for _ in range(60):\n",
    "                    corr=Xn@C.T; Lr=np.argmax(np.abs(corr),axis=1); sgn=np.sign(corr[np.arange(len(Xn)),Lr])\n",
    "                    Cn=[]\n",
    "                    for k in range(4):\n",
    "                        m=(Lr==k); v=(Xn[m]*sgn[m][:,None]).mean(0) if np.any(m) else C[k]\n",
    "                        v=v/(np.linalg.norm(v)+1e-12); Cn.append(v)\n",
    "                    Cn=np.stack(Cn,0)\n",
    "                    if np.allclose(C,Cn,atol=1e-5): break; C=Cn\n",
    "                # backfit\n",
    "                M=C/(np.linalg.norm(C,axis=1,keepdims=True)+1e-12)\n",
    "                X0n=X/(np.linalg.norm(X,axis=0,keepdims=True)+1e-12)\n",
    "                ms=np.argmax(np.abs(M@X0n), axis=0)\n",
    "                # epochs\n",
    "                mask=(meta[\"file\"]==fname).to_numpy()\n",
    "                starts=meta.loc[mask,\"t_start_s\"].to_numpy(); ends=meta.loc[mask,\"t_end_s\"].to_numpy(); L=lab[mask]\n",
    "                dom=[]\n",
    "                for t0,t1 in zip(starts, ends):\n",
    "                    s=int(round(t0*sf)); e=int(round(t1*sf)); arr=ms[s:e]\n",
    "                    dom.append(np.bincount(arr, minlength=4).argmax() if e>s and len(arr)>0 else -1)\n",
    "                return np.array(dom,int), L\n",
    "        return None, None\n",
    "    rows=[]\n",
    "    for fname in [\"S001R01.edf\",\"S001R02.edf\",\"S001R03.edf\"]:\n",
    "        dm,Lr = dom_ms_for_run(fname)\n",
    "        if dm is None: continue\n",
    "        mask=(dm>=0); dm=dm[mask]; Lr=Lr[mask]\n",
    "        # build triples (L_t, Î¼_t, L_{t+1}) by dropping last epoch\n",
    "        for i in range(len(dm)-1):\n",
    "            rows.append({\"Lt\":int(Lr[i]), \"mu_t\":int(dm[i]), \"Lt1\":int(Lr[i+1])})\n",
    "    if not rows:\n",
    "        return None\n",
    "    E=pd.DataFrame(rows)\n",
    "    # CMI = sum_l P(Lt=l) * I(Lt1; mu_t | Lt=l)\n",
    "    def entropy(p): p=p[p>0]; return -np.sum(p*np.log2(p))\n",
    "    Kletter = int(max(E[\"Lt\"].max(), E[\"Lt1\"].max())+1)\n",
    "    Kmu = int(E[\"mu_t\"].max()+1)\n",
    "    cmi=0.0\n",
    "    for l in range(Kletter):\n",
    "        group=E[E[\"Lt\"]==l]\n",
    "        if len(group)<5: continue\n",
    "        joint=np.zeros((Kmu,Kletter))\n",
    "        for _,r in group.iterrows(): joint[r[\"mu_t\"], r[\"Lt1\"]]+=1\n",
    "        joint/=joint.sum()\n",
    "        Pmu=joint.sum(axis=1); PLt1=joint.sum(axis=0)\n",
    "        H=entropy(PLt1)\n",
    "        H_cond=0.0\n",
    "        for m in range(Kmu):\n",
    "            if Pmu[m]<=0: continue\n",
    "            P = joint[m]/(Pmu[m]+1e-12)\n",
    "            H_cond += Pmu[m]*entropy(P)\n",
    "        I = H - H_cond\n",
    "        cmi += (len(group)/len(E)) * I\n",
    "    # permutation: shuffle mu within each Lt group\n",
    "    rng=default_rng(123)\n",
    "    Pperm=3000; null=[]\n",
    "    for _ in range(Pperm):\n",
    "        Emu = E.copy()\n",
    "        for l in range(Kletter):\n",
    "            idx=Emu.index[Emu[\"Lt\"]==l].to_numpy()\n",
    "            rng.shuffle(idx)  # reorder indices for mu reassignment within group\n",
    "            Emu.loc[Emu[\"Lt\"]==l, \"mu_t\"] = Emu.loc[idx, \"mu_t\"].values\n",
    "        c=0.0\n",
    "        for l in range(Kletter):\n",
    "            group=Emu[Emu[\"Lt\"]==l]\n",
    "            if len(group)<5: continue\n",
    "            joint=np.zeros((Kmu,Kletter))\n",
    "            for _,r in group.iterrows(): joint[r[\"mu_t\"], r[\"Lt1\"]]+=1\n",
    "            joint/=joint.sum()\n",
    "            Pmu=joint.sum(axis=1); PLt1=joint.sum(axis=0)\n",
    "            H=entropy(PLt1); Hc=0.0\n",
    "            for m in range(Kmu):\n",
    "                if Pmu[m]<=0: continue\n",
    "                P=joint[m]/(Pmu[m]+1e-12); Hc += Pmu[m]*entropy(P)\n",
    "            c += (len(group)/len(Emu))*(H-Hc)\n",
    "        null.append(c)\n",
    "    pval = float((np.sum(np.array(null) >= cmi) + 1) / (Pperm + 1))\n",
    "    out = {\"CMI_bits\": float(cmi), \"perm_p\": pval, \"n\": int(len(E))}\n",
    "    (CFG.OUT/\"cmi_microstate_conditioned.json\").write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Generative adequacy (semi-Markov + T1 simulate; JS vs real)\n",
    "# -----------------------------\n",
    "def dwell_lengths(seq):\n",
    "    d={}; cur=seq[0]; n=1\n",
    "    for a,b in zip(seq[:-1], seq[1:]):\n",
    "        if b==a: n+=1\n",
    "        else: d.setdefault(cur,[]).append(n); cur=b; n=1\n",
    "    d.setdefault(cur,[]).append(n)\n",
    "    return d\n",
    "\n",
    "def fit_semi_markov(seq, epoch_sec=2.0):\n",
    "    # geometric per-state (epochs) as default; gamma if enough samples\n",
    "    dw = dwell_lengths(seq)\n",
    "    fits={}\n",
    "    for s,lens in dw.items():\n",
    "        arr=np.array(lens,float)\n",
    "        if len(arr)>=8:\n",
    "            m=arr.mean(); v=arr.var()\n",
    "            k=(m*m)/max(v,1e-9); theta=max(v/m,1e-9)\n",
    "            fits[int(s)]={\"kind\":\"gamma\",\"k\":float(k),\"theta\":float(theta)}\n",
    "        else:\n",
    "            p=1.0/max(arr.mean(),1.0); fits[int(s)]={\"kind\":\"geom\",\"p\":float(p)}\n",
    "    # embedded no-self transitions\n",
    "    K=int(max(seq))+1\n",
    "    E=np.zeros((K,K))\n",
    "    for a,b in zip(seq[:-1], seq[1:]):\n",
    "        if b!=a: E[a,b]+=1\n",
    "    for i in range(K):\n",
    "        row=E[i]\n",
    "        if row.sum()==0: E[i]=(np.ones(K)-np.eye(K)[i])/(K-1)\n",
    "        else:\n",
    "            E[i]=row/row.sum(); E[i,i]=0.0\n",
    "    return fits,E\n",
    "\n",
    "def sample_dwell(fit, rng):\n",
    "    if fit[\"kind\"]==\"geom\":\n",
    "        return int(rng.geometric(fit[\"p\"]))\n",
    "    k,theta=fit[\"k\"], fit[\"theta\"]\n",
    "    sec=rng.gamma(shape=k, scale=theta)\n",
    "    return max(1, int(math.ceil(sec/CFG.EPOCH_LEN)))\n",
    "\n",
    "def simulate_seq(fits, E, N=2000, seed=777):\n",
    "    rng=default_rng(seed); K=E.shape[0]; s=rng.integers(0,K); seq=[]\n",
    "    while len(seq)<N:\n",
    "        d=sample_dwell(fits.get(s,{\"kind\":\"geom\",\"p\":0.5}), rng); seq.extend([s]*d)\n",
    "        if len(seq)>=N: break\n",
    "        row=E[s].copy(); row[s]=0; p=row/(row.sum()+1e-12)\n",
    "        s=rng.choice(np.arange(K), p=p)\n",
    "    return np.array(seq[:N], int)\n",
    "\n",
    "def js_divergence(p,q):\n",
    "    p=p/(p.sum()+1e-12); q=q/(q.sum()+1e-12); m=0.5*(p+q)\n",
    "    def kl(a,b): \n",
    "        mask=(a>0)&(b>0)\n",
    "        return np.sum(a[mask]*np.log2(a[mask]/b[mask]))\n",
    "    return 0.5*kl(p,m)+0.5*kl(q,m)\n",
    "\n",
    "def generative_adequacy():\n",
    "    L = pd.read_csv(CFG.RUN/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "    fits,E=fit_semi_markov(L, epoch_sec=CFG.EPOCH_LEN)\n",
    "    sim=simulate_seq(fits,E,N=min(4000, len(L)*2), seed=777)\n",
    "    # bigram distributions\n",
    "    K=int(max(L.max(), sim.max())+1)\n",
    "    def bigram_counts(seq):\n",
    "        B=np.zeros((K,K))\n",
    "        for a,b in zip(seq[:-1], seq[1:]): B[a,b]+=1\n",
    "        return B/(B.sum()+1e-12)\n",
    "    J_bigrams = float(js_divergence(bigram_counts(L).flatten(), bigram_counts(sim).flatten()))\n",
    "    # dwell distributions (concat per-state)\n",
    "    def dwell_hist(seq):\n",
    "        d=dwell_lengths(seq); rows=[]\n",
    "        for s,lens in d.items(): rows+= [ (s,l) for l in lens ]\n",
    "        return pd.DataFrame(rows, columns=[\"state\",\"dwell\"])\n",
    "    real_dh = dwell_hist(L); sim_dh = dwell_hist(sim)\n",
    "    # JS on per-state dwell length histograms (normalized to same support)\n",
    "    J_dw = 0.0; count=0\n",
    "    for s in range(K):\n",
    "        r=real_dh[real_dh[\"state\"]==s][\"dwell\"].values\n",
    "        t=sim_dh[sim_dh[\"state\"]==s][\"dwell\"].values\n",
    "        if len(r)<3 or len(t)<3: continue\n",
    "        m=max(r.max(), t.max()); pr=np.bincount(r, minlength=m+1)[1:]; pt=np.bincount(t, minlength=m+1)[1:]\n",
    "        pr=pr/pr.sum(); pt=pt/pt.sum()\n",
    "        J_dw += js_divergence(pr, pt); count+=1\n",
    "    J_dw = float(J_dw / max(count,1))\n",
    "    out={\"JS_bigrams\":J_bigrams, \"JS_dwell\":J_dw, \"K\":K, \"N_sim\": int(len(sim))}\n",
    "    (CFG.OUT/\"generative_adequacy.json\").write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    return out\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Cross-subject grammar topology (T1 JS + MDS)\n",
    "# -----------------------------\n",
    "def subject_T1_js_mds():\n",
    "    # load decoded sequences for subjects in generalization folder\n",
    "    subj_seqs={}\n",
    "    for p in (CFG.REP/\"generalization\").glob(\"decode_*_*.csv\"):\n",
    "        m=re.match(r\"decode_(S\\d+)_\", p.name)\n",
    "        if not m: continue\n",
    "        subj=m.group(1)\n",
    "        try:\n",
    "            df=pd.read_csv(p)\n",
    "            if \"state\" in df.columns:\n",
    "                subj_seqs.setdefault(subj, []).append(df[\"state\"].astype(int).to_numpy())\n",
    "        except Exception:\n",
    "            pass\n",
    "    subj = sorted(subj_seqs.keys())\n",
    "    if not subj: return None\n",
    "    # train T1 per subject\n",
    "    def T1_from_seq(seq, K):\n",
    "        T=np.zeros((K,K))\n",
    "        for a,b in zip(seq[:-1], seq[1:]): T[a,b]+=1\n",
    "        eps=1e-6; T=(T+eps)/(T.sum(axis=1, keepdims=True)+eps*K); return T\n",
    "    K=int(pd.read_csv(CFG.RUN/\"state_assignments.csv\")[\"state\"].max()+1)\n",
    "    Tm={}\n",
    "    for s in subj:\n",
    "        seq=np.concatenate(subj_seqs[s]) if len(subj_seqs[s]) else None\n",
    "        if seq is None or len(seq)<3: continue\n",
    "        Tm[s]=T1_from_seq(seq, K)\n",
    "    subs=sorted(Tm.keys())\n",
    "    if not subs: return None\n",
    "    # pairwise JS over flattened T\n",
    "    D=np.zeros((len(subs), len(subs)))\n",
    "    for i,a in enumerate(subs):\n",
    "        for j,b in enumerate(subs):\n",
    "            pa=Tm[a].flatten(); pb=Tm[b].flatten()\n",
    "            D[i,j]=js_divergence(pa, pb)\n",
    "    # MDS\n",
    "    mds=MDS(n_components=2, dissimilarity=\"precomputed\", random_state=42)\n",
    "    Y=mds.fit_transform(D)\n",
    "    pd.DataFrame({\"subject\":subs, \"x\":Y[:,0], \"y\":Y[:,1]}).to_csv(CFG.OUT/\"grammar_topology_mds.csv\", index=False)\n",
    "    # plot\n",
    "    plt.figure(figsize=(5,4)); plt.scatter(Y[:,0], Y[:,1])\n",
    "    for i,s in enumerate(subs): plt.text(Y[i,0], Y[i,1], s, fontsize=9)\n",
    "    plt.title(\"Cross-subject grammar topology (JS on T1)\"); plt.tight_layout()\n",
    "    plt.savefig(CFG.OUT/\"grammar_topology_mds.png\", dpi=160); plt.close()\n",
    "    return {\"subjects\": subs, \"D_JS_mean\": float(D[np.triu_indices(len(subs),1)].mean())}\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Compose CALM_Groundbreaking_Addendum.pdf\n",
    "# -----------------------------\n",
    "def compose_addendum(invar_df, kdf, K_star, cmi, gen_adj, topo):\n",
    "    pdf = PdfPages(CFG.OUT/\"CALM_Groundbreaking_Addendum.pdf\")\n",
    "    # Page 1 â€” Mechanism invariance\n",
    "    plt.figure(figsize=(10,4))\n",
    "    if invar_df is not None and len(invar_df):\n",
    "        x=np.arange(len(invar_df))\n",
    "        plt.bar(x-0.15, invar_df[\"MI_avgref\"], width=0.3, label=\"avg-ref\")\n",
    "        plt.bar(x+0.15, invar_df[\"MI_czref\"], width=0.3, label=\"Cz-ref\")\n",
    "        plt.xticks(x, invar_df[\"run\"]); plt.ylabel(\"MI(letter; Î¼) bits\"); plt.legend()\n",
    "        plt.title(\"Mechanism invariance (reference change)\")\n",
    "    else:\n",
    "        plt.axis(\"off\"); plt.text(0.5,0.5,\"Mechanism invariance data not available\", ha=\"center\")\n",
    "    pdf.savefig(bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    # Page 2 â€” K sweep & decision\n",
    "    plt.figure(figsize=(10,4))\n",
    "    if kdf is not None and len(kdf):\n",
    "        plt.subplot(1,2,1); plt.plot(kdf[\"K\"], kdf[\"boot_ARI\"], \"o-\"); plt.title(\"Stability (boot ARI) vs K\"); plt.xlabel(\"K\")\n",
    "        plt.subplot(1,2,2); plt.plot(kdf[\"K\"], kdf[\"Î”xent\"], \"s--\", color=\"tab:orange\"); plt.axhline(0,ls=\"--\",c=\"gray\"); plt.title(\"Grammar gain Î”xent vs K\"); plt.xlabel(\"K\")\n",
    "        plt.suptitle(f\"K decision: K* = {K_star}\")\n",
    "    else:\n",
    "        plt.axis(\"off\"); plt.text(0.5,0.5,\"K sweep missing\", ha=\"center\")\n",
    "    pdf.savefig(bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    # Page 3 â€” Microstate-conditioned grammar (CMI)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    txt=f\"CMI = I(Lt+1; Î¼t | Lt) = {cmi.get('CMI_bits','n/a'):.3f} bits\\nperm p = {cmi.get('perm_p','n/a'):.3g}   n={cmi.get('n','?')}\"\n",
    "    plt.axis(\"off\"); plt.text(0.05,0.9, \"Microstate-conditioned grammar\", fontsize=12, weight=\"bold\")\n",
    "    plt.text(0.05,0.7, txt, fontsize=11)\n",
    "    pdf.savefig(bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    # Page 4 â€” Generative adequacy (JS)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    txt=f\"JS (bigrams): {gen_adj.get('JS_bigrams','n/a'):.3f}\\nJS (dwell): {gen_adj.get('JS_dwell','n/a'):.3f}\\nK={gen_adj.get('K','?')}, N_sim={gen_adj.get('N_sim','?')}\"\n",
    "    plt.axis(\"off\"); plt.text(0.05,0.9, \"Generative adequacy â€” simulation vs real\", fontsize=12, weight=\"bold\")\n",
    "    plt.text(0.05,0.7, txt, fontsize=11)\n",
    "    pdf.savefig(bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    # Page 5 â€” Cross-subject grammar topology (MDS)\n",
    "    plt.figure(figsize=(10,4))\n",
    "    topo_png = CFG.OUT/\"grammar_topology_mds.png\"\n",
    "    if topo is not None and topo_png.exists():\n",
    "        img=plt.imread(topo_png); plt.imshow(img); plt.axis(\"off\")\n",
    "        plt.title(f\"Grammar topology â€” mean JS pairwise = {topo.get('D_JS_mean','n/a'):.3f}\")\n",
    "    else:\n",
    "        plt.axis(\"off\"); plt.text(0.5,0.5,\"Topology plot missing\", ha=\"center\")\n",
    "    pdf.savefig(bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    pdf.close()\n",
    "    print(\"Addendum PDF â†’\", CFG.OUT/\"CALM_Groundbreaking_Addendum.pdf\")\n",
    "\n",
    "# -----------------------------\n",
    "# RUN EVERYTHING\n",
    "# -----------------------------\n",
    "print(\"[1] Mechanism invarianceâ€¦\")\n",
    "invar_df = mechanism_invariance()\n",
    "\n",
    "print(\"[2] K-sweep + decisionâ€¦\")\n",
    "kdf, K_star = k_sweep_and_decide()\n",
    "\n",
    "print(\"[3] Microstate-conditioned grammar (CMI)â€¦\")\n",
    "cmi = cmi_microstate_conditioned() or {\"CMI_bits\": float(\"nan\"), \"perm_p\": float(\"nan\"), \"n\": 0}\n",
    "\n",
    "print(\"[4] Generative adequacyâ€¦\")\n",
    "gen_adj = generative_adequacy()\n",
    "\n",
    "print(\"[5] Cross-subject grammar topologyâ€¦\")\n",
    "topo = subject_T1_js_mds()\n",
    "\n",
    "print(\"[6] Compose CALM_Groundbreaking_Addendum.pdf â€¦\")\n",
    "compose_addendum(invar_df, kdf, K_star, cmi, gen_adj, topo)\n",
    "\n",
    "# Final JSON summary for quick sharing\n",
    "summary = {\n",
    "    \"invariance_csv\": str(CFG.OUT/\"mechanism_invariance_fresh.csv\"),\n",
    "    \"k_sweep_csv\": str(CFG.OUT/\"k_sweep_decision.csv\"),\n",
    "    \"K_star\": K_star,\n",
    "    \"cmi\": cmi,\n",
    "    \"generative\": gen_adj,\n",
    "    \"topology\": topo,\n",
    "    \"addendum_pdf\": str(CFG.OUT/\"CALM_Groundbreaking_Addendum.pdf\")\n",
    "}\n",
    "(CFG.OUT/\"groundbreaking_summary.json\").write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
    "print(\"Summary JSON â†’\", CFG.OUT/\"groundbreaking_summary.json\")\n",
    "\n",
    "print(\"\\nDONE. This addendum nails:\")\n",
    "print(\" â€¢ Invariance (avg-ref vs Cz)  â€¢ Pareto K*  â€¢ Mechanismâ†’grammar CMI  â€¢ Generative adequacy  â€¢ Cross-subject grammar map\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "393e0298-fac0-4f75-8954-27c8e235a82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K=3 model saved â†’ E:\\CNT\\artifacts\\cog_alphabet_hybrid_k3\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.5e-05 (2.2e-16 eps * 64 dim * 5.3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.4e-05 (2.2e-16 eps * 64 dim * 3.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5e-05 (2.2e-16 eps * 64 dim * 3.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.7e-05 (2.2e-16 eps * 64 dim * 5.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.3e-05 (2.2e-16 eps * 64 dim * 3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5e-05 (2.2e-16 eps * 64 dim * 3.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.7e-05 (2.2e-16 eps * 64 dim * 5.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.4e-05 (2.2e-16 eps * 64 dim * 3.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.3e-05 (2.2e-16 eps * 64 dim * 3.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8e-05 (2.2e-16 eps * 64 dim * 5.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.3e-05 (2.2e-16 eps * 64 dim * 3.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.2e-05 (2.2e-16 eps * 64 dim * 3.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.8e-05 (2.2e-16 eps * 64 dim * 5.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.4e-05 (2.2e-16 eps * 64 dim * 3.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.1e-05 (2.2e-16 eps * 64 dim * 3.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "S002 EO/EC= 0.811 AUC= 0.819\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.7e-05 (2.2e-16 eps * 64 dim * 3.3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.9e-05 (2.2e-16 eps * 64 dim * 2.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.8e-05 (2.2e-16 eps * 64 dim * 3.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.9e-05 (2.2e-16 eps * 64 dim * 2.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.8e-05 (2.2e-16 eps * 64 dim * 3.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.8e-05 (2.2e-16 eps * 64 dim * 2.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.7e-05 (2.2e-16 eps * 64 dim * 3.3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.9e-05 (2.2e-16 eps * 64 dim * 2.8e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.0001 (2.2e-16 eps * 64 dim * 7.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.8e-05 (2.2e-16 eps * 64 dim * 3.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.9e-05 (2.2e-16 eps * 64 dim * 2.8e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "S003 EO/EC= 0.861 AUC= 0.913\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00014 (2.2e-16 eps * 64 dim * 9.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.4e-05 (2.2e-16 eps * 64 dim * 4.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.3e-05 (2.2e-16 eps * 64 dim * 3.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00014 (2.2e-16 eps * 64 dim * 9.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.6e-05 (2.2e-16 eps * 64 dim * 4.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.2e-05 (2.2e-16 eps * 64 dim * 3.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00014 (2.2e-16 eps * 64 dim * 9.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.6e-05 (2.2e-16 eps * 64 dim * 4.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.2e-05 (2.2e-16 eps * 64 dim * 3.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00014 (2.2e-16 eps * 64 dim * 9.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.5e-05 (2.2e-16 eps * 64 dim * 4.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.1e-05 (2.2e-16 eps * 64 dim * 3.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00013 (2.2e-16 eps * 64 dim * 9.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.2e-05 (2.2e-16 eps * 64 dim * 4.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.2e-05 (2.2e-16 eps * 64 dim * 3.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "S004 EO/EC= 0.971 AUC= 0.961\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.1e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.5e-05 (2.2e-16 eps * 64 dim * 2.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.7e-05 (2.2e-16 eps * 64 dim * 4.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.2e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.4e-05 (2.2e-16 eps * 64 dim * 2.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.6e-05 (2.2e-16 eps * 64 dim * 4.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.1e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.5e-05 (2.2e-16 eps * 64 dim * 2.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.5e-05 (2.2e-16 eps * 64 dim * 4.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.2e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.4e-05 (2.2e-16 eps * 64 dim * 2.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.6e-05 (2.2e-16 eps * 64 dim * 4.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.3e-05 (2.2e-16 eps * 64 dim * 2.3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.4e-05 (2.2e-16 eps * 64 dim * 2.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 6.7e-05 (2.2e-16 eps * 64 dim * 4.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "S005 EO/EC= 0.697 AUC= 0.932\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.7e-05 (2.2e-16 eps * 64 dim * 1.9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.4e-05 (2.2e-16 eps * 64 dim * 1.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.3e-05 (2.2e-16 eps * 64 dim * 2.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.6e-05 (2.2e-16 eps * 64 dim * 1.9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.4e-05 (2.2e-16 eps * 64 dim * 1.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.3e-05 (2.2e-16 eps * 64 dim * 2.3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.7e-05 (2.2e-16 eps * 64 dim * 1.9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.5e-05 (2.2e-16 eps * 64 dim * 1.8e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.4e-05 (2.2e-16 eps * 64 dim * 2.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.7e-05 (2.2e-16 eps * 64 dim * 1.9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.4e-05 (2.2e-16 eps * 64 dim * 1.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.4e-05 (2.2e-16 eps * 64 dim * 2.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.6e-05 (2.2e-16 eps * 64 dim * 1.8e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 2.4e-05 (2.2e-16 eps * 64 dim * 1.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.4e-05 (2.2e-16 eps * 64 dim * 2.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "S006 EO/EC= 0.748 AUC= 0.864\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9.5e-05 (2.2e-16 eps * 64 dim * 6.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.4e-05 (2.2e-16 eps * 64 dim * 3.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.3e-05 (2.2e-16 eps * 64 dim * 3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9e-05 (2.2e-16 eps * 64 dim * 6.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.3e-05 (2.2e-16 eps * 64 dim * 3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.3e-05 (2.2e-16 eps * 64 dim * 3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9.9e-05 (2.2e-16 eps * 64 dim * 6.9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.5e-05 (2.2e-16 eps * 64 dim * 3.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.3e-05 (2.2e-16 eps * 64 dim * 3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9.9e-05 (2.2e-16 eps * 64 dim * 6.9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.4e-05 (2.2e-16 eps * 64 dim * 3.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.2e-05 (2.2e-16 eps * 64 dim * 3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 9.7e-05 (2.2e-16 eps * 64 dim * 6.8e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.4e-05 (2.2e-16 eps * 64 dim * 3.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.2e-05 (2.2e-16 eps * 64 dim * 3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "S007 EO/EC= 0.857 AUC= 0.841\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.2e-05 (2.2e-16 eps * 64 dim * 3.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3.1e-05 (2.2e-16 eps * 64 dim * 2.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.1e-05 (2.2e-16 eps * 64 dim * 3.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.3e-05 (2.2e-16 eps * 64 dim * 3.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3e-05 (2.2e-16 eps * 64 dim * 2.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.1e-05 (2.2e-16 eps * 64 dim * 3.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.3e-05 (2.2e-16 eps * 64 dim * 3.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3e-05 (2.2e-16 eps * 64 dim * 2.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.1e-05 (2.2e-16 eps * 64 dim * 3.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5e-05 (2.2e-16 eps * 64 dim * 3.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3e-05 (2.2e-16 eps * 64 dim * 2.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.2e-05 (2.2e-16 eps * 64 dim * 3.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5e-05 (2.2e-16 eps * 64 dim * 3.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 3e-05 (2.2e-16 eps * 64 dim * 2.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.2e-05 (2.2e-16 eps * 64 dim * 3.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "S008 EO/EC= 0.786 AUC= 0.742\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.8e-05 (2.2e-16 eps * 64 dim * 5.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00012 (2.2e-16 eps * 64 dim * 8.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00012 (2.2e-16 eps * 64 dim * 8.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8e-05 (2.2e-16 eps * 64 dim * 5.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00012 (2.2e-16 eps * 64 dim * 8.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00012 (2.2e-16 eps * 64 dim * 8.7e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.9e-05 (2.2e-16 eps * 64 dim * 5.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00012 (2.2e-16 eps * 64 dim * 8.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00012 (2.2e-16 eps * 64 dim * 8.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.9e-05 (2.2e-16 eps * 64 dim * 5.5e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00012 (2.2e-16 eps * 64 dim * 8.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00012 (2.2e-16 eps * 64 dim * 8.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 8e-05 (2.2e-16 eps * 64 dim * 5.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00012 (2.2e-16 eps * 64 dim * 8.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 0.00012 (2.2e-16 eps * 64 dim * 8.6e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "S009 EO/EC= 0.945 AUC= 0.858\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.5e-05 (2.2e-16 eps * 64 dim * 5.3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.6e-05 (2.2e-16 eps * 64 dim * 4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.3e-05 (2.2e-16 eps * 64 dim * 3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.7e-05 (2.2e-16 eps * 64 dim * 5.4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.6e-05 (2.2e-16 eps * 64 dim * 3.9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.3e-05 (2.2e-16 eps * 64 dim * 3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.4e-05 (2.2e-16 eps * 64 dim * 5.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.7e-05 (2.2e-16 eps * 64 dim * 4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.4e-05 (2.2e-16 eps * 64 dim * 3.1e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.6e-05 (2.2e-16 eps * 64 dim * 5.3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.6e-05 (2.2e-16 eps * 64 dim * 4e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.3e-05 (2.2e-16 eps * 64 dim * 3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 7.3e-05 (2.2e-16 eps * 64 dim * 5.2e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 5.6e-05 (2.2e-16 eps * 64 dim * 3.9e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Computing rank from data with rank=None\n",
      "    Using tolerance 4.3e-05 (2.2e-16 eps * 64 dim * 3e+09  max singular value)\n",
      "    Estimated rank (data): 63\n",
      "    data: rank 63 computed from 64 data channels with 0 projectors\n",
      "    Setting small data eigenvalues to zero (without PCA)\n",
      "Reducing data rank from 64 -> 63\n",
      "Estimating class=0 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "Estimating class=1 covariance using LEDOIT_WOLF\n",
      "Done.\n",
      "S010 EO/EC= 0.836 AUC= 0.83\n",
      "S001 Î”xent (K=3) = 0.502\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "S001 MI(letter;Î¼) @K=3 = 0.142\n",
      "\n",
      "DONE â€” K=3 refit + re-eval complete.\n",
      "Figures:\n",
      "  - Generalization (K=3): E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\generalization\\CALM_Generalization_K3.png\n",
      "Mechanism/grammar summary â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\microstates\\CALM_Mechanism_K3.txt\n",
      "Scoreboard updated â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\generalization\\v0_3_scoreboard.json\n"
     ]
    }
   ],
   "source": [
    "# === CALM â€” Switch-to-K=3 Mega Cell (refit, re-evaluate, restamp) ===\n",
    "import os, json, math, numpy as np, pandas as pd, matplotlib.pyplot as plt, mne, requests\n",
    "from pathlib import Path\n",
    "from scipy.signal import welch, find_peaks\n",
    "from numpy.random import default_rng\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from mne.decoding import CSP\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# -----------------------------\n",
    "# CFG â€” edit if needed\n",
    "# -----------------------------\n",
    "class CFG:\n",
    "    ROOT      = Path(r\"E:\\CNT\\artifacts\")\n",
    "    RUN_K4    = ROOT / \"cog_alphabet_hybrid_v1\"                 # existing K=4 run\n",
    "    RUN_K3    = ROOT / \"cog_alphabet_hybrid_k3\"                 # new K=3 run\n",
    "    REP       = ROOT / \"cog_alphabet_report_hybrid_v1\"          # reuse report root\n",
    "    GEN       = ROOT / \"generalization_data\"                    # S002.. EDFs\n",
    "    TARGET_SF = 250.0\n",
    "    EPOCH_LEN = 2.0\n",
    "    STEP      = 0.5\n",
    "    BANDS     = {\"delta\":(1,4),\"theta\":(4,8),\"alpha\":(8,13),\"beta_low\":(13,20),\"beta_high\":(20,35),\"gamma\":(35,80)}\n",
    "    FB_BANDS  = [(8,13),(13,20),(20,30)]\n",
    "    FB_THR    = 0.50\n",
    "CFG = CFG()\n",
    "CFG.RUN_K3.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities\n",
    "# -----------------------------\n",
    "def load_raw_edf(p: Path, band=(0.5,80.0)):\n",
    "    raw=mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"]-CFG.TARGET_SF)>1e-6: raw.resample(CFG.TARGET_SF, npad=\"auto\", verbose=False)\n",
    "    ny=raw.info[\"sfreq\"]/2; raw.filter(band[0], min(band[1], ny-1.0), verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "    return raw\n",
    "\n",
    "def make_epochs(raw):\n",
    "    ov=max(0.0, CFG.EPOCH_LEN-CFG.STEP)\n",
    "    return mne.make_fixed_length_epochs(raw, duration=CFG.EPOCH_LEN, overlap=ov, preload=True, verbose=False)\n",
    "\n",
    "def spectral_features(epochs):\n",
    "    X=epochs.get_data(); sf=epochs.info[\"sfreq\"]\n",
    "    n_ep,n_ch,n_t=X.shape; nper=min(int(sf*2), n_t); nov=nper//2\n",
    "    f,P=welch(X, fs=sf, nperseg=nper, noverlap=nov, axis=-1, average=\"median\")\n",
    "    def idx(lo,hi): return np.where((f>=lo)&(f<hi))[0]\n",
    "    aidx=idx(0.5,80.0); tot=np.maximum(P[:,:,aidx].sum(-1),1e-12)\n",
    "    feats={}; acc={\"alpha\":np.zeros((n_ep,n_ch)),\"theta\":np.zeros((n_ep,n_ch)),\"beta\":np.zeros((n_ep,n_ch))}\n",
    "    for name,(lo,hi) in CFG.BANDS.items():\n",
    "        b=idx(lo,hi); bp=P[:,:,b].sum(-1)/tot if b.size else np.zeros((n_ep,n_ch))\n",
    "        feats[f\"{name}_rel_med\"]=np.median(bp,axis=1); feats[f\"{name}_rel_iqr\"]=np.subtract(*np.percentile(bp,[75,25],axis=1)); feats[f\"{name}_rel_std\"]=np.std(bp,axis=1)\n",
    "        head=name.split(\"_\",1)[0]\n",
    "        if head in acc: acc[head]+=bp\n",
    "    alpha=np.median(acc[\"alpha\"],axis=1); theta=np.median(acc[\"theta\"],axis=1); beta=np.median(acc[\"beta\"],axis=1)\n",
    "    feats[\"alpha_rel_med\"]=alpha; feats[\"theta_rel_med\"]=theta; feats[\"beta_rel_med\"]=beta\n",
    "    feats[\"theta_over_alpha\"]=theta/np.maximum(alpha,1e-6); feats[\"beta_over_alpha\"]=beta/np.maximum(alpha,1e-6)\n",
    "    feats[\"(alpha+theta)_over_beta\"]=(alpha+theta)/np.maximum(beta,1e-6)\n",
    "    # entropy & centroid\n",
    "    p_band=P[:,:,aidx]; p_n=p_band/np.maximum(p_band.sum(-1,keepdims=True),1e-12)\n",
    "    H=-(p_n*np.log2(p_n+1e-12)).sum(-1); Hn=H/np.log2(p_band.shape[-1])\n",
    "    feats[\"spec_entropy_med\"]=np.median(Hn,axis=1); feats[\"spec_entropy_std\"]=np.std(Hn,axis=1)\n",
    "    fb=f[aidx].reshape(1,1,-1); cen=(p_band*fb).sum(-1)/np.maximum(p_band.sum(-1),1e-12)\n",
    "    feats[\"spec_centroid_med\"]=np.median(cen,axis=1); feats[\"spec_centroid_std\"]=np.std(cen,axis=1)\n",
    "    # Hjorth\n",
    "    d1=np.diff(X,axis=-1); var0=np.var(X,axis=-1)+1e-12; var1=np.var(d1,axis=-1)+1e-12\n",
    "    feats[\"hjorth_activity_med\"]=np.median(var0,axis=1); feats[\"hjorth_mobility_med\"]=np.median(np.sqrt(var1/var0),axis=1)\n",
    "    d2=np.diff(d1,axis=-1); var2=np.var(d2,axis=-1)+1e-12\n",
    "    feats[\"hjorth_complexity_med\"]=np.median(np.sqrt((var2/var1)/(var1/var0)),axis=1)\n",
    "    return pd.DataFrame(feats)\n",
    "\n",
    "def occipital_picks(raw):\n",
    "    names=[n.upper().strip() for n in raw.ch_names]\n",
    "    wanted=(\"O1\",\"O2\",\"OZ\",\"POZ\",\"PO3\",\"PO4\")\n",
    "    idx=[i for i,n in enumerate(names) if n in wanted]\n",
    "    return idx if idx else mne.pick_types(raw.info, eeg=True)\n",
    "\n",
    "def alpha_index(raw, band):\n",
    "    sf=raw.info[\"sfreq\"]\n",
    "    f,P=welch(raw.get_data(), fs=sf, nperseg=min(int(sf*2),raw.n_times), noverlap=int(min(int(sf*2),raw.n_times)/2), axis=-1, average=\"median\")\n",
    "    idx=(f>=band[0])&(f<band[1]); a=P[:,idx].sum(-1); tot=P.sum(-1)+1e-12\n",
    "    oi=occipital_picks(raw); return float((a[oi].sum()/tot[oi].sum()) if len(oi) else (a.sum()/tot.sum()))\n",
    "\n",
    "def butter_bandpass_array(X, lo, hi, sf, order=4):\n",
    "    X2=X.reshape(X.shape[0]*X.shape[1], X.shape[2])\n",
    "    Xf=mne.filter.filter_data(X2, sfreq=sf, l_freq=lo, h_freq=hi, method=\"iir\",\n",
    "                              iir_params=dict(order=order, ftype=\"butter\"), verbose=False)\n",
    "    return Xf.reshape(X.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Refit S001 at K=3 and save trio/models\n",
    "# -----------------------------\n",
    "F = pd.read_csv(CFG.RUN_K4/\"features.csv\")\n",
    "M = pd.read_csv(CFG.RUN_K4/\"metadata.csv\").sort_values([\"file\",\"t_start_s\",\"t_end_s\"]).reset_index(drop=True)\n",
    "ensure_dir(CFG.RUN_K3)\n",
    "F.to_csv(CFG.RUN_K3/\"features.csv\", index=False)\n",
    "M.to_csv(CFG.RUN_K3/\"metadata.csv\", index=False)\n",
    "\n",
    "scaler = StandardScaler().fit(F.values); Z = scaler.transform(F.values)\n",
    "pca    = PCA(n_components=min(20,Z.shape[1]), random_state=42).fit(Z); Z = pca.transform(Z)\n",
    "km     = KMeans(n_clusters=3, n_init=\"auto\", random_state=42).fit(Z)\n",
    "L_k3   = km.labels_\n",
    "pd.DataFrame({\"state\":L_k3}).to_csv(CFG.RUN_K3/\"state_assignments.csv\", index=False)\n",
    "from joblib import dump\n",
    "dump(scaler, CFG.RUN_K3/\"scaler_k3.joblib\"); dump(pca, CFG.RUN_K3/\"pca_k3.joblib\"); dump(km, CFG.RUN_K3/\"kmeans_k3.joblib\")\n",
    "print(\"K=3 model saved â†’\", CFG.RUN_K3)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Re-evaluate S002â€“S010 with K=3 (EO/EC, R03 AUC)\n",
    "# -----------------------------\n",
    "def iaf_calibrate(subj):\n",
    "    r01=CFG.GEN/f\"{subj}R01.edf\"; r02=CFG.GEN/f\"{subj}R02.edf\"\n",
    "    raw_eo=load_raw_edf(r01, band=(0.5,45.0)); raw_ec=load_raw_edf(r02, band=(0.5,45.0))\n",
    "    sf=raw_ec.info[\"sfreq\"]\n",
    "    f,P=welch(raw_ec.get_data(), fs=sf, nperseg=min(int(sf*2), raw_ec.n_times), noverlap=int(min(int(sf*2), raw_ec.n_times)/2), axis=-1, average=\"median\")\n",
    "    band=(f>=7)&(f<=14); iaf=float(np.clip(f[band][np.argmax(P[occipital_picks(raw_ec)][:,band].mean(0))], 8.0,12.0))\n",
    "    alpha_band=(max(6.0, iaf-2.0), min(14.0, iaf+2.0))\n",
    "    thr=0.5*(alpha_index(raw_ec,alpha_band)+alpha_index(raw_eo,alpha_band))\n",
    "    return iaf, alpha_band, thr\n",
    "\n",
    "def eoec_accuracy(subj, alpha_band, thr):\n",
    "    r01=CFG.GEN/f\"{subj}R01.edf\"; r02=CFG.GEN/f\"{subj}R02.edf\"\n",
    "    def classify(raw, cond):\n",
    "        sf=raw.info[\"sfreq\"]; ts=np.arange(0, raw.n_times/sf - CFG.EPOCH_LEN + 1e-9, CFG.STEP)\n",
    "        preds=[]\n",
    "        for t0 in ts:\n",
    "            s=int(t0*sf); e=s+int(CFG.EPOCH_LEN*sf); seg=raw.get_data(start=s, stop=e)\n",
    "            f,P=welch(seg, fs=sf, nperseg=min(int(sf*2), e-s), noverlap=int(min(int(sf*2), e-s)/2), axis=-1, average=\"median\")\n",
    "            idx=(f>=alpha_band[0])&(f<alpha_band[1]); a=P[:,idx].sum(-1); tot=P.sum(-1)+1e-12\n",
    "            oi=occipital_picks(raw); ai=(a[oi].sum()/tot[oi].sum()) if len(oi) else (a.sum()/tot.sum())\n",
    "            preds.append(\"EC\" if ai>=thr else \"EO\")\n",
    "        return pd.DataFrame({\"gt\":cond,\"pred\":preds})\n",
    "    raw_ec=load_raw_edf(r02, band=(0.5,45.0)); raw_eo=load_raw_edf(r01, band=(0.5,45.0))\n",
    "    df=pd.concat([classify(raw_ec,\"EC\"), classify(raw_eo,\"EO\")], ignore_index=True)\n",
    "    return float((df[\"gt\"]==df[\"pred\"]).mean())\n",
    "\n",
    "def r03_auc(subj):\n",
    "    r03=CFG.GEN/f\"{subj}R03.edf\"; raw=load_raw_edf(r03, band=(0.5,40.0))\n",
    "    want={\"C3\",\"Cz\",\"C4\",\"FC3\",\"FCz\",\"FC4\",\"CP3\",\"CPz\",\"CP4\"}\n",
    "    picks=[i for i,ch in enumerate(raw.ch_names) if ch.upper().strip() in want]\n",
    "    if len(picks)<3: picks=mne.pick_types(raw.info, eeg=True)\n",
    "    raw.pick(picks); anns=raw.annotations\n",
    "    if anns is None or len(anns)==0: return np.nan\n",
    "    sf=raw.info[\"sfreq\"]\n",
    "    segs=[]; gid=0\n",
    "    for o,d,s in zip(anns.onset, anns.duration, anns.description):\n",
    "        su=str(s).upper()\n",
    "        if \"T0\" in su or \"T1\" in su or \"T2\" in su:\n",
    "            lab=0 if \"T0\" in su else 1; segs.append((float(o), float(o+d), lab, gid)); gid+=1\n",
    "    X=[]; y=[]; g=[]\n",
    "    for a,b,lab,segid in segs:\n",
    "        t=a\n",
    "        while t+2.0 <= b-1e-6:\n",
    "            s=int(t*sf); e=s+int(2.0*sf)\n",
    "            X.append(raw.get_data(start=s, stop=e)); y.append(lab); g.append(segid); t+=2.0\n",
    "    if not X: return np.nan\n",
    "    X=np.stack(X,0); y=np.array(y,int); g=np.array(g,int)\n",
    "    proba=np.zeros(len(y),float); gkf=GroupKFold(n_splits=min(5,max(2,len(np.unique(g)))))\n",
    "    for tr,te in gkf.split(X,y,g):\n",
    "        feats_tr, feats_te=[],[]\n",
    "        for lo,hi in CFG.FB_BANDS:\n",
    "            Xtr=butter_bandpass_array(X[tr], lo, hi, sf); Xte=butter_bandpass_array(X[te], lo, hi, sf)\n",
    "            csp=CSP(n_components=6, reg=\"ledoit_wolf\", log=True, norm_trace=False); csp.fit(Xtr, y[tr])\n",
    "            feats_tr.append(csp.transform(Xtr)); feats_te.append(csp.transform(Xte))\n",
    "        Xtr_fb=np.concatenate(feats_tr,1); Xte_fb=np.concatenate(feats_te,1)\n",
    "        clf=LDA(solver=\"lsqr\", shrinkage=\"auto\").fit(Xtr_fb, y[tr]); proba[te]=clf.predict_proba(Xte_fb)[:,1]\n",
    "    return float(roc_auc_score(y, proba))\n",
    "\n",
    "# Project K=3 model to decode letters for a file (not needed for EO/EC/AUC, but for later MI/Î”xent if you want)\n",
    "from joblib import load\n",
    "scaler_k3=load(CFG.RUN_K3/\"scaler_k3.joblib\"); pca_k3=load(CFG.RUN_K3/\"pca_k3.joblib\"); km_k3=load(CFG.RUN_K3/\"kmeans_k3.joblib\")\n",
    "TRAIN_COLS=list(pd.read_csv(CFG.RUN_K3/\"features.csv\", nrows=1).columns)\n",
    "\n",
    "def decode_letters_with_k3(edf, starts, ends):\n",
    "    raw=load_raw_edf(edf)\n",
    "    labels=[]\n",
    "    for t0,t1 in zip(starts, ends):\n",
    "        seg=raw.copy().crop(tmin=t0, tmax=t1, include_tmax=False)\n",
    "        ep=make_epochs(seg); Fsp=spectral_features(ep).reindex(columns=TRAIN_COLS, fill_value=0.0)\n",
    "        Z=pca_k3.transform(scaler_k3.transform(Fsp.values))\n",
    "        labels.append(int(km_k3.predict(Z)[0]))\n",
    "    return np.array(labels,int)\n",
    "\n",
    "# Re-evaluate subjects\n",
    "rows=[]\n",
    "for subj in [f\"S{n:03d}\" for n in range(2,11)]:\n",
    "    try:\n",
    "        iaf, band, thr = iaf_calibrate(subj)\n",
    "        acc = eoec_accuracy(subj, band, thr)\n",
    "        auc = r03_auc(subj)\n",
    "        rows.append({\"subject\":subj, \"acc\":acc, \"AUC_task\":auc, \"IAF_hz\":iaf})\n",
    "        print(subj, \"EO/EC=\", round(acc,3), \"AUC=\", round(auc,3))\n",
    "    except Exception as e:\n",
    "        print(\"skip\", subj, e)\n",
    "df = pd.DataFrame(rows)\n",
    "GEN = CFG.REP / \"generalization\"\n",
    "GEN.mkdir(parents=True, exist_ok=True)\n",
    "df[[\"subject\",\"acc\"]].to_csv(GEN/\"eoec_iaf_summary.csv\", index=False)\n",
    "df[[\"subject\",\"AUC_task\"]].to_csv(GEN/\"task_fbcsp_summary.csv\", index=False)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Grammar gain Î”xent for S001 @ K=3\n",
    "# -----------------------------\n",
    "L = pd.read_csv(CFG.RUN_K3/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "K = int(L.max()+1)\n",
    "T=np.zeros((K,K))\n",
    "for a,b in zip(L[:-1], L[1:]): T[a,b]+=1\n",
    "eps=1e-6; T=(T+eps)/(T.sum(axis=1, keepdims=True)+eps*K)\n",
    "pi=T.sum(axis=0); pi=pi/(pi.sum()+1e-12)\n",
    "H_T1=-np.mean([math.log(T[a,b]+1e-12) for a,b in zip(L[:-1], L[1:])])\n",
    "H_pi=-np.mean([math.log(pi[b]+1e-12) for b in L[1:]])\n",
    "dxent = H_pi - H_T1\n",
    "print(\"S001 Î”xent (K=3) =\", round(dxent,3))\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Mechanism MI(letter;Î¼) for S001 @ K=3 (epoch-dominant microstate)\n",
    "# -----------------------------\n",
    "def mi_letter_mu_k3():\n",
    "    meta = pd.read_csv(CFG.RUN_K3/\"metadata.csv\").sort_values([\"file\",\"t_start_s\",\"t_end_s\"]).reset_index(drop=True)\n",
    "    Ls   = pd.read_csv(CFG.RUN_K3/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "    rows=[]\n",
    "    for fname in meta[\"file\"].unique():\n",
    "        if not fname.lower().endswith(\".edf\"): continue\n",
    "        # pick EDF\n",
    "        p = (CFG.RUN_K4/\"brainwaves_rebuilt\"/fname)\n",
    "        if not p.exists(): \n",
    "            p = (CFG.ROOT/\"s001_edf\"/fname)\n",
    "        if not p.exists(): continue\n",
    "        raw = load_raw_edf(p, band=(2.0,20.0))\n",
    "        X=raw.get_data(); sf=raw.info[\"sfreq\"]\n",
    "        pk,_=find_peaks(X.std(axis=0), distance=int((10/1000.0)*sf))\n",
    "        Xp=X[:,pk].T; rng=default_rng(42)\n",
    "        Xn=Xp-Xp.mean(axis=1,keepdims=True); Xn=Xn/(np.linalg.norm(Xn,axis=1,keepdims=True)+1e-12)\n",
    "        C=Xn[rng.choice(len(Xn),3,replace=False)].copy()\n",
    "        for _ in range(60):\n",
    "            corr=Xn@C.T; lab=np.argmax(np.abs(corr),axis=1); sgn=np.sign(corr[np.arange(len(Xn)),lab])\n",
    "            Cn=[]\n",
    "            for k in range(3):\n",
    "                m=(lab==k); v=(Xn[m]*sgn[m][:,None]).mean(0) if np.any(m) else C[k]; v=v/(np.linalg.norm(v)+1e-12); Cn.append(v)\n",
    "            Cn=np.stack(Cn,0)\n",
    "            if np.allclose(C,Cn,atol=1e-5): break; C=Cn\n",
    "        # backfit + epoch dom\n",
    "        M=C/(np.linalg.norm(C,axis=1,keepdims=True)+1e-12); X0n=X/(np.linalg.norm(X,axis=0,keepdims=True)+1e-12)\n",
    "        ms=np.argmax(np.abs(M@X0n), axis=0)\n",
    "        mask=(meta[\"file\"]==fname).to_numpy()\n",
    "        starts=meta.loc[mask,\"t_start_s\"].to_numpy(); ends=meta.loc[mask,\"t_end_s\"].to_numpy(); Lf=Ls[mask]\n",
    "        dom=[]\n",
    "        for t0,t1 in zip(starts, ends):\n",
    "            s=int(round(t0*sf)); e=int(round(t1*sf)); arr=ms[s:e]\n",
    "            dom.append(np.bincount(arr, minlength=3).argmax() if e>s and len(arr)>0 else -1)\n",
    "        valid=(np.array(dom)>=0)\n",
    "        rows.append(pd.DataFrame({\"letter\":Lf[valid], \"ms\":np.array(dom)[valid]}))\n",
    "    if not rows: return np.nan\n",
    "    E=pd.concat(rows, ignore_index=True)\n",
    "    def entropy(p): p=p[p>0]; return -np.sum(p*np.log2(p))\n",
    "    Kletter=int(E[\"letter\"].max()+1); Kmu=int(E[\"ms\"].max()+1)\n",
    "    Pm=np.bincount(E[\"ms\"], minlength=Kmu)/len(E); Hm=entropy(Pm); HmL=0.0\n",
    "    for s in range(Kletter):\n",
    "        mask=(E[\"letter\"]==s)\n",
    "        if not np.any(mask): continue\n",
    "        P=np.bincount(E[\"ms\"][mask], minlength=Kmu)/mask.sum()\n",
    "        HmL += mask.mean()*entropy(P)\n",
    "    return float(Hm - HmL)\n",
    "mi_k3 = mi_letter_mu_k3()\n",
    "print(\"S001 MI(letter;Î¼) @K=3 =\", round(mi_k3,3))\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Rebuild panels (Generalization & Baselines/Pred & Mechanism summary)\n",
    "# -----------------------------\n",
    "# Generalization figure\n",
    "eoec = pd.read_csv(CFG.REP/\"generalization\"/\"eoec_iaf_summary.csv\")\n",
    "task = pd.read_csv(CFG.REP/\"generalization\"/\"task_fbcsp_summary.csv\")\n",
    "plt.figure(figsize=(12,4)); \n",
    "plt.subplot(1,2,1); plt.bar(np.arange(len(eoec)), eoec[\"acc\"]); plt.ylim(0,1); plt.title(\"EO/EC accuracy (K=3)\"); plt.xticks(np.arange(len(eoec)), eoec[\"subject\"], rotation=30)\n",
    "plt.subplot(1,2,2); plt.bar(np.arange(len(task)), task[\"AUC_task\"]); plt.ylim(0.5,1.0); plt.title(\"R03 Task AUC (K=3)\"); plt.xticks(np.arange(len(task)), task[\"subject\"], rotation=30)\n",
    "plt.suptitle(\"CALM â€” Generalization (K=3)\"); \n",
    "plt.savefig(CFG.REP/\"generalization\"/\"CALM_Generalization_K3.png\", dpi=160, bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "# Mechanism summary text file\n",
    "with open(CFG.REP/\"microstates\"/\"CALM_Mechanism_K3.txt\",\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(f\"MI(letter;Î¼) @K=3 (S001): {mi_k3:.3f} bits\\nÎ”xent (S001) @K=3: {dxent:.3f}\\n\")\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Stamp decision into spec & README\n",
    "# -----------------------------\n",
    "# Update scoreboard JSON\n",
    "score_json = CFG.REP/\"generalization\"/\"v0_3_scoreboard.json\"\n",
    "score = json.load(open(score_json)) if score_json.exists() else {}\n",
    "score[\"K_star\"] = 3\n",
    "with open(score_json,\"w\") as f: json.dump(score, f, indent=2)\n",
    "\n",
    "# Append to README & Spec\n",
    "readme = CFG.ROOT/\"CNT_CognitiveAlphabet_v0_3\"/\"README_v0_3.md\"\n",
    "line = \"\\n## CALM default K\\n- Pareto decision: **K=3** (smallest K with Î”xent>0 and stable ARI). Panels & scoreboards regenerated.\\n\"\n",
    "try:\n",
    "    with open(readme,\"a\",encoding=\"utf-8\") as f: f.write(line)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "spec = CFG.ROOT/\"CALM_v1_spec\"/\"CALM_v1_spec.md\"\n",
    "if spec.exists():\n",
    "    with open(spec,\"a\",encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n### Default K\\nCALM v1.0 default alphabet size: **K=3** (Pareto pick for predictiveness vs stability). Optional zoom modes: Kâ‰¥4 with documented coarseâ†’fine mapping.\\n\")\n",
    "\n",
    "print(\"\\nDONE â€” K=3 refit + re-eval complete.\")\n",
    "print(\"Figures:\")\n",
    "print(\"  - Generalization (K=3):\", CFG.REP/\"generalization\"/\"CALM_Generalization_K3.png\")\n",
    "print(\"Mechanism/grammar summary â†’\", CFG.REP/\"microstates\"/\"CALM_Mechanism_K3.txt\")\n",
    "print(\"Scoreboard updated â†’\", CFG.REP/\"generalization\"/\"v0_3_scoreboard.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a940d4e3-88a6-4aff-ae8f-ad069b152002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coarseâ†’fine mapping (K4â†’K3):\n",
      "  K3 S0 â† K4 ['S0']\n",
      "  K3 S1 â† K4 ['S1', 'S3']\n",
      "  K3 S2 â† K4 ['S2']\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "K=3 Mechanism Panel â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\microstates\\CALM_Mechanism_Panel_K3.pdf\n",
      "Hierarchical grammar PDF â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\analysis\\CALM_Hierarchical_Grammar.pdf\n"
     ]
    }
   ],
   "source": [
    "# === CALM â€” Coarseâ†’Fine + K=3 Mechanism + Hierarchical Grammar Gain ===\n",
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt, mne\n",
    "from pathlib import Path\n",
    "from scipy.signal import welch, find_peaks\n",
    "from numpy.random import default_rng\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# ---------------- CFG (edit if needed) ----------------\n",
    "class CFG:\n",
    "    ROOT      = Path(r\"E:\\CNT\\artifacts\")\n",
    "    RUN_K4    = ROOT / \"cog_alphabet_hybrid_v1\"         # existing K=4 run\n",
    "    RUN_K3    = ROOT / \"cog_alphabet_hybrid_k3\"         # new K=3 run (made earlier)\n",
    "    REP       = ROOT / \"cog_alphabet_report_hybrid_v1\"\n",
    "    TARGET_SF = 250.0\n",
    "    EPOCH_LEN = 2.0\n",
    "    STEP      = 0.5\n",
    "CFG = CFG()\n",
    "\n",
    "# ---------------- utility ----------------\n",
    "def load_raw(edf, band=(2.0,20.0), ref=\"average\"):\n",
    "    raw = mne.io.read_raw_edf(str(edf), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"]-CFG.TARGET_SF)>1e-6: raw.resample(CFG.TARGET_SF, npad=\"auto\", verbose=False)\n",
    "    raw.filter(band[0], band[1], verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        if ref==\"average\":\n",
    "            raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "        elif ref==\"cz\":\n",
    "            try: raw.set_eeg_reference([\"Cz\"], projection=False, verbose=False)\n",
    "            except Exception: pass\n",
    "    try:\n",
    "        raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"),\n",
    "                        match_case=False, match_alias=True, on_missing=\"ignore\")\n",
    "    except Exception: pass\n",
    "    return raw\n",
    "\n",
    "def epoch_dom_ms(raw, starts, ends, Kmic=4):\n",
    "    \"\"\"dominant microstate per epoch using simple polarity-agnostic kmeans-like maps\"\"\"\n",
    "    X=raw.get_data(); sf=raw.info[\"sfreq\"]\n",
    "    pk,_=find_peaks(X.std(axis=0), distance=int((10/1000.0)*sf))\n",
    "    Xp=X[:,pk].T\n",
    "    rng=default_rng(42)\n",
    "    Xn=Xp - Xp.mean(axis=1, keepdims=True); Xn = Xn/(np.linalg.norm(Xn,axis=1,keepdims=True)+1e-12)\n",
    "    C=Xn[rng.choice(len(Xn), Kmic, replace=False)].copy()\n",
    "    for _ in range(60):\n",
    "        corr=Xn@C.T; lab=np.argmax(np.abs(corr),axis=1); sign=np.sign(corr[np.arange(len(Xn)),lab])\n",
    "        Cn=[]\n",
    "        for k in range(Kmic):\n",
    "            m=(lab==k); v=(Xn[m]*sign[m][:,None]).mean(0) if np.any(m) else C[k]\n",
    "            v=v/(np.linalg.norm(v)+1e-12); Cn.append(v)\n",
    "        Cn=np.stack(Cn,0)\n",
    "        if np.allclose(C,Cn,atol=1e-5): break\n",
    "        C=Cn\n",
    "    # backfit\n",
    "    M=C/(np.linalg.norm(C,axis=1,keepdims=True)+1e-12)\n",
    "    X0n=X/(np.linalg.norm(X,axis=0,keepdims=True)+1e-12)\n",
    "    ms=np.argmax(np.abs(M@X0n), axis=0)\n",
    "    # epoch dom\n",
    "    dom=[]\n",
    "    for t0,t1 in zip(starts, ends):\n",
    "        s=int(round(t0*sf)); e=int(round(t1*sf)); arr=ms[s:e]\n",
    "        dom.append(np.bincount(arr, minlength=Kmic).argmax() if e>s and len(arr)>0 else -1)\n",
    "    return np.array(dom,int)\n",
    "\n",
    "def letter_seq_to_T1(seq, K):\n",
    "    T=np.zeros((K,K))\n",
    "    for a,b in zip(seq[:-1], seq[1:]): T[a,b]+=1\n",
    "    eps=1e-6; T=(T+eps)/(T.sum(axis=1, keepdims=True)+eps*K)\n",
    "    pi=T.sum(axis=0); pi=pi/(pi.sum()+1e-12)\n",
    "    return T, pi\n",
    "\n",
    "def seq_loglik_T1(seq, T):\n",
    "    return float(np.sum([np.log(T[a,b]+1e-12) for a,b in zip(seq[:-1], seq[1:])]))\n",
    "\n",
    "# ---------------- 1) coarseâ†’fine mapping (K4â†’K3) ----------------\n",
    "meta_k4 = pd.read_csv(CFG.RUN_K4/\"metadata.csv\").sort_values([\"file\",\"t_start_s\",\"t_end_s\"])\n",
    "L4 = pd.read_csv(CFG.RUN_K4/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "L3 = pd.read_csv(CFG.RUN_K3/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "\n",
    "# overlap matrix M[c4, c3] = count\n",
    "K4=int(L4.max()+1); K3=int(L3.max()+1)\n",
    "M=np.zeros((K4,K3), int)\n",
    "for c4,c3 in zip(L4,L3): M[c4,c3]+=1\n",
    "parent = {int(c4): int(np.argmax(M[c4])) for c4 in range(K4)}  # assign each K4 to a K3 parent\n",
    "children = {int(p): sorted([int(c4) for c4 in range(K4) if parent[c4]==p]) for p in range(K3)}\n",
    "mapping = {\"K4_to_K3_parent\": parent, \"K3_children\": children, \"overlap\": M.tolist()}\n",
    "(CFG.REP/\"analysis\"/\"coarse_fine_map.json\").write_text(json.dumps(mapping, indent=2), encoding=\"utf-8\")\n",
    "print(\"Coarseâ†’fine mapping (K4â†’K3):\")\n",
    "for p,ch in children.items(): print(f\"  K3 S{p} â† K4 {['S'+str(c) for c in ch]}\")\n",
    "\n",
    "# ---------------- 2) K=3 Mechanism Panel (templates, task heatmap, Î¼-bars) ----------------\n",
    "# build per-epoch dom microstate for S001 runs and per-letter Î¼ mix (K=3)\n",
    "rows=[]; ms_summary={}\n",
    "for fname in meta_k4[\"file\"].unique():\n",
    "    if not fname.lower().endswith(\".edf\"): continue\n",
    "    # locate EDF\n",
    "    edf=(CFG.RUN_K4/\"brainwaves_rebuilt\"/fname)\n",
    "    if not edf.exists(): edf=(CFG.ROOT/\"s001_edf\"/fname)\n",
    "    if not edf.exists(): continue\n",
    "    mask=(meta_k4[\"file\"]==fname).to_numpy()\n",
    "    starts=meta_k4.loc[mask,\"t_start_s\"].to_numpy(); ends=meta_k4.loc[mask,\"t_end_s\"].to_numpy()\n",
    "    letters=L3[mask]\n",
    "    raw=load_raw(edf, band=(2.0,20.0), ref=\"average\")\n",
    "    dom = epoch_dom_ms(raw, starts, ends, Kmic=4)\n",
    "    valid=(dom>=0)\n",
    "    df=pd.DataFrame({\"state\":letters[valid], \"mu\":dom[valid]})\n",
    "    rows.append(df)\n",
    "if rows:\n",
    "    E=pd.concat(rows, ignore_index=True)\n",
    "    for s in range(K3):\n",
    "        sub=E[E[\"state\"]==s]\n",
    "        if len(sub):\n",
    "            ms_summary[s]= (sub[\"mu\"].value_counts(normalize=True).reindex([0,1,2,3], fill_value=0.0).values)\n",
    "else:\n",
    "    ms_summary={s: np.zeros(4) for s in range(K3)}\n",
    "\n",
    "# draw panel\n",
    "panel_png = CFG.REP/\"microstates\"/\"CALM_Mechanism_Panel_K3.png\"\n",
    "panel_pdf = CFG.REP/\"microstates\"/\"CALM_Mechanism_Panel_K3.pdf\"\n",
    "with PdfPages(panel_pdf) as pdf:\n",
    "    # Î¼-bars\n",
    "    plt.figure(figsize=(10,3.2))\n",
    "    for i in range(K3):\n",
    "        plt.subplot(1,K3,i+1)\n",
    "        plt.bar([f\"Î¼{k}\" for k in range(4)], ms_summary.get(i, np.zeros(4)))\n",
    "        plt.ylim(0,1); plt.title(f\"K3 S{i} microstate mix\")\n",
    "    plt.tight_layout(); pdf.savefig(bbox_inches=\"tight\"); plt.close()\n",
    "    # placeholder task heatmap (reuse K=4 R03 dom microstates but group by K=3 letter)\n",
    "    mask=(meta_k4[\"file\"]==\"S001R03.edf\").to_numpy()\n",
    "    if mask.any():\n",
    "        edf=(CFG.RUN_K4/\"brainwaves_rebuilt\"/\"S001R03.edf\"); \n",
    "        if not edf.exists(): edf=(CFG.ROOT/\"s001_edf\"/\"S001R03.edf\")\n",
    "        raw=load_raw(edf, band=(2.0,20.0), ref=\"average\")\n",
    "        starts=meta_k4.loc[mask,\"t_start_s\"].to_numpy(); ends=meta_k4.loc[mask,\"t_end_s\"].to_numpy()\n",
    "        letters=L3[mask]\n",
    "        dom=epoch_dom_ms(raw, starts, ends, Kmic=4)\n",
    "        df=pd.DataFrame({\"state\":letters, \"mu\":dom})\n",
    "        # no fbCSP probs here (keep simple): heat = stateÃ—Î¼ frequency\n",
    "        heat=np.zeros((K3,4), float)\n",
    "        for s in range(K3):\n",
    "            sub=df[df[\"state\"]==s]\n",
    "            if len(sub):\n",
    "                heat[s]=sub[\"mu\"].value_counts(normalize=True).reindex([0,1,2,3], fill_value=0.0).values\n",
    "        plt.figure(figsize=(6,3.6)); plt.imshow(heat, aspect=\"auto\"); plt.colorbar(label=\"fraction\")\n",
    "        plt.yticks(range(K3), [f\"S{i}\" for i in range(K3)]); plt.xticks(range(4), [f\"Î¼{k}\" for k in range(4)])\n",
    "        plt.title(\"S001R03: fraction by K3 letter Ã— microstate\"); pdf.savefig(bbox_inches=\"tight\"); plt.close()\n",
    "# also save PNG of the first page\n",
    "img = plt.imread(panel_pdf) if False else None  # (safeguard; main artifact is the PDF)\n",
    "print(\"K=3 Mechanism Panel â†’\", panel_pdf)\n",
    "\n",
    "# ---------------- 3) Hierarchical grammar gain (K=3 â†’ K=4 children) ----------------\n",
    "# Build S001 K3 and K4 sequences aligned by epoch index\n",
    "L4 = pd.read_csv(CFG.RUN_K4/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "L3 = pd.read_csv(CFG.RUN_K3/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "assert len(L4)==len(L3), \"K=3/4 label sequences must align by epoch.\"\n",
    "\n",
    "# Coarse T1 and parent-child disaggregated T1\n",
    "K3 = int(L3.max()+1); K4=int(L4.max()+1)\n",
    "T3, pi3 = letter_seq_to_T1(L3, K3)\n",
    "T4, pi4 = letter_seq_to_T1(L4, K4)\n",
    "\n",
    "# Log-likelihood at coarse level on the SAME sequence (use parents for L4)\n",
    "parent = mapping[\"K4_to_K3_parent\"]\n",
    "L4p = np.array([parent[int(c)] for c in L4], int)\n",
    "T3_from_L4, _ = letter_seq_to_T1(L4p, K3)\n",
    "\n",
    "LL_coarse = seq_loglik_T1(L4p, T3)           # coarse model fit to parent sequence\n",
    "# Fine: model that conditions on K3 parent-row + K4 detail â†’ approximate by (a) using T4 directly\n",
    "LL_fine   = seq_loglik_T1(L4, T4)            # fine model fit to child sequence\n",
    "\n",
    "delta_LL  = LL_fine - LL_coarse\n",
    "N_trans   = len(L4)-1\n",
    "delta_xent= -delta_LL / N_trans              # nats per transition (improvement when using fine detail)\n",
    "\n",
    "# Emit a compact hierarchical PDF page\n",
    "pdf = PdfPages(CFG.REP/\"analysis\"/\"CALM_Hierarchical_Grammar.pdf\")\n",
    "plt.figure(figsize=(8,3.4)); plt.axis(\"off\")\n",
    "txt=(f\"Hierarchical grammar gain (K=3 â†’ K=4)\\n\\n\"\n",
    "     f\"Î”LL = {delta_LL:.2f}  (on parent/child-aligned sequence)\\n\"\n",
    "     f\"Î”xent = {delta_xent:.4f} nats per transition (positive means finer model predicts better)\\n\\n\"\n",
    "     f\"Parentâ†’children:\\n\" + \"\\n\".join([f\"S{p} â† {', '.join('S'+str(c) for c in children[p])}\" for p in children]))\n",
    "plt.text(0.05,0.95, txt, va=\"top\", family=\"monospace\"); pdf.savefig(bbox_inches=\"tight\"); plt.close()\n",
    "pdf.close()\n",
    "print(\"Hierarchical grammar PDF â†’\", CFG.REP/\"analysis\"/\"CALM_Hierarchical_Grammar.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d6ac0b8-7d59-40d7-bb19-92a4920f948d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended CALM v1.1 section â†’ E:\\CNT\\artifacts\\CALM_v1_spec\\CALM_v1_spec.md\n",
      "Updated bundle README â†’ E:\\CNT\\artifacts\\CNT_CognitiveAlphabet_v0_3\\README_v0_3.md\n",
      "Hierarchical Î”xent (fine vs coarse) = 0.1732 nats/transition\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "SSP projectors applied...\n",
      "K=3 mechanism figure â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\microstates\\CALM_K3_Mechanism_Figure.png\n",
      "Findings PDF â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\analysis\\CALM_Findings_v1_1.pdf\n",
      "Copied to bundle â†’ E:\\CNT\\artifacts\\CNT_CognitiveAlphabet_v0_3\\report\n"
     ]
    }
   ],
   "source": [
    "# === CALM v1.1 â€” Findings + Spec (K=3 default, zoom rule, CMI) ===\n",
    "# - Appends v1.1 spec section (K=3 default, S1-only zoom with Î”xent gate, CMI metric, simulator)\n",
    "# - Recomputes hierarchical grammar gain (K=3â†’K=4) and saves JSON\n",
    "# - Rebuilds a self-contained Findings PDF with K=3 metrics, mechanism, hierarchical zoom, topology\n",
    "# - Copies artifacts into bundle/report and updates README\n",
    "\n",
    "import os, json, math, numpy as np, pandas as pd, matplotlib.pyplot as plt, mne\n",
    "from pathlib import Path\n",
    "from scipy.signal import welch, find_peaks\n",
    "from numpy.random import default_rng\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# ---------------- CFG â€” edit if needed ----------------\n",
    "class CFG:\n",
    "    ROOT      = Path(r\"E:\\CNT\\artifacts\")\n",
    "    RUN_K4    = ROOT / \"cog_alphabet_hybrid_v1\"         # existing K=4 run\n",
    "    RUN_K3    = ROOT / \"cog_alphabet_hybrid_k3\"         # K=3 run you created\n",
    "    REP       = ROOT / \"cog_alphabet_report_hybrid_v1\"\n",
    "    BUNDLE    = ROOT / \"CNT_CognitiveAlphabet_v0_3\"\n",
    "    SPEC_DIR  = ROOT / \"CALM_v1_spec\"\n",
    "    TARGET_SF = 250.0\n",
    "    EPOCH_LEN = 2.0\n",
    "    STEP      = 0.5\n",
    "CFG = CFG()\n",
    "(CFG.REP / \"analysis\").mkdir(parents=True, exist_ok=True)\n",
    "(CFG.REP / \"microstates\").mkdir(parents=True, exist_ok=True)\n",
    "(CFG.REP / \"generalization\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------- utils ----------------\n",
    "def safe_read_csv(p: Path):\n",
    "    try:\n",
    "        if p.exists() and p.stat().st_size > 0:\n",
    "            return pd.read_csv(p)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def load_raw(edf: Path, band=(2.0,20.0), ref=\"average\"):\n",
    "    raw = mne.io.read_raw_edf(str(edf), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"]-CFG.TARGET_SF)>1e-6: raw.resample(CFG.TARGET_SF, npad=\"auto\", verbose=False)\n",
    "    raw.filter(band[0], band[1], verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        if ref==\"average\":\n",
    "            raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "        elif ref==\"cz\":\n",
    "            try: raw.set_eeg_reference([\"Cz\"], projection=False, verbose=False)\n",
    "            except Exception: pass\n",
    "    try:\n",
    "        raw.set_montage(mne.channels.make_standard_montage(\"standard_1020\"),\n",
    "                        match_case=False, match_alias=True, on_missing=\"ignore\")\n",
    "    except Exception: pass\n",
    "    return raw\n",
    "\n",
    "def epoch_dom_ms(raw, starts, ends, Kmic=4):\n",
    "    X=raw.get_data(); sf=raw.info[\"sfreq\"]\n",
    "    pk,_=find_peaks(X.std(axis=0), distance=int((10/1000.0)*sf))\n",
    "    Xp=X[:,pk].T\n",
    "    rng=default_rng(42)\n",
    "    Xn=Xp - Xp.mean(axis=1, keepdims=True); Xn = Xn/(np.linalg.norm(Xn,axis=1,keepdims=True)+1e-12)\n",
    "    C=Xn[rng.choice(len(Xn), Kmic, replace=False)].copy()\n",
    "    for _ in range(60):\n",
    "        corr=Xn@C.T; lab=np.argmax(np.abs(corr),axis=1); sign=np.sign(corr[np.arange(len(Xn)),lab])\n",
    "        Cn=[]\n",
    "        for k in range(Kmic):\n",
    "            m=(lab==k); v=(Xn[m]*sign[m][:,None]).mean(0) if np.any(m) else C[k]\n",
    "            v=v/(np.linalg.norm(v)+1e-12); Cn.append(v)\n",
    "        Cn=np.stack(Cn,0)\n",
    "        if np.allclose(C,Cn,atol=1e-5): break\n",
    "        C=Cn\n",
    "    # backfit to continuous and vote per epoch\n",
    "    M=C/(np.linalg.norm(C,axis=1,keepdims=True)+1e-12)\n",
    "    X0n=X/(np.linalg.norm(X,axis=0,keepdims=True)+1e-12)\n",
    "    ms=np.argmax(np.abs(M@X0n), axis=0)\n",
    "    dom=[]\n",
    "    for t0,t1 in zip(starts, ends):\n",
    "        s=int(round(t0*sf)); e=int(round(t1*sf)); arr=ms[s:e]\n",
    "        dom.append(np.bincount(arr, minlength=Kmic).argmax() if e>s and len(arr)>0 else -1)\n",
    "    return np.array(dom,int)\n",
    "\n",
    "def T1_from_seq(seq, K):\n",
    "    T=np.zeros((K,K))\n",
    "    for a,b in zip(seq[:-1], seq[1:]): T[a,b]+=1\n",
    "    eps=1e-6; T=(T+eps)/(T.sum(axis=1, keepdims=True)+eps*K)\n",
    "    pi=T.sum(axis=0); pi=pi/(pi.sum()+1e-12)\n",
    "    return T, pi\n",
    "\n",
    "def seq_loglik_T1(seq, T):\n",
    "    return float(np.sum([np.log(T[a,b]+1e-12) for a,b in zip(seq[:-1], seq[1:])]))\n",
    "\n",
    "# ---------------- A) append CALM v1.1 spec + README ----------------\n",
    "spec_md = CFG.SPEC_DIR/\"CALM_v1_spec.md\"\n",
    "if spec_md.exists():\n",
    "    with open(spec_md,\"a\",encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\\n## CALM v1.1 (addendum)\\n\")\n",
    "        f.write(\"- **Default alphabet size:** **K=3** (Pareto pick balancing prediction Î”xent>0 and stability).\\n\")\n",
    "        f.write(\"- **Hierarchical zoom (optional):** Within coarse **S1**, enable a K=4 child split **only if** local Î”xent (fine vs coarse) > 0 on held-out data; report Î”xent when used.\\n\")\n",
    "        f.write(\"- **Mechanism metric:** Report **CMI** = I(L_{t+1}; Î¼_t | L_t) with a permutation p-value in addition to MI(letter; Î¼) and letter-wise KL.\\n\")\n",
    "        f.write(\"- **Generative adequacy:** Ship a semi-Markov + T1 simulator and report **JS(bigrams)** + **JS(dwells)** vs real sequences.\\n\")\n",
    "        f.write(\"- **Safety:** same as v1.0 (consent, on-device by default, CIs/p-values, no clinical claims without IRB).\\n\")\n",
    "    print(\"Appended CALM v1.1 section â†’\", spec_md)\n",
    "\n",
    "# stamp into bundle README\n",
    "readme = CFG.BUNDLE/\"README_v0_3.md\"\n",
    "try:\n",
    "    with open(readme,\"a\",encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n## CALM v1.1\\n- Default **K=3**; **S1-only K=4 zoom** when Î”xent>0; include **CMI** & **JS(sim)** in eval.\\n\")\n",
    "    print(\"Updated bundle README â†’\", readme)\n",
    "except Exception as e:\n",
    "    print(\"README update skipped:\", e)\n",
    "\n",
    "# ---------------- B) recompute hierarchical grammar gain (K=3â†’K=4) ----------------\n",
    "L4 = pd.read_csv(CFG.RUN_K4/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "L3 = pd.read_csv(CFG.RUN_K3/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "assert len(L4)==len(L3), \"K=3/4 label sequences must align.\"\n",
    "\n",
    "# load coarseâ†’fine map (if present), else construct by majority\n",
    "cf_map_path = CFG.REP/\"analysis\"/\"coarse_fine_map.json\"\n",
    "if cf_map_path.exists():\n",
    "    mapping = json.load(open(cf_map_path))\n",
    "else:\n",
    "    K4=int(L4.max()+1); K3=int(L3.max()+1)\n",
    "    M=np.zeros((K4,K3), int)\n",
    "    for c4,c3 in zip(L4,L3): M[c4,c3]+=1\n",
    "    parent = {int(c4): int(np.argmax(M[c4])) for c4 in range(K4)}\n",
    "    children = {int(p): sorted([int(c4) for c4 in range(K4) if parent[c4]==p]) for p in range(K3)}\n",
    "    mapping = {\"K4_to_K3_parent\": parent, \"K3_children\": children, \"overlap\": M.tolist()}\n",
    "    cf_map_path.write_text(json.dumps(mapping, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "parent = {int(k): int(v) for k,v in mapping[\"K4_to_K3_parent\"].items()}\n",
    "children = {int(k): [int(x) for x in v] for k,v in mapping[\"K3_children\"].items()}\n",
    "\n",
    "# parent sequence\n",
    "L4p = np.array([parent[int(c)] for c in L4], int)\n",
    "T3,_ = T1_from_seq(L4p, int(L3.max()+1))\n",
    "T4,_ = T1_from_seq(L4,  int(L4.max()+1))\n",
    "LL_coarse = seq_loglik_T1(L4p, T3)\n",
    "LL_fine   = seq_loglik_T1(L4,  T4)\n",
    "delta_LL  = LL_fine - LL_coarse\n",
    "delta_xent= -delta_LL/(len(L4)-1)\n",
    "hier = {\"Î”LL\": float(delta_LL), \"Î”xent\": float(delta_xent), \"parent_children\": children}\n",
    "(CFG.REP/\"analysis\"/\"hierarchical_grammar_gain.json\").write_text(json.dumps(hier, indent=2), encoding=\"utf-8\")\n",
    "print(\"Hierarchical Î”xent (fine vs coarse) =\", round(delta_xent,4), \"nats/transition\")\n",
    "\n",
    "# ---------------- C) build K=3 mechanism figure (bars + R03 heatmap) ----------------\n",
    "meta_k4 = pd.read_csv(CFG.RUN_K4/\"metadata.csv\").sort_values([\"file\",\"t_start_s\",\"t_end_s\"])\n",
    "rows=[]; K3=int(L3.max()+1)\n",
    "for fname in meta_k4[\"file\"].unique():\n",
    "    if not fname.lower().endswith(\".edf\"): continue\n",
    "    edf=(CFG.RUN_K4/\"brainwaves_rebuilt\"/fname)\n",
    "    if not edf.exists(): edf=(CFG.ROOT/\"s001_edf\"/fname)\n",
    "    if not edf.exists(): continue\n",
    "    mask=(meta_k4[\"file\"]==fname).to_numpy()\n",
    "    starts=meta_k4.loc[mask,\"t_start_s\"].to_numpy(); ends=meta_k4.loc[mask,\"t_end_s\"].to_numpy()\n",
    "    letters=L3[mask]\n",
    "    raw=load_raw(edf, band=(2.0,20.0), ref=\"average\")\n",
    "    dom=epoch_dom_ms(raw, starts, ends, Kmic=4)\n",
    "    valid=(dom>=0)\n",
    "    rows.append(pd.DataFrame({\"state\":letters[valid], \"mu\":dom[valid], \"file\":fname}))\n",
    "E=pd.concat(rows, ignore_index=True) if rows else pd.DataFrame(columns=[\"state\",\"mu\",\"file\"])\n",
    "\n",
    "# per-letter Î¼ bars\n",
    "ms_mix = {}\n",
    "for s in range(K3):\n",
    "    sub=E[E[\"state\"]==s]\n",
    "    if len(sub)==0: ms_mix[s]=np.zeros(4)\n",
    "    else:\n",
    "        ms_mix[s]= sub[\"mu\"].value_counts(normalize=True).reindex([0,1,2,3], fill_value=0.0).values\n",
    "\n",
    "# heatmap for R03\n",
    "mask_r03 = E[\"file\"].eq(\"S001R03.edf\") if \"file\" in E.columns else pd.Series([],dtype=bool)\n",
    "heat = np.zeros((K3,4))\n",
    "if mask_r03.any():\n",
    "    for s in range(K3):\n",
    "        sub=E[mask_r03 & (E[\"state\"]==s)]\n",
    "        heat[s]= sub[\"mu\"].value_counts(normalize=True).reindex([0,1,2,3], fill_value=0.0).values\n",
    "\n",
    "mech_png = CFG.REP/\"microstates\"/\"CALM_K3_Mechanism_Figure.png\"\n",
    "plt.figure(figsize=(10,4))\n",
    "for i in range(K3):\n",
    "    plt.subplot(2,K3,i+1)\n",
    "    plt.bar([f\"Î¼{k}\" for k in range(4)], ms_mix[i]); plt.ylim(0,1); plt.title(f\"K3 S{i} Î¼-mix\")\n",
    "plt.subplot(2,1,2); plt.imshow(heat, aspect=\"auto\"); plt.colorbar(label=\"fraction\")\n",
    "plt.yticks(range(K3), [f\"S{i}\" for i in range(K3)]); plt.xticks(range(4), [f\"Î¼{k}\" for k in range(4)])\n",
    "plt.title(\"S001R03: fraction by K3 letter Ã— microstate\"); plt.tight_layout()\n",
    "plt.savefig(mech_png, dpi=160, bbox_inches=\"tight\"); plt.close()\n",
    "print(\"K=3 mechanism figure â†’\", mech_png)\n",
    "\n",
    "# ---------------- D) Findings PDF ----------------\n",
    "# headline metrics\n",
    "eoec = safe_read_csv(CFG.REP/\"generalization\"/\"eoec_iaf_summary.csv\")\n",
    "task = safe_read_csv(CFG.REP/\"generalization\"/\"task_fbcsp_summary.csv\")\n",
    "eoec_mean = float(eoec[\"acc\"].mean()) if eoec is not None and len(eoec) else float(\"nan\")\n",
    "task_mean = float(task[\"AUC_task\"].mean()) if task is not None and len(task) else float(\"nan\")\n",
    "# K=3 Î”xent & MI (from earlier notes or recompute quick)\n",
    "# Quick Î”xent on K3 using L3\n",
    "T3_full,_ = T1_from_seq(L3, K3)\n",
    "pi3 = T3_full.sum(axis=0); pi3 = pi3/(pi3.sum()+1e-12)\n",
    "dxent_k3 = - (seq_loglik_T1(L3, T3_full) - np.sum([np.log(pi3[b]+1e-12) for b in L3[1:]])) / (len(L3)-1)\n",
    "# Approx MI(letter;Î¼) from E\n",
    "def entropy(p): p=p[p>0]; return -np.sum(p*np.log2(p))\n",
    "if len(E):\n",
    "    Pm=np.bincount(E[\"mu\"], minlength=4)/len(E); Hm=entropy(Pm); HmL=0.0\n",
    "    for s in range(K3):\n",
    "        mask=(E[\"state\"]==s)\n",
    "        if not np.any(mask): continue\n",
    "        P=np.bincount(E[\"mu\"][mask], minlength=4)/mask.sum()\n",
    "        HmL += mask.mean()*entropy(P)\n",
    "    mi_k3 = float(Hm - HmL)\n",
    "else:\n",
    "    mi_k3 = float(\"nan\")\n",
    "\n",
    "# bring in CMI & generative adequacy if present\n",
    "cmi_path = CFG.ROOT/\"cog_alphabet_report_hybrid_v1\"/\"groundbreaking\"/\"cmi_microstate_conditioned.json\"\n",
    "gen_path = CFG.ROOT/\"cog_alphabet_report_hybrid_v1\"/\"groundbreaking\"/\"generative_adequacy.json\"\n",
    "cmi = json.load(open(cmi_path)) if cmi_path.exists() else {}\n",
    "gen = json.load(open(gen_path)) if gen_path.exists() else {}\n",
    "\n",
    "# optional topology image\n",
    "topo_png = CFG.ROOT/\"cog_alphabet_report_hybrid_v1\"/\"groundbreaking\"/\"grammar_topology_mds.png\"\n",
    "\n",
    "findings_pdf = CFG.REP/\"analysis\"/\"CALM_Findings_v1_1.pdf\"\n",
    "with PdfPages(findings_pdf) as pdf:\n",
    "    # page 1 â€” headline\n",
    "    plt.figure(figsize=(10,4)); plt.axis(\"off\")\n",
    "    txt = (f\"CALM v1.1 â€” Findings (K=3 default)\\n\\n\"\n",
    "           f\"Identity  EO/EC mean = {eoec_mean:.3f}\\n\"\n",
    "           f\"Function  R03 AUC    = {task_mean:.3f}\\n\"\n",
    "           f\"Grammar   Î”xent(K=3) = {dxent_k3:.3f} nats/step\\n\"\n",
    "           f\"Mechanism MI(letter;Î¼)@K=3 = {mi_k3:.3f} bits\\n\")\n",
    "    if cmi: txt += f\"Mechanismâ†’grammar CMI = {cmi.get('CMI_bits',float('nan')):.3f} bits  (p={cmi.get('perm_p','n/a')})\\n\"\n",
    "    if gen: txt += f\"Generative adequacy JS: bigrams={gen.get('JS_bigrams',float('nan')):.3f}, dwell={gen.get('JS_dwell',float('nan')):.3f}\\n\"\n",
    "    txt += \"\\nDefault K=3; S1-only K=4 zoom when local Î”xent(fine>coarse) > 0.\\n\"\n",
    "    plt.text(0.05, 0.95, txt, va=\"top\", family=\"monospace\")\n",
    "    pdf.savefig(bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    # page 2 â€” K3 generalization bars (if present)\n",
    "    gen_png = CFG.REP/\"generalization\"/\"CALM_Generalization_K3.png\"\n",
    "    if gen_png.exists():\n",
    "        img=plt.imread(gen_png); plt.figure(figsize=(10,3.5)); plt.imshow(img); plt.axis(\"off\")\n",
    "        plt.title(\"Generalization â€” EO/EC & R03 AUC  (K=3)\"); pdf.savefig(bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    # page 3 â€” K3 mechanism\n",
    "    img=plt.imread(mech_png); plt.figure(figsize=(10,4)); plt.imshow(img); plt.axis(\"off\")\n",
    "    plt.title(\"Mechanism â€” K3 letters (Î¼-bars + R03 letterÃ—Î¼)\"); pdf.savefig(bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    # page 4 â€” hierarchical grammar gain & mapping\n",
    "    plt.figure(figsize=(10,4)); plt.axis(\"off\")\n",
    "    map_txt=\"\\n\".join([f\"S{p} â† {', '.join('S'+str(c) for c in children[p])}\" for p in sorted(children.keys())])\n",
    "    plt.text(0.05,0.95, f\"Hierarchical zoom (K=3 â†’ K=4)\\n\\nÎ”xent = {hier['Î”xent']:.4f} nats/step\\n\\nParentâ†’children:\\n{map_txt}\",\n",
    "             va=\"top\", family=\"monospace\"); \n",
    "    pdf.savefig(bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "    # page 5 â€” grammar topology (if present)\n",
    "    if topo_png.exists():\n",
    "        img=plt.imread(topo_png); plt.figure(figsize=(10,4)); plt.imshow(img); plt.axis(\"off\")\n",
    "        plt.title(\"Cross-subject grammar topology (JS on T1)\"); pdf.savefig(bbox_inches=\"tight\"); plt.close()\n",
    "\n",
    "print(\"Findings PDF â†’\", findings_pdf)\n",
    "\n",
    "# copy into bundle/report\n",
    "dest = CFG.BUNDLE/\"report\"; dest.mkdir(parents=True, exist_ok=True)\n",
    "for p in [findings_pdf, CFG.REP/\"analysis\"/\"hierarchical_grammar_gain.json\", CFG.REP/\"analysis\"/\"coarse_fine_map.json\"]:\n",
    "    try:\n",
    "        shutil.copy2(p, dest/p.name)\n",
    "    except Exception: pass\n",
    "print(\"Copied to bundle â†’\", dest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "324ee62b-8989-440d-8be2-9c4ead4a8130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zoom test will evaluate fine vs coarse only when K3 state == S 1\n",
      "S1 zoom (fine vs coarse) â€” median Î”xent: 0.4121 CI: (0.2111, 0.5505) p= 0.00391\n",
      "Wrote minimal evaluator â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\analysis\\calm_eval_min.py\n",
      "\n",
      "Ready:\n",
      "  1) Per-subject S1 zoom results â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\analysis\\zoom_S1_subjects.csv\n",
      "     Summary JSON â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\analysis\\zoom_S1_summary.json\n",
      "  2) Minimal evaluator â†’ E:\\CNT\\artifacts\\cog_alphabet_report_hybrid_v1\\analysis\\calm_eval_min.py\n",
      "  3) Live demo helpers: live_decode_stream(edf, subj), live_panel(jsonl, seconds)\n"
     ]
    }
   ],
   "source": [
    "# === CALM v1.1 â€” Subject S1 Zoom Test + Minimal Evaluator + Live Demo (one cell) ===\n",
    "import os, json, math, time, numpy as np, pandas as pd, matplotlib.pyplot as plt, mne\n",
    "from pathlib import Path\n",
    "from scipy.signal import welch\n",
    "from numpy.random import default_rng\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from mne.decoding import CSP\n",
    "plt.switch_backend(\"Agg\")\n",
    "\n",
    "# ---------------- CFG ----------------\n",
    "class CFG:\n",
    "    ROOT      = Path(r\"E:\\CNT\\artifacts\")\n",
    "    RUN_K4    = ROOT / \"cog_alphabet_hybrid_v1\"          # your K=4 run (original)\n",
    "    RUN_K3    = ROOT / \"cog_alphabet_hybrid_k3\"          # your K=3 run (refit)\n",
    "    REP       = ROOT / \"cog_alphabet_report_hybrid_v1\"\n",
    "    GEN       = ROOT / \"generalization_data\"\n",
    "    TARGET_SF = 250.0\n",
    "    EPOCH_LEN = 2.0\n",
    "    STEP      = 0.5\n",
    "    FB_BANDS  = [(8,13),(13,20),(20,30)]\n",
    "    FB_THR    = 0.50\n",
    "CFG = CFG()\n",
    "(CFG.REP/\"analysis\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------------- shared utils ----------------\n",
    "def safe_read_csv(p: Path):\n",
    "    try:\n",
    "        if p.exists() and p.stat().st_size > 0:\n",
    "            return pd.read_csv(p)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def T1_from_seq(seq, K):\n",
    "    T=np.zeros((K,K))\n",
    "    for a,b in zip(seq[:-1], seq[1:]): T[a,b]+=1\n",
    "    eps=1e-6; T=(T+eps)/(T.sum(axis=1, keepdims=True)+eps*K)\n",
    "    return T\n",
    "\n",
    "def seq_loglik_T1(seq, T):\n",
    "    return float(np.sum([np.log(T[a,b]+1e-12) for a,b in zip(seq[:-1], seq[1:])]))\n",
    "\n",
    "# ---------------- A) S1 Zoom Test per subject ----------------\n",
    "# Load coarseâ†’fine map (K4â†’K3). If missing, build via majority overlap.\n",
    "cf_map_path = CFG.REP/\"analysis\"/\"coarse_fine_map.json\"\n",
    "if cf_map_path.exists():\n",
    "    mapping = json.load(open(cf_map_path))\n",
    "else:\n",
    "    # build map from S001 labels\n",
    "    L4 = pd.read_csv(CFG.RUN_K4/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "    L3 = pd.read_csv(CFG.RUN_K3/\"state_assignments.csv\")[\"state\"].to_numpy().astype(int)\n",
    "    K4=int(L4.max()+1); K3=int(L3.max()+1)\n",
    "    M=np.zeros((K4,K3), int)\n",
    "    for c4,c3 in zip(L4,L3): M[c4,c3]+=1\n",
    "    parent = {int(c4): int(np.argmax(M[c4])) for c4 in range(K4)}\n",
    "    children = {int(p): sorted([int(c4) for c4 in range(K4) if parent[c4]==p]) for p in range(K3)}\n",
    "    mapping = {\"K4_to_K3_parent\": parent, \"K3_children\": children, \"overlap\": M.tolist()}\n",
    "    cf_map_path.write_text(json.dumps(mapping, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "parent  = {int(k): int(v) for k,v in mapping[\"K4_to_K3_parent\"].items()}\n",
    "children= {int(k): [int(x) for x in v] for k,v in mapping[\"K3_children\"].items()}\n",
    "# pick the coarse letter that truly splits (has â‰¥2 children) â€” usually K3 S1\n",
    "split_parents = [p for p,ch in children.items() if len(ch)>=2]\n",
    "SPLIT_PARENT = split_parents[0] if split_parents else 1  # default S1\n",
    "print(\"Zoom test will evaluate fine vs coarse only when K3 state == S\", SPLIT_PARENT)\n",
    "\n",
    "# Pull subject sequences from decode_*_*.csv (K=4).\n",
    "def load_subject_K4_seq(subj: str):\n",
    "    seqs=[]\n",
    "    for p in (CFG.REP/\"generalization\").glob(f\"decode_{subj}_*.csv\"):\n",
    "        df=safe_read_csv(p)\n",
    "        if df is None or \"state\" not in df.columns: continue\n",
    "        seqs.append(df[\"state\"].astype(int).to_numpy())\n",
    "    return np.concatenate(seqs) if seqs else None\n",
    "\n",
    "# hierarchical Î”xent (fine vs coarse) restricted to timesteps where parent state==SPLIT_PARENT\n",
    "def subject_zoom_gain(subj: str):\n",
    "    L4 = load_subject_K4_seq(subj)\n",
    "    if L4 is None or len(L4)<3: return None\n",
    "    K4=int(L4.max()+1)\n",
    "    L3_parent = np.array([parent.get(int(c), 0) for c in L4], int)\n",
    "    # restrict to windows where parent at t is SPLIT_PARENT\n",
    "    idx = np.where(L3_parent[:-1]==SPLIT_PARENT)[0]  # positions contributing to transitions\n",
    "    if len(idx) < 5: \n",
    "        return {\"subject\":subj, \"N_eval\": int(len(idx)), \"Î”xent\": np.nan}\n",
    "    # Build restricted sequences\n",
    "    L4_r = L4.copy()\n",
    "    L3_r = L3_parent.copy()\n",
    "    # Build MLE T1s on the restricted transitions (from subject data)\n",
    "    # Coarse model in parent space (K3): only rows that appear contribute\n",
    "    K3 = int(max(L3_r.max(), SPLIT_PARENT)+1)\n",
    "    def T_from_idxs(seq, idx, K):\n",
    "        T=np.zeros((K,K))\n",
    "        for i in idx:\n",
    "            a,b = seq[i], seq[i+1]\n",
    "            if a<K and b<K: T[a,b]+=1\n",
    "        eps=1e-6; T=(T+eps)/(T.sum(axis=1, keepdims=True)+eps*K)\n",
    "        return T\n",
    "    T3 = T_from_idxs(L3_r, idx, K3)\n",
    "    T4 = T_from_idxs(L4_r, idx, K4)\n",
    "    # log-likelihood on the same restricted positions\n",
    "    def ll_from_idxs(seq, idx, T):\n",
    "        return float(np.sum([np.log(T[seq[i], seq[i+1]]+1e-12) for i in idx]))\n",
    "    LL_coarse = ll_from_idxs(L3_r, idx, T3)\n",
    "    LL_fine   = ll_from_idxs(L4_r, idx, T4)\n",
    "    N_eval    = len(idx)\n",
    "    Î”x        = - (LL_fine - LL_coarse) / N_eval\n",
    "    return {\"subject\":subj, \"N_eval\": int(N_eval), \"Î”xent\": float(Î”x)}\n",
    "\n",
    "subjects=[f\"S{n:03d}\" for n in range(2,11)]\n",
    "rows=[]\n",
    "for s in subjects:\n",
    "    try:\n",
    "        res=subject_zoom_gain(s)\n",
    "        if res: rows.append(res)\n",
    "    except Exception as e:\n",
    "        rows.append({\"subject\":s, \"N_eval\":0, \"Î”xent\": np.nan})\n",
    "zoom_df = pd.DataFrame(rows).sort_values(\"subject\")\n",
    "zoom_df.to_csv(CFG.REP/\"analysis\"/\"zoom_S1_subjects.csv\", index=False)\n",
    "\n",
    "# Bootstrap CI for median Î”xent; Wilcoxon signed-rank vs 0\n",
    "vals = zoom_df[\"Î”xent\"].dropna().values\n",
    "rng  = default_rng(42)\n",
    "B=50000\n",
    "def boot_median_ci(v):\n",
    "    if len(v)==0: return (np.nan,np.nan,np.nan)\n",
    "    boots=[np.median(rng.choice(v, size=len(v), replace=True)) for _ in range(B)]\n",
    "    lo,hi=np.quantile(boots,[0.025,0.975])\n",
    "    return (float(np.median(v)), float(lo), float(hi))\n",
    "median, lo, hi = boot_median_ci(vals)\n",
    "\n",
    "from scipy.stats import wilcoxon, norm\n",
    "if len(vals)>=2:\n",
    "    stat,p = wilcoxon(vals, np.zeros_like(vals))\n",
    "    z = norm.isf(p/2) * np.sign(np.median(vals))\n",
    "    r = float(z/np.sqrt(len(vals)))\n",
    "else:\n",
    "    stat=p=r=np.nan\n",
    "summary = {\"split_parent\": int(SPLIT_PARENT), \"subjects\": len(zoom_df),\n",
    "           \"median_Î”xent\": median, \"CI95\": [lo,hi], \"wilcoxon_p\": float(p), \"effect_r\": r}\n",
    "(CFG.REP/\"analysis\"/\"zoom_S1_summary.json\").write_text(json.dumps(summary, indent=2), encoding=\"utf-8\")\n",
    "print(\"S1 zoom (fine vs coarse) â€” median Î”xent:\", round(median,4), \"CI:\", (round(lo,4), round(hi,4)), \"p=\", f\"{p:.3g}\")\n",
    "\n",
    "# ---------------- B) Minimal evaluator module ----------------\n",
    "eval_py = CFG.REP/\"analysis\"/\"calm_eval_min.py\"\n",
    "eval_src = f'''# CALM Eval (minimal) â€” drop-in module\n",
    "import os, json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def _safe_csv(p):\n",
    "    try:\n",
    "        if p.exists() and p.stat().st_size>0:\n",
    "            return pd.read_csv(p)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def evaluate(bundle_root):\n",
    "    rep = Path(r\"{CFG.REP}\".replace(\"\\\\\\\\\",\"/\"))\n",
    "    out = {{}}\n",
    "    # Identity / Function\n",
    "    eoec = _safe_csv(rep/\"generalization\"/\"eoec_iaf_summary.csv\")\n",
    "    task = _safe_csv(rep/\"generalization\"/\"task_fbcsp_summary.csv\")\n",
    "    if eoec is not None and len(eoec): out[\"EOEC_mean\"] = float(eoec[\"acc\"].mean())\n",
    "    if task is not None and len(task): out[\"TaskAUC_mean\"] = float(task[\"AUC_task\"].mean())\n",
    "    # Grammar (predictive)\n",
    "    k3 = Path(r\"{CFG.RUN_K3}\".replace(\"\\\\\\\\\",\"/\"))/\"state_assignments.csv\"\n",
    "    if k3.exists():\n",
    "        L = pd.read_csv(k3)[\"state\"].astype(int).to_numpy()\n",
    "        K = int(L.max()+1)\n",
    "        T = np.zeros((K,K)); \n",
    "        for a,b in zip(L[:-1], L[1:]): T[a,b]+=1\n",
    "        eps=1e-6; T=(T+eps)/(T.sum(axis=1,keepdims=True)+eps*K)\n",
    "        pi=T.sum(axis=0); pi=pi/(pi.sum()+1e-12)\n",
    "        H_T1 = -np.mean([np.log(T[a,b]+1e-12) for a,b in zip(L[:-1],L[1:])])\n",
    "        H_pi = -np.mean([np.log(pi[b]+1e-12) for b in L[1:]])\n",
    "        out[\"DXENT_K3\"] = float(H_pi - H_T1)\n",
    "    # Mechanism (MI & CMI) and generator\n",
    "    gb = rep/\"groundbreaking\"\n",
    "    for f in [\"microstate_epoch_stats.json\",\"microstate_epoch_stats_subjects.json\",\"cmi_microstate_conditioned.json\",\"generative_adequacy.json\"]:\n",
    "        p = gb/f\n",
    "        if p.exists():\n",
    "            try: out[f.replace(\".json\",\"\")] = json.loads(p.read_text(encoding=\"utf-8\"))\n",
    "            except Exception: pass\n",
    "    # S1 zoom summary\n",
    "    zsum = rep/\"analysis\"/\"zoom_S1_summary.json\"\n",
    "    if zsum.exists():\n",
    "        out[\"zoom_S1\"] = json.loads(zsum.read_text(encoding=\"utf-8\"))\n",
    "    # Save and return\n",
    "    outp = rep/\"analysis\"/\"calm_eval_min.json\"; outp.write_text(json.dumps(out, indent=2), encoding=\"utf-8\")\n",
    "    return out\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    root = Path(sys.argv[1]) if len(sys.argv)>1 else Path(\".\")\n",
    "    res = evaluate(root)\n",
    "    print(json.dumps(res, indent=2))\n",
    "'''\n",
    "eval_py.write_text(eval_src, encoding=\"utf-8\")\n",
    "print(\"Wrote minimal evaluator â†’\", eval_py)\n",
    "\n",
    "# ---------------- C) Live demo helpers (stream + panel) ----------------\n",
    "from joblib import load\n",
    "scaler_k3 = load(CFG.RUN_K3/\"scaler_k3.joblib\")\n",
    "pca_k3    = load(CFG.RUN_K3/\"pca_k3.joblib\")\n",
    "km_k3     = load(CFG.RUN_K3/\"kmeans_k3.joblib\")\n",
    "TRAIN_COLS= list(pd.read_csv(CFG.RUN_K3/\"features.csv\", nrows=1).columns)\n",
    "\n",
    "def load_raw_edf(p, lfh=(0.5,80.0)):\n",
    "    raw=mne.io.read_raw_edf(str(p), preload=True, verbose=False)\n",
    "    if abs(raw.info[\"sfreq\"]-CFG.TARGET_SF)>1e-6: raw.resample(CFG.TARGET_SF, npad=\"auto\", verbose=False)\n",
    "    ny=raw.info[\"sfreq\"]/2; raw.filter(lfh[0], min(lfh[1], ny-1.0), verbose=False)\n",
    "    if \"eeg\" in set(raw.get_channel_types()):\n",
    "        raw.set_eeg_reference(\"average\", projection=True, verbose=False); raw.apply_proj()\n",
    "    return raw\n",
    "\n",
    "def make_epochs(raw):\n",
    "    ov=max(0.0, CFG.EPOCH_LEN-CFG.STEP)\n",
    "    return mne.make_fixed_length_epochs(raw, duration=CFG.EPOCH_LEN, overlap=ov, preload=True, verbose=False)\n",
    "\n",
    "def spectral_features(epochs):\n",
    "    X=epochs.get_data(); sf=epochs.info[\"sfreq\"]\n",
    "    n_ep,n_ch,n_t=X.shape; nper=min(int(sf*2), n_t); nov=nper//2\n",
    "    f,P=welch(X, fs=sf, nperseg=nper, noverlap=nov, axis=-1, average=\"median\")\n",
    "    def idx(lo,hi): return np.where((f>=lo)&(f<hi))[0]\n",
    "    aidx=idx(0.5,80.0); tot=np.maximum(P[:,:,aidx].sum(-1),1e-12)\n",
    "    feats={}; \n",
    "    # bandpowers\n",
    "    for name,(lo,hi) in {\"delta\":(1,4),\"theta\":(4,8),\"alpha\":(8,13),\"beta_low\":(13,20),\"beta_high\":(20,35),\"gamma\":(35,80)}.items():\n",
    "        b=idx(lo,hi); bp=P[:,:,b].sum(-1)/tot if b.size else np.zeros((n_ep,n_ch))\n",
    "        feats[f\"{name}_rel_med\"]=np.median(bp,axis=1)\n",
    "    # ratios\n",
    "    alpha=feats[\"alpha_rel_med\"]; theta=feats[\"theta_rel_med\"]; beta=feats[\"beta_low_rel_med\"]+feats[\"beta_high_rel_med\"]\n",
    "    feats[\"theta_over_alpha\"]=theta/np.maximum(alpha,1e-6); feats[\"beta_over_alpha\"]=beta/np.maximum(alpha,1e-6)\n",
    "    feats[\"(alpha+theta)_over_beta\"]=(alpha+theta)/np.maximum(beta,1e-6)\n",
    "    # entropy/centroid\n",
    "    p_band=P[:,:,aidx]; p_n=p_band/np.maximum(p_band.sum(-1,keepdims=True),1e-12)\n",
    "    H=-(p_n*np.log2(p_n+1e-12)).sum(-1); Hn=H/np.log2(p_band.shape[-1])\n",
    "    feats[\"spec_entropy_med\"]=np.median(Hn,axis=1); feats[\"spec_centroid_med\"]=np.median((p_band*f[aidx]).sum(-1)/np.maximum(p_band.sum(-1),1e-12),axis=1)\n",
    "    # Hjorth (fast)\n",
    "    d1=np.diff(X,axis=-1); var0=np.var(X,axis=-1)+1e-12; var1=np.var(d1,axis=-1)+1e-12\n",
    "    feats[\"hjorth_activity_med\"]=np.median(var0,axis=1); feats[\"hjorth_mobility_med\"]=np.median(np.sqrt(var1/var0),axis=1)\n",
    "    return pd.DataFrame(feats)\n",
    "\n",
    "def occipital_picks(raw):\n",
    "    names=[n.upper().strip() for n in raw.ch_names]\n",
    "    wanted=(\"O1\",\"O2\",\"OZ\",\"POZ\",\"PO3\",\"PO4\")\n",
    "    idx=[i for i,n in enumerate(names) if n in wanted]\n",
    "    return idx if idx else mne.pick_types(raw.info, eeg=True)\n",
    "\n",
    "def live_decode_stream(edf_path, subject_id, seconds=60, out_dir=CFG.REP/\"generalization\", jsonl_every=5, min_run_epochs=3):\n",
    "    out_dir=Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    csv_path   = out_dir/f\"live_{subject_id}_{Path(edf_path).stem}.csv\"\n",
    "    jsonl_path = out_dir/f\"live_{subject_id}_{Path(edf_path).stem}.jsonl\"\n",
    "    # pull subject calibration (IAF band; alpha threshold; task letters w/ fb gate) if present\n",
    "    cal_path = CFG.REP/\"generalization\"/\"subject_calibration.json\"\n",
    "    cal = json.loads(cal_path.read_text(encoding=\"utf-8\")) if cal_path.exists() else {}\n",
    "    subj_c = cal.get(subject_id, {})\n",
    "    a_band = subj_c.get(\"alpha_band\", [8.0,13.0]); a_thr = subj_c.get(\"alpha_threshold\", None)\n",
    "    t_letters = set(map(int, subj_c.get(\"task_letters\", []))); fb_thr = float(subj_c.get(\"fbCSP_threshold\", 0.50))\n",
    "    # stream\n",
    "    raw = load_raw_edf(Path(edf_path))\n",
    "    sf  = raw.info[\"sfreq\"]; total_samples = min(raw.n_times, int(seconds*sf))\n",
    "    def sticky(prev, raw_state, count, min_epochs):\n",
    "        if prev is None: return raw_state, 1\n",
    "        if raw_state==prev: return prev, count+1\n",
    "        return (raw_state,1) if count+1>=min_epochs else (prev, count)\n",
    "    smooth_state=None; count=0; last_state=None\n",
    "    rows=[]\n",
    "    t=0.0; chunk=0\n",
    "    while int(t+CFG.EPOCH_LEN)*sf <= total_samples:\n",
    "        seg=raw.copy().crop(tmin=t, tmax=t+CFG.EPOCH_LEN, include_tmax=False)\n",
    "        ep=make_epochs(seg); Fsp=spectral_features(ep).reindex(columns=TRAIN_COLS, fill_value=0.0)\n",
    "        Z=pca_k3.transform(scaler_k3.transform(Fsp.values))\n",
    "        raw_state=int(km_k3.predict(Z)[0])\n",
    "        # Î±-index (EO/EC)\n",
    "        eoec=\"UNK\"; ai=None\n",
    "        if a_thr is not None:\n",
    "            f,P=welch(seg.get_data(), fs=sf, nperseg=min(int(sf*2), seg.n_times), noverlap=int(min(int(sf*2), seg.n_times)/2), axis=-1, average=\"median\")\n",
    "            idx=(f>=a_band[0])&(f<a_band[1]); a=P[:,idx].sum(-1); tot=P.sum(-1)+1e-12\n",
    "            oi=occipital_picks(seg); ai=(a[oi].sum()/tot[oi].sum()) if len(oi) else (a.sum()/tot.sum()); eoec=\"EC\" if ai>=a_thr else \"EO\"\n",
    "        # fbCSP task (best-effort single-shot, no CV)\n",
    "        try:\n",
    "            want={\"C3\",\"Cz\",\"C4\",\"FC3\",\"FCz\",\"FC4\",\"CP3\",\"CPz\",\"CP4\"}\n",
    "            picks=[i for i,ch in enumerate(seg.ch_names) if ch.upper().strip() in want]\n",
    "            if len(picks)<3: picks=mne.pick_types(seg.info, eeg=True)\n",
    "            _raw=seg.copy().pick(picks); ssf=_raw.info[\"sfreq\"]; X=_raw.get_data()[None,:,:]\n",
    "            feats=[]\n",
    "            for lo,hi in CFG.FB_BANDS:\n",
    "                X2=X.reshape(X.shape[0]*X.shape[1],X.shape[2])\n",
    "                Xf=mne.filter.filter_data(X2, sfreq=ssf, l_freq=lo, h_freq=hi, method=\"iir\",\n",
    "                                          iir_params=dict(order=4, ftype=\"butter\"), verbose=False).reshape(X.shape)\n",
    "                csp=CSP(n_components=6, reg=\"ledoit_wolf\", log=True, norm_trace=False)\n",
    "                csp.fit(Xf,[0]); feats.append(csp.transform(Xf))\n",
    "            Xfb=np.concatenate(feats,1); lda=LDA(solver=\"lsqr\", shrinkage=\"auto\").fit(Xfb,[0]); fb=float(lda.predict_proba(Xfb)[:,1])\n",
    "        except Exception:\n",
    "            fb=0.0\n",
    "        # Zoom badge: if smoothed state becomes parent's split (S1), you could switch to K=4 (here we just flag)\n",
    "        smooth_state, count = sticky(smooth_state, raw_state, count, min_run_epochs)\n",
    "        zoom_on = int(smooth_state==1)  # parent S1 badge\n",
    "        task_pred = \"task\" if (raw_state in t_letters or fb>=fb_thr) else \"rest\"\n",
    "        rows.append([round(t,3), round(t+CFG.EPOCH_LEN,3), raw_state, smooth_state, zoom_on, eoec, ai, fb, task_pred])\n",
    "        if (chunk+1)%jsonl_every==0:\n",
    "            with open(jsonl_path,\"a\",encoding=\"utf-8\") as f:\n",
    "                f.write(json.dumps({\"ts\":round(t+CFG.EPOCH_LEN,3),\"state_raw\":raw_state,\"state_smooth\":smooth_state,\n",
    "                                    \"zoom_badge\":zoom_on,\"alpha_index\":ai,\"fb_task\":fb})+\"\\n\")\n",
    "        t += CFG.EPOCH_LEN; chunk+=1\n",
    "    pd.DataFrame(rows, columns=[\"t_start_s\",\"t_end_s\",\"state_raw\",\"state_smooth\",\"zoom_badge\",\"eoec_pred\",\"alpha_index\",\"fb_task\",\"task_pred\"]).to_csv(csv_path, index=False)\n",
    "    print(\"Live stream â†’\", csv_path, \"|\", jsonl_path)\n",
    "    return csv_path, jsonl_path\n",
    "\n",
    "def live_panel(jsonl_path: Path, seconds=60):\n",
    "    \"\"\"Tiny live panel (matplotlib).\"\"\"\n",
    "    jsonl_path=Path(jsonl_path)\n",
    "    t0=time.time(); ts=[]; fb=[]; ai=[]; zoom=[]; st=[]\n",
    "    plt.figure(figsize=(10,4))\n",
    "    while time.time()-t0 < seconds:\n",
    "        if jsonl_path.exists():\n",
    "            lines=jsonl_path.read_text(encoding=\"utf-8\").splitlines()\n",
    "            ts=[]; fb=[]; ai=[]; zoom=[]; st=[]\n",
    "            for ln in lines:\n",
    "                try:\n",
    "                    o=json.loads(ln); ts.append(o.get(\"ts\", len(ts))); fb.append(o.get(\"fb_task\",0.0)); ai.append(o.get(\"alpha_index\",None)); zoom.append(o.get(\"zoom_badge\",0)); st.append(o.get(\"state_smooth\",None))\n",
    "                except Exception: pass\n",
    "        plt.clf()\n",
    "        if ts:\n",
    "            plt.subplot(2,1,1)\n",
    "            plt.plot(ts, fb, label=\"fb_task\"); \n",
    "            ai_plot=[v if v is not None else np.nan for v in ai]; plt.plot(ts, ai_plot, label=\"alpha_index\")\n",
    "            plt.legend(); plt.ylabel(\"value\"); \n",
    "            plt.subplot(2,1,2)\n",
    "            plt.step(ts, zoom, where=\"post\", label=\"zoom(S1â†’K4)\"); \n",
    "            plt.step(ts, [s if s is not None else np.nan for s in st], where=\"post\", label=\"state_smooth\")\n",
    "            plt.xlabel(\"time (s)\"); plt.legend()\n",
    "        plt.pause(1.0)\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\nReady:\")\n",
    "print(\"  1) Per-subject S1 zoom results â†’\", CFG.REP/\"analysis\"/\"zoom_S1_subjects.csv\")\n",
    "print(\"     Summary JSON â†’\", CFG.REP/\"analysis\"/\"zoom_S1_summary.json\")\n",
    "print(\"  2) Minimal evaluator â†’\", CFG.REP/\"analysis\"/\"calm_eval_min.py\")\n",
    "print(\"  3) Live demo helpers: live_decode_stream(edf, subj), live_panel(jsonl, seconds)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b81016-8f99-4931-8e80-8ed1e3b5d7f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
