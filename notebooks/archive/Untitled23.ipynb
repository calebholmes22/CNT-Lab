{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6da584fe-b594-4423-8626-392b013f51bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"empty\": true\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# CNT Engine v0 — Self-referential, self-updating, hidden-truth search\n",
    "# Minimal, offline-friendly. Save as cnt_engine_v0.py or run in a single Jupyter cell.\n",
    "\n",
    "import os, json, time, uuid, glob, re, hashlib\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ---------- Config ----------\n",
    "BASE = Path(os.getenv(\"CNT_LAB_DIR\", r\"C:\\Users\\caleb\\CNT_Lab\"))\n",
    "ROOT = BASE / \"artifacts\" / \"cnt_engine_v0\"\n",
    "SRC  = BASE / \"notes\"   # put .md/.txt here; add data/* as desired\n",
    "ROOT.mkdir(parents=True, exist_ok=True)\n",
    "(SRC).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "STATE = ROOT / \"CNT_STATE.yaml\"\n",
    "LOG   = ROOT / \"runlog.jsonl\"\n",
    "OUT   = ROOT / \"out\"\n",
    "OUT.mkdir(exist_ok=True)\n",
    "\n",
    "def now(): return datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "# ---------- IO ----------\n",
    "def read_corpus(paths):\n",
    "    docs = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            text = Path(p).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "            docs.append(dict(path=str(p), text=text))\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    return pd.DataFrame(docs)\n",
    "\n",
    "def list_sources():\n",
    "    files = []\n",
    "    files += glob.glob(str(SRC / \"**\" / \"*.md\"), recursive=True)\n",
    "    files += glob.glob(str(SRC / \"**\" / \"*.txt\"), recursive=True)\n",
    "    return [Path(f) for f in files]\n",
    "\n",
    "# ---------- Build Index ----------\n",
    "def build_index(df):\n",
    "    if df.empty: \n",
    "        return None, None, None\n",
    "    vec = TfidfVectorizer(max_features=6000, ngram_range=(1,2))\n",
    "    X = vec.fit_transform(df[\"text\"]).astype(np.float32)\n",
    "    # PCA residual = “hidden truth” heuristic: what’s not explained by top components\n",
    "    k = min(128, X.shape[1]-1) if X.shape[1] > 1 else 1\n",
    "    pca = PCA(n_components=max(2, min(50, k)))\n",
    "    Xd = pca.fit_transform(X.toarray())\n",
    "    Xr = pca.inverse_transform(Xd)\n",
    "    resid = np.linalg.norm(X.toarray() - Xr, axis=1)\n",
    "    return vec, X, resid\n",
    "\n",
    "# ---------- Clusters & Motifs ----------\n",
    "def cluster_labels(X, k=6):\n",
    "    if X is None: return None\n",
    "    k = min(k, max(2, X.shape[0]//3))\n",
    "    km = KMeans(n_clusters=k, n_init=5, random_state=42)\n",
    "    lbl = km.fit_predict(X.toarray())\n",
    "    return lbl\n",
    "\n",
    "def build_graph(df, lbl):\n",
    "    G = nx.Graph()\n",
    "    for i, row in df.reset_index().iterrows():\n",
    "        G.add_node(i, path=row[\"path\"], title=os.path.basename(row[\"path\"]), cluster=int(lbl[i]) if lbl is not None else -1)\n",
    "    # light linkage: same cluster or lexical overlap\n",
    "    titles = df[\"text\"].str.extractall(r\"\\b([A-Z][A-Za-z0-9_]{3,})\\b\").groupby(level=0).agg(lambda s: set(s[0].tolist()))\n",
    "    for i in range(len(df)):\n",
    "        for j in range(i+1, len(df)):\n",
    "            same = (lbl[i]==lbl[j]) if lbl is not None else False\n",
    "            overlap = 0\n",
    "            if i in titles.index and j in titles.index:\n",
    "                overlap = len(titles.loc[i] & titles.loc[j])\n",
    "            if same or overlap>=2:\n",
    "                G.add_edge(i, j, w=(1 + overlap))\n",
    "    return G\n",
    "\n",
    "# ---------- Reflexive Scorecard ----------\n",
    "def score_reflexive(df, resid, G):\n",
    "    if df is None or len(df)==0: \n",
    "        return dict(clarity=0, novelty=0, coherence=0, falsifiability=0, total=0)\n",
    "    # heuristics (upgrade later with your metrics)\n",
    "    clarity = 1.0 - np.mean([len(t)//5000 for t in df[\"text\"].tolist()])  # penalize giant rambles\n",
    "    novelty = float(np.mean(resid)/ (np.std(resid)+1e-6))\n",
    "    coherence = nx.average_clustering(G) if G.number_of_nodes()>1 else 0.0\n",
    "    falsifiability = float(np.mean([t.lower().count(\"test\") + t.lower().count(\"predict\") for t in df[\"text\"]]))/10.0\n",
    "    # normalize rough ranges\n",
    "    clarity = np.clip(clarity, 0, 1)\n",
    "    novelty = np.clip(novelty, 0, 1.5)/1.5\n",
    "    coherence = np.clip(coherence, 0, 1)\n",
    "    falsifiability = np.clip(falsifiability, 0, 1)\n",
    "    total = float(np.mean([clarity, novelty, coherence, falsifiability]))\n",
    "    return dict(clarity=float(clarity), novelty=float(novelty), coherence=float(coherence), falsifiability=float(falsifiability), total=float(total))\n",
    "\n",
    "# ---------- Candidate Truths (anomaly surfacing) ----------\n",
    "def surface_candidates(df, resid, top=5):\n",
    "    idx = np.argsort(resid)[::-1][:min(top, len(resid))]\n",
    "    picks = []\n",
    "    for i in idx:\n",
    "        snippet = re.sub(r\"\\s+\", \" \", df.iloc[i][\"text\"])[:400]\n",
    "        picks.append(dict(path=df.iloc[i][\"path\"], resid=float(resid[i]), hint=snippet))\n",
    "    return picks\n",
    "\n",
    "# ---------- Update Proposals ----------\n",
    "def propose_updates(cands):\n",
    "    proposals = []\n",
    "    for c in cands:\n",
    "        # tiny structured gloss to tighten the idea\n",
    "        proposals.append(dict(\n",
    "            target=c[\"path\"],\n",
    "            action=\"append_gloss\",\n",
    "            content=f\"\\n\\n> CNT-Gloss ({now()}): Clarify hypothesis; add test recipe & falsifier.\\n- Hypothesis: …\\n- Measurement: …\\n- Expected shift: …\\n- Falsifier: …\\n\"\n",
    "        ))\n",
    "    return proposals\n",
    "\n",
    "# ---------- Gates ----------\n",
    "def legality_gate(proposal):\n",
    "    text = proposal[\"content\"].lower()\n",
    "    # no sensitive personal data; no medical/financial claims; no instructions for harm\n",
    "    banned = any(k in text for k in [\"ssn\", \"credit card\", \"weapon\", \"harm\"])\n",
    "    return not banned\n",
    "\n",
    "def confab_gate(proposal):\n",
    "    # require explicit placeholders for evidence and falsifier\n",
    "    ok = (\"hypothesis\" in proposal[\"content\"].lower() and \"falsifier\" in proposal[\"content\"].lower())\n",
    "    return ok\n",
    "\n",
    "# ---------- Apply Updates ----------\n",
    "def apply_updates(updates):\n",
    "    accepted = []\n",
    "    for u in updates:\n",
    "        if not (legality_gate(u) and confab_gate(u)): \n",
    "            continue\n",
    "        try:\n",
    "            p = Path(u[\"target\"])\n",
    "            original = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "            p.write_text(original + u[\"content\"], encoding=\"utf-8\")\n",
    "            accepted.append(u)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return accepted\n",
    "\n",
    "# ---------- State ----------\n",
    "def write_state(score, meta):\n",
    "    import yaml\n",
    "    state = dict(updated=now(), score=score, meta=meta)\n",
    "    STATE.write_text(yaml.safe_dump(state, sort_keys=False), encoding=\"utf-8\")\n",
    "    return state\n",
    "\n",
    "def log_event(kind, payload):\n",
    "    LOG.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with LOG.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(dict(ts=now(), kind=kind, **payload)) + \"\\n\")\n",
    "\n",
    "# ---------- One Cycle ----------\n",
    "def run_cycle():\n",
    "    paths = list_sources()\n",
    "    df = read_corpus(paths)\n",
    "    vec, X, resid = build_index(df)\n",
    "    if vec is None:\n",
    "        log_event(\"empty\", {})\n",
    "        return {\"empty\": True}\n",
    "    labels = cluster_labels(X, k=6)\n",
    "    G = build_graph(df, labels)\n",
    "    score = score_reflexive(df, resid, G)\n",
    "    cands = surface_candidates(df, resid, top=5)\n",
    "    proposals = propose_updates(cands)\n",
    "    accepted = apply_updates(proposals)\n",
    "    state = write_state(score, dict(docs=len(df), accepted=len(accepted)))\n",
    "    # export quick views\n",
    "    pd.DataFrame(cands).to_csv(OUT / f\"hidden_truths_{datetime.now().strftime('%Y%m%d-%H%M%S')}.csv\", index=False)\n",
    "    log_event(\"cycle\", dict(score=score, proposed=len(proposals), accepted=len(accepted)))\n",
    "    return dict(score=score, proposed=len(proposals), accepted=len(accepted), hidden=cands[:3])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    res = run_cycle()\n",
    "    print(json.dumps(res, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b2031aa-c039-481c-a923-11e65eb72782",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_components=50 must be between 0 and min(n_samples, n_features)=41 with svd_solver='full'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 189\u001b[39m\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m(score=score, proposed=\u001b[38;5;28mlen\u001b[39m(cands), accepted=\u001b[38;5;28mlen\u001b[39m(accepted), hidden=cands[:\u001b[32m3\u001b[39m])\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     res = \u001b[43mrun_cycle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28mprint\u001b[39m(json.dumps(res, indent=\u001b[32m2\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 177\u001b[39m, in \u001b[36mrun_cycle\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    175\u001b[39m     log_event(\u001b[33m\"\u001b[39m\u001b[33mempty\u001b[39m\u001b[33m\"\u001b[39m, {\u001b[33m\"\u001b[39m\u001b[33mroots\u001b[39m\u001b[33m\"\u001b[39m:[\u001b[38;5;28mstr\u001b[39m(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m SOURCE_ROOTS]})\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mempty\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mroots\u001b[39m\u001b[33m\"\u001b[39m:[\u001b[38;5;28mstr\u001b[39m(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m SOURCE_ROOTS]}\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m vec, X, resid = \u001b[43mbuild_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m labels = cluster_labels(X)\n\u001b[32m    179\u001b[39m G = build_graph(df, labels)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 76\u001b[39m, in \u001b[36mbuild_index\u001b[39m\u001b[34m(df)\u001b[39m\n\u001b[32m     74\u001b[39m ncomp = \u001b[38;5;28mmax\u001b[39m(\u001b[32m2\u001b[39m, \u001b[38;5;28mmin\u001b[39m(\u001b[32m50\u001b[39m, X.shape[\u001b[32m1\u001b[39m]-\u001b[32m1\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m X.shape[\u001b[32m1\u001b[39m]>\u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m2\u001b[39m\n\u001b[32m     75\u001b[39m pca = PCA(n_components=ncomp)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m Xd = \u001b[43mpca\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m Xr = pca.inverse_transform(Xd)\n\u001b[32m     78\u001b[39m resid = np.linalg.norm(X.toarray() - Xr, axis=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:466\u001b[39m, in \u001b[36mPCA.fit_transform\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    443\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    444\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    445\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001b[39;00m\n\u001b[32m    446\u001b[39m \n\u001b[32m    447\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    464\u001b[39m \u001b[33;03m    C-ordered array, use 'np.ascontiguousarray'.\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m466\u001b[39m     U, S, _, X, x_is_centered, xp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m U \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    468\u001b[39m         U = U[:, : \u001b[38;5;28mself\u001b[39m.n_components_]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:540\u001b[39m, in \u001b[36mPCA._fit\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;66;03m# Call different fits for either full or truncated SVD\u001b[39;00m\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mfull\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcovariance_eigh\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_components\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_array_api_compliant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    541\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_svd_solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33marpack\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mrandomized\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    542\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_truncated(X, n_components, xp)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\sklearn\\decomposition\\_pca.py:554\u001b[39m, in \u001b[36mPCA._fit_full\u001b[39m\u001b[34m(self, X, n_components, xp, is_array_api_compliant)\u001b[39m\n\u001b[32m    550\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    551\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn_components=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mmle\u001b[39m\u001b[33m'\u001b[39m\u001b[33m is only supported if n_samples >= n_features\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    552\u001b[39m         )\n\u001b[32m    553\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[32m0\u001b[39m <= n_components <= \u001b[38;5;28mmin\u001b[39m(n_samples, n_features):\n\u001b[32m--> \u001b[39m\u001b[32m554\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    555\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mn_components=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_components\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be between 0 and \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    556\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmin(n_samples, n_features)=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmin\u001b[39m(n_samples,\u001b[38;5;250m \u001b[39mn_features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    557\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msvd_solver=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._fit_svd_solver\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    558\u001b[39m     )\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.mean_ = xp.mean(X, axis=\u001b[32m0\u001b[39m)\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# When X is a scipy sparse matrix, self.mean_ is a numpy matrix, so we need\u001b[39;00m\n\u001b[32m    562\u001b[39m \u001b[38;5;66;03m# to transform it to a 1D array. Note that this is not the case when X\u001b[39;00m\n\u001b[32m    563\u001b[39m \u001b[38;5;66;03m# is a scipy sparse array.\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;66;03m# TODO: remove the following two lines when scikit-learn only depends\u001b[39;00m\n\u001b[32m    565\u001b[39m \u001b[38;5;66;03m# on scipy versions that no longer support scipy.sparse matrices.\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: n_components=50 must be between 0 and min(n_samples, n_features)=41 with svd_solver='full'"
     ]
    }
   ],
   "source": [
    "# CNT Engine v0 — rooted to C:\\Users\\caleb\\CNT_Lab\n",
    "# Save as: C:\\Users\\caleb\\CNT_Lab\\cnt_engine_v0.py  (or run as a single Jupyter cell)\n",
    "\n",
    "import os, sys, json, time, glob, re\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ---------- Config ----------\n",
    "LAB_ROOT = Path(os.getenv(\"CNT_LAB_DIR\", r\"C:\\Users\\caleb\\CNT_Lab\")).resolve()\n",
    "\n",
    "# Add/adjust any subfolders you actually use for writing text\n",
    "SOURCE_ROOTS = [\n",
    "    LAB_ROOT / \"notes\",\n",
    "    LAB_ROOT / \"artifacts\" / \"cnt_scroll\",\n",
    "    LAB_ROOT / \"artifacts\" / \"cnt_codex\",\n",
    "    LAB_ROOT / \"notebooks\",\n",
    "]\n",
    "\n",
    "ROOT = LAB_ROOT / \"artifacts\" / \"cnt_engine_v0\"\n",
    "OUT  = ROOT / \"out\"\n",
    "STATE= ROOT / \"CNT_STATE.yaml\"\n",
    "LOG  = ROOT / \"runlog.jsonl\"\n",
    "ROOT.mkdir(parents=True, exist_ok=True); OUT.mkdir(exist_ok=True)\n",
    "\n",
    "def now(): return datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "# ---------- Source discovery ----------\n",
    "def list_sources():\n",
    "    files = []\n",
    "    for root in SOURCE_ROOTS:\n",
    "        root.mkdir(parents=True, exist_ok=True)\n",
    "        files += glob.glob(str(root / \"**\" / \"*.md\"), recursive=True)\n",
    "        files += glob.glob(str(root / \"**\" / \"*.txt\"), recursive=True)\n",
    "    return [Path(f) for f in files]\n",
    "\n",
    "def auto_seed_if_empty():\n",
    "    paths = list_sources()\n",
    "    if paths: \n",
    "        return\n",
    "    seed_dir = LAB_ROOT / \"notes\"\n",
    "    seed_dir.mkdir(parents=True, exist_ok=True)\n",
    "    seed = seed_dir / \"cnt_seed.md\"\n",
    "    seed.write_text(\n",
    "        \"# CNT Seed\\n\\n\"\n",
    "        \"Hypothesis: Certain glyph–field pairings lower entropy drift.\\n\"\n",
    "        \"Test: Re-run θ metrics on EEG segments with glyph overlay vs baseline.\\n\"\n",
    "        \"Expected: Δθ > 0 with CI > 95% for overlay.\\n\"\n",
    "        \"Falsifier: No uplift or negative drift after 1k permutations.\\n\",\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "# ---------- IO ----------\n",
    "def read_corpus(paths):\n",
    "    rows = []\n",
    "    for p in paths:\n",
    "        try:\n",
    "            rows.append(dict(path=str(p), text=Path(p).read_text(encoding=\"utf-8\", errors=\"ignore\")))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# ---------- Index & residuals ----------\n",
    "def build_index(df):\n",
    "    if df.empty: return None, None, None\n",
    "    vec = TfidfVectorizer(max_features=6000, ngram_range=(1,2))\n",
    "    X = vec.fit_transform(df[\"text\"]).astype(np.float32)\n",
    "    ncomp = max(2, min(50, X.shape[1]-1)) if X.shape[1]>1 else 2\n",
    "    pca = PCA(n_components=ncomp)\n",
    "    Xd = pca.fit_transform(X.toarray())\n",
    "    Xr = pca.inverse_transform(Xd)\n",
    "    resid = np.linalg.norm(X.toarray() - Xr, axis=1)\n",
    "    return vec, X, resid\n",
    "\n",
    "def cluster_labels(X, k=6):\n",
    "    if X is None: return None\n",
    "    k = min(max(2, X.shape[0]//3), max(6, min(12, X.shape[0]-1)))\n",
    "    km = KMeans(n_clusters=k, n_init=5, random_state=42)\n",
    "    return km.fit_predict(X.toarray())\n",
    "\n",
    "def build_graph(df, lbl):\n",
    "    G = nx.Graph()\n",
    "    for i, row in df.reset_index().iterrows():\n",
    "        G.add_node(i, path=row[\"path\"], title=os.path.basename(row[\"path\"]), cluster=int(lbl[i]) if lbl is not None else -1)\n",
    "    # Light lexical linkage\n",
    "    caps = df[\"text\"].str.extractall(r\"\\b([A-Z][A-Za-z0-9_]{3,})\\b\").groupby(level=0).agg(lambda s: set(s[0].tolist()))\n",
    "    for i in range(len(df)):\n",
    "        for j in range(i+1, len(df)):\n",
    "            same = (lbl[i]==lbl[j]) if lbl is not None else False\n",
    "            overlap = 0\n",
    "            if i in caps.index and j in caps.index:\n",
    "                overlap = len(caps.loc[i] & caps.loc[j])\n",
    "            if same or overlap>=2:\n",
    "                G.add_edge(i, j, w=(1+overlap))\n",
    "    return G\n",
    "\n",
    "def score_reflexive(df, resid, G):\n",
    "    if df is None or len(df)==0:\n",
    "        return dict(clarity=0, novelty=0, coherence=0, falsifiability=0, total=0)\n",
    "    clarity = 1.0 - np.mean([min(len(t),10000)/10000 for t in df[\"text\"].tolist()])\n",
    "    novelty = float(np.mean(resid)/(np.std(resid)+1e-6))\n",
    "    coherence = (nx.average_clustering(G) if G.number_of_nodes()>1 else 0.0)\n",
    "    falsifiability = float(np.mean([t.lower().count(\"test\")+t.lower().count(\"predict\") for t in df[\"text\"]]))/10.0\n",
    "    clarity = np.clip(clarity, 0, 1); novelty = np.clip(novelty/1.5, 0, 1); coherence = np.clip(coherence, 0, 1); falsifiability = np.clip(falsifiability, 0, 1)\n",
    "    total = float(np.mean([clarity, novelty, coherence, falsifiability]))\n",
    "    return dict(clarity=float(clarity), novelty=float(novelty), coherence=float(coherence), falsifiability=float(falsifiability), total=float(total))\n",
    "\n",
    "def surface_candidates(df, resid, top=5):\n",
    "    idx = np.argsort(resid)[::-1][:min(top, len(resid))]\n",
    "    picks = []\n",
    "    for i in idx:\n",
    "        snippet = re.sub(r\"\\s+\", \" \", df.iloc[i][\"text\"])[:400]\n",
    "        picks.append(dict(path=df.iloc[i][\"path\"], resid=float(resid[i]), hint=snippet))\n",
    "    return picks\n",
    "\n",
    "def propose_updates(cands):\n",
    "    return [\n",
    "        dict(\n",
    "            target=c[\"path\"],\n",
    "            action=\"append_gloss\",\n",
    "            content=(\n",
    "                f\"\\n\\n> CNT-Gloss ({now()}): Clarify hypothesis; add test recipe & falsifier.\\n\"\n",
    "                f\"- Hypothesis: …\\n- Measurement: …\\n- Expected shift: …\\n- Falsifier: …\\n\"\n",
    "            ),\n",
    "        )\n",
    "        for c in cands\n",
    "    ]\n",
    "\n",
    "def legality_gate(text: str):\n",
    "    bad = any(k in text for k in [\"ssn\",\"credit card\",\"weapon\",\"harm\"])\n",
    "    return not bad\n",
    "\n",
    "def confab_gate(text: str):\n",
    "    t = text.lower()\n",
    "    return (\"hypothesis\" in t and \"falsifier\" in t)\n",
    "\n",
    "def apply_updates(updates):\n",
    "    accepted = []\n",
    "    for u in updates:\n",
    "        try:\n",
    "            content = u[\"content\"]\n",
    "            if not (legality_gate(content) and confab_gate(content)): \n",
    "                continue\n",
    "            p = Path(u[\"target\"])\n",
    "            original = p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "            p.write_text(original + content, encoding=\"utf-8\")\n",
    "            accepted.append(u)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return accepted\n",
    "\n",
    "def write_state(score, meta):\n",
    "    try:\n",
    "        import yaml\n",
    "        STATE.write_text(yaml.safe_dump(dict(updated=now(), score=score, meta=meta), sort_keys=False), encoding=\"utf-8\")\n",
    "    except Exception:\n",
    "        STATE.write_text(json.dumps(dict(updated=now(), score=score, meta=meta), indent=2), encoding=\"utf-8\")\n",
    "\n",
    "def log_event(kind, payload):\n",
    "    LOG.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with LOG.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(dict(ts=now(), kind=kind, **payload)) + \"\\n\")\n",
    "\n",
    "def run_cycle():\n",
    "    auto_seed_if_empty()\n",
    "    paths = list_sources()\n",
    "    df = read_corpus(paths)\n",
    "    if df.empty:\n",
    "        log_event(\"empty\", {\"roots\":[str(p) for p in SOURCE_ROOTS]})\n",
    "        return {\"empty\": True, \"roots\":[str(p) for p in SOURCE_ROOTS]}\n",
    "    vec, X, resid = build_index(df)\n",
    "    labels = cluster_labels(X)\n",
    "    G = build_graph(df, labels)\n",
    "    score = score_reflexive(df, resid, G)\n",
    "    cands = surface_candidates(df, resid, top=5)\n",
    "    accepted = apply_updates(propose_updates(cands))\n",
    "    write_state(score, dict(docs=len(df), accepted=len(accepted)))\n",
    "    pd.DataFrame(cands).to_csv(OUT / f\"hidden_truths_{datetime.now().strftime('%Y%m%d-%H%M%S')}.csv\", index=False)\n",
    "    log_event(\"cycle\", dict(score=score, proposed=len(cands), accepted=len(accepted)))\n",
    "    return dict(score=score, proposed=len(cands), accepted=len(accepted), hidden=cands[:3])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    res = run_cycle()\n",
    "    print(json.dumps(res, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c82dbd67-9c37-464e-93a8-5c8035ea17ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_index(df):\n",
    "    if df.empty:\n",
    "        return None, None, None\n",
    "    vec = TfidfVectorizer(max_features=6000, ngram_range=(1,2))\n",
    "    X = vec.fit_transform(df[\"text\"]).astype(np.float32)\n",
    "\n",
    "    n_samples, n_features = X.shape\n",
    "    # choose a safe SVD rank within bounds\n",
    "    nmax = max(2, min(n_samples - 1, n_features - 1, 128))\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    svd = TruncatedSVD(n_components=nmax, random_state=42)\n",
    "    Xd = svd.fit_transform(X)                # stays sparse-friendly\n",
    "    Xr = svd.inverse_transform(Xd)           # available on TruncatedSVD\n",
    "    resid = np.linalg.norm(X.toarray() - Xr, axis=1)\n",
    "    return vec, X, resid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8554e81-7e12-4438-aa94-7fea271ca8cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:294\u001b[39m, in \u001b[36mSeriesGroupBy.aggregate\u001b[39m\u001b[34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_python_agg_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m    296\u001b[39m     \u001b[38;5;66;03m# KeyError raised in test_groupby.test_basic is bc the func does\u001b[39;00m\n\u001b[32m    297\u001b[39m     \u001b[38;5;66;03m#  a dictionary lookup on group.name, but group name is not\u001b[39;00m\n\u001b[32m    298\u001b[39m     \u001b[38;5;66;03m#  pinned in _python_agg_general, only in _aggregate_named\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:327\u001b[39m, in \u001b[36mSeriesGroupBy._python_agg_general\u001b[39m\u001b[34m(self, func, *args, **kwargs)\u001b[39m\n\u001b[32m    326\u001b[39m obj = \u001b[38;5;28mself\u001b[39m._obj_with_exclusions\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_grouper\u001b[49m\u001b[43m.\u001b[49m\u001b[43magg_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m res = obj._constructor(result, name=obj.name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:873\u001b[39m, in \u001b[36mBaseGrouper.agg_series\u001b[39m\u001b[34m(self, obj, func, preserve_dtype)\u001b[39m\n\u001b[32m    871\u001b[39m     preserve_dtype = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_aggregate_series_pure_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    875\u001b[39m npvalues = lib.maybe_convert_objects(result, try_float=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:894\u001b[39m, in \u001b[36mBaseGrouper._aggregate_series_pure_python\u001b[39m\u001b[34m(self, obj, func)\u001b[39m\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(splitter):\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m     res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    895\u001b[39m     res = extract_result(res)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:324\u001b[39m, in \u001b[36mSeriesGroupBy._python_agg_general.<locals>.<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    323\u001b[39m     warn_alias_replacement(\u001b[38;5;28mself\u001b[39m, orig_func, alias)\n\u001b[32m--> \u001b[39m\u001b[32m324\u001b[39m f = \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m obj = \u001b[38;5;28mself\u001b[39m._obj_with_exclusions\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mbuild_graph.<locals>.<lambda>\u001b[39m\u001b[34m(s)\u001b[39m\n\u001b[32m     78\u001b[39m     G.add_node(i, path=row[\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m], cluster=\u001b[38;5;28mint\u001b[39m(lbl[i]) \u001b[38;5;28;01mif\u001b[39;00m lbl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m -\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m caps = df[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m].str.extractall(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mb([A-Z][A-Za-z0-9_]\u001b[39m\u001b[33m{\u001b[39m\u001b[33m3,})\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m).groupby(level=\u001b[32m0\u001b[39m).agg(\u001b[38;5;28;01mlambda\u001b[39;00m s: \u001b[38;5;28mset\u001b[39m(\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m.tolist()))\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:1133\u001b[39m, in \u001b[36mSeries.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1132\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[32m-> \u001b[39m\u001b[32m1133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[32m   1136\u001b[39m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:1249\u001b[39m, in \u001b[36mSeries._get_value\u001b[39m\u001b[34m(self, label, takeable)\u001b[39m\n\u001b[32m   1248\u001b[39m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:3059\u001b[39m, in \u001b[36mMultiIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3058\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m3059\u001b[39m     loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_level_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3060\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _maybe_to_slice(loc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:3437\u001b[39m, in \u001b[36mMultiIndex._get_level_indexer\u001b[39m\u001b[34m(self, key, level, indexer)\u001b[39m\n\u001b[32m   3435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m start == end:\n\u001b[32m   3436\u001b[39m     \u001b[38;5;66;03m# The label is present in self.levels[level] but unused:\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3437\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m   3438\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(start, end)\n",
      "\u001b[31mKeyError\u001b[39m: 0",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 175\u001b[39m\n\u001b[32m    173\u001b[39m \u001b[38;5;66;03m# ---------- EXECUTE ----------\u001b[39;00m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m==\u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     res=\u001b[43mrun_cycle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m     \u001b[38;5;28mprint\u001b[39m(json.dumps(res,indent=\u001b[32m2\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 164\u001b[39m, in \u001b[36mrun_cycle\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    162\u001b[39m vec,X,resid=build_index(df)\n\u001b[32m    163\u001b[39m labels=cluster_labels(X)\n\u001b[32m--> \u001b[39m\u001b[32m164\u001b[39m G=\u001b[43mbuild_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m score=score_reflexive(df,resid,G)\n\u001b[32m    166\u001b[39m cands=surface_candidates(df,resid,top=\u001b[32m5\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mbuild_graph\u001b[39m\u001b[34m(df, lbl)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m df.reset_index().iterrows():\n\u001b[32m     78\u001b[39m     G.add_node(i, path=row[\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m], cluster=\u001b[38;5;28mint\u001b[39m(lbl[i]) \u001b[38;5;28;01mif\u001b[39;00m lbl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m -\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m caps = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextractall\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mb([A-Z][A-Za-z0-9_]\u001b[39;49m\u001b[33;43m{\u001b[39;49m\u001b[33;43m3,})\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df)):\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i+\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:1482\u001b[39m, in \u001b[36mDataFrameGroupBy.aggregate\u001b[39m\u001b[34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m   1480\u001b[39m gba = GroupByApply(\u001b[38;5;28mself\u001b[39m, [func], args=(), kwargs={})\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1482\u001b[39m     result = \u001b[43mgba\u001b[49m\u001b[43m.\u001b[49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1484\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mNo objects to concatenate\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:193\u001b[39m, in \u001b[36mApply.agg\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agg_dict_like()\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(func):\n\u001b[32m    192\u001b[39m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magg_list_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(func):\n\u001b[32m    196\u001b[39m     f = com.get_cython_func(func)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:326\u001b[39m, in \u001b[36mApply.agg_list_like\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magg_list_like\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> DataFrame | Series:\n\u001b[32m    319\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[33;03m    Compute aggregation in the case of a list-like argument.\u001b[39;00m\n\u001b[32m    321\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    324\u001b[39m \u001b[33;03m    Result of aggregation.\u001b[39;00m\n\u001b[32m    325\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43magg_or_apply_list_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43magg\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1566\u001b[39m, in \u001b[36mGroupByApply.agg_or_apply_list_like\u001b[39m\u001b[34m(self, op_name)\u001b[39m\n\u001b[32m   1561\u001b[39m \u001b[38;5;66;03m# Only set as_index=True on groupby objects, not Window or Resample\u001b[39;00m\n\u001b[32m   1562\u001b[39m \u001b[38;5;66;03m# that inherit from this class.\u001b[39;00m\n\u001b[32m   1563\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m com.temp_setattr(\n\u001b[32m   1564\u001b[39m     obj, \u001b[33m\"\u001b[39m\u001b[33mas_index\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, condition=\u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[33m\"\u001b[39m\u001b[33mas_index\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1565\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1566\u001b[39m     keys, results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_list_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1567\u001b[39m result = \u001b[38;5;28mself\u001b[39m.wrap_results_list_like(keys, results)\n\u001b[32m   1568\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:385\u001b[39m, in \u001b[36mApply.compute_list_like\u001b[39m\u001b[34m(self, op_name, selected_obj, kwargs)\u001b[39m\n\u001b[32m    379\u001b[39m colg = obj._gotitem(col, ndim=\u001b[32m1\u001b[39m, subset=selected_obj.iloc[:, index])\n\u001b[32m    380\u001b[39m args = (\n\u001b[32m    381\u001b[39m     [\u001b[38;5;28mself\u001b[39m.axis, *\u001b[38;5;28mself\u001b[39m.args]\n\u001b[32m    382\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m include_axis(op_name, colg)\n\u001b[32m    383\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args\n\u001b[32m    384\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m new_res = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcolg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m results.append(new_res)\n\u001b[32m    387\u001b[39m indices.append(index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:257\u001b[39m, in \u001b[36mSeriesGroupBy.aggregate\u001b[39m\u001b[34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m    255\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine\u001b[39m\u001b[33m\"\u001b[39m] = engine\n\u001b[32m    256\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mengine_kwargs\u001b[39m\u001b[33m\"\u001b[39m] = engine_kwargs\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_aggregate_multiple_funcs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m relabeling:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# columns is not narrowed by mypy from relabeling flag\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# for mypy\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:362\u001b[39m, in \u001b[36mSeriesGroupBy._aggregate_multiple_funcs\u001b[39m\u001b[34m(self, arg, *args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m idx, (name, func) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(arg):\n\u001b[32m    361\u001b[39m         key = base.OutputKey(label=name, position=idx)\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m         results[key] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, DataFrame) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m results.values()):\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m concat\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:299\u001b[39m, in \u001b[36mSeriesGroupBy.aggregate\u001b[39m\u001b[34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[39m\n\u001b[32m    294\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._python_agg_general(func, *args, **kwargs)\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[32m    296\u001b[39m     \u001b[38;5;66;03m# KeyError raised in test_groupby.test_basic is bc the func does\u001b[39;00m\n\u001b[32m    297\u001b[39m     \u001b[38;5;66;03m#  a dictionary lookup on group.name, but group name is not\u001b[39;00m\n\u001b[32m    298\u001b[39m     \u001b[38;5;66;03m#  pinned in _python_agg_general, only in _aggregate_named\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_aggregate_named\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    301\u001b[39m     warnings.warn(\n\u001b[32m    302\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPinning the groupby key to each group in \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    303\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.agg is deprecated, and cases that \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    308\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    309\u001b[39m     )\n\u001b[32m    311\u001b[39m     \u001b[38;5;66;03m# result is a dict whose keys are the elements of result_index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:461\u001b[39m, in \u001b[36mSeriesGroupBy._aggregate_named\u001b[39m\u001b[34m(self, func, *args, **kwargs)\u001b[39m\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._grouper.get_iterator(\n\u001b[32m    456\u001b[39m     \u001b[38;5;28mself\u001b[39m._obj_with_exclusions, axis=\u001b[38;5;28mself\u001b[39m.axis\n\u001b[32m    457\u001b[39m ):\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m# needed for pandas/tests/groupby/test_groupby.py::test_basic_aggregations\u001b[39;00m\n\u001b[32m    459\u001b[39m     \u001b[38;5;28mobject\u001b[39m.\u001b[34m__setattr__\u001b[39m(group, \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m, name)\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m     output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    462\u001b[39m     output = ops.extract_result(output)\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m initialized:\n\u001b[32m    464\u001b[39m         \u001b[38;5;66;03m# We only do this validation on the first iteration\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mbuild_graph.<locals>.<lambda>\u001b[39m\u001b[34m(s)\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m df.reset_index().iterrows():\n\u001b[32m     78\u001b[39m     G.add_node(i, path=row[\u001b[33m\"\u001b[39m\u001b[33mpath\u001b[39m\u001b[33m\"\u001b[39m], cluster=\u001b[38;5;28mint\u001b[39m(lbl[i]) \u001b[38;5;28;01mif\u001b[39;00m lbl \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m -\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m caps = df[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m].str.extractall(\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mb([A-Z][A-Za-z0-9_]\u001b[39m\u001b[33m{\u001b[39m\u001b[33m3,})\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m).groupby(level=\u001b[32m0\u001b[39m).agg(\u001b[38;5;28;01mlambda\u001b[39;00m s: \u001b[38;5;28mset\u001b[39m(\u001b[43ms\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m.tolist()))\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(df)):\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i+\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df)):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:1133\u001b[39m, in \u001b[36mSeries.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1130\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[key]\n\u001b[32m   1132\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[32m-> \u001b[39m\u001b[32m1133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[38;5;66;03m# Convert generator to list before going through hashable part\u001b[39;00m\n\u001b[32m   1136\u001b[39m \u001b[38;5;66;03m# (We will iterate through the generator there to check for slices)\u001b[39;00m\n\u001b[32m   1137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\pandas\\core\\series.py:1249\u001b[39m, in \u001b[36mSeries._get_value\u001b[39m\u001b[34m(self, label, takeable)\u001b[39m\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[label]\n\u001b[32m   1248\u001b[39m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1249\u001b[39m loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(loc):\n\u001b[32m   1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._values[loc]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:3059\u001b[39m, in \u001b[36mMultiIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3056\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m mask\n\u001b[32m   3058\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m3059\u001b[39m     loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_level_indexer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   3060\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _maybe_to_slice(loc)\n\u001b[32m   3062\u001b[39m keylen = \u001b[38;5;28mlen\u001b[39m(key)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\CNT_Lab\\.venv\\Lib\\site-packages\\pandas\\core\\indexes\\multi.py:3437\u001b[39m, in \u001b[36mMultiIndex._get_level_indexer\u001b[39m\u001b[34m(self, key, level, indexer)\u001b[39m\n\u001b[32m   3433\u001b[39m     end = algos.searchsorted(level_codes, idx, side=\u001b[33m\"\u001b[39m\u001b[33mright\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   3435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m start == end:\n\u001b[32m   3436\u001b[39m     \u001b[38;5;66;03m# The label is present in self.levels[level] but unused:\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3437\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m   3438\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mslice\u001b[39m(start, end)\n",
      "\u001b[31mKeyError\u001b[39m: 0"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# CNT Engine — Self-Referential / Self-Updating / Hidden-Truth Search\n",
    "# Rooted at:  C:\\Users\\caleb\\CNT_Lab\n",
    "# ============================================================\n",
    "\n",
    "import os, json, glob, re, time\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "LAB_ROOT = Path(os.getenv(\"CNT_LAB_DIR\", r\"C:\\Users\\caleb\\CNT_Lab\")).resolve()\n",
    "SOURCE_ROOTS = [\n",
    "    LAB_ROOT / \"notes\",\n",
    "    LAB_ROOT / \"artifacts\" / \"cnt_scroll\",\n",
    "    LAB_ROOT / \"artifacts\" / \"cnt_codex\",\n",
    "    LAB_ROOT / \"notebooks\",\n",
    "]\n",
    "ROOT  = LAB_ROOT / \"artifacts\" / \"cnt_engine_megacell\"\n",
    "OUT   = ROOT / \"out\"\n",
    "STATE = ROOT / \"CNT_STATE.yaml\"\n",
    "LOG   = ROOT / \"runlog.jsonl\"\n",
    "for p in [ROOT, OUT, *SOURCE_ROOTS]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def now(): return datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "# ---------- DISCOVER / SEED ----------\n",
    "def list_sources():\n",
    "    files = []\n",
    "    for root in SOURCE_ROOTS:\n",
    "        files += glob.glob(str(root / \"**\" / \"*.md\"), recursive=True)\n",
    "        files += glob.glob(str(root / \"**\" / \"*.txt\"), recursive=True)\n",
    "    return [Path(f) for f in files]\n",
    "\n",
    "def auto_seed_if_empty():\n",
    "    paths = list_sources()\n",
    "    if paths: return\n",
    "    seed = SOURCE_ROOTS[0] / \"cnt_seed.md\"\n",
    "    seed.write_text(\n",
    "        \"# CNT Seed\\n\\n\"\n",
    "        \"Hypothesis: Certain glyph–field pairings lower entropy drift.\\n\"\n",
    "        \"Test: Re-run θ metrics on EEG segments with glyph overlay vs baseline.\\n\"\n",
    "        \"Expected: Δθ > 0 with CI > 95% for overlay.\\n\"\n",
    "        \"Falsifier: No uplift or negative drift after 1 k permutations.\\n\",\n",
    "        encoding=\"utf-8\"\n",
    "    )\n",
    "\n",
    "# ---------- INDEX & RESIDUALS ----------\n",
    "def build_index(df):\n",
    "    if df.empty: return None, None, None\n",
    "    vec = TfidfVectorizer(max_features=6000, ngram_range=(1,2))\n",
    "    X = vec.fit_transform(df[\"text\"]).astype(np.float32)\n",
    "    n_samples, n_features = X.shape\n",
    "    nmax = max(2, min(n_samples - 1, n_features - 1, 128))\n",
    "    svd = TruncatedSVD(n_components=nmax, random_state=42)\n",
    "    Xd = svd.fit_transform(X)\n",
    "    Xr = svd.inverse_transform(Xd)\n",
    "    resid = np.linalg.norm(X.toarray() - Xr, axis=1)\n",
    "    return vec, X, resid\n",
    "\n",
    "# ---------- CLUSTERS & GRAPH ----------\n",
    "def cluster_labels(X, k=6):\n",
    "    if X is None: return None\n",
    "    k = max(2, min(k, X.shape[0]-1))\n",
    "    km = KMeans(n_clusters=k, n_init=5, random_state=42)\n",
    "    return km.fit_predict(X.toarray())\n",
    "\n",
    "def build_graph(df, lbl):\n",
    "    G = nx.Graph()\n",
    "    for i, row in df.reset_index().iterrows():\n",
    "        G.add_node(i, path=row[\"path\"], cluster=int(lbl[i]) if lbl is not None else -1)\n",
    "    caps = df[\"text\"].str.extractall(r\"\\b([A-Z][A-Za-z0-9_]{3,})\\b\").groupby(level=0).agg(lambda s: set(s[0].tolist()))\n",
    "    for i in range(len(df)):\n",
    "        for j in range(i+1, len(df)):\n",
    "            same = (lbl[i]==lbl[j]) if lbl is not None else False\n",
    "            overlap = 0\n",
    "            if i in caps.index and j in caps.index:\n",
    "                overlap = len(caps.loc[i] & caps.loc[j])\n",
    "            if same or overlap>=2:\n",
    "                G.add_edge(i, j, w=(1+overlap))\n",
    "    return G\n",
    "\n",
    "# ---------- SCORES ----------\n",
    "def score_reflexive(df, resid, G):\n",
    "    if df is None or len(df)==0:\n",
    "        return dict(clarity=0, novelty=0, coherence=0, falsifiability=0, total=0)\n",
    "    clarity = 1.0 - np.mean([min(len(t),10000)/10000 for t in df[\"text\"].tolist()])\n",
    "    novelty = float(np.mean(resid)/(np.std(resid)+1e-6))\n",
    "    coherence = nx.average_clustering(G) if G.number_of_nodes()>1 else 0.0\n",
    "    falsifiability = float(np.mean([t.lower().count(\"test\")+t.lower().count(\"predict\") for t in df[\"text\"]]))/10.0\n",
    "    clarity = np.clip(clarity,0,1); novelty = np.clip(novelty/1.5,0,1)\n",
    "    coherence = np.clip(coherence,0,1); falsifiability = np.clip(falsifiability,0,1)\n",
    "    total = float(np.mean([clarity,novelty,coherence,falsifiability]))\n",
    "    return dict(clarity=float(clarity), novelty=float(novelty),\n",
    "                coherence=float(coherence), falsifiability=float(falsifiability),\n",
    "                total=float(total))\n",
    "\n",
    "# ---------- SURFACING / UPDATES ----------\n",
    "def surface_candidates(df, resid, top=5):\n",
    "    idx = np.argsort(resid)[::-1][:min(top, len(resid))]\n",
    "    picks=[]\n",
    "    for i in idx:\n",
    "        snippet=re.sub(r\"\\s+\",\" \",df.iloc[i][\"text\"])[:400]\n",
    "        picks.append(dict(path=df.iloc[i][\"path\"],resid=float(resid[i]),hint=snippet))\n",
    "    return picks\n",
    "\n",
    "def propose_updates(cands):\n",
    "    return [dict(\n",
    "        target=c[\"path\"],\n",
    "        content=(\n",
    "            f\"\\n\\n> CNT-Gloss ({now()}): Clarify hypothesis; add test recipe & falsifier.\\n\"\n",
    "            \"- Hypothesis: …\\n- Measurement: …\\n- Expected shift: …\\n- Falsifier: …\\n\"\n",
    "        )\n",
    "    ) for c in cands]\n",
    "\n",
    "def legality_gate(text): \n",
    "    bad=any(k in text for k in[\"ssn\",\"credit card\",\"weapon\",\"harm\"])\n",
    "    return not bad\n",
    "\n",
    "def confab_gate(text):\n",
    "    t=text.lower(); return (\"hypothesis\" in t and \"falsifier\" in t)\n",
    "\n",
    "def apply_updates(updates):\n",
    "    accepted=[]\n",
    "    for u in updates:\n",
    "        try:\n",
    "            if not (legality_gate(u[\"content\"]) and confab_gate(u[\"content\"])): \n",
    "                continue\n",
    "            p=Path(u[\"target\"])\n",
    "            p.write_text(p.read_text(encoding=\"utf-8\",errors=\"ignore\")+u[\"content\"],encoding=\"utf-8\")\n",
    "            accepted.append(u)\n",
    "        except Exception: pass\n",
    "    return accepted\n",
    "\n",
    "# ---------- STATE / LOG ----------\n",
    "def write_state(score,meta):\n",
    "    try:\n",
    "        import yaml\n",
    "        STATE.write_text(yaml.safe_dump(dict(updated=now(),score=score,meta=meta),sort_keys=False),encoding=\"utf-8\")\n",
    "    except Exception:\n",
    "        STATE.write_text(json.dumps(dict(updated=now(),score=score,meta=meta),indent=2),encoding=\"utf-8\")\n",
    "\n",
    "def log_event(kind,payload):\n",
    "    with LOG.open(\"a\",encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(dict(ts=now(),kind=kind,**payload))+\"\\n\")\n",
    "\n",
    "# ---------- MAIN CYCLE ----------\n",
    "def run_cycle():\n",
    "    auto_seed_if_empty()\n",
    "    paths=list_sources()\n",
    "    df=pd.DataFrame([dict(path=str(p),text=p.read_text(encoding=\"utf-8\",errors=\"ignore\")) for p in paths])\n",
    "    if df.empty:\n",
    "        log_event(\"empty\",{\"roots\":[str(r) for r in SOURCE_ROOTS]})\n",
    "        return {\"empty\":True,\"roots\":[str(r) for r in SOURCE_ROOTS]}\n",
    "    vec,X,resid=build_index(df)\n",
    "    labels=cluster_labels(X)\n",
    "    G=build_graph(df,labels)\n",
    "    score=score_reflexive(df,resid,G)\n",
    "    cands=surface_candidates(df,resid,top=5)\n",
    "    accepted=apply_updates(propose_updates(cands))\n",
    "    write_state(score,dict(docs=len(df),accepted=len(accepted)))\n",
    "    pd.DataFrame(cands).to_csv(OUT/f\"hidden_truths_{datetime.now().strftime('%Y%m%d-%H%M%S')}.csv\",index=False)\n",
    "    log_event(\"cycle\",dict(score=score,proposed=len(cands),accepted=len(accepted)))\n",
    "    return dict(score=score,proposed=len(cands),accepted=len(accepted),hidden=cands[:3])\n",
    "\n",
    "# ---------- EXECUTE ----------\n",
    "if __name__==\"__main__\":\n",
    "    res=run_cycle()\n",
    "    print(json.dumps(res,indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bf57ec-118c-4754-97e8-c1ef1721a94b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
