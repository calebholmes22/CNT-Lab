# === G-DREN Daily Runner v0.1 — retune sweep → dashboard → history CSVs ===
import os, json, time
from pathlib import Path
import numpy as np, pandas as pd

# ---- knobs (tweak as you like) ----
FA_PER_YEAR_TARGET = 0.60       # false-alarm budget for regime tuner
COOLDOWN_DAYS      = 14
MAX_WARN_DAYS      = 12
HYST_EXIT_DAYS     = 3
QGRID              = np.linspace(0.965, 0.990, 51)
WIN_RANK           = 512
VOTE_THRESH_THETA  = 0.25
VOTE_THRESH_GAMMA  = 0.08
LOOKAHEAD_DAYS     = 14         # regime horizon used in sweep scoring

# If features look stale (> 3 days old), we *warn* but do not recompute here.
STALE_FEATURE_DAYS = 3

# ---- paths ----
CNT  = Path(os.environ.get("CNT_LAB_DIR", "E:/CNT")).resolve()
ROOT = CNT / "artifacts" / "g_dren"
DASH = ROOT / "dashboard"
ROOT.mkdir(parents=True, exist_ok=True); DASH.mkdir(parents=True, exist_ok=True)
TODAY = pd.Timestamp.today().normalize()

def _latest(globs):
    c=[]
    for g in globs: c += list(ROOT.glob(g))
    return sorted(c, key=lambda p: p.stat().st_mtime, reverse=True)[0] if c else None

def _load_csv(fp):
    df = pd.read_csv(fp, index_col=0, parse_dates=True)
    df.index = pd.DatetimeIndex(df.index).tz_localize(None).normalize()
    return df[~df.index.duplicated(keep="last")].sort_index()

def _pct_rank(s: pd.Series, win=512):
    def pr(w):
        v = w[-1]; a = np.sort(w)
        return np.searchsorted(a, v, side="right")/len(w)
    return s.rolling(win, min_periods=win//2).apply(pr, raw=True)

def _transitions(df, col="alert", n=5):
    a = df[col].astype(str)
    trans = (a != a.shift(1)).fillna(True)
    pts = df.index[trans]
    out=[]
    for i in range(len(pts)-1, -1, -1):
        t = pts[i]; state = a.loc[t]
        days = (pts[i+1]-t).days if i+1 < len(pts) else (df.index[-1]-t).days + 1
        out.append((str(t.date()), state, int(days)))
        if len(out)>=n: break
    return out

# ---------- 1) Locate latest Spike + Regime feature/alert files ----------
spike_alerts = _latest(["*_v02b/gdren_v02b_alerts.csv", "*_v02b/*alerts*.csv"])
reg_features = _latest(["*_v03a_lite_fix/gdren_v03a_lite_fix_features.csv"])
if reg_features is None:
    raise FileNotFoundError("No v03a_lite_fix_features.csv found — run the v0.3a-lite (fix) cell once to seed features.")

age_days = (pd.Timestamp.fromtimestamp(reg_features.stat().st_mtime).normalize() - TODAY).days
if age_days < -STALE_FEATURE_DAYS:
    print(f"[warn] Regime features are {abs(age_days)} days old → consider re-running v0.3a-lite fix to refresh.")

DF = _load_csv(reg_features)

# ---------- 2) Build Regime hybrid DRIVER & run retune sweep ----------
required = ["RISK","EVENT_ONSET","Theta_mkt","Theta_vix","Theta_cred","Gamma_mkt_vix","AR1_mkt"]
miss = [c for c in required if c not in DF.columns]
if miss: raise ValueError(f"Missing columns in features CSV: {miss}")

risk   = pd.Series(DF["RISK"].astype(float), index=DF.index).fillna(0.0)
Theta_v= DF["Theta_vix"].astype(float).values
Theta_c= DF["Theta_cred"].astype(float).values
Theta_m= DF["Theta_mkt"].astype(float).values
Gamma_v= DF["Gamma_mkt_vix"].astype(float).values
AR1_m  = DF["AR1_mkt"].astype(float).values
onset  = DF["EVENT_ONSET"].astype(int).values
idx    = DF.index

risk_pct = _pct_rank(risk, win=WIN_RANK).fillna(0.0)
cons_raw = (0.45*pd.Series(Theta_v, index=idx) +
            0.35*pd.Series(Theta_c, index=idx) +
            0.15*pd.Series(np.maximum(0, AR1_m), index=idx) +
            0.05*pd.Series(np.nan_to_num(Gamma_v), index=idx))
cons_pct = _pct_rank(cons_raw, win=WIN_RANK).fillna(0.0)
driver   = pd.Series(np.maximum(risk_pct.values, cons_pct.values), index=idx, name="DRIVER")

votes_base = ((Theta_v>=VOTE_THRESH_THETA).astype(int) +
              (Theta_c>=VOTE_THRESH_THETA).astype(int) +
              (Theta_m>=VOTE_THRESH_THETA).astype(int) +
              (Gamma_v>=VOTE_THRESH_GAMMA).astype(int) +
              ((driver.diff(7).fillna(0.0).values)>0).astype(int))

def make_alert(driver_s, q, gate_min=2, persist_win=3, persist_req=1):
    t = float(np.quantile(driver_s.values[np.isfinite(driver_s.values)], q))
    persist = ((driver_s>=t).astype(int).rolling(persist_win).sum().fillna(0).values >= persist_req)
    state=[]; in_warn=False; below=0; wlen=0
    dv = driver_s.values
    for r,v,p in zip(dv, votes_base, persist):
        if in_warn:
            wlen += 1
            below = below+1 if r < 0.9*t else 0
            if below>=HYST_EXIT_DAYS or wlen>=MAX_WARN_DAYS:
                in_warn=False; wlen=0; state.append('OK')
            else: state.append('WARNING')
        else:
            if (r>=t) and p and (v>=gate_min):
                in_warn=True; below=0; wlen=1; state.append('WARNING')
            else: state.append('OK')
    return pd.Series(state, index=driver_s.index), t

def score(alert_s, horizon, ix):
    a = alert_s.loc[ix]
    starts = (a.eq("WARNING") & ~a.shift(1).eq("WARNING")).fillna(False).values
    s_ix = np.where(starts)[0].tolist()
    kept=[]; last=-10**9
    for s in s_ix:
        if s-last >= COOLDOWN_DAYS: kept.append(s); last=s
    s_ix = kept
    e_slice = onset[a.index.get_indexer(ix)]
    hits=0
    for s in s_ix:
        if e_slice[s+1:min(s+1+horizon, len(e_slice))].any(): hits += 1
    fa = int(len(s_ix)-hits)
    years = max(1e-9, (ix[-1]-ix[0]).days/365.25)
    return {"starts": len(s_ix), "hits": hits, "fa_py": fa/years}, pd.Index(a.index[s_ix])

# Train slice for calibration (≤2015-12-31)
ix_train = idx[idx <= pd.Timestamp("2015-12-31")]
if len(ix_train) < 2000:
    ix_train = idx[: int(len(idx)*0.6)]  # fallback split

best=None
for gate_min in [1,2,3]:
    for pwin in [3,5]:
        for preq in [1,2]:
            for q in QGRID:
                alert_q, thr = make_alert(driver, q, gate_min, pwin, preq)
                res,_ = score(alert_q, LOOKAHEAD_DAYS, ix_train)
                cand = {"q": float(q), "thr": float(thr), "gate": gate_min, "pwin": pwin, "preq": preq, **res}
                if (best is None) or \
                   ((res["fa_py"] <= FA_PER_YEAR_TARGET) and (res["hits"] > best.get("hits_ok",-1))) or \
                   ((res["fa_py"] <= FA_PER_YEAR_TARGET) and (res["hits"] == best.get("hits_ok",-1)) and (res["fa_py"] < best["fa_py"])) or \
                   ((best["fa_py"] > FA_PER_YEAR_TARGET) and (res["fa_py"] < best["fa_py"])):
                    best = cand.copy(); best["hits_ok"] = res["hits"]

# apply to full series
alert_final, thr_final = make_alert(driver, best["q"], best["gate"], best["pwin"], best["preq"])
full_res, starts_idx   = score(alert_final, LOOKAHEAD_DAYS, idx)

# Save regime sweep outputs
RUN = ROOT / time.strftime("%Y%m%d-%H%M%SZ", time.gmtime()) / "v03a_daily"
RUN.mkdir(parents=True, exist_ok=True)
out_df = DF.assign(risk_pct=risk_pct, cons_pct=cons_pct, DRIVER=driver, alert=alert_final)
cols = ["RISK","risk_pct","cons_pct","DRIVER","alert","EVENT_ONSET","Theta_mkt","Theta_vix","Theta_cred","Gamma_mkt_vix","AR1_mkt"]
out_df[cols].to_csv(RUN / "gdren_v03a_daily_alerts.csv")
(Path(RUN / "gdren_v03a_daily_metrics.json")).write_text(json.dumps({
    "q": best["q"], "thr": best["thr"], "gate_min_votes": best["gate"],
    "persist_win": best["pwin"], "persist_req": best["preq"],
    "train_hits": int(best["hits"]), "train_fa_per_year": float(best["fa_py"]),
    "full_starts": int(full_res["starts"]), "full_hits": int(full_res["hits"]),
    "full_fa_per_year": float(full_res["fa_py"]), "horizon_days": LOOKAHEAD_DAYS
}, indent=2))

# ---------- 3) Update dashboard + overlay ----------
def _status_line(name, date, base_alert, extras):
    parts = [f"{name:10s} — {base_alert:8s} as of {date}"]
    if extras: parts.append(" | " + " ".join(extras))
    return "".join(parts)

lines=[]; payload={"generated_utc": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())}

# SPIKE status
if spike_alerts and spike_alerts.exists():
    S = _load_csv(spike_alerts)
    s_last = (S.index[S.index<=TODAY][-1] if any(S.index<=TODAY) else S.index[-1])
    s_alert = str(S.loc[s_last, "alert"])
    s_nexus = float(S.loc[s_last, "NEXUS"]) if "NEXUS" in S.columns and pd.notna(S.loc[s_last, "NEXUS"]) else None
    lines.append(_status_line("SPIKE", str(s_last.date()), s_alert, [f"NEXUS={s_nexus:.3f}"] if s_nexus is not None else []))
    s_hist = _transitions(S, "alert", n=5)
    payload["spike"] = {"file": str(spike_alerts), "asof": str(s_last.date()), "alert": s_alert, "nexus": s_nexus, "recent": s_hist}
else:
    lines.append("SPIKE      — (no recent v0.2b alerts found)")
    payload["spike"] = {"file": None, "alert": None, "recent": []}

# REGIME status (from today’s RUN)
R = _load_csv(RUN / "gdren_v03a_daily_alerts.csv")
r_last = (R.index[R.index<=TODAY][-1] if any(R.index<=TODAY) else R.index[-1])
base_alert = str(R.loc[r_last, "alert"])
driver_last= float(R.loc[r_last, "DRIVER"]) if "DRIVER" in R.columns else None
risk_last  = float(R.loc[r_last, "RISK"])   if "RISK"   in R.columns else None

# policy PREWARN echo: driver≥thr & Γ strong
Gamma_v_last = float(R.loc[r_last, "Gamma_mkt_vix"]) if "Gamma_mkt_vix" in R.columns else 0.0
persist_hits = int((R["DRIVER"].iloc[max(0, R.index.get_loc(r_last)-2):R.index.get_loc(r_last)+1] >= best["thr"]).sum()) if "DRIVER" in R.columns else 0
persist_ok   = (persist_hits >= 1)
policy_alert = base_alert
if base_alert != "WARNING" and np.isfinite(driver_last) and driver_last>=best["thr"] and persist_ok and (Gamma_v_last >= 0.75):
    policy_alert = "PREWARN"

extras = []
if risk_last is not None:   extras.append(f"RISK={risk_last:.3f}")
if driver_last is not None: extras.append(f"DRIVER={driver_last:.3f}")
if policy_alert == "PREWARN" and base_alert != "WARNING": extras.append("policy=PREWARN(Γ strong)")

lines.append(_status_line("REGIME", str(r_last.date()), policy_alert, extras))
r_hist = _transitions(R, "alert", n=5)

payload["regime"] = {
    "file": str(RUN / "gdren_v03a_daily_alerts.csv"),
    "asof": str(r_last.date()),
    "alert": base_alert, "policy_alert": policy_alert,
    "driver": driver_last, "risk": risk_last,
    "threshold": float(best["thr"]), "q": float(best["q"]),
    "gate_min_votes": int(best["gate"]), "persist_win": int(best["pwin"]), "persist_req": int(best["preq"]),
    "recent": r_hist
}

# write JSON
(DASH / "g_dren_dashboard_status.json").write_text(json.dumps(payload, indent=2))

# ---------- 4) Export transitions CSVs ----------
def export_transitions_csv(fp_in, fp_out):
    D = _load_csv(fp_in)
    hist = _transitions(D, "alert", n=9999)  # full history
    pd.DataFrame(hist, columns=["date","state","days"]).to_csv(fp_out, index=False)

if spike_alerts and spike_alerts.exists():
    export_transitions_csv(spike_alerts, DASH / "spike_transitions.csv")
export_transitions_csv(RUN / "gdren_v03a_daily_alerts.csv", DASH / "regime_transitions.csv")

# ---------- 5) Print status ----------
print("\n".join(lines))
print(f"\n[dashboard] {DASH/'g_dren_dashboard_status.json'}")
print(f"[history  ] {DASH/'spike_transitions.csv'} (if SPIKE present)")
print(f"[history  ] {DASH/'regime_transitions.csv'}")
